----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_codec/hydration/v1/temporal_hydrate_time_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
from datetime import (
    datetime,
    time,
    timedelta,
    timezone,
)


from src.neo4j._optional_deps import (
    np,
    pd,
)
from src.neo4j.time import (
    Date,
    DateTime,
    Duration,
    MAX_YEAR,
    MIN_YEAR,
    NANO_SECONDS,
    Time,
)
from src.neo4j._codec.packstream import Structure


ANY_BUILTIN_DATETIME = datetime(1970, 1, 1)


def get_date_unix_epoch():
    return Date(1970, 1, 1)


def get_date_unix_epoch_ordinal():
    return get_date_unix_epoch().to_ordinal()


def get_datetime_unix_epoch_utc():
    from pytz import utc
    return DateTime(1970, 1, 1, 0, 0, 0, utc)


def hydrate_date(days):
    """ Hydrator for `Date` values.

    :param days:
    :returns: Date
    """
    return Date.from_ordinal(get_date_unix_epoch_ordinal() + days)


def dehydrate_date(value):
    """ Dehydrator for `date` values.

    :param value:
    :type value: Date
    :returns:
    """
    return Structure(b"D", value.toordinal() - get_date_unix_epoch().toordinal())


def hydrate_time(nanoseconds, tz=None):
    """ Hydrator for `Time` and `LocalTime` values.

    :param nanoseconds:
    :param tz:
    :returns: Time
    """
    from pytz import FixedOffset
    seconds, nanoseconds = map(int, divmod(nanoseconds, 1000000000))
    minutes, seconds = map(int, divmod(seconds, 60))
    hours, minutes = map(int, divmod(minutes, 60))
    t = Time(hours, minutes, seconds, nanoseconds)
    if tz is None:
        return t
    tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)
    zone = FixedOffset(tz_offset_minutes)
    return zone.localize(t)


def dehydrate_time(value):
    """ Dehydrator for `time` values.

    :param value:
    :type value: Time
    :returns:
    """
    if isinstance(value, Time):
        nanoseconds = value.ticks
    elif isinstance(value, time):
        nanoseconds = (3600000000000 * value.hour + 60000000000 * value.minute +
                       1000000000 * value.second + 1000 * value.microsecond)
    else:
        raise TypeError("Value must be a neo4j.time.Time or a datetime.time")
    if value.tzinfo:
        return Structure(b"T", nanoseconds,
                         int(value.tzinfo.utcoffset(value).total_seconds()))
    else:
        return Structure(b"t", nanoseconds)


def hydrate_datetime(seconds, nanoseconds, tz=None):
    """ Hydrator for `DateTime` and `LocalDateTime` values.

    :param seconds:
    :param nanoseconds:
    :param tz:
    :returns: datetime
    """
    from pytz import (
        FixedOffset,
        timezone,
    )
    minutes, seconds = map(int, divmod(seconds, 60))
    hours, minutes = map(int, divmod(minutes, 60))
    days, hours = map(int, divmod(hours, 24))
    t = DateTime.combine(
        Date.from_ordinal(get_date_unix_epoch_ordinal() + days),
        Time(hours, minutes, seconds, nanoseconds)
    )
    if tz is None:
        return t
    if isinstance(tz, int):
        tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)
        zone = FixedOffset(tz_offset_minutes)
    else:
        zone = timezone(tz)
    return zone.localize(t)


def dehydrate_datetime(value):
    """ Dehydrator for `datetime` values.

    :param value:
    :type value: datetime or DateTime
    :returns:
    """

    def seconds_and_nanoseconds(dt):
        if isinstance(dt, datetime):
            dt = DateTime.from_native(dt)
        zone_epoch = DateTime(1970, 1, 1, tzinfo=dt.tzinfo)
        dt_clock_time = dt.to_clock_time()
        zone_epoch_clock_time = zone_epoch.to_clock_time()
        t = dt_clock_time - zone_epoch_clock_time
        return t.seconds, t.nanoseconds

    tz = value.tzinfo
    if tz is None:
        # without time zone
        from pytz import utc
        value = utc.localize(value)
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b"d", seconds, nanoseconds)
    elif hasattr(tz, "zone") and tz.zone and isinstance(tz.zone, str):
        # with named pytz time zone
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b"f", seconds, nanoseconds, tz.zone)
    elif hasattr(tz, "key") and tz.key and isinstance(tz.key, str):
        # with named zoneinfo (Python 3.9+) time zone
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b"f", seconds, nanoseconds, tz.key)
    else:
        if isinstance(tz, timezone):
            # offset of the timezone is constant, so any date will do
            offset = tz.utcoffset(ANY_BUILTIN_DATETIME)
        else:
            offset = tz.utcoffset(value)
        # with time offset
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b"F", seconds, nanoseconds,
                         int(offset.total_seconds()))


if np is not None:
    def dehydrate_np_datetime(value):
        """ Dehydrator for `numpy.datetime64` values.

        :param value:
        :type value: numpy.datetime64
        :returns:
        """
        if np.isnat(value):
            return None
        year = value.astype("datetime64[Y]").astype(int) + 1970
        if not 0 < year <= 9999:
            # while we could encode years outside the range, they would fail
            # when retrieved from the database.
            raise ValueError(f"Year out of range ({MIN_YEAR:d}..{MAX_YEAR:d}) "
                             f"found {year}")
        seconds = value.astype(np.dtype("datetime64[s]")).astype(int)
        nanoseconds = (value.astype(np.dtype("datetime64[ns]")).astype(int)
                       % NANO_SECONDS)
        return Structure(b"d", seconds, nanoseconds)


if pd is not None:
    def dehydrate_pandas_datetime(value):
        """ Dehydrator for `pandas.Timestamp` values.

        :param value:
        :type value: pandas.Timestamp
        :returns:
        """
        return dehydrate_datetime(
            DateTime(
                value.year,
                value.month,
                value.day,
                value.hour,
                value.minute,
                value.second,
                value.microsecond * 1000 + value.nanosecond,
                value.tzinfo,
            )
        )


def hydrate_duration(months, days, seconds, nanoseconds):
    """ Hydrator for `Duration` values.

    :param months:
    :param days:
    :param seconds:
    :param nanoseconds:
    :returns: `duration` namedtuple
    """
    return Duration(months=months, days=days, seconds=seconds, nanoseconds=nanoseconds)


def dehydrate_duration(value):
    """ Dehydrator for `duration` values.

    :param value:
    :type value: Duration
    :returns:
    """
    return Structure(b"E", value.months, value.days, value.seconds, value.nanoseconds)


def dehydrate_timedelta(value):
    """ Dehydrator for `timedelta` values.

    :param value:
    :type value: timedelta
    :returns:
    """
    months = 0
    days = value.days
    seconds = value.seconds
    nanoseconds = 1000 * value.microseconds
    return Structure(b"E", months, days, seconds, nanoseconds)


if np is not None:
    _NUMPY_DURATION_UNITS = {
        "Y": "years",
        "M": "months",
        "W": "weeks",
        "D": "days",
        "h": "hours",
        "m": "minutes",
        "s": "seconds",
        "ms": "milliseconds",
        "us": "microseconds",
        "ns": "nanoseconds",
    }

    def dehydrate_np_timedelta(value):
        """ Dehydrator for `numpy.timedelta64` values.

        :param value:
        :type value: numpy.timedelta64
        :returns:
        """
        if np.isnat(value):
            return None
        unit, step_size = np.datetime_data(value)
        numer = int(value.astype(int))
        # raise RuntimeError((type(numer), type(step_size)))
        kwarg = _NUMPY_DURATION_UNITS.get(unit)
        if kwarg is not None:
            return dehydrate_duration(Duration(**{kwarg: numer * step_size}))
        return dehydrate_duration(Duration(
            nanoseconds=value.astype("timedelta64[ns]").astype(int)
        ))


if pd is not None:
    def dehydrate_pandas_timedelta(value):
        """ Dehydrator for `pandas.Timedelta` values.

        :param value:
        :type value: pandas.Timedelta
        :returns:
        """
        return dehydrate_duration(Duration(
            nanoseconds=value.value
        ))

if __name__ == "__main__":
    isT=True
    try:
        res1 = hydrate_time(3723000000004, 3600)
        res2 = hydrate_time(3723000000004, None)
        if not str(res1)=="01:02:03.000000004+01:00" or not str(res2)=="01:02:03.000000004":
            isT=False
    except:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")

    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver\\data_passk_platform1\\62e60f43d76274f8a4026e28/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver\\data_passk_platform1\\62e60f43d76274f8a4026e28/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     print(args0)
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     print(args1)
    #     res0 = hydrate_time(args0,args1)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    # if not isT:
    #     raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_codec/hydration/v1/temporal_dehydrate_timedelta_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
from datetime import (
    datetime,
    time,
    timedelta,
    timezone,
)


from src.neo4j._optional_deps import (
    np,
    pd,
)
from src.neo4j.time import (
    Date,
    DateTime,
    Duration,
    MAX_YEAR,
    MIN_YEAR,
    NANO_SECONDS,
    Time,
)
from src.neo4j._codec.packstream import Structure


ANY_BUILTIN_DATETIME = datetime(1970, 1, 1)


def get_date_unix_epoch():
    return Date(1970, 1, 1)


def get_date_unix_epoch_ordinal():
    return get_date_unix_epoch().to_ordinal()


def get_datetime_unix_epoch_utc():
    from pytz import utc
    return DateTime(1970, 1, 1, 0, 0, 0, utc)


def hydrate_date(days):
    """ Hydrator for `Date` values.

    :param days:
    :returns: Date
    """
    return Date.from_ordinal(get_date_unix_epoch_ordinal() + days)


def dehydrate_date(value):
    """ Dehydrator for `date` values.

    :param value:
    :type value: Date
    :returns:
    """
    return Structure(b"D", value.toordinal() - get_date_unix_epoch().toordinal())


def hydrate_time(nanoseconds, tz=None):
    """ Hydrator for `Time` and `LocalTime` values.

    :param nanoseconds:
    :param tz:
    :returns: Time
    """
    from pytz import FixedOffset
    seconds, nanoseconds = map(int, divmod(nanoseconds, 1000000000))
    minutes, seconds = map(int, divmod(seconds, 60))
    hours, minutes = map(int, divmod(minutes, 60))
    t = Time(hours, minutes, seconds, nanoseconds)
    if tz is None:
        return t
    tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)
    zone = FixedOffset(tz_offset_minutes)
    return zone.localize(t)


def dehydrate_time(value):
    """ Dehydrator for `time` values.

    :param value:
    :type value: Time
    :returns:
    """
    if isinstance(value, Time):
        nanoseconds = value.ticks
    elif isinstance(value, time):
        nanoseconds = (3600000000000 * value.hour + 60000000000 * value.minute +
                       1000000000 * value.second + 1000 * value.microsecond)
    else:
        raise TypeError("Value must be a neo4j.time.Time or a datetime.time")
    if value.tzinfo:
        return Structure(b"T", nanoseconds,
                         int(value.tzinfo.utcoffset(value).total_seconds()))
    else:
        return Structure(b"t", nanoseconds)


def hydrate_datetime(seconds, nanoseconds, tz=None):
    """ Hydrator for `DateTime` and `LocalDateTime` values.

    :param seconds:
    :param nanoseconds:
    :param tz:
    :returns: datetime
    """
    from pytz import (
        FixedOffset,
        timezone,
    )
    minutes, seconds = map(int, divmod(seconds, 60))
    hours, minutes = map(int, divmod(minutes, 60))
    days, hours = map(int, divmod(hours, 24))
    t = DateTime.combine(
        Date.from_ordinal(get_date_unix_epoch_ordinal() + days),
        Time(hours, minutes, seconds, nanoseconds)
    )
    if tz is None:
        return t
    if isinstance(tz, int):
        tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)
        zone = FixedOffset(tz_offset_minutes)
    else:
        zone = timezone(tz)
    return zone.localize(t)


def dehydrate_datetime(value):
    """ Dehydrator for `datetime` values.

    :param value:
    :type value: datetime or DateTime
    :returns:
    """

    def seconds_and_nanoseconds(dt):
        if isinstance(dt, datetime):
            dt = DateTime.from_native(dt)
        zone_epoch = DateTime(1970, 1, 1, tzinfo=dt.tzinfo)
        dt_clock_time = dt.to_clock_time()
        zone_epoch_clock_time = zone_epoch.to_clock_time()
        t = dt_clock_time - zone_epoch_clock_time
        return t.seconds, t.nanoseconds

    tz = value.tzinfo
    if tz is None:
        # without time zone
        from pytz import utc
        value = utc.localize(value)
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b"d", seconds, nanoseconds)
    elif hasattr(tz, "zone") and tz.zone and isinstance(tz.zone, str):
        # with named pytz time zone
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b"f", seconds, nanoseconds, tz.zone)
    elif hasattr(tz, "key") and tz.key and isinstance(tz.key, str):
        # with named zoneinfo (Python 3.9+) time zone
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b"f", seconds, nanoseconds, tz.key)
    else:
        if isinstance(tz, timezone):
            # offset of the timezone is constant, so any date will do
            offset = tz.utcoffset(ANY_BUILTIN_DATETIME)
        else:
            offset = tz.utcoffset(value)
        # with time offset
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b"F", seconds, nanoseconds,
                         int(offset.total_seconds()))


if np is not None:
    def dehydrate_np_datetime(value):
        """ Dehydrator for `numpy.datetime64` values.

        :param value:
        :type value: numpy.datetime64
        :returns:
        """
        if np.isnat(value):
            return None
        year = value.astype("datetime64[Y]").astype(int) + 1970
        if not 0 < year <= 9999:
            # while we could encode years outside the range, they would fail
            # when retrieved from the database.
            raise ValueError(f"Year out of range ({MIN_YEAR:d}..{MAX_YEAR:d}) "
                             f"found {year}")
        seconds = value.astype(np.dtype("datetime64[s]")).astype(int)
        nanoseconds = (value.astype(np.dtype("datetime64[ns]")).astype(int)
                       % NANO_SECONDS)
        return Structure(b"d", seconds, nanoseconds)


if pd is not None:
    def dehydrate_pandas_datetime(value):
        """ Dehydrator for `pandas.Timestamp` values.

        :param value:
        :type value: pandas.Timestamp
        :returns:
        """
        return dehydrate_datetime(
            DateTime(
                value.year,
                value.month,
                value.day,
                value.hour,
                value.minute,
                value.second,
                value.microsecond * 1000 + value.nanosecond,
                value.tzinfo,
            )
        )


def hydrate_duration(months, days, seconds, nanoseconds):
    """ Hydrator for `Duration` values.

    :param months:
    :param days:
    :param seconds:
    :param nanoseconds:
    :returns: `duration` namedtuple
    """
    return Duration(months=months, days=days, seconds=seconds, nanoseconds=nanoseconds)


def dehydrate_duration(value):
    """ Dehydrator for `duration` values.

    :param value:
    :type value: Duration
    :returns:
    """
    return Structure(b"E", value.months, value.days, value.seconds, value.nanoseconds)


def dehydrate_timedelta(value):
    """ Dehydrator for `timedelta` values.

    :param value:
    :type value: timedelta
    :returns:
    """
    months = 0
    days = value.days
    seconds = value.seconds
    nanoseconds = 1000 * value.microseconds
    return Structure(b"E", months, days, seconds, nanoseconds)


if np is not None:
    _NUMPY_DURATION_UNITS = {
        "Y": "years",
        "M": "months",
        "W": "weeks",
        "D": "days",
        "h": "hours",
        "m": "minutes",
        "s": "seconds",
        "ms": "milliseconds",
        "us": "microseconds",
        "ns": "nanoseconds",
    }

    def dehydrate_np_timedelta(value):
        """ Dehydrator for `numpy.timedelta64` values.

        :param value:
        :type value: numpy.timedelta64
        :returns:
        """
        if np.isnat(value):
            return None
        unit, step_size = np.datetime_data(value)
        numer = int(value.astype(int))
        # raise RuntimeError((type(numer), type(step_size)))
        kwarg = _NUMPY_DURATION_UNITS.get(unit)
        if kwarg is not None:
            return dehydrate_duration(Duration(**{kwarg: numer * step_size}))
        return dehydrate_duration(Duration(
            nanoseconds=value.astype("timedelta64[ns]").astype(int)
        ))


if pd is not None:
    def dehydrate_pandas_timedelta(value):
        """ Dehydrator for `pandas.Timedelta` values.

        :param value:
        :type value: pandas.Timedelta
        :returns:
        """
        return dehydrate_duration(Duration(
            nanoseconds=value.value
        ))

if __name__ == "__main__":
    isT=True
    input1=timedelta(days=1, seconds=2, microseconds=3)
    input2=timedelta(days=-1, seconds=2, microseconds=3)
    res1=dehydrate_timedelta(input1)
    res2 = dehydrate_timedelta(input2)
    try:
        if res1.fields[0]!=0 or res1.fields[1]!=1 or res1.fields[2]!=2 or res1.fields[3]!=3000:
            isT=False
        if res2.fields[0] != 0 or res2.fields[1] != -1 or res2.fields[2]!=2 or res2.fields[3]!=3000:
            isT = False
    except:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_codec/hydration/v1/temporal_dehydrate_time_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from datetime import (
    datetime,
    time,
    timedelta,
    timezone,
)

import pytz
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
from src import neo4j
from src.neo4j._optional_deps import (
    np,
    pd,
)
from src.neo4j.time import (
    Date,
    DateTime,
    Duration,
    MAX_YEAR,
    MIN_YEAR,
    NANO_SECONDS,
    Time,
)
from src.neo4j._codec.packstream import Structure


ANY_BUILTIN_DATETIME = datetime(1970, 1, 1)


def get_date_unix_epoch():
    return Date(1970, 1, 1)


def get_date_unix_epoch_ordinal():
    return get_date_unix_epoch().to_ordinal()


def get_datetime_unix_epoch_utc():
    from pytz import utc
    return DateTime(1970, 1, 1, 0, 0, 0, utc)


def hydrate_date(days):
    """ Hydrator for `Date` values.

    :param days:
    :returns: Date
    """
    return Date.from_ordinal(get_date_unix_epoch_ordinal() + days)


def dehydrate_date(value):
    """ Dehydrator for `date` values.

    :param value:
    :type value: Date
    :returns:
    """
    return Structure(b"D", value.toordinal() - get_date_unix_epoch().toordinal())


def hydrate_time(nanoseconds, tz=None):
    """ Hydrator for `Time` and `LocalTime` values.

    :param nanoseconds:
    :param tz:
    :returns: Time
    """
    from pytz import FixedOffset
    seconds, nanoseconds = map(int, divmod(nanoseconds, 1000000000))
    minutes, seconds = map(int, divmod(seconds, 60))
    hours, minutes = map(int, divmod(minutes, 60))
    t = Time(hours, minutes, seconds, nanoseconds)
    if tz is None:
        return t
    tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)
    zone = FixedOffset(tz_offset_minutes)
    return zone.localize(t)


def dehydrate_time(value):
    """ Dehydrator for `time` values.

    :param value:
    :type value: Time
    :returns:
    """
    if isinstance(value, Time):
        nanoseconds = value.ticks
    elif isinstance(value, time):
        nanoseconds = (3600000000000 * value.hour + 60000000000 * value.minute +
                       1000000000 * value.second + 1000 * value.microsecond)
    else:
        raise TypeError("Value must be a neo4j.time.Time or a datetime.time")
    if value.tzinfo:
        return Structure(b"T", nanoseconds,
                         int(value.tzinfo.utcoffset(value).total_seconds()))
    else:
        return Structure(b"t", nanoseconds)


def hydrate_datetime(seconds, nanoseconds, tz=None):
    """ Hydrator for `DateTime` and `LocalDateTime` values.

    :param seconds:
    :param nanoseconds:
    :param tz:
    :returns: datetime
    """
    from pytz import (
        FixedOffset,
        timezone,
    )
    minutes, seconds = map(int, divmod(seconds, 60))
    hours, minutes = map(int, divmod(minutes, 60))
    days, hours = map(int, divmod(hours, 24))
    t = DateTime.combine(
        Date.from_ordinal(get_date_unix_epoch_ordinal() + days),
        Time(hours, minutes, seconds, nanoseconds)
    )
    if tz is None:
        return t
    if isinstance(tz, int):
        tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)
        zone = FixedOffset(tz_offset_minutes)
    else:
        zone = timezone(tz)
    return zone.localize(t)


def dehydrate_datetime(value):
    """ Dehydrator for `datetime` values.

    :param value:
    :type value: datetime or DateTime
    :returns:
    """

    def seconds_and_nanoseconds(dt):
        if isinstance(dt, datetime):
            dt = DateTime.from_native(dt)
        zone_epoch = DateTime(1970, 1, 1, tzinfo=dt.tzinfo)
        dt_clock_time = dt.to_clock_time()
        zone_epoch_clock_time = zone_epoch.to_clock_time()
        t = dt_clock_time - zone_epoch_clock_time
        return t.seconds, t.nanoseconds

    tz = value.tzinfo
    if tz is None:
        # without time zone
        from pytz import utc
        value = utc.localize(value)
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b"d", seconds, nanoseconds)
    elif hasattr(tz, "zone") and tz.zone and isinstance(tz.zone, str):
        # with named pytz time zone
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b"f", seconds, nanoseconds, tz.zone)
    elif hasattr(tz, "key") and tz.key and isinstance(tz.key, str):
        # with named zoneinfo (Python 3.9+) time zone
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b"f", seconds, nanoseconds, tz.key)
    else:
        if isinstance(tz, timezone):
            # offset of the timezone is constant, so any date will do
            offset = tz.utcoffset(ANY_BUILTIN_DATETIME)
        else:
            offset = tz.utcoffset(value)
        # with time offset
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b"F", seconds, nanoseconds,
                         int(offset.total_seconds()))


if np is not None:
    def dehydrate_np_datetime(value):
        """ Dehydrator for `numpy.datetime64` values.

        :param value:
        :type value: numpy.datetime64
        :returns:
        """
        if np.isnat(value):
            return None
        year = value.astype("datetime64[Y]").astype(int) + 1970
        if not 0 < year <= 9999:
            # while we could encode years outside the range, they would fail
            # when retrieved from the database.
            raise ValueError(f"Year out of range ({MIN_YEAR:d}..{MAX_YEAR:d}) "
                             f"found {year}")
        seconds = value.astype(np.dtype("datetime64[s]")).astype(int)
        nanoseconds = (value.astype(np.dtype("datetime64[ns]")).astype(int)
                       % NANO_SECONDS)
        return Structure(b"d", seconds, nanoseconds)


if pd is not None:
    def dehydrate_pandas_datetime(value):
        """ Dehydrator for `pandas.Timestamp` values.

        :param value:
        :type value: pandas.Timestamp
        :returns:
        """
        return dehydrate_datetime(
            DateTime(
                value.year,
                value.month,
                value.day,
                value.hour,
                value.minute,
                value.second,
                value.microsecond * 1000 + value.nanosecond,
                value.tzinfo,
            )
        )


def hydrate_duration(months, days, seconds, nanoseconds):
    """ Hydrator for `Duration` values.

    :param months:
    :param days:
    :param seconds:
    :param nanoseconds:
    :returns: `duration` namedtuple
    """
    return Duration(months=months, days=days, seconds=seconds, nanoseconds=nanoseconds)


def dehydrate_duration(value):
    """ Dehydrator for `duration` values.

    :param value:
    :type value: Duration
    :returns:
    """
    return Structure(b"E", value.months, value.days, value.seconds, value.nanoseconds)


def dehydrate_timedelta(value):
    """ Dehydrator for `timedelta` values.

    :param value:
    :type value: timedelta
    :returns:
    """
    months = 0
    days = value.days
    seconds = value.seconds
    nanoseconds = 1000 * value.microseconds
    return Structure(b"E", months, days, seconds, nanoseconds)


if np is not None:
    _NUMPY_DURATION_UNITS = {
        "Y": "years",
        "M": "months",
        "W": "weeks",
        "D": "days",
        "h": "hours",
        "m": "minutes",
        "s": "seconds",
        "ms": "milliseconds",
        "us": "microseconds",
        "ns": "nanoseconds",
    }

    def dehydrate_np_timedelta(value):
        """ Dehydrator for `numpy.timedelta64` values.

        :param value:
        :type value: numpy.timedelta64
        :returns:
        """
        if np.isnat(value):
            return None
        unit, step_size = np.datetime_data(value)
        numer = int(value.astype(int))
        # raise RuntimeError((type(numer), type(step_size)))
        kwarg = _NUMPY_DURATION_UNITS.get(unit)
        if kwarg is not None:
            return dehydrate_duration(Duration(**{kwarg: numer * step_size}))
        return dehydrate_duration(Duration(
            nanoseconds=value.astype("timedelta64[ns]").astype(int)
        ))


if pd is not None:
    def dehydrate_pandas_timedelta(value):
        """ Dehydrator for `pandas.Timedelta` values.

        :param value:
        :type value: pandas.Timedelta
        :returns:
        """
        return dehydrate_duration(Duration(
            nanoseconds=value.value
        ))

if __name__ == "__main__":
    import dill
    import os
    isT=True

    input1 = neo4j.time.Time(1, 2, 3, 4, tzinfo=pytz.FixedOffset(60))
    input2 = time(1, 2, 3, 4, tzinfo=pytz.FixedOffset(60))
    input3 = neo4j.time.Time(1, 2, 3, 4)
    input4 = time(1, 2, 3, 4)
    res1 = dehydrate_time(input1)
    res2 = dehydrate_time(input2)
    res3 = dehydrate_time(input3)
    res4 = dehydrate_time(input4)

    try:
        if res1.fields[0]!=3723000000004 or res1.fields[1]!=3600:
            isT=False
        if res2.fields[0] != 3723000004000 or res1.fields[1] != 3600:
            isT = False
        if res3.fields[0] != 3723000000004:
            isT = False
        if res4.fields[0] != 3723000004000:
            isT = False
    except:
        isT=False
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver\\data_passk_platform1\\62e60f37d76274f8a4026dfd\\"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver\\data_passk_platform1\\62e60f37d76274f8a4026dfd\\"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     # print(type(content["input"]["args"][0]["bytes"]))
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         # print(content["input"]["args"][0]["bytes"])
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #         # print(dill.loads(args0))
    #         # print(dill.loads(args0["$binary"]["base64"]))
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     print(args0)
    #     # print(type(args0))
    #     input=neo4j.time.Time(1, 2, 3, 4, tzinfo=pytz.FixedOffset(60))
    #     res0 = dehydrate_time(input)
    #     # print(res0.fields)
    #     # print(content["output"][0])
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_codec/hydration/v1/spatial_dehydrate_point_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# from ctypes.wintypes import POINT
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
from src.neo4j._spatial import (
    Point,
    srid_table,
)
from src.neo4j.packstream import Structure

def hydrate_point(srid, *coordinates):
    """ Create a new instance of a Point subclass from a raw
    set of fields. The subclass chosen is determined by the
    given SRID; a ValueError will be raised if no such
    subclass can be found.
    """
    try:
        point_class, dim = srid_table[srid]
    except KeyError:
        point = Point(coordinates)
        point.srid = srid
        return point
    else:
        if len(coordinates) != dim:
            raise ValueError("SRID %d requires %d coordinates (%d provided)" % (srid, dim, len(coordinates)))
        return point_class(coordinates)


def dehydrate_point(value):
    """ Dehydrator for Point data.

    :param value:
    :type value: Point
    :returns:
    """
    dim = len(value)
    if dim == 2:
        return Structure(b"X", value.srid, *value)
    elif dim == 3:
        return Structure(b"Y", value.srid, *value)
    else:
        raise ValueError("Cannot dehydrate Point with %d dimensions" % dim)


__all__ = [
    "hydrate_point",
    "dehydrate_point",
]

if __name__ == "__main__":
    isT=True
    try:
        input1 = Point((1.0, 3.1))
        input1.srid = 12345
        res1 = dehydrate_point(input1)
        input2 = Point([1.0, - 2.0, 3.1])
        input2.srid = 12345
        res2 = dehydrate_point(input2)
        input3 = Point([1.0, 3.1])
        input3.srid = 12345
        res3 = dehydrate_point(input3)
        input4 = Point([1.0, - 2.0, 3.1])
        input4.srid = 12345
        res4 = dehydrate_point(input4)
        input5 = Point([1.0, 3.1])
        input5.srid = 12345
        res5 = dehydrate_point(input5)
        input6 = Point([1.0, - 2.0, 3.1])
        input6.srid = 12345
        res6 = dehydrate_point(input6)
        if res1.tag!=b'X' or res1.fields!=[12345,1.0,3.1]:
            isT=False
        if res2.tag!=b'Y' or res2.fields!=[12345, 1.0, -2.0, 3.1]:
            isT=False
        if res3.tag!=b'X' or res3.fields!=[12345,1.0,3.1]:
            isT=False
        if res4.tag!=b'Y' or res4.fields!=[12345,1.0, -2.0, 3.1]:
            isT=False
        if res5.tag!=b'X' or res5.fields!=[12345,1.0,3.1]:
            isT=False
        if res6.tag!=b'Y' or res6.fields!=[12345,1.0, -2.0, 3.1]:
            isT=False
    except:
        isT=False
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver\\data_passk_platform1\\62e60f33d76274f8a4026de9/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver\\data_passk_platform1\\62e60f33d76274f8a4026de9/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     print(args0)
    #     # res0 = dehydrate_point(args0)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_data_keys_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from __future__ import annotations
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
import typing as t
from abc import (
    ABCMeta,
    abstractmethod,
)
from collections.abc import (
    Mapping,
    Sequence,
    Set,
)
from functools import reduce
from operator import xor as xor_operator

from src.neo4j._codec.hydration import BrokenHydrationObject
from src.neo4j._conf import iter_items
from src.neo4j._meta import deprecated
from src.neo4j.exceptions import BrokenRecordError
from src.neo4j.graph import (
    Node,
    Path,
    Relationship,
)


_T = t.TypeVar("_T")
_K = t.Union[int, str]


class Record(tuple, Mapping):
    """ A :class:`.Record` is an immutable ordered collection of key-value
    pairs. It is generally closer to a :py:class:`namedtuple` than to a
    :py:class:`OrderedDict` in as much as iteration of the collection will
    yield values rather than keys.
    """

    __keys: t.Tuple[str]

    def __new__(cls, iterable=()):
        keys = []
        values = []
        for key, value in iter_items(iterable):
            keys.append(key)
            values.append(value)
        inst = tuple.__new__(cls, values)
        inst.__keys = tuple(keys)
        return inst

    def _broken_record_error(self, index):
        return BrokenRecordError(
            f"Record contains broken data at {index} ('{self.__keys[index]}')"
        )

    def _super_getitem_single(self, index):
        value = super().__getitem__(index)
        if isinstance(value, BrokenHydrationObject):
            raise self._broken_record_error(index) from value.error
        return value

    def __repr__(self) -> str:
        return "<%s %s>" % (
            self.__class__.__name__,
            " ".join("%s=%r" % (field, value)
                     for field, value in zip(self.__keys, super().__iter__()))
        )

    __str__ = __repr__

    def __eq__(self, other: object) -> bool:
        """ In order to be flexible regarding comparison, the equality rules
        for a record permit comparison with any other Sequence or Mapping.

        :param other:
        :returns:
        """
        compare_as_sequence = isinstance(other, Sequence)
        compare_as_mapping = isinstance(other, Mapping)
        if compare_as_sequence and compare_as_mapping:
            other = t.cast(t.Mapping, other)
            return list(self) == list(other) and dict(self) == dict(other)
        elif compare_as_sequence:
            other = t.cast(t.Sequence, other)
            return list(self) == list(other)
        elif compare_as_mapping:
            other = t.cast(t.Mapping, other)
            return dict(self) == dict(other)
        else:
            return False

    def __ne__(self, other: object) -> bool:
        return not self.__eq__(other)

    def __hash__(self):
        return reduce(xor_operator, map(hash, self.items()))

    def __iter__(self) -> t.Iterator[t.Any]:
        for i, v in enumerate(super().__iter__()):
            if isinstance(v, BrokenHydrationObject):
                raise self._broken_record_error(i) from v.error
            yield v

    def __getitem__(  # type: ignore[override]
        self, key: t.Union[_K, slice]
    ) -> t.Any:
        if isinstance(key, slice):
            keys = self.__keys[key]
            values = super().__getitem__(key)
            return self.__class__(zip(keys, values))
        try:
            index = self.index(key)
        except IndexError:
            return None
        else:
            return self._super_getitem_single(index)

    # TODO: 6.0 - remove
    @deprecated("This method is deprecated and will be removed in the future.")
    def __getslice__(self, start, stop):
        key = slice(start, stop)
        keys = self.__keys[key]
        values = tuple(self)[key]
        return self.__class__(zip(keys, values))

    def get(self, key: str, default: t.Optional[object] = None) -> t.Any:
        """ Obtain a value from the record by key, returning a default
        value if the key does not exist.

        :param key: a key
        :param default: default value

        :returns: a value
        """
        try:
            index = self.__keys.index(str(key))
        except ValueError:
            return default
        if 0 <= index < len(self):
            return self._super_getitem_single(index)
        else:
            return default

    def index(self, key: _K) -> int:  # type: ignore[override]
        """ Return the index of the given item.

        :param key: a key

        :returns: index
        """
        if isinstance(key, int):
            if 0 <= key < len(self.__keys):
                return key
            raise IndexError(key)
        elif isinstance(key, str):
            try:
                return self.__keys.index(key)
            except ValueError as exc:
                raise KeyError(key) from exc
        else:
            raise TypeError(key)

    def value(
        self, key: _K = 0, default: t.Optional[object] = None
    ) -> t.Any:
        """ Obtain a single value from the record by index or key. If no
        index or key is specified, the first value is returned. If the
        specified item does not exist, the default value is returned.

        :param key: an index or key
        :param default: default value

        :returns: a single value
        """
        try:
            index = self.index(key)
        except (IndexError, KeyError):
            return default
        else:
            return self[index]

    def keys(self) -> t.List[str]:  # type: ignore[override]
        """ Return the keys of the record.

        :returns: list of key names
        """
        return list(self.__keys)

    def values(self, *keys: _K) -> t.List[t.Any]:  # type: ignore[override]
        """ Return the values of the record, optionally filtering to
        include only certain values by index or key.

        :param keys: indexes or keys of the items to include; if none
                     are provided, all values will be included

        :returns: list of values
        """
        if keys:
            d: t.List[t.Any] = []
            for key in keys:
                try:
                    i = self.index(key)
                except KeyError:
                    d.append(None)
                else:
                    d.append(self[i])
            return d
        return list(self)

    def items(self, *keys):
        """ Return the fields of the record as a list of key and value tuples

        :returns: a list of value tuples
        """
        if keys:
            d = []
            for key in keys:
                try:
                    i = self.index(key)
                except KeyError:
                    d.append((key, None))
                else:
                    d.append((self.__keys[i], self[i]))
            return d
        return list((self.__keys[i], self._super_getitem_single(i))
                    for i in range(len(self)))

    def data(self, *keys: _K) -> t.Dict[str, t.Any]:
        """ Return the keys and values of this record as a dictionary,
        optionally including only certain values by index or key. Keys
        provided in the items that are not in the record will be
        inserted with a value of :data:`None`; indexes provided
        that are out of bounds will trigger an :exc:`IndexError`.

        :param keys: indexes or keys of the items to include; if none
                      are provided, all values will be included

        :returns: dictionary of values, keyed by field name

        :raises: :exc:`IndexError` if an out-of-bounds index is specified
        """
        return RecordExporter().transform(dict(self.items(*keys)))


class DataTransformer(metaclass=ABCMeta):
    """ Abstract base class for transforming data from one form into
    another.
    """

    @abstractmethod
    def transform(self, x):
        """ Transform a value, or collection of values.

        :param x: input value
        :returns: output value
        """


class RecordExporter(DataTransformer):
    """ Transformer class used by the :meth:`.Record.data` method.
    """

    def transform(self, x):
        if isinstance(x, Node):
            return self.transform(dict(x))
        elif isinstance(x, Relationship):
            return (self.transform(dict(x.start_node)),
                    x.__class__.__name__,
                    self.transform(dict(x.end_node)))
        elif isinstance(x, Path):
            path = [self.transform(x.start_node)]
            for i, relationship in enumerate(x.relationships):
                path.append(self.transform(relationship.__class__.__name__))
                path.append(self.transform(x.nodes[i + 1]))
            return path
        elif isinstance(x, str):
            return x
        elif isinstance(x, Sequence):
            typ = type(x)
            return typ(map(self.transform, x))
        elif isinstance(x, Set):
            typ = type(x)
            return typ(map(self.transform, x))
        elif isinstance(x, Mapping):
            typ = type(x)
            return typ((k, self.transform(v)) for k, v in x.items())
        else:
            return x


class RecordTableRowExporter(DataTransformer):
    """Transformer class used by the :meth:`.Result.to_df` method."""

    def transform(self, x):
        assert isinstance(x, Mapping)
        typ = type(x)
        return typ(item
                   for k, v in x.items()
                   for item in self._transform(
                       v, prefix=k.replace("\\", "\\\\").replace(".", "\\.")
                   ).items())

    def _transform(self, x, prefix):
        if isinstance(x, Node):
            res = {
                "%s().element_id" % prefix: x.element_id,
                "%s().labels" % prefix: x.labels,
            }
            res.update(("%s().prop.%s" % (prefix, k), v) for k, v in x.items())
            return res
        elif isinstance(x, Relationship):
            res = {
                "%s->.element_id" % prefix: x.element_id,
                "%s->.start.element_id" % prefix: x.start_node.element_id,
                "%s->.end.element_id" % prefix: x.end_node.element_id,
                "%s->.type" % prefix: x.__class__.__name__,
            }
            res.update(("%s->.prop.%s" % (prefix, k), v) for k, v in x.items())
            return res
        elif isinstance(x, Path) or isinstance(x, str):
            return {prefix: x}
        elif isinstance(x, Sequence):
            return dict(
                item
                for i, v in enumerate(x)
                for item in self._transform(
                    v, prefix="%s[].%i" % (prefix, i)
                ).items()
            )
        elif isinstance(x, Mapping):
            typ = type(x)
            return typ(
                item
                for k, v in x.items()
                for item in self._transform(
                    v, prefix="%s{}.%s" % (prefix, k.replace("\\", "\\\\")
                                                    .replace(".", "\\."))
                ).items()
            )
        else:
            return {prefix: x}

if __name__ == "__main__":
    import dill
    import os
    isT=True
    dictt={"key1":"value1","key2":"value2","key3":"value3"}

    temp_class = Record(dictt)
    if "key1" not in dictt.keys() or "key2" not in dictt.keys() or "key3" not in dictt.keys():
        isT=False

    if not isT:
        raise Exception("Result not True!!!")



----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_sync/io/_bolt_protocol_handlers_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from __future__ import annotations
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
import abc
import asyncio
import typing as t
from collections import deque
from logging import getLogger
from time import perf_counter

from src.neo4j._async_compat.network import BoltSocket
from src.neo4j._async_compat.util import Util
from src.neo4j._codec.hydration import v1 as hydration_v1
from src.neo4j._codec.packstream import v1 as packstream_v1
from src.neo4j._conf import PoolConfig
from src.neo4j._deadline import Deadline
from src.neo4j._exceptions import (
    BoltError,
    BoltHandshakeError,
)
from src.neo4j._meta import USER_AGENT
from src.neo4j.addressing import ResolvedAddress
from src.neo4j.api import (
    Auth,
    ServerInfo,
    Version,
)
from src.neo4j.exceptions import (
    AuthError,
    ConfigurationError,
    DriverError,
    IncompleteCommit,
    ServiceUnavailable,
    SessionExpired,
)
from src.neo4j._sync.io._common import (
    CommitResponse,
    Inbox,
    Outbox,
)


# Set up logger
log = getLogger("neo4j")


class ServerStateManagerBase(abc.ABC):
    @abc.abstractmethod
    def __init__(self, init_state, on_change=None):
        ...

    @abc.abstractmethod
    def transition(self, message, metadata):
        ...

    @abc.abstractmethod
    def failed(self):
        ...


class ClientStateManagerBase(abc.ABC):
    @abc.abstractmethod
    def __init__(self, init_state, on_change=None):
        ...

    @abc.abstractmethod
    def transition(self, message):
        ...


class Bolt:
    """ Server connection for Bolt protocol.

    A :class:`.Bolt` should be constructed following a
    successful .open()

    Bolt handshake and takes the socket over which
    the handshake was carried out.
    """

    # TODO: let packer/unpacker know of hydration (give them hooks?)
    # TODO: make sure query parameter dehydration gets clear error message.

    PACKER_CLS = packstream_v1.Packer
    UNPACKER_CLS = packstream_v1.Unpacker
    HYDRATION_HANDLER_CLS = hydration_v1.HydrationHandler

    MAGIC_PREAMBLE = b"\x60\x60\xB0\x17"

    PROTOCOL_VERSION: Version = None  # type: ignore[assignment]

    # flag if connection needs RESET to go back to READY state
    is_reset = False

    # The socket
    in_use = False

    # When the connection was last put back into the pool
    idle_since = float("-inf")
    # The database name the connection was last used with
    # (BEGIN for explicit transactions, RUN for auto-commit transactions)
    last_database: t.Optional[str] = None

    # The socket
    _closing = False
    _closed = False
    _defunct = False

    #: The pool of which this connection is a member
    pool = None

    # Store the id of the most recent ran query to be able to reduce sent bits by
    # using the default (-1) to refer to the most recent query when pulling
    # results for it.
    most_recent_qid = None

    def __init__(self, unresolved_address, sock, max_connection_lifetime, *,
                 auth=None, auth_manager=None, user_agent=None,
                 routing_context=None, notifications_min_severity=None,
                 notifications_disabled_categories=None):
        self.unresolved_address = unresolved_address
        self.socket = sock
        self.local_port = self.socket.getsockname()[1]
        self.server_info = ServerInfo(
            ResolvedAddress(sock.getpeername(),
                            host_name=unresolved_address.host),
            self.PROTOCOL_VERSION
        )
        # so far `connection.recv_timeout_seconds` is the only available
        # configuration hint that exists. Therefore, all hints can be stored at
        # connection level. This might change in the future.
        self.configuration_hints = {}
        self.patch = {}
        self.outbox = Outbox(
            self.socket, on_error=self._set_defunct_write,
            packer_cls=self.PACKER_CLS
        )
        self.inbox = Inbox(
            self.socket, on_error=self._set_defunct_read,
            unpacker_cls=self.UNPACKER_CLS
        )
        self.hydration_handler = self.HYDRATION_HANDLER_CLS()
        self.responses = deque()
        self._max_connection_lifetime = max_connection_lifetime
        self._creation_timestamp = perf_counter()
        self.routing_context = routing_context
        self.idle_since = perf_counter()

        # Determine the user agent
        if user_agent:
            self.user_agent = user_agent
        else:
            self.user_agent = USER_AGENT

        self.auth = auth
        self.auth_dict = self._to_auth_dict(auth)
        self.auth_manager = auth_manager

        self.notifications_min_severity = notifications_min_severity
        self.notifications_disabled_categories = \
            notifications_disabled_categories

    def __del__(self):
        if not asyncio.iscoroutinefunction(self.close):
            self.close()

    @abc.abstractmethod
    def _get_server_state_manager(self) -> ServerStateManagerBase:
        ...

    @abc.abstractmethod
    def _get_client_state_manager(self) -> ClientStateManagerBase:
        ...

    @classmethod
    def _to_auth_dict(cls, auth):
        # Determine auth details
        if not auth:
            return {}
        elif isinstance(auth, tuple) and 2 <= len(auth) <= 3:
            return vars(Auth("basic", *auth))
        else:
            try:
                return vars(auth)
            except (KeyError, TypeError):
                raise AuthError("Cannot determine auth details from %r" % auth)

    @property
    def connection_id(self):
        return self.server_info._metadata.get("connection_id", "<unknown id>")

    @property
    @abc.abstractmethod
    def supports_multiple_results(self):
        """ Boolean flag to indicate if the connection version supports multiple
        queries to be buffered on the server side (True) or if all results need
        to be eagerly pulled before sending the next RUN (False).
        """
        pass

    @property
    @abc.abstractmethod
    def supports_multiple_databases(self):
        """ Boolean flag to indicate if the connection version supports multiple
        databases.
        """
        pass

    @property
    @abc.abstractmethod
    def supports_re_auth(self):
        """Whether the connection version supports re-authentication."""
        pass

    def assert_re_auth_support(self):
        if not self.supports_re_auth:
            raise ConfigurationError(
                "User switching is not supported for Bolt "
                f"Protocol {self.PROTOCOL_VERSION!r}. Server Agent "
                f"{self.server_info.agent!r}"
            )

    @property
    @abc.abstractmethod
    def supports_notification_filtering(self):
        """Whether the connection version supports re-authentication."""
        pass

    def assert_notification_filtering_support(self):
        if not self.supports_notification_filtering:
            raise ConfigurationError(
                "Notification filtering is not supported for the Bolt "
                f"Protocol {self.PROTOCOL_VERSION!r}. Server Agent "
                f"{self.server_info.agent!r}"
            )

    # [bolt-version-bump] search tag when changing bolt version support
    @classmethod
    def protocol_handlers(cls, protocol_version=None):
        """ Return a dictionary of available Bolt protocol handlers,
        keyed by version tuple. If an explicit protocol version is
        provided, the dictionary will contain either zero or one items,
        depending on whether that version is supported. If no protocol
        version is provided, all available versions will be returned.

        :param protocol_version: tuple identifying a specific protocol
            version (e.g. (3, 5)) or None
        :returns: dictionary of version tuple to handler class for all
            relevant and supported protocol versions
        :raise TypeError: if protocol version is not passed in a tuple
        """

        # Carry out Bolt subclass imports locally to avoid circular dependency issues.
        from src.neo4j._sync.io._bolt3 import Bolt3
        from src.neo4j._sync.io._bolt4 import (
            Bolt4x1,
            Bolt4x2,
            Bolt4x3,
            Bolt4x4,
        )
        from src.neo4j._sync.io._bolt5 import (
            Bolt5x0,
            Bolt5x1,
            Bolt5x2,
            Bolt5x3,
        )

        handlers = {
            Bolt3.PROTOCOL_VERSION: Bolt3,
            # 4.0 unsupported because no space left in the handshake
            Bolt4x1.PROTOCOL_VERSION: Bolt4x1,
            Bolt4x2.PROTOCOL_VERSION: Bolt4x2,
            Bolt4x3.PROTOCOL_VERSION: Bolt4x3,
            Bolt4x4.PROTOCOL_VERSION: Bolt4x4,
            Bolt5x0.PROTOCOL_VERSION: Bolt5x0,
            Bolt5x1.PROTOCOL_VERSION: Bolt5x1,
            Bolt5x2.PROTOCOL_VERSION: Bolt5x2,
            Bolt5x3.PROTOCOL_VERSION: Bolt5x3,
        }

        if protocol_version is None:
            return handlers

        if not isinstance(protocol_version, tuple):
            raise TypeError("Protocol version must be specified as a tuple")

        if protocol_version in handlers:
            return {protocol_version: handlers[protocol_version]}

        return {}

    @classmethod
    def version_list(cls, versions, limit=4):
        """ Return a list of supported protocol versions in order of
        preference. The number of protocol versions (or ranges)
        returned is limited to four.
        """
        # In fact, 4.3 is the fist version to support ranges. However, the
        # range support got backported to 4.2. But even if the server is too
        # old to have the backport, negotiating BOLT 4.1 is no problem as it's
        # equivalent to 4.2
        first_with_range_support = Version(4, 2)
        result = []
        for version in versions:
            if (result
                    and version >= first_with_range_support
                    and result[-1][0] == version[0]
                    and result[-1][1][1] == version[1] + 1):
                # can use range to encompass this version
                result[-1][1][1] = version[1]
                continue
            result.append(Version(version[0], [version[1], version[1]]))
            if len(result) == 4:
                break
        return result

    @classmethod
    def get_handshake(cls):
        """ Return the supported Bolt versions as bytes.
        The length is 16 bytes as specified in the Bolt version negotiation.
        :returns: bytes
        """
        supported_versions = sorted(cls.protocol_handlers().keys(), reverse=True)
        offered_versions = cls.version_list(supported_versions)
        return b"".join(version.to_bytes() for version in offered_versions).ljust(16, b"\x00")

    @classmethod
    def ping(cls, address, *, deadline=None, pool_config=None):
        """ Attempt to establish a Bolt connection, returning the
        agreed Bolt protocol version if successful.
        """
        if pool_config is None:
            pool_config = PoolConfig()
        if deadline is None:
            deadline = Deadline(None)

        try:
            s, protocol_version, handshake, data = \
                BoltSocket.connect(
                    address,
                    tcp_timeout=pool_config.connection_timeout,
                    deadline=deadline,
                    custom_resolver=pool_config.resolver,
                    ssl_context=pool_config.get_ssl_context(),
                    keep_alive=pool_config.keep_alive,
                )
        except (ServiceUnavailable, SessionExpired, BoltHandshakeError):
            return None
        else:
            BoltSocket.close_socket(s)
            return protocol_version

    # [bolt-version-bump] search tag when changing bolt version support
    @classmethod
    def open(
        cls, address, *, auth_manager=None, deadline=None,
        routing_context=None, pool_config=None
    ):
        """Open a new Bolt connection to a given server address.

        :param address:
        :param auth_manager:
        :param deadline: how long to wait for the connection to be established
        :param routing_context: dict containing routing context
        :param pool_config:

        :returns: connected Bolt instance

        :raise BoltHandshakeError:
            raised if the Bolt Protocol can not negotiate a protocol version.
        :raise ServiceUnavailable: raised if there was a connection issue.
        """

        if pool_config is None:
            pool_config = PoolConfig()
        if deadline is None:
            deadline = Deadline(None)

        s, protocol_version, handshake, data = \
            BoltSocket.connect(
                address,
                tcp_timeout=pool_config.connection_timeout,
                deadline=deadline,
                custom_resolver=pool_config.resolver,
                ssl_context=pool_config.get_ssl_context(),
                keep_alive=pool_config.keep_alive,
            )

        pool_config.protocol_version = protocol_version

        # Carry out Bolt subclass imports locally to avoid circular dependency
        # issues.
        if protocol_version == (5, 3):
            from ._bolt5 import Bolt5x3
            bolt_cls = Bolt5x3
        elif protocol_version == (5, 2):
            from ._bolt5 import Bolt5x2
            bolt_cls = Bolt5x2
        elif protocol_version == (5, 1):
            from ._bolt5 import Bolt5x1
            bolt_cls = Bolt5x1
        elif protocol_version == (5, 0):
            from ._bolt5 import Bolt5x0
            bolt_cls = Bolt5x0
        elif protocol_version == (4, 4):
            from ._bolt4 import Bolt4x4
            bolt_cls = Bolt4x4
        elif protocol_version == (4, 3):
            from ._bolt4 import Bolt4x3
            bolt_cls = Bolt4x3
        elif protocol_version == (4, 2):
            from ._bolt4 import Bolt4x2
            bolt_cls = Bolt4x2
        elif protocol_version == (4, 1):
            from ._bolt4 import Bolt4x1
            bolt_cls = Bolt4x1
        # Implementation for 4.0 exists, but there was no space left in the
        # handshake to offer this version to the server. Hence, the server
        # should never request us to speak bolt 4.0.
        # elif protocol_version == (4, 0):
        #     from ._bolt4 import AsyncBolt4x0
        #     bolt_cls = AsyncBolt4x0
        elif protocol_version == (3, 0):
            from ._bolt3 import Bolt3
            bolt_cls = Bolt3
        else:
            log.debug("[#%04X]  C: <CLOSE>", s.getsockname()[1])
            BoltSocket.close_socket(s)

            supported_versions = cls.protocol_handlers().keys()
            raise BoltHandshakeError(
                "The neo4j server does not support communication with this "
                "driver. This driver has support for Bolt protocols "
                "{}.".format(tuple(map(str, supported_versions))),
                address=address, request_data=handshake, response_data=data
            )

        try:
            auth = Util.callback(auth_manager.get_auth)
        except asyncio.CancelledError as e:
            log.debug("[#%04X]  C: <KILL> open auth manager failed: %r",
                      s.getsockname()[1], e)
            s.kill()
            raise
        except Exception as e:
            log.debug("[#%04X]  C: <CLOSE> open auth manager failed: %r",
                      s.getsockname()[1], e)
            s.close()
            raise

        connection = bolt_cls(
            address, s, pool_config.max_connection_lifetime, auth=auth,
            auth_manager=auth_manager, user_agent=pool_config.user_agent,
            routing_context=routing_context,
            notifications_min_severity=pool_config.notifications_min_severity,
            notifications_disabled_categories=
                pool_config.notifications_disabled_categories
        )

        try:
            connection.socket.set_deadline(deadline)
            try:
                connection.hello()
            finally:
                connection.socket.set_deadline(None)
        except (
            Exception,
            # Python 3.8+: CancelledError is a subclass of BaseException
            asyncio.CancelledError,
        ) as e:
            log.debug("[#%04X]  C: <OPEN FAILED> %r", connection.local_port, e)
            connection.kill()
            raise

        return connection

    @property
    @abc.abstractmethod
    def encrypted(self):
        pass

    @property
    @abc.abstractmethod
    def der_encoded_server_certificate(self):
        pass

    @abc.abstractmethod
    def hello(self, dehydration_hooks=None, hydration_hooks=None):
        """ Appends a HELLO message to the outgoing queue, sends it and consumes
         all remaining messages.

        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        """
        pass

    @abc.abstractmethod
    def logon(self, dehydration_hooks=None, hydration_hooks=None):
        """Append a LOGON message to the outgoing queue."""
        pass

    @abc.abstractmethod
    def logoff(self, dehydration_hooks=None, hydration_hooks=None):
        """Append a LOGOFF message to the outgoing queue."""
        pass

    def mark_unauthenticated(self):
        """Mark the connection as unauthenticated."""
        self.auth_dict = {}

    def re_auth(
        self, auth, auth_manager, force=False,
        dehydration_hooks=None, hydration_hooks=None,
    ):
        """Append LOGON, LOGOFF to the outgoing queue.

        If auth is the same as the current auth, this method does nothing.

        :returns: whether the auth was changed
        """
        new_auth_dict = self._to_auth_dict(auth)
        if not force and new_auth_dict == self.auth_dict:
            self.auth_manager = auth_manager
            self.auth = auth
            return False
        self.logoff(dehydration_hooks=dehydration_hooks,
                     hydration_hooks=hydration_hooks)
        self.auth_dict = new_auth_dict
        self.auth_manager = auth_manager
        self.auth = auth
        self.logon(dehydration_hooks=dehydration_hooks,
                    hydration_hooks=hydration_hooks)
        return True


    @abc.abstractmethod
    def route(
        self, database=None, imp_user=None, bookmarks=None,
        dehydration_hooks=None, hydration_hooks=None
    ):
        """ Fetch a routing table from the server for the given
        `database`. For Bolt 4.3 and above, this appends a ROUTE
        message; for earlier versions, a procedure call is made via
        the regular Cypher execution mechanism. In all cases, this is
        sent to the network, and a response is fetched.

        :param database: database for which to fetch a routing table
            Requires Bolt 4.0+.
        :param imp_user: the user to impersonate
            Requires Bolt 4.4+.
        :param bookmarks: iterable of bookmark values after which this
                          transaction should begin
        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        """
        pass

    @abc.abstractmethod
    def run(self, query, parameters=None, mode=None, bookmarks=None,
            metadata=None, timeout=None, db=None, imp_user=None,
            notifications_min_severity=None,
            notifications_disabled_categories=None, dehydration_hooks=None,
            hydration_hooks=None, **handlers):
        """ Appends a RUN message to the output queue.

        :param query: Cypher query string
        :param parameters: dictionary of Cypher parameters
        :param mode: access mode for routing - "READ" or "WRITE" (default)
        :param bookmarks: iterable of bookmark values after which this transaction should begin
        :param metadata: custom metadata dictionary to attach to the transaction
        :param timeout: timeout for transaction execution (seconds)
        :param db: name of the database against which to begin the transaction
            Requires Bolt 4.0+.
        :param imp_user: the user to impersonate
            Requires Bolt 4.4+.
        :param notifications_min_severity:
            minimum severity of notifications to be received.
            Requires Bolt 5.2+.
        :param notifications_disabled_categories:
            list of notification categories to be disabled.
            Requires Bolt 5.2+.
        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        :param handlers: handler functions passed into the returned Response object
        """
        pass

    @abc.abstractmethod
    def discard(self, n=-1, qid=-1, dehydration_hooks=None,
                hydration_hooks=None, **handlers):
        """ Appends a DISCARD message to the output queue.

        :param n: number of records to discard, default = -1 (ALL)
        :param qid: query ID to discard for, default = -1 (last query)
        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        :param handlers: handler functions passed into the returned Response object
        """
        pass

    @abc.abstractmethod
    def pull(self, n=-1, qid=-1, dehydration_hooks=None, hydration_hooks=None,
             **handlers):
        """ Appends a PULL message to the output queue.

        :param n: number of records to pull, default = -1 (ALL)
        :param qid: query ID to pull for, default = -1 (last query)
        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        :param handlers: handler functions passed into the returned Response object
        """
        pass

    @abc.abstractmethod
    def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,
              db=None, imp_user=None, notifications_min_severity=None,
              notifications_disabled_categories=None, dehydration_hooks=None,
              hydration_hooks=None, **handlers):
        """ Appends a BEGIN message to the output queue.

        :param mode: access mode for routing - "READ" or "WRITE" (default)
        :param bookmarks: iterable of bookmark values after which this transaction should begin
        :param metadata: custom metadata dictionary to attach to the transaction
        :param timeout: timeout for transaction execution (seconds)
        :param db: name of the database against which to begin the transaction
            Requires Bolt 4.0+.
        :param imp_user: the user to impersonate
            Requires Bolt 4.4+
        :param notifications_min_severity:
            minimum severity of notifications to be received.
            Requires Bolt 5.2+.
        :param notifications_disabled_categories:
            list of notification categories to be disabled.
            Requires Bolt 5.2+.
        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        :param handlers: handler functions passed into the returned Response object
        :returns: Response object
        """
        pass

    @abc.abstractmethod
    def commit(self, dehydration_hooks=None, hydration_hooks=None, **handlers):
        """ Appends a COMMIT message to the output queue.

        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        """
        pass

    @abc.abstractmethod
    def rollback(self, dehydration_hooks=None, hydration_hooks=None, **handlers):
        """ Appends a ROLLBACK message to the output queue.

        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything."""
        pass

    @abc.abstractmethod
    def reset(self, dehydration_hooks=None, hydration_hooks=None):
        """ Appends a RESET message to the outgoing queue, sends it and consumes
         all remaining messages.

        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        """
        pass

    @abc.abstractmethod
    def goodbye(self, dehydration_hooks=None, hydration_hooks=None):
        """Append a GOODBYE message to the outgoing queue.

        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        """
        pass

    def new_hydration_scope(self):
        return self.hydration_handler.new_hydration_scope()

    def _append(self, signature, fields=(), response=None,
                dehydration_hooks=None):
        """ Appends a message to the outgoing queue.

        :param signature: the signature of the message
        :param fields: the fields of the message as a tuple
        :param response: a response object to handle callbacks
        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        """
        self.outbox.append_message(signature, fields, dehydration_hooks)
        self.responses.append(response)
        if response:
            self._get_client_state_manager().transition(response.message)

    def _send_all(self):
        if self.outbox.flush():
            self.idle_since = perf_counter()

    def send_all(self):
        """ Send all queued messages to the server.
        """
        if self.closed():
            raise ServiceUnavailable(
                "Failed to write to closed connection {!r} ({!r})".format(
                    self.unresolved_address, self.server_info.address
                )
            )
        if self.defunct():
            raise ServiceUnavailable(
                "Failed to write to defunct connection {!r} ({!r})".format(
                    self.unresolved_address, self.server_info.address
                )
            )

        self._send_all()

    @abc.abstractmethod
    def _process_message(self, tag, fields):
        """ Receive at most one message from the server, if available.

        :returns: 2-tuple of number of detail messages and number of summary
                 messages fetched
        """
        pass

    def fetch_message(self):
        if self._closed:
            raise ServiceUnavailable(
                "Failed to read from closed connection {!r} ({!r})".format(
                    self.unresolved_address, self.server_info.address
                )
            )
        if self._defunct:
            raise ServiceUnavailable(
                "Failed to read from defunct connection {!r} ({!r})".format(
                    self.unresolved_address, self.server_info.address
                )
            )
        if not self.responses:
            return 0, 0

        # Receive exactly one message
        tag, fields = self.inbox.pop(
            hydration_hooks=self.responses[0].hydration_hooks
        )
        res = self._process_message(tag, fields)
        self.idle_since = perf_counter()
        return res

    def fetch_all(self):
        """ Fetch all outstanding messages.

        :returns: 2-tuple of number of detail messages and number of summary
                 messages fetched
        """
        detail_count = summary_count = 0
        while self.responses:
            response = self.responses[0]
            while not response.complete:
                detail_delta, summary_delta = self.fetch_message()
                detail_count += detail_delta
                summary_count += summary_delta
        return detail_count, summary_count

    def _set_defunct_read(self, error=None, silent=False):
        message = "Failed to read from defunct connection {!r} ({!r})".format(
            self.unresolved_address, self.server_info.address
        )
        self._set_defunct(message, error=error, silent=silent)

    def _set_defunct_write(self, error=None, silent=False):
        message = "Failed to write data to connection {!r} ({!r})".format(
            self.unresolved_address, self.server_info.address
        )
        self._set_defunct(message, error=error, silent=silent)

    def _set_defunct(self, message, error=None, silent=False):
        direct_driver = getattr(self.pool, "is_direct_pool", False)
        user_cancelled = isinstance(error, asyncio.CancelledError)

        if error:
            log.debug("[#%04X]  _: <CONNECTION> error: %r", self.local_port,
                      error)
        if not user_cancelled:
            log.error(message)
        # We were attempting to receive data but the connection
        # has unexpectedly terminated. So, we need to close the
        # connection from the client side, and remove the address
        # from the connection pool.
        self._defunct = True
        if user_cancelled:
            self.kill()
            raise error  # cancellation error should not be re-written
        if not self._closing:
            # If we fail while closing the connection, there is no need to
            # remove the connection from the pool, nor to try to close the
            # connection again.
            self.close()
            if self.pool and not self._get_server_state_manager().failed():
                self.pool.deactivate(address=self.unresolved_address)

        # Iterate through the outstanding responses, and if any correspond
        # to COMMIT requests then raise an error to signal that we are
        # unable to confirm that the COMMIT completed successfully.
        if silent:
            return
        for response in self.responses:
            if isinstance(response, CommitResponse):
                if error:
                    raise IncompleteCommit(message) from error
                else:
                    raise IncompleteCommit(message)

        if direct_driver:
            if error:
                raise ServiceUnavailable(message) from error
            else:
                raise ServiceUnavailable(message)
        else:
            if error:
                raise SessionExpired(message) from error
            else:
                raise SessionExpired(message)

    def stale(self):
        return (self._stale
                or (0 <= self._max_connection_lifetime
                    <= perf_counter() - self._creation_timestamp))

    _stale = False

    def set_stale(self):
        self._stale = True

    def close(self):
        """Close the connection."""
        if self._closed or self._closing:
            return
        self._closing = True
        if not self._defunct:
            self.goodbye()
            try:
                self._send_all()
            except (OSError, BoltError, DriverError) as exc:
                log.debug("[#%04X]  _: <CONNECTION> ignoring failed close %r",
                          self.local_port, exc)
        log.debug("[#%04X]  C: <CLOSE>", self.local_port)
        try:
            self.socket.close()
        except OSError:
            pass
        finally:
            self._closed = True

    def kill(self):
        """Close the socket most violently. No flush, no goodbye, no mercy."""
        if self._closed:
            return
        log.debug("[#%04X]  C: <KILL>", self.local_port)
        self._closing = True
        try:
            self.socket.kill()
        except OSError as exc:
            log.debug("[#%04X]  _: <CONNECTION> ignoring failed kill %r",
                      self.local_port, exc)
        finally:
            self._closed = True

    def closed(self):
        return self._closed

    def defunct(self):
        return self._defunct

    def is_idle_for(self, timeout):
        """Check if connection has been idle for at least the given timeout.

        :param timeout: timeout in seconds
        :type timeout: float

        :rtype: bool
        """
        return perf_counter() - self.idle_since > timeout


BoltSocket.Bolt = Bolt  # type: ignore


def tx_timeout_as_ms(timeout: float) -> int:
    """Round transaction timeout to milliseconds.

    Values in (0, 1], else values are rounded using the built-in round()
    function (round n.5 values to nearest even).

    :param timeout: timeout in seconds (must be >= 0)

    :returns: timeout in milliseconds (rounded)

    :raise ValueError: if timeout is negative
    """
    try:
        timeout = float(timeout)
    except (TypeError, ValueError) as e:
        err_type = type(e)
        msg = "Timeout must be specified as a number of seconds"
        raise err_type(msg) from None
    if timeout < 0:
        raise ValueError("Timeout must be a positive number or 0.")
    ms = int(round(1000 * timeout))
    if ms == 0 and timeout > 0:
        # Special case for 0 < timeout < 0.5 ms.
        # This would be rounded to 0 ms, but the server interprets this as
        # infinite timeout. So we round to the smallest possible timeout: 1 ms.
        ms = 1
    return ms

if __name__ == "__main__":
    import dill
    import os
    from src.neo4j._sync.io._bolt3 import Bolt3
    from src.neo4j._sync.io._bolt4 import (
        Bolt4x1,
        Bolt4x2,
        Bolt4x3,
        Bolt4x4,
    )
    from src.neo4j._sync.io._bolt5 import (
        Bolt5x0,
        Bolt5x1,
        Bolt5x2,
        Bolt5x3,
    )

    handlers = {
        Bolt3.PROTOCOL_VERSION: Bolt3,
        # 4.0 unsupported because no space left in the handshake
        Bolt4x1.PROTOCOL_VERSION: Bolt4x1,
        Bolt4x2.PROTOCOL_VERSION: Bolt4x2,
        Bolt4x3.PROTOCOL_VERSION: Bolt4x3,
        Bolt4x4.PROTOCOL_VERSION: Bolt4x4,
        Bolt5x0.PROTOCOL_VERSION: Bolt5x0,
        Bolt5x1.PROTOCOL_VERSION: Bolt5x1,
        Bolt5x2.PROTOCOL_VERSION: Bolt5x2,
        Bolt5x3.PROTOCOL_VERSION: Bolt5x3,
    }

    isT = True
    input1=(Bolt4x2.PROTOCOL_VERSION)
    input2=(Version(3,1))
    input3=(Version(4,3))
    input4=None
    try:
        res1=Bolt.protocol_handlers(input1)
        if Version(4,2) in res1.keys():
            if res1[Version(4,2)].__name__!="Bolt4x2":
                isT=False
        res2=Bolt.protocol_handlers(input2)
        if res2!={}:
            isT=False
        res3=Bolt.protocol_handlers(input3)
        if Version(4,3) in res3.keys():
            if res3[Version(4,3)].__name__!="Bolt4x3":
                isT=False
        res4=Bolt.protocol_handlers(input4)
        if len(res4)!=9:
            isT=False
    except:
        isT=False
    # for l in os.listdir(
    #     "/home/travis/builds/repos/neo4j---neo4j-python-driver/data_passk_platform/62e60ecfd76274f8a4026d6a/"):
    #     f = open(
    #         "/home/travis/builds/repos/neo4j---neo4j-python-driver/data_passk_platform/62e60ecfd76274f8a4026d6a/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     address = ("127.0.0.1", 7687)
    #     max_connection_lifetime = 0
    #     # temp_class = AsyncBolt(address,AsyncFakeSocket(address),max_connection_lifetime)
    #     # temp_class = Bolt(address,AsyncFakeSocket(address),max_connection_lifetime)
    #     # temp_class.__dict__.update(object_class)
    #     res0 = object_class.protocol_handlers(args1)
    #     print(str(res0))
    #     print(content["output"][0])
    #     if not ( dill.dumps(str(res0))== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_work/query_unit_of_work_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from __future__ import annotations

import typing as t


if t.TYPE_CHECKING:
    import typing_extensions as te

    _T = t.TypeVar("_T")


class Query:
    """A query with attached extra data.

    This wrapper class for queries is used to attach extra data to queries
    passed to :meth:`.Session.run` and :meth:`.AsyncSession.run`, fulfilling
    a similar role as :func:`.unit_of_work` for transactions functions.

    :param text: The query text.
    :param metadata: metadata attached to the query.
    :param timeout: seconds.
    """
    def __init__(
        self,
        text: te.LiteralString,
        metadata: t.Optional[t.Dict[str, t.Any]] = None,
        timeout: t.Optional[float] = None
    ) -> None:
        self.text = text

        self.metadata = metadata
        self.timeout = timeout

    def __str__(self) -> te.LiteralString:
        return str(self.text)

#query_unit_of_work_passk_validte
def unit_of_work(
    metadata: t.Optional[t.Dict[str, t.Any]] = None,
    timeout: t.Optional[float] = None
) -> t.Callable[[_T], _T]:
    """Decorator giving extra control over transaction function configuration.

    This function is a decorator for transaction functions that allows extra
    control over how the transaction is carried out.

    For example, a timeout may be applied::

        from neo4j import unit_of_work


        @unit_of_work(timeout=100)
        def count_people_tx(tx):
            result = tx.run("MATCH (a:Person) RETURN count(a) AS persons")
            record = result.single()
            return record["persons"]

    :param metadata:
        a dictionary with metadata.
        Specified metadata will be attached to the executing transaction
        and visible in the output of ``SHOW TRANSACTIONS YIELD *``
        It will also get logged to the ``query.log``.
        This functionality makes it easier to tag transactions and is
        equivalent to the ``dbms.setTXMetaData`` procedure, see
        https://neo4j.com/docs/cypher-manual/current/clauses/transaction-clauses/#query-listing-transactions
        and https://neo4j.com/docs/operations-manual/current/reference/procedures/
        for reference.

    :param timeout:
        the transaction timeout in seconds.
        Transactions that execute longer than the configured timeout will be
        terminated by the database.
        This functionality allows to limit query/transaction execution time.
        Specified timeout overrides the default timeout configured in the
        database using ``dbms.transaction.timeout`` setting.
        Values higher than ``dbms.transaction.timeout`` will be ignored and
        will fall back to default (unless using Neo4j < 4.2).
        Value should not represent a negative duration.
        A zero duration will make the transaction execute indefinitely.
        None will use the default timeout configured in the database.
    """

    def wrapper(f):

        def wrapped(*args, **kwargs):
            return f(*args, **kwargs)

        wrapped.metadata = metadata
        wrapped.timeout = timeout
        return wrapped

    return wrapper


if __name__ == "__main__":
    import dill
    import os
    isT=True
    @unit_of_work(timeout=100)
    def count_people_tx(input_arg):
        return input_arg


    input_args="input value"
    output_args = count_people_tx(input_args)
    # print(input_args,output_args)
    if input_args!=output_args:
        isT=False

    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_data_index_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from __future__ import annotations
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
import typing as t
from abc import (
    ABCMeta,
    abstractmethod,
)
from collections.abc import (
    Mapping,
    Sequence,
    Set,
)
from functools import reduce
from operator import xor as xor_operator

from src.neo4j._codec.hydration import BrokenHydrationObject
from src.neo4j._conf import iter_items
from src.neo4j._meta import deprecated
from src.neo4j.exceptions import BrokenRecordError
from src.neo4j.graph import (
    Node,
    Path,
    Relationship,
)


_T = t.TypeVar("_T")
_K = t.Union[int, str]


class Record(tuple, Mapping):
    """ A :class:`.Record` is an immutable ordered collection of key-value
    pairs. It is generally closer to a :py:class:`namedtuple` than to a
    :py:class:`OrderedDict` in as much as iteration of the collection will
    yield values rather than keys.
    """

    __keys: t.Tuple[str]

    def __new__(cls, iterable=()):
        keys = []
        values = []
        for key, value in iter_items(iterable):
            keys.append(key)
            values.append(value)
        inst = tuple.__new__(cls, values)
        inst.__keys = tuple(keys)
        return inst

    def _broken_record_error(self, index):
        return BrokenRecordError(
            f"Record contains broken data at {index} ('{self.__keys[index]}')"
        )

    def _super_getitem_single(self, index):
        value = super().__getitem__(index)
        if isinstance(value, BrokenHydrationObject):
            raise self._broken_record_error(index) from value.error
        return value

    def __repr__(self) -> str:
        return "<%s %s>" % (
            self.__class__.__name__,
            " ".join("%s=%r" % (field, value)
                     for field, value in zip(self.__keys, super().__iter__()))
        )

    __str__ = __repr__

    def __eq__(self, other: object) -> bool:
        """ In order to be flexible regarding comparison, the equality rules
        for a record permit comparison with any other Sequence or Mapping.

        :param other:
        :returns:
        """
        compare_as_sequence = isinstance(other, Sequence)
        compare_as_mapping = isinstance(other, Mapping)
        if compare_as_sequence and compare_as_mapping:
            other = t.cast(t.Mapping, other)
            return list(self) == list(other) and dict(self) == dict(other)
        elif compare_as_sequence:
            other = t.cast(t.Sequence, other)
            return list(self) == list(other)
        elif compare_as_mapping:
            other = t.cast(t.Mapping, other)
            return dict(self) == dict(other)
        else:
            return False

    def __ne__(self, other: object) -> bool:
        return not self.__eq__(other)

    def __hash__(self):
        return reduce(xor_operator, map(hash, self.items()))

    def __iter__(self) -> t.Iterator[t.Any]:
        for i, v in enumerate(super().__iter__()):
            if isinstance(v, BrokenHydrationObject):
                raise self._broken_record_error(i) from v.error
            yield v

    def __getitem__(  # type: ignore[override]
        self, key: t.Union[_K, slice]
    ) -> t.Any:
        if isinstance(key, slice):
            keys = self.__keys[key]
            values = super().__getitem__(key)
            return self.__class__(zip(keys, values))
        try:
            index = self.index(key)
        except IndexError:
            return None
        else:
            return self._super_getitem_single(index)

    # TODO: 6.0 - remove
    @deprecated("This method is deprecated and will be removed in the future.")
    def __getslice__(self, start, stop):
        key = slice(start, stop)
        keys = self.__keys[key]
        values = tuple(self)[key]
        return self.__class__(zip(keys, values))

    def get(self, key: str, default: t.Optional[object] = None) -> t.Any:
        """ Obtain a value from the record by key, returning a default
        value if the key does not exist.

        :param key: a key
        :param default: default value

        :returns: a value
        """
        try:
            index = self.__keys.index(str(key))
        except ValueError:
            return default
        if 0 <= index < len(self):
            return self._super_getitem_single(index)
        else:
            return default

    def index(self, key: _K) -> int:  # type: ignore[override]
        """ Return the index of the given item.

        :param key: a key

        :returns: index
        """
        if isinstance(key, int):
            if 0 <= key < len(self.__keys):
                return key
            raise IndexError(key)
        elif isinstance(key, str):
            try:
                return self.__keys.index(key)
            except ValueError as exc:
                raise KeyError(key) from exc
        else:
            raise TypeError(key)

    def value(
        self, key: _K = 0, default: t.Optional[object] = None
    ) -> t.Any:
        """ Obtain a single value from the record by index or key. If no
        index or key is specified, the first value is returned. If the
        specified item does not exist, the default value is returned.

        :param key: an index or key
        :param default: default value

        :returns: a single value
        """
        try:
            index = self.index(key)
        except (IndexError, KeyError):
            return default
        else:
            return self[index]

    def keys(self) -> t.List[str]:  # type: ignore[override]
        """ Return the keys of the record.

        :returns: list of key names
        """
        return list(self.__keys)

    def values(self, *keys: _K) -> t.List[t.Any]:  # type: ignore[override]
        """ Return the values of the record, optionally filtering to
        include only certain values by index or key.

        :param keys: indexes or keys of the items to include; if none
                     are provided, all values will be included

        :returns: list of values
        """
        if keys:
            d: t.List[t.Any] = []
            for key in keys:
                try:
                    i = self.index(key)
                except KeyError:
                    d.append(None)
                else:
                    d.append(self[i])
            return d
        return list(self)

    def items(self, *keys):
        """ Return the fields of the record as a list of key and value tuples

        :returns: a list of value tuples
        """
        if keys:
            d = []
            for key in keys:
                try:
                    i = self.index(key)
                except KeyError:
                    d.append((key, None))
                else:
                    d.append((self.__keys[i], self[i]))
            return d
        return list((self.__keys[i], self._super_getitem_single(i))
                    for i in range(len(self)))

    def data(self, *keys: _K) -> t.Dict[str, t.Any]:
        """ Return the keys and values of this record as a dictionary,
        optionally including only certain values by index or key. Keys
        provided in the items that are not in the record will be
        inserted with a value of :data:`None`; indexes provided
        that are out of bounds will trigger an :exc:`IndexError`.

        :param keys: indexes or keys of the items to include; if none
                      are provided, all values will be included

        :returns: dictionary of values, keyed by field name

        :raises: :exc:`IndexError` if an out-of-bounds index is specified
        """
        return RecordExporter().transform(dict(self.items(*keys)))


class DataTransformer(metaclass=ABCMeta):
    """ Abstract base class for transforming data from one form into
    another.
    """

    @abstractmethod
    def transform(self, x):
        """ Transform a value, or collection of values.

        :param x: input value
        :returns: output value
        """


class RecordExporter(DataTransformer):
    """ Transformer class used by the :meth:`.Record.data` method.
    """

    def transform(self, x):
        if isinstance(x, Node):
            return self.transform(dict(x))
        elif isinstance(x, Relationship):
            return (self.transform(dict(x.start_node)),
                    x.__class__.__name__,
                    self.transform(dict(x.end_node)))
        elif isinstance(x, Path):
            path = [self.transform(x.start_node)]
            for i, relationship in enumerate(x.relationships):
                path.append(self.transform(relationship.__class__.__name__))
                path.append(self.transform(x.nodes[i + 1]))
            return path
        elif isinstance(x, str):
            return x
        elif isinstance(x, Sequence):
            typ = type(x)
            return typ(map(self.transform, x))
        elif isinstance(x, Set):
            typ = type(x)
            return typ(map(self.transform, x))
        elif isinstance(x, Mapping):
            typ = type(x)
            return typ((k, self.transform(v)) for k, v in x.items())
        else:
            return x


class RecordTableRowExporter(DataTransformer):
    """Transformer class used by the :meth:`.Result.to_df` method."""

    def transform(self, x):
        assert isinstance(x, Mapping)
        typ = type(x)
        return typ(item
                   for k, v in x.items()
                   for item in self._transform(
                       v, prefix=k.replace("\\", "\\\\").replace(".", "\\.")
                   ).items())

    def _transform(self, x, prefix):
        if isinstance(x, Node):
            res = {
                "%s().element_id" % prefix: x.element_id,
                "%s().labels" % prefix: x.labels,
            }
            res.update(("%s().prop.%s" % (prefix, k), v) for k, v in x.items())
            return res
        elif isinstance(x, Relationship):
            res = {
                "%s->.element_id" % prefix: x.element_id,
                "%s->.start.element_id" % prefix: x.start_node.element_id,
                "%s->.end.element_id" % prefix: x.end_node.element_id,
                "%s->.type" % prefix: x.__class__.__name__,
            }
            res.update(("%s->.prop.%s" % (prefix, k), v) for k, v in x.items())
            return res
        elif isinstance(x, Path) or isinstance(x, str):
            return {prefix: x}
        elif isinstance(x, Sequence):
            return dict(
                item
                for i, v in enumerate(x)
                for item in self._transform(
                    v, prefix="%s[].%i" % (prefix, i)
                ).items()
            )
        elif isinstance(x, Mapping):
            typ = type(x)
            return typ(
                item
                for k, v in x.items()
                for item in self._transform(
                    v, prefix="%s{}.%s" % (prefix, k.replace("\\", "\\\\")
                                                    .replace(".", "\\."))
                ).items()
            )
        else:
            return {prefix: x}

if __name__ == "__main__":
    import dill
    import os
    isT=True
    try:
        r=Record({0:"zero",1:"one","name":"smart student","empire":"bbb"})
        res1=r.index(0)
        res2 = r.index(1)
        res3 = r.index(0)
        res4 = r.index(1)
        res5 = r.index("name")
        res6 = r.index("empire")
        if res1!=0 or res2!=1 or res3!=0 or res4!=1 or res5!=2 or res6!=3:
            isT=False
        try:
            res7 = r.index("notin")
            isT=False
        except:
            pass

    except:
        isT=False

    if not isT:
        raise Exception("Result not True!!!")
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver\\data_passk_platform1\\62e60e05d76274f8a4026cfd/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver\\data_passk_platform1\\62e60e05d76274f8a4026cfd\\"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     # object_class=dill.loads(content["input"]["args"][0]["bytes"])
    #     print(args1)
    # #     temp_class=Record()
    # #     temp_class.__dict__.update(object_class)
    # #     res0 = temp_class.index(args1)
    # #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    # #         isT=False
    # #         break
    # # if not isT:
    # #     raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_data_values_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from __future__ import annotations
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
import typing as t
from abc import (
    ABCMeta,
    abstractmethod,
)
from collections.abc import (
    Mapping,
    Sequence,
    Set,
)
from functools import reduce
from operator import xor as xor_operator

from src.neo4j._codec.hydration import BrokenHydrationObject
from src.neo4j._conf import iter_items
from src.neo4j._meta import deprecated
from src.neo4j.exceptions import BrokenRecordError
from src.neo4j.graph import (
    Node,
    Path,
    Relationship,
)


_T = t.TypeVar("_T")
_K = t.Union[int, str]


class Record(tuple, Mapping):
    """ A :class:`.Record` is an immutable ordered collection of key-value
    pairs. It is generally closer to a :py:class:`namedtuple` than to a
    :py:class:`OrderedDict` in as much as iteration of the collection will
    yield values rather than keys.
    """

    __keys: t.Tuple[str]

    def __new__(cls, iterable=()):
        keys = []
        values = []
        for key, value in iter_items(iterable):
            keys.append(key)
            values.append(value)
        inst = tuple.__new__(cls, values)
        inst.__keys = tuple(keys)
        return inst

    def _broken_record_error(self, index):
        return BrokenRecordError(
            f"Record contains broken data at {index} ('{self.__keys[index]}')"
        )

    def _super_getitem_single(self, index):
        value = super().__getitem__(index)
        if isinstance(value, BrokenHydrationObject):
            raise self._broken_record_error(index) from value.error
        return value

    def __repr__(self) -> str:
        return "<%s %s>" % (
            self.__class__.__name__,
            " ".join("%s=%r" % (field, value)
                     for field, value in zip(self.__keys, super().__iter__()))
        )

    __str__ = __repr__

    def __eq__(self, other: object) -> bool:
        """ In order to be flexible regarding comparison, the equality rules
        for a record permit comparison with any other Sequence or Mapping.

        :param other:
        :returns:
        """
        compare_as_sequence = isinstance(other, Sequence)
        compare_as_mapping = isinstance(other, Mapping)
        if compare_as_sequence and compare_as_mapping:
            other = t.cast(t.Mapping, other)
            return list(self) == list(other) and dict(self) == dict(other)
        elif compare_as_sequence:
            other = t.cast(t.Sequence, other)
            return list(self) == list(other)
        elif compare_as_mapping:
            other = t.cast(t.Mapping, other)
            return dict(self) == dict(other)
        else:
            return False

    def __ne__(self, other: object) -> bool:
        return not self.__eq__(other)

    def __hash__(self):
        return reduce(xor_operator, map(hash, self.items()))

    def __iter__(self) -> t.Iterator[t.Any]:
        for i, v in enumerate(super().__iter__()):
            if isinstance(v, BrokenHydrationObject):
                raise self._broken_record_error(i) from v.error
            yield v

    def __getitem__(  # type: ignore[override]
        self, key: t.Union[_K, slice]
    ) -> t.Any:
        if isinstance(key, slice):
            keys = self.__keys[key]
            values = super().__getitem__(key)
            return self.__class__(zip(keys, values))
        try:
            index = self.index(key)
        except IndexError:
            return None
        else:
            return self._super_getitem_single(index)

    # TODO: 6.0 - remove
    @deprecated("This method is deprecated and will be removed in the future.")
    def __getslice__(self, start, stop):
        key = slice(start, stop)
        keys = self.__keys[key]
        values = tuple(self)[key]
        return self.__class__(zip(keys, values))

    def get(self, key: str, default: t.Optional[object] = None) -> t.Any:
        """ Obtain a value from the record by key, returning a default
        value if the key does not exist.

        :param key: a key
        :param default: default value

        :returns: a value
        """
        try:
            index = self.__keys.index(str(key))
        except ValueError:
            return default
        if 0 <= index < len(self):
            return self._super_getitem_single(index)
        else:
            return default

    def index(self, key: _K) -> int:  # type: ignore[override]
        """ Return the index of the given item.

        :param key: a key

        :returns: index
        """
        if isinstance(key, int):
            if 0 <= key < len(self.__keys):
                return key
            raise IndexError(key)
        elif isinstance(key, str):
            try:
                return self.__keys.index(key)
            except ValueError as exc:
                raise KeyError(key) from exc
        else:
            raise TypeError(key)

    def value(
        self, key: _K = 0, default: t.Optional[object] = None
    ) -> t.Any:
        """ Obtain a single value from the record by index or key. If no
        index or key is specified, the first value is returned. If the
        specified item does not exist, the default value is returned.

        :param key: an index or key
        :param default: default value

        :returns: a single value
        """
        try:
            index = self.index(key)
        except (IndexError, KeyError):
            return default
        else:
            return self[index]

    def keys(self) -> t.List[str]:  # type: ignore[override]
        """ Return the keys of the record.

        :returns: list of key names
        """
        return list(self.__keys)

    def values(self, *keys: _K) -> t.List[t.Any]:  # type: ignore[override]
        """ Return the values of the record, optionally filtering to
        include only certain values by index or key.

        :param keys: indexes or keys of the items to include; if none
                     are provided, all values will be included

        :returns: list of values
        """
        if keys:
            d: t.List[t.Any] = []
            for key in keys:
                try:
                    i = self.index(key)
                except KeyError:
                    d.append(None)
                else:
                    d.append(self[i])
            return d
        return list(self)

    def items(self, *keys):
        """ Return the fields of the record as a list of key and value tuples

        :returns: a list of value tuples
        """
        if keys:
            d = []
            for key in keys:
                try:
                    i = self.index(key)
                except KeyError:
                    d.append((key, None))
                else:
                    d.append((self.__keys[i], self[i]))
            return d
        return list((self.__keys[i], self._super_getitem_single(i))
                    for i in range(len(self)))

    def data(self, *keys: _K) -> t.Dict[str, t.Any]:
        """ Return the keys and values of this record as a dictionary,
        optionally including only certain values by index or key. Keys
        provided in the items that are not in the record will be
        inserted with a value of :data:`None`; indexes provided
        that are out of bounds will trigger an :exc:`IndexError`.

        :param keys: indexes or keys of the items to include; if none
                      are provided, all values will be included

        :returns: dictionary of values, keyed by field name

        :raises: :exc:`IndexError` if an out-of-bounds index is specified
        """
        return RecordExporter().transform(dict(self.items(*keys)))


class DataTransformer(metaclass=ABCMeta):
    """ Abstract base class for transforming data from one form into
    another.
    """

    @abstractmethod
    def transform(self, x):
        """ Transform a value, or collection of values.

        :param x: input value
        :returns: output value
        """


class RecordExporter(DataTransformer):
    """ Transformer class used by the :meth:`.Record.data` method.
    """

    def transform(self, x):
        if isinstance(x, Node):
            return self.transform(dict(x))
        elif isinstance(x, Relationship):
            return (self.transform(dict(x.start_node)),
                    x.__class__.__name__,
                    self.transform(dict(x.end_node)))
        elif isinstance(x, Path):
            path = [self.transform(x.start_node)]
            for i, relationship in enumerate(x.relationships):
                path.append(self.transform(relationship.__class__.__name__))
                path.append(self.transform(x.nodes[i + 1]))
            return path
        elif isinstance(x, str):
            return x
        elif isinstance(x, Sequence):
            typ = type(x)
            return typ(map(self.transform, x))
        elif isinstance(x, Set):
            typ = type(x)
            return typ(map(self.transform, x))
        elif isinstance(x, Mapping):
            typ = type(x)
            return typ((k, self.transform(v)) for k, v in x.items())
        else:
            return x


class RecordTableRowExporter(DataTransformer):
    """Transformer class used by the :meth:`.Result.to_df` method."""

    def transform(self, x):
        assert isinstance(x, Mapping)
        typ = type(x)
        return typ(item
                   for k, v in x.items()
                   for item in self._transform(
                       v, prefix=k.replace("\\", "\\\\").replace(".", "\\.")
                   ).items())

    def _transform(self, x, prefix):
        if isinstance(x, Node):
            res = {
                "%s().element_id" % prefix: x.element_id,
                "%s().labels" % prefix: x.labels,
            }
            res.update(("%s().prop.%s" % (prefix, k), v) for k, v in x.items())
            return res
        elif isinstance(x, Relationship):
            res = {
                "%s->.element_id" % prefix: x.element_id,
                "%s->.start.element_id" % prefix: x.start_node.element_id,
                "%s->.end.element_id" % prefix: x.end_node.element_id,
                "%s->.type" % prefix: x.__class__.__name__,
            }
            res.update(("%s->.prop.%s" % (prefix, k), v) for k, v in x.items())
            return res
        elif isinstance(x, Path) or isinstance(x, str):
            return {prefix: x}
        elif isinstance(x, Sequence):
            return dict(
                item
                for i, v in enumerate(x)
                for item in self._transform(
                    v, prefix="%s[].%i" % (prefix, i)
                ).items()
            )
        elif isinstance(x, Mapping):
            typ = type(x)
            return typ(
                item
                for k, v in x.items()
                for item in self._transform(
                    v, prefix="%s{}.%s" % (prefix, k.replace("\\", "\\\\")
                                                    .replace(".", "\\."))
                ).items()
            )
        else:
            return {prefix: x}

if __name__ == "__main__":
    import dill
    import os

    isT = True
    try:
        r = Record(
            {0: "zero", 1: "one", "name": "smart student", "empire": "bbb"})
        if r.values()!=['zero', 'one', 'smart student', 'bbb']:
            isT=False
    except:
        isT=False
    # for l in os.listdir(
    #     "C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver\\data_passk_platform1\\62e60da4d76274f8a4026cf1/"):
    #     f = open(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver\\data_passk_platform1\\62e60da4d76274f8a4026cf1/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     print(type(content))
    #     print(type(content["input"]["args"][0]["bytes"]))
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     print(type(object_class))
    #     temp_class = Record()
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.values()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_data_data_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from __future__ import annotations

import typing as t
from abc import (
    ABCMeta,
    abstractmethod,
)
from collections.abc import (
    Mapping,
    Sequence,
    Set,
)
from functools import reduce
from operator import xor as xor_operator
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
from src.neo4j._codec.hydration import BrokenHydrationObject
from src.neo4j._conf import iter_items
from src.neo4j._meta import deprecated
from src.neo4j.exceptions import BrokenRecordError
from src.neo4j.graph import (
    Node,
    Path,
    Relationship,
)


_T = t.TypeVar("_T")
_K = t.Union[int, str]


class Record(tuple, Mapping):
    """ A :class:`.Record` is an immutable ordered collection of key-value
    pairs. It is generally closer to a :py:class:`namedtuple` than to a
    :py:class:`OrderedDict` in as much as iteration of the collection will
    yield values rather than keys.
    """

    __keys: t.Tuple[str]

    def __new__(cls, iterable=()):
        keys = []
        values = []
        for key, value in iter_items(iterable):
            keys.append(key)
            values.append(value)
        inst = tuple.__new__(cls, values)
        inst.__keys = tuple(keys)
        return inst

    def _broken_record_error(self, index):
        return BrokenRecordError(
            f"Record contains broken data at {index} ('{self.__keys[index]}')"
        )

    def _super_getitem_single(self, index):
        value = super().__getitem__(index)
        if isinstance(value, BrokenHydrationObject):
            raise self._broken_record_error(index) from value.error
        return value

    def __repr__(self) -> str:
        return "<%s %s>" % (
            self.__class__.__name__,
            " ".join("%s=%r" % (field, value)
                     for field, value in zip(self.__keys, super().__iter__()))
        )

    __str__ = __repr__

    def __eq__(self, other: object) -> bool:
        """ In order to be flexible regarding comparison, the equality rules
        for a record permit comparison with any other Sequence or Mapping.

        :param other:
        :returns:
        """
        compare_as_sequence = isinstance(other, Sequence)
        compare_as_mapping = isinstance(other, Mapping)
        if compare_as_sequence and compare_as_mapping:
            other = t.cast(t.Mapping, other)
            return list(self) == list(other) and dict(self) == dict(other)
        elif compare_as_sequence:
            other = t.cast(t.Sequence, other)
            return list(self) == list(other)
        elif compare_as_mapping:
            other = t.cast(t.Mapping, other)
            return dict(self) == dict(other)
        else:
            return False

    def __ne__(self, other: object) -> bool:
        return not self.__eq__(other)

    def __hash__(self):
        return reduce(xor_operator, map(hash, self.items()))

    def __iter__(self) -> t.Iterator[t.Any]:
        for i, v in enumerate(super().__iter__()):
            if isinstance(v, BrokenHydrationObject):
                raise self._broken_record_error(i) from v.error
            yield v

    def __getitem__(  # type: ignore[override]
        self, key: t.Union[_K, slice]
    ) -> t.Any:
        if isinstance(key, slice):
            keys = self.__keys[key]
            values = super().__getitem__(key)
            return self.__class__(zip(keys, values))
        try:
            index = self.index(key)
        except IndexError:
            return None
        else:
            return self._super_getitem_single(index)

    # TODO: 6.0 - remove
    @deprecated("This method is deprecated and will be removed in the future.")
    def __getslice__(self, start, stop):
        key = slice(start, stop)
        keys = self.__keys[key]
        values = tuple(self)[key]
        return self.__class__(zip(keys, values))

    def get(self, key: str, default: t.Optional[object] = None) -> t.Any:
        """ Obtain a value from the record by key, returning a default
        value if the key does not exist.

        :param key: a key
        :param default: default value

        :returns: a value
        """
        try:
            index = self.__keys.index(str(key))
        except ValueError:
            return default
        if 0 <= index < len(self):
            return self._super_getitem_single(index)
        else:
            return default

    def index(self, key: _K) -> int:  # type: ignore[override]
        """ Return the index of the given item.

        :param key: a key

        :returns: index
        """
        if isinstance(key, int):
            if 0 <= key < len(self.__keys):
                return key
            raise IndexError(key)
        elif isinstance(key, str):
            try:
                return self.__keys.index(key)
            except ValueError as exc:
                raise KeyError(key) from exc
        else:
            raise TypeError(key)

    def value(
        self, key: _K = 0, default: t.Optional[object] = None
    ) -> t.Any:
        """ Obtain a single value from the record by index or key. If no
        index or key is specified, the first value is returned. If the
        specified item does not exist, the default value is returned.

        :param key: an index or key
        :param default: default value

        :returns: a single value
        """
        try:
            index = self.index(key)
        except (IndexError, KeyError):
            return default
        else:
            return self[index]

    def keys(self) -> t.List[str]:  # type: ignore[override]
        """ Return the keys of the record.

        :returns: list of key names
        """
        return list(self.__keys)

    def values(self, *keys: _K) -> t.List[t.Any]:  # type: ignore[override]
        """ Return the values of the record, optionally filtering to
        include only certain values by index or key.

        :param keys: indexes or keys of the items to include; if none
                     are provided, all values will be included

        :returns: list of values
        """
        if keys:
            d: t.List[t.Any] = []
            for key in keys:
                try:
                    i = self.index(key)
                except KeyError:
                    d.append(None)
                else:
                    d.append(self[i])
            return d
        return list(self)

    def items(self, *keys):
        """ Return the fields of the record as a list of key and value tuples

        :returns: a list of value tuples
        """
        if keys:
            d = []
            for key in keys:
                try:
                    i = self.index(key)
                except KeyError:
                    d.append((key, None))
                else:
                    d.append((self.__keys[i], self[i]))
            return d
        return list((self.__keys[i], self._super_getitem_single(i))
                    for i in range(len(self)))

    def data(self, *keys: _K) -> t.Dict[str, t.Any]:
        """ Return the keys and values of this record as a dictionary,
        optionally including only certain values by index or key. Keys
        provided in the items that are not in the record will be
        inserted with a value of :data:`None`; indexes provided
        that are out of bounds will trigger an :exc:`IndexError`.

        :param keys: indexes or keys of the items to include; if none
                      are provided, all values will be included

        :returns: dictionary of values, keyed by field name

        :raises: :exc:`IndexError` if an out-of-bounds index is specified
        """
        return RecordExporter().transform(dict(self.items(*keys)))


class DataTransformer(metaclass=ABCMeta):
    """ Abstract base class for transforming data from one form into
    another.
    """

    @abstractmethod
    def transform(self, x):
        """ Transform a value, or collection of values.

        :param x: input value
        :returns: output value
        """


class RecordExporter(DataTransformer):
    """ Transformer class used by the :meth:`.Record.data` method.
    """

    def transform(self, x):
        if isinstance(x, Node):
            return self.transform(dict(x))
        elif isinstance(x, Relationship):
            return (self.transform(dict(x.start_node)),
                    x.__class__.__name__,
                    self.transform(dict(x.end_node)))
        elif isinstance(x, Path):
            path = [self.transform(x.start_node)]
            for i, relationship in enumerate(x.relationships):
                path.append(self.transform(relationship.__class__.__name__))
                path.append(self.transform(x.nodes[i + 1]))
            return path
        elif isinstance(x, str):
            return x
        elif isinstance(x, Sequence):
            typ = type(x)
            return typ(map(self.transform, x))
        elif isinstance(x, Set):
            typ = type(x)
            return typ(map(self.transform, x))
        elif isinstance(x, Mapping):
            typ = type(x)
            return typ((k, self.transform(v)) for k, v in x.items())
        else:
            return x


class RecordTableRowExporter(DataTransformer):
    """Transformer class used by the :meth:`.Result.to_df` method."""

    def transform(self, x):
        assert isinstance(x, Mapping)
        typ = type(x)
        return typ(item
                   for k, v in x.items()
                   for item in self._transform(
                       v, prefix=k.replace("\\", "\\\\").replace(".", "\\.")
                   ).items())

    def _transform(self, x, prefix):
        if isinstance(x, Node):
            res = {
                "%s().element_id" % prefix: x.element_id,
                "%s().labels" % prefix: x.labels,
            }
            res.update(("%s().prop.%s" % (prefix, k), v) for k, v in x.items())
            return res
        elif isinstance(x, Relationship):
            res = {
                "%s->.element_id" % prefix: x.element_id,
                "%s->.start.element_id" % prefix: x.start_node.element_id,
                "%s->.end.element_id" % prefix: x.end_node.element_id,
                "%s->.type" % prefix: x.__class__.__name__,
            }
            res.update(("%s->.prop.%s" % (prefix, k), v) for k, v in x.items())
            return res
        elif isinstance(x, Path) or isinstance(x, str):
            return {prefix: x}
        elif isinstance(x, Sequence):
            return dict(
                item
                for i, v in enumerate(x)
                for item in self._transform(
                    v, prefix="%s[].%i" % (prefix, i)
                ).items()
            )
        elif isinstance(x, Mapping):
            typ = type(x)
            return typ(
                item
                for k, v in x.items()
                for item in self._transform(
                    v, prefix="%s{}.%s" % (prefix, k.replace("\\", "\\\\")
                                                    .replace(".", "\\."))
                ).items()
            )
        else:
            return {prefix: x}

if __name__ == "__main__":
    import dill
    import os
    isT=True
    dictt = (("key1","value1"), ("key1","value1"), ("key1","value1"), ("key2","value2"))

    temp_class = Record(dictt)
    res0 = temp_class.data()
    if "key1" not in res0.keys() or "key2" not in res0.keys():
        isT = False
    if isT:
        if res0["key1"]!="value1" or res0["key2"]!="value2":
            isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_codec/packstream/v1/__init___pop_u16_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import typing as t
from codecs import decode
from contextlib import contextmanager
from struct import (
    pack as struct_pack,
    unpack as struct_unpack,
)
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
from src.neo4j._optional_deps import (
    np,
    pd,
)
from src.neo4j._codec.hydration import DehydrationHooks
from src.neo4j._codec.packstream._common import Structure


NONE_VALUES: t.Tuple = (None,)
TRUE_VALUES: t.Tuple = (True,)
FALSE_VALUES: t.Tuple = (False,)
INT_TYPES: t.Tuple[t.Type, ...] = (int,)
FLOAT_TYPES: t.Tuple[t.Type, ...] = (float,)
# we can't put tuple here because spatial types subclass tuple,
# and we don't want to treat them as sequences
SEQUENCE_TYPES: t.Tuple[t.Type, ...] = (list,)
MAPPING_TYPES: t.Tuple[t.Type, ...] = (dict,)
BYTES_TYPES: t.Tuple[t.Type, ...] = (bytes, bytearray)


if np is not None:
    TRUE_VALUES = (*TRUE_VALUES, np.bool_(True))
    FALSE_VALUES = (*FALSE_VALUES, np.bool_(False))
    INT_TYPES = (*INT_TYPES, np.integer)
    FLOAT_TYPES = (*FLOAT_TYPES, np.floating)
    SEQUENCE_TYPES = (*SEQUENCE_TYPES, np.ndarray)

if pd is not None:
    NONE_VALUES = (*NONE_VALUES, pd.NA)
    SEQUENCE_TYPES = (*SEQUENCE_TYPES, pd.Series, pd.Categorical,
                      pd.core.arrays.ExtensionArray)
    MAPPING_TYPES = (*MAPPING_TYPES, pd.DataFrame)


PACKED_UINT_8 = [struct_pack(">B", value) for value in range(0x100)]
PACKED_UINT_16 = [struct_pack(">H", value) for value in range(0x10000)]

UNPACKED_UINT_8 = {bytes(bytearray([x])): x for x in range(0x100)}
UNPACKED_UINT_16 = {struct_pack(">H", x): x for x in range(0x10000)}

INT64_MIN = -(2 ** 63)
INT64_MAX = 2 ** 63


class Packer:

    def __init__(self, stream):
        self.stream = stream
        self._write = self.stream.write

    def _pack_raw(self, data):
        self._write(data)

    def pack(self, data, dehydration_hooks=None):
        self._pack(data,
                   dehydration_hooks=self._inject_hooks(dehydration_hooks))

    @classmethod
    def _inject_hooks(cls, dehydration_hooks=None):
        if dehydration_hooks is None:
            return DehydrationHooks(
                exact_types={tuple: list},
                subtypes={}
            )
        return dehydration_hooks.extend(
            exact_types={tuple: list},
            subtypes={}
        )


    def _pack(self, value, dehydration_hooks=None):
        write = self._write

        # None
        if any(value is v for v in NONE_VALUES):
            write(b"\xC0")  # NULL

        # Boolean
        elif any(value is v for v in TRUE_VALUES):
            write(b"\xC3")
        elif any(value is v for v in FALSE_VALUES):
            write(b"\xC2")

        # Float (only double precision is supported)
        elif isinstance(value, FLOAT_TYPES):
            write(b"\xC1")
            write(struct_pack(">d", value))

        # Integer
        elif isinstance(value, INT_TYPES):
            value = int(value)
            if -0x10 <= value < 0x80:
                write(PACKED_UINT_8[value % 0x100])
            elif -0x80 <= value < -0x10:
                write(b"\xC8")
                write(PACKED_UINT_8[value % 0x100])
            elif -0x8000 <= value < 0x8000:
                write(b"\xC9")
                write(PACKED_UINT_16[value % 0x10000])
            elif -0x80000000 <= value < 0x80000000:
                write(b"\xCA")
                write(struct_pack(">i", value))
            elif INT64_MIN <= value < INT64_MAX:
                write(b"\xCB")
                write(struct_pack(">q", value))
            else:
                raise OverflowError("Integer %s out of range" % value)

        # String
        elif isinstance(value, str):
            encoded = value.encode("utf-8")
            self._pack_string_header(len(encoded))
            self._pack_raw(encoded)

        # Bytes
        elif isinstance(value, BYTES_TYPES):
            self._pack_bytes_header(len(value))
            self._pack_raw(value)

        # List
        elif isinstance(value, SEQUENCE_TYPES):
            self._pack_list_header(len(value))
            for item in value:
                self._pack(item, dehydration_hooks)

        # Map
        elif isinstance(value, MAPPING_TYPES):
            self._pack_map_header(len(value.keys()))
            for key, item in value.items():
                if not isinstance(key, str):
                    raise TypeError(
                        "Map keys must be strings, not {}".format(type(key))
                    )
                self._pack(key, dehydration_hooks)
                self._pack(item, dehydration_hooks)

        # Structure
        elif isinstance(value, Structure):
            self.pack_struct(value.tag, value.fields)

        # Other if in dehydration hooks
        else:
            if dehydration_hooks:
                transformer = dehydration_hooks.get_transformer(value)
                if transformer is not None:
                    self._pack(transformer(value), dehydration_hooks)
                    return

            raise ValueError("Values of type %s are not supported" % type(value))

    def _pack_bytes_header(self, size):
        write = self._write
        if size < 0x100:
            write(b"\xCC")
            write(PACKED_UINT_8[size])
        elif size < 0x10000:
            write(b"\xCD")
            write(PACKED_UINT_16[size])
        elif size < 0x100000000:
            write(b"\xCE")
            write(struct_pack(">I", size))
        else:
            raise OverflowError("Bytes header size out of range")

    def _pack_string_header(self, size):
        write = self._write
        if size <= 0x0F:
            write(bytes((0x80 | size,)))
        elif size < 0x100:
            write(b"\xD0")
            write(PACKED_UINT_8[size])
        elif size < 0x10000:
            write(b"\xD1")
            write(PACKED_UINT_16[size])
        elif size < 0x100000000:
            write(b"\xD2")
            write(struct_pack(">I", size))
        else:
            raise OverflowError("String header size out of range")

    def _pack_list_header(self, size):
        write = self._write
        if size <= 0x0F:
            write(bytes((0x90 | size,)))
        elif size < 0x100:
            write(b"\xD4")
            write(PACKED_UINT_8[size])
        elif size < 0x10000:
            write(b"\xD5")
            write(PACKED_UINT_16[size])
        elif size < 0x100000000:
            write(b"\xD6")
            write(struct_pack(">I", size))
        else:
            raise OverflowError("List header size out of range")

    def _pack_map_header(self, size):
        write = self._write
        if size <= 0x0F:
            write(bytes((0xA0 | size,)))
        elif size < 0x100:
            write(b"\xD8")
            write(PACKED_UINT_8[size])
        elif size < 0x10000:
            write(b"\xD9")
            write(PACKED_UINT_16[size])
        elif size < 0x100000000:
            write(b"\xDA")
            write(struct_pack(">I", size))
        else:
            raise OverflowError("Map header size out of range")

    def pack_struct(self, signature, fields, dehydration_hooks=None):
        self._pack_struct(
            signature, fields,
            dehydration_hooks=self._inject_hooks(dehydration_hooks)
        )

    def _pack_struct(self, signature, fields, dehydration_hooks=None):
        if len(signature) != 1 or not isinstance(signature, bytes):
            raise ValueError("Structure signature must be a single byte value")
        write = self._write
        size = len(fields)
        if size <= 0x0F:
            write(bytes((0xB0 | size,)))
        else:
            raise OverflowError("Structure size out of range")
        write(signature)
        for field in fields:
            self._pack(field, dehydration_hooks)

    @staticmethod
    def new_packable_buffer():
        return PackableBuffer()


class PackableBuffer:
    def __init__(self):
        self.data = bytearray()
        # export write method for packer; "inline" for performance
        self.write = self.data.extend
        self.clear = self.data.clear
        self._tmp_buffering = 0

    @contextmanager
    def tmp_buffer(self):
        self._tmp_buffering += 1
        old_len = len(self.data)
        try:
            yield
        except Exception:
            del self.data[old_len:]
            raise
        finally:
            self._tmp_buffering -= 1

    def is_tmp_buffering(self):
        return bool(self._tmp_buffering)


class Unpacker:

    def __init__(self, unpackable):
        self.unpackable = unpackable

    def reset(self):
        self.unpackable.reset()

    def read(self, n=1):
        return self.unpackable.read(n)

    def read_u8(self):
        return self.unpackable.read_u8()

    def unpack(self, hydration_hooks=None):
        value = self._unpack(hydration_hooks=hydration_hooks)
        if hydration_hooks and type(value) in hydration_hooks:
            return hydration_hooks[type(value)](value)
        return value

    def _unpack(self, hydration_hooks=None):
        marker = self.read_u8()

        if marker == -1:
            raise ValueError("Nothing to unpack")

        # Tiny Integer
        if 0x00 <= marker <= 0x7F:
            return marker
        elif 0xF0 <= marker <= 0xFF:
            return marker - 0x100

        # Null
        elif marker == 0xC0:
            return None

        # Float
        elif marker == 0xC1:
            value, = struct_unpack(">d", self.read(8))
            return value

        # Boolean
        elif marker == 0xC2:
            return False
        elif marker == 0xC3:
            return True

        # Integer
        elif marker == 0xC8:
            return struct_unpack(">b", self.read(1))[0]
        elif marker == 0xC9:
            return struct_unpack(">h", self.read(2))[0]
        elif marker == 0xCA:
            return struct_unpack(">i", self.read(4))[0]
        elif marker == 0xCB:
            return struct_unpack(">q", self.read(8))[0]

        # Bytes
        elif marker == 0xCC:
            size, = struct_unpack(">B", self.read(1))
            return self.read(size).tobytes()
        elif marker == 0xCD:
            size, = struct_unpack(">H", self.read(2))
            return self.read(size).tobytes()
        elif marker == 0xCE:
            size, = struct_unpack(">I", self.read(4))
            return self.read(size).tobytes()

        else:
            marker_high = marker & 0xF0
            # String
            if marker_high == 0x80:  # TINY_STRING
                return decode(self.read(marker & 0x0F), "utf-8")
            elif marker == 0xD0:  # STRING_8:
                size, = struct_unpack(">B", self.read(1))
                return decode(self.read(size), "utf-8")
            elif marker == 0xD1:  # STRING_16:
                size, = struct_unpack(">H", self.read(2))
                return decode(self.read(size), "utf-8")
            elif marker == 0xD2:  # STRING_32:
                size, = struct_unpack(">I", self.read(4))
                return decode(self.read(size), "utf-8")

            # List
            elif 0x90 <= marker <= 0x9F or 0xD4 <= marker <= 0xD6:
                return list(self._unpack_list_items(
                    marker, hydration_hooks=hydration_hooks)
                )

            # Map
            elif 0xA0 <= marker <= 0xAF or 0xD8 <= marker <= 0xDA:
                return self._unpack_map(
                    marker, hydration_hooks=hydration_hooks
                )

            # Structure
            elif 0xB0 <= marker <= 0xBF:
                size, tag = self._unpack_structure_header(marker)
                value = Structure(tag, *([None] * size))
                for i in range(len(value)):
                    value[i] = self.unpack(hydration_hooks=hydration_hooks)
                return value

            else:
                raise ValueError("Unknown PackStream marker %02X" % marker)

    def _unpack_list_items(self, marker, hydration_hooks=None):
        marker_high = marker & 0xF0
        if marker_high == 0x90:
            size = marker & 0x0F
            if size == 0:
                return
            elif size == 1:
                yield self.unpack(hydration_hooks=hydration_hooks)
            else:
                for _ in range(size):
                    yield self.unpack(hydration_hooks=hydration_hooks)
        elif marker == 0xD4:  # LIST_8:
            size, = struct_unpack(">B", self.read(1))
            for _ in range(size):
                yield self.unpack(hydration_hooks=hydration_hooks)
        elif marker == 0xD5:  # LIST_16:
            size, = struct_unpack(">H", self.read(2))
            for _ in range(size):
                yield self.unpack(hydration_hooks=hydration_hooks)
        elif marker == 0xD6:  # LIST_32:
            size, = struct_unpack(">I", self.read(4))
            for _ in range(size):
                yield self.unpack(hydration_hooks=hydration_hooks)
        else:
            return

    def unpack_map(self, hydration_hooks=None):
        marker = self.read_u8()
        return self._unpack_map(marker, hydration_hooks=hydration_hooks)

    def _unpack_map(self, marker, hydration_hooks=None):
        marker_high = marker & 0xF0
        if marker_high == 0xA0:
            size = marker & 0x0F
            value = {}
            for _ in range(size):
                key = self.unpack(hydration_hooks=hydration_hooks)
                value[key] = self.unpack(hydration_hooks=hydration_hooks)
            return value
        elif marker == 0xD8:  # MAP_8:
            size, = struct_unpack(">B", self.read(1))
            value = {}
            for _ in range(size):
                key = self.unpack(hydration_hooks=hydration_hooks)
                value[key] = self.unpack(hydration_hooks=hydration_hooks)
            return value
        elif marker == 0xD9:  # MAP_16:
            size, = struct_unpack(">H", self.read(2))
            value = {}
            for _ in range(size):
                key = self.unpack(hydration_hooks=hydration_hooks)
                value[key] = self.unpack(hydration_hooks=hydration_hooks)
            return value
        elif marker == 0xDA:  # MAP_32:
            size, = struct_unpack(">I", self.read(4))
            value = {}
            for _ in range(size):
                key = self.unpack(hydration_hooks=hydration_hooks)
                value[key] = self.unpack(hydration_hooks=hydration_hooks)
            return value
        else:
            return None

    def unpack_structure_header(self):
        marker = self.read_u8()
        if marker == -1:
            return None, None
        else:
            return self._unpack_structure_header(marker)

    def _unpack_structure_header(self, marker):
        marker_high = marker & 0xF0
        if marker_high == 0xB0:  # TINY_STRUCT
            signature = self.read(1).tobytes()
            return marker & 0x0F, signature
        else:
            raise ValueError("Expected structure, found marker %02X" % marker)

    @staticmethod
    def new_unpackable_buffer():
        return UnpackableBuffer()


class UnpackableBuffer:

    initial_capacity = 8192

    def __init__(self, data=None):
        if data is None:
            self.data = bytearray(self.initial_capacity)
            self.used = 0
        else:
            self.data = bytearray(data)
            self.used = len(self.data)
        self.p = 0

    def reset(self):
        self.used = 0
        self.p = 0

    def read(self, n=1):
        view = memoryview(self.data)
        q = self.p + n
        subview = view[self.p:q]
        self.p = q
        return subview

    def read_u8(self):
        if self.used - self.p >= 1:
            value = self.data[self.p]
            self.p += 1
            return value
        else:
            return -1

    def pop_u16(self):
        """ Remove the last two bytes of data, returning them as a big-endian
        16-bit unsigned integer.
        """
        if self.used >= 2:
            value = 0x100 * self.data[self.used - 2] + self.data[self.used - 1]
            self.used -= 2
            return value
        else:
            return -1

if __name__ == "__main__":
    isT=True
    try:
        unpack1 = UnpackableBuffer()
        res1=unpack1.pop_u16()
        unpack2 = UnpackableBuffer([1, 2, 3, 4, 5, 6, 2, 1])
        res2=unpack2.pop_u16()
        res3 = unpack2.pop_u16()
        res4 = unpack2.pop_u16()
        if res1!=-1 or res2!=513 or res3!=1286 or res4!=772:
            isT=False
    except:
        isT=False
    # for l in os.listdir("/home/travis/builds/repos/neo4j---neo4j-python-driver/data_passk_platform/62e6087bd76274f8a4026bfa/"):
    #     f = open("/home/travis/builds/repos/neo4j---neo4j-python-driver/data_passk_platform/62e6087bd76274f8a4026bfa/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     object_class=dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class=UnpackableBuffer()
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.pop_u16()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_async/io/_bolt3_discard_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from __future__ import annotations
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
import typing as t
from enum import Enum
from logging import getLogger
from ssl import SSLSocket

from src.neo4j._exceptions import BoltProtocolError
from src.neo4j.api import (
    READ_ACCESS,
    Version,
)
from src.neo4j.exceptions import (
    ConfigurationError,
    DatabaseUnavailable,
    ForbiddenOnReadOnlyDatabase,
    Neo4jError,
    NotALeader,
    ServiceUnavailable,
)
from src.neo4j._async.io._bolt import (
    AsyncBolt,
    ClientStateManagerBase,
    ServerStateManagerBase,
    tx_timeout_as_ms,
)
class AsyncFakeSocket:

    def __init__(self, address, unpacker_cls=None):
        self.address = address
        self.captured = b""
        self.messages = None
        if unpacker_cls is not None:
            self.messages = AsyncInbox(
                self, on_error=print, unpacker_cls=unpacker_cls
            )

    def getsockname(self):
        return "127.0.0.1", 0xFFFF

    def getpeername(self):
        return self.address

    async def recv_into(self, buffer, nbytes):
        data = self.captured[:nbytes]
        actual = len(data)
        buffer[:actual] = data
        self.captured = self.captured[actual:]
        return actual

    async def sendall(self, data):
        self.captured += data

    def close(self):
        return

    async def pop_message(self):
        assert self.messages
        return await self.messages.pop(None)
from src.neo4j._async.io._common import (
    check_supported_server_product,
    CommitResponse,
    InitResponse,
    Response, AsyncInbox,
)


log = getLogger("neo4j")


class BoltStates(Enum):
    CONNECTED = "CONNECTED"
    READY = "READY"
    STREAMING = "STREAMING"
    TX_READY_OR_TX_STREAMING = "TX_READY||TX_STREAMING"
    FAILED = "FAILED"


class ServerStateManager(ServerStateManagerBase):
    _STATE_TRANSITIONS: t.Dict[Enum, t.Dict[str, Enum]] = {
        BoltStates.CONNECTED: {
            "hello": BoltStates.READY,
        },
        BoltStates.READY: {
            "run": BoltStates.STREAMING,
            "begin": BoltStates.TX_READY_OR_TX_STREAMING,
        },
        BoltStates.STREAMING: {
            "pull": BoltStates.READY,
            "discard": BoltStates.READY,
            "reset": BoltStates.READY,
        },
        BoltStates.TX_READY_OR_TX_STREAMING: {
            "commit": BoltStates.READY,
            "rollback": BoltStates.READY,
            "reset": BoltStates.READY,
        },
        BoltStates.FAILED: {
            "reset": BoltStates.READY,
        }
    }

    def __init__(self, init_state, on_change=None):
        self.state = init_state
        self._on_change = on_change

    def transition(self, message, metadata):
        if metadata.get("has_more"):
            return
        state_before = self.state
        self.state = self._STATE_TRANSITIONS\
            .get(self.state, {})\
            .get(message, self.state)
        if state_before != self.state and callable(self._on_change):
            self._on_change(state_before, self.state)

    def failed(self):
        return self.state == BoltStates.FAILED


class ClientStateManager(ClientStateManagerBase):
    _STATE_TRANSITIONS: t.Dict[Enum, t.Dict[str, Enum]] = {
        BoltStates.CONNECTED: {
            "hello": BoltStates.READY,
        },
        BoltStates.READY: {
            "run": BoltStates.STREAMING,
            "begin": BoltStates.TX_READY_OR_TX_STREAMING,
        },
        BoltStates.STREAMING: {
            "begin": BoltStates.TX_READY_OR_TX_STREAMING,
            "reset": BoltStates.READY,
        },
        BoltStates.TX_READY_OR_TX_STREAMING: {
            "commit": BoltStates.READY,
            "rollback": BoltStates.READY,
            "reset": BoltStates.READY,
        },
    }

    def __init__(self, init_state, on_change=None):
        self.state = init_state
        self._on_change = on_change

    def transition(self, message):
        state_before = self.state
        self.state = self._STATE_TRANSITIONS \
            .get(self.state, {}) \
            .get(message, self.state)
        if state_before != self.state and callable(self._on_change):
            self._on_change(state_before, self.state)


class AsyncBolt3(AsyncBolt):
    """ Protocol handler for Bolt 3.

    This is supported by Neo4j versions 3.5, 4.0, 4.1, 4.2, 4.3, and 4.4.
    """

    PROTOCOL_VERSION = Version(3, 0)

    supports_multiple_results = False

    supports_multiple_databases = False

    supports_re_auth = False

    supports_notification_filtering = False

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._server_state_manager = ServerStateManager(
            BoltStates.CONNECTED, on_change=self._on_server_state_change
        )
        self._client_state_manager = ClientStateManager(
            BoltStates.CONNECTED, on_change=self._on_client_state_change
        )

    def _on_server_state_change(self, old_state, new_state):
        log.debug("[#%04X]  _: <CONNECTION> server state: %s > %s",
                  self.local_port, old_state.name, new_state.name)

    def _get_server_state_manager(self) -> ServerStateManagerBase:
        return self._server_state_manager

    def _on_client_state_change(self, old_state, new_state):
        log.debug("[#%04X]  _: <CONNECTION> client state: %s > %s",
                  self.local_port, old_state.name, new_state.name)

    def _get_client_state_manager(self) -> ClientStateManagerBase:
        return self._client_state_manager

    @property
    def is_reset(self):
        # We can't be sure of the server's state if there are still pending
        # responses. Unless the last message we sent was RESET. In that case
        # the server state will always be READY when we're done.
        if self.responses:
            return self.responses[-1] and self.responses[-1].message == "reset"
        return self._server_state_manager.state == BoltStates.READY

    @property
    def encrypted(self):
        return isinstance(self.socket, SSLSocket)

    @property
    def der_encoded_server_certificate(self):
        return self.socket.getpeercert(binary_form=True)

    def get_base_headers(self):
        return {
            "user_agent": self.user_agent,
        }

    async def hello(self, dehydration_hooks=None, hydration_hooks=None):
        if (
            self.notifications_min_severity is not None
            or self.notifications_disabled_categories is not None
        ):
            self.assert_notification_filtering_support()
        headers = self.get_base_headers()
        headers.update(self.auth_dict)
        logged_headers = dict(headers)
        if "credentials" in logged_headers:
            logged_headers["credentials"] = "*******"
        log.debug("[#%04X]  C: HELLO %r", self.local_port, logged_headers)
        self._append(b"\x01", (headers,),
                     response=InitResponse(self, "hello", hydration_hooks,
                                           on_success=self.server_info.update),
                     dehydration_hooks=dehydration_hooks)
        await self.send_all()
        await self.fetch_all()
        check_supported_server_product(self.server_info.agent)

    def logon(self, dehydration_hooks=None, hydration_hooks=None):
        """Append a LOGON message to the outgoing queue."""
        self.assert_re_auth_support()

    def logoff(self, dehydration_hooks=None, hydration_hooks=None):
        """Append a LOGOFF message to the outgoing queue."""
        self.assert_re_auth_support()

    async def route(
        self, database=None, imp_user=None, bookmarks=None,
        dehydration_hooks=None, hydration_hooks=None
    ):
        if database is not None:
            raise ConfigurationError(
                "Database name parameter for selecting database is not "
                "supported in Bolt Protocol {!r}. Database name {!r}. "
                "Server Agent {!r}".format(
                    self.PROTOCOL_VERSION, database, self.server_info.agent
                )
            )
        if imp_user is not None:
            raise ConfigurationError(
                "Impersonation is not supported in Bolt Protocol {!r}. "
                "Trying to impersonate {!r}.".format(
                    self.PROTOCOL_VERSION, imp_user
                )
            )

        metadata = {}
        records = []

        # Ignoring database and bookmarks because there is no multi-db support.
        # The bookmarks are only relevant for making sure a previously created
        # db exists before querying a routing table for it.
        self.run(
            "CALL dbms.cluster.routing.getRoutingTable($context)",  # This is an internal procedure call. Only available if the Neo4j 3.5 is setup with clustering.
            {"context": self.routing_context},
            mode="r",                                               # Bolt Protocol Version(3, 0) supports mode="r"
            dehydration_hooks=dehydration_hooks,
            hydration_hooks=hydration_hooks,
            on_success=metadata.update
        )
        self.pull(dehydration_hooks=None, hydration_hooks=None,
                  on_success=metadata.update, on_records=records.extend)
        await self.send_all()
        await self.fetch_all()
        routing_info = [dict(zip(metadata.get("fields", ()), values)) for values in records]
        return routing_info

    def run(self, query, parameters=None, mode=None, bookmarks=None,
            metadata=None, timeout=None, db=None, imp_user=None,
            notifications_min_severity=None,
            notifications_disabled_categories=None, dehydration_hooks=None,
            hydration_hooks=None, **handlers):
        if db is not None:
            raise ConfigurationError(
                "Database name parameter for selecting database is not "
                "supported in Bolt Protocol {!r}. Database name {!r}.".format(
                    self.PROTOCOL_VERSION, db
                )
            )
        if imp_user is not None:
            raise ConfigurationError(
                "Impersonation is not supported in Bolt Protocol {!r}. "
                "Trying to impersonate {!r}.".format(
                    self.PROTOCOL_VERSION, imp_user
                )
            )
        if (
            notifications_min_severity is not None
            or notifications_disabled_categories is not None
        ):
            self.assert_notification_filtering_support()
        if not parameters:
            parameters = {}
        extra = {}
        if mode in (READ_ACCESS, "r"):
            extra["mode"] = "r"  # It will default to mode "w" if nothing is specified
        if bookmarks:
            try:
                extra["bookmarks"] = list(bookmarks)
            except TypeError:
                raise TypeError("Bookmarks must be provided within an iterable")
        if metadata:
            try:
                extra["tx_metadata"] = dict(metadata)
            except TypeError:
                raise TypeError("Metadata must be coercible to a dict")
        if timeout is not None:
            extra["tx_timeout"] = tx_timeout_as_ms(timeout)
        fields = (query, parameters, extra)
        log.debug("[#%04X]  C: RUN %s", self.local_port, " ".join(map(repr, fields)))
        self._append(b"\x10", fields,
                     Response(self, "run", hydration_hooks, **handlers),
                     dehydration_hooks=dehydration_hooks)

    def discard(self, n=-1, qid=-1, dehydration_hooks=None,
                hydration_hooks=None, **handlers):
        # Just ignore n and qid, it is not supported in the Bolt 3 Protocol.
        log.debug("[#%04X]  C: DISCARD_ALL", self.local_port)
        self._append(b"\x2F", (),
                     Response(self, "discard", hydration_hooks, **handlers),
                     dehydration_hooks=dehydration_hooks)

    def pull(self, n=-1, qid=-1, dehydration_hooks=None, hydration_hooks=None,
             **handlers):
        # Just ignore n and qid, it is not supported in the Bolt 3 Protocol.
        log.debug("[#%04X]  C: PULL_ALL", self.local_port)
        self._append(b"\x3F", (),
                     Response(self, "pull", hydration_hooks, **handlers),
                     dehydration_hooks=dehydration_hooks)

    def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,
              db=None, imp_user=None, notifications_min_severity=None,
              notifications_disabled_categories=None, dehydration_hooks=None,
              hydration_hooks=None, **handlers):
        if db is not None:
            raise ConfigurationError(
                "Database name parameter for selecting database is not "
                "supported in Bolt Protocol {!r}. Database name {!r}.".format(
                    self.PROTOCOL_VERSION, db
                )
            )
        if imp_user is not None:
            raise ConfigurationError(
                "Impersonation is not supported in Bolt Protocol {!r}. "
                "Trying to impersonate {!r}.".format(
                    self.PROTOCOL_VERSION, imp_user
                )
            )
        if (
            notifications_min_severity is not None
            or notifications_disabled_categories is not None
        ):
            self.assert_notification_filtering_support()
        extra = {}
        if mode in (READ_ACCESS, "r"):
            extra["mode"] = "r"  # It will default to mode "w" if nothing is specified
        if bookmarks:
            try:
                extra["bookmarks"] = list(bookmarks)
            except TypeError:
                raise TypeError("Bookmarks must be provided within an iterable")
        if metadata:
            try:
                extra["tx_metadata"] = dict(metadata)
            except TypeError:
                raise TypeError("Metadata must be coercible to a dict")
        if timeout is not None:
            extra["tx_timeout"] = tx_timeout_as_ms(timeout)
        log.debug("[#%04X]  C: BEGIN %r", self.local_port, extra)
        self._append(b"\x11", (extra,),
                     Response(self, "begin", hydration_hooks, **handlers),
                     dehydration_hooks=dehydration_hooks)

    def commit(self, dehydration_hooks=None, hydration_hooks=None, **handlers):
        log.debug("[#%04X]  C: COMMIT", self.local_port)
        self._append(b"\x12", (),
                     CommitResponse(self, "commit", hydration_hooks,
                                    **handlers),
                     dehydration_hooks=dehydration_hooks)

    def rollback(self, dehydration_hooks=None, hydration_hooks=None,
                 **handlers):
        log.debug("[#%04X]  C: ROLLBACK", self.local_port)
        self._append(b"\x13", (),
                     Response(self, "rollback", hydration_hooks, **handlers),
                     dehydration_hooks=dehydration_hooks)

    async def reset(self, dehydration_hooks=None, hydration_hooks=None):
        """ Add a RESET message to the outgoing queue, send
        it and consume all remaining messages.
        """

        def fail(metadata):
            raise BoltProtocolError("RESET failed %r" % metadata, address=self.unresolved_address)

        log.debug("[#%04X]  C: RESET", self.local_port)
        self._append(b"\x0F",
                     response=Response(self, "reset", hydration_hooks,
                                       on_failure=fail),
                     dehydration_hooks=dehydration_hooks)
        await self.send_all()
        await self.fetch_all()

    def goodbye(self, dehydration_hooks=None, hydration_hooks=None):
        log.debug("[#%04X]  C: GOODBYE", self.local_port)
        self._append(b"\x02", (), dehydration_hooks=dehydration_hooks)

    async def _process_message(self, tag, fields):
        """ Process at most one message from the server, if available.

        :returns: 2-tuple of number of detail messages and number of summary
                 messages fetched
        """
        details = []
        summary_signature = summary_metadata = None
        if tag == b"\x71":  # RECORD
            details = fields
        elif fields:
            summary_signature = tag
            summary_metadata = fields[0]
        else:
            summary_signature = tag

        if details:
            log.debug("[#%04X]  S: RECORD * %d", self.local_port, len(details))  # Do not log any data
            await self.responses[0].on_records(details)

        if summary_signature is None:
            return len(details), 0

        response = self.responses.popleft()
        response.complete = True
        if summary_signature == b"\x70":
            log.debug("[#%04X]  S: SUCCESS %r", self.local_port, summary_metadata)
            self._server_state_manager.transition(response.message,
                                                  summary_metadata)
            await response.on_success(summary_metadata or {})
        elif summary_signature == b"\x7E":
            log.debug("[#%04X]  S: IGNORED", self.local_port)
            await response.on_ignored(summary_metadata or {})
        elif summary_signature == b"\x7F":
            log.debug("[#%04X]  S: FAILURE %r", self.local_port, summary_metadata)
            self._server_state_manager.state = BoltStates.FAILED
            try:
                await response.on_failure(summary_metadata or {})
            except (ServiceUnavailable, DatabaseUnavailable):
                if self.pool:
                    await self.pool.deactivate(address=self.unresolved_address)
                raise
            except (NotALeader, ForbiddenOnReadOnlyDatabase):
                if self.pool:
                    await self.pool.on_write_failure(
                        address=self.unresolved_address,
                        database=self.last_database,
                    )
                raise
            except Neo4jError as e:
                await self.pool.on_neo4j_error(e, self)
                raise
        else:
            raise BoltProtocolError("Unexpected response message with signature %02X" % summary_signature, address=self.unresolved_address)

        return len(details), 1

class add():
    host=""
    port=0
    def __int__(self):
        self.host="127.0.0.1"
        self.port = 7687
if __name__ == "__main__":
    isT=True
    address = ("127.0.0.1", 7687)
    address_dict = add()
    max_connection_lifetime = 0
    temp_class = AsyncBolt3(address_dict, AsyncFakeSocket(address),
                            max_connection_lifetime)
    try:
        temp_class.discard()
        if temp_class.responses.popleft().message != "discard":
            isT=False
    except:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_async/io/_bolt3_begin_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from __future__ import annotations
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
import typing as t
from enum import Enum
from logging import getLogger
from ssl import SSLSocket

from src.neo4j._exceptions import BoltProtocolError
from src.neo4j.api import (
    READ_ACCESS,
    Version,
)
from src.neo4j.exceptions import (
    ConfigurationError,
    DatabaseUnavailable,
    ForbiddenOnReadOnlyDatabase,
    Neo4jError,
    NotALeader,
    ServiceUnavailable,
)
from src.neo4j._async.io._bolt import (
    AsyncBolt,
    ClientStateManagerBase,
    ServerStateManagerBase,
    tx_timeout_as_ms,
)
class AsyncFakeSocket:

    def __init__(self, address, unpacker_cls=None):
        self.address = address
        self.captured = b""
        self.messages = None
        if unpacker_cls is not None:
            self.messages = AsyncInbox(
                self, on_error=print, unpacker_cls=unpacker_cls
            )

    def getsockname(self):
        return "127.0.0.1", 0xFFFF

    def getpeername(self):
        return self.address

    async def recv_into(self, buffer, nbytes):
        data = self.captured[:nbytes]
        actual = len(data)
        buffer[:actual] = data
        self.captured = self.captured[actual:]
        return actual

    async def sendall(self, data):
        self.captured += data

    def close(self):
        return

    async def pop_message(self):
        assert self.messages
        return await self.messages.pop(None)
from src.neo4j._async.io._common import (
    check_supported_server_product,
    CommitResponse,
    InitResponse,
    Response, AsyncInbox,
)


log = getLogger("neo4j")


class BoltStates(Enum):
    CONNECTED = "CONNECTED"
    READY = "READY"
    STREAMING = "STREAMING"
    TX_READY_OR_TX_STREAMING = "TX_READY||TX_STREAMING"
    FAILED = "FAILED"


class ServerStateManager(ServerStateManagerBase):
    _STATE_TRANSITIONS: t.Dict[Enum, t.Dict[str, Enum]] = {
        BoltStates.CONNECTED: {
            "hello": BoltStates.READY,
        },
        BoltStates.READY: {
            "run": BoltStates.STREAMING,
            "begin": BoltStates.TX_READY_OR_TX_STREAMING,
        },
        BoltStates.STREAMING: {
            "pull": BoltStates.READY,
            "discard": BoltStates.READY,
            "reset": BoltStates.READY,
        },
        BoltStates.TX_READY_OR_TX_STREAMING: {
            "commit": BoltStates.READY,
            "rollback": BoltStates.READY,
            "reset": BoltStates.READY,
        },
        BoltStates.FAILED: {
            "reset": BoltStates.READY,
        }
    }

    def __init__(self, init_state, on_change=None):
        self.state = init_state
        self._on_change = on_change

    def transition(self, message, metadata):
        if metadata.get("has_more"):
            return
        state_before = self.state
        self.state = self._STATE_TRANSITIONS\
            .get(self.state, {})\
            .get(message, self.state)
        if state_before != self.state and callable(self._on_change):
            self._on_change(state_before, self.state)

    def failed(self):
        return self.state == BoltStates.FAILED


class ClientStateManager(ClientStateManagerBase):
    _STATE_TRANSITIONS: t.Dict[Enum, t.Dict[str, Enum]] = {
        BoltStates.CONNECTED: {
            "hello": BoltStates.READY,
        },
        BoltStates.READY: {
            "run": BoltStates.STREAMING,
            "begin": BoltStates.TX_READY_OR_TX_STREAMING,
        },
        BoltStates.STREAMING: {
            "begin": BoltStates.TX_READY_OR_TX_STREAMING,
            "reset": BoltStates.READY,
        },
        BoltStates.TX_READY_OR_TX_STREAMING: {
            "commit": BoltStates.READY,
            "rollback": BoltStates.READY,
            "reset": BoltStates.READY,
        },
    }

    def __init__(self, init_state, on_change=None):
        self.state = init_state
        self._on_change = on_change

    def transition(self, message):
        state_before = self.state
        self.state = self._STATE_TRANSITIONS \
            .get(self.state, {}) \
            .get(message, self.state)
        if state_before != self.state and callable(self._on_change):
            self._on_change(state_before, self.state)


class AsyncBolt3(AsyncBolt):
    """ Protocol handler for Bolt 3.

    This is supported by Neo4j versions 3.5, 4.0, 4.1, 4.2, 4.3, and 4.4.
    """

    PROTOCOL_VERSION = Version(3, 0)

    supports_multiple_results = False

    supports_multiple_databases = False

    supports_re_auth = False

    supports_notification_filtering = False

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._server_state_manager = ServerStateManager(
            BoltStates.CONNECTED, on_change=self._on_server_state_change
        )
        self._client_state_manager = ClientStateManager(
            BoltStates.CONNECTED, on_change=self._on_client_state_change
        )

    def _on_server_state_change(self, old_state, new_state):
        log.debug("[#%04X]  _: <CONNECTION> server state: %s > %s",
                  self.local_port, old_state.name, new_state.name)

    def _get_server_state_manager(self) -> ServerStateManagerBase:
        return self._server_state_manager

    def _on_client_state_change(self, old_state, new_state):
        log.debug("[#%04X]  _: <CONNECTION> client state: %s > %s",
                  self.local_port, old_state.name, new_state.name)

    def _get_client_state_manager(self) -> ClientStateManagerBase:
        return self._client_state_manager

    @property
    def is_reset(self):
        # We can't be sure of the server's state if there are still pending
        # responses. Unless the last message we sent was RESET. In that case
        # the server state will always be READY when we're done.
        if self.responses:
            return self.responses[-1] and self.responses[-1].message == "reset"
        return self._server_state_manager.state == BoltStates.READY

    @property
    def encrypted(self):
        return isinstance(self.socket, SSLSocket)

    @property
    def der_encoded_server_certificate(self):
        return self.socket.getpeercert(binary_form=True)

    def get_base_headers(self):
        return {
            "user_agent": self.user_agent,
        }

    async def hello(self, dehydration_hooks=None, hydration_hooks=None):
        if (
            self.notifications_min_severity is not None
            or self.notifications_disabled_categories is not None
        ):
            self.assert_notification_filtering_support()
        headers = self.get_base_headers()
        headers.update(self.auth_dict)
        logged_headers = dict(headers)
        if "credentials" in logged_headers:
            logged_headers["credentials"] = "*******"
        log.debug("[#%04X]  C: HELLO %r", self.local_port, logged_headers)
        self._append(b"\x01", (headers,),
                     response=InitResponse(self, "hello", hydration_hooks,
                                           on_success=self.server_info.update),
                     dehydration_hooks=dehydration_hooks)
        await self.send_all()
        await self.fetch_all()
        check_supported_server_product(self.server_info.agent)

    def logon(self, dehydration_hooks=None, hydration_hooks=None):
        """Append a LOGON message to the outgoing queue."""
        self.assert_re_auth_support()

    def logoff(self, dehydration_hooks=None, hydration_hooks=None):
        """Append a LOGOFF message to the outgoing queue."""
        self.assert_re_auth_support()

    async def route(
        self, database=None, imp_user=None, bookmarks=None,
        dehydration_hooks=None, hydration_hooks=None
    ):
        if database is not None:
            raise ConfigurationError(
                "Database name parameter for selecting database is not "
                "supported in Bolt Protocol {!r}. Database name {!r}. "
                "Server Agent {!r}".format(
                    self.PROTOCOL_VERSION, database, self.server_info.agent
                )
            )
        if imp_user is not None:
            raise ConfigurationError(
                "Impersonation is not supported in Bolt Protocol {!r}. "
                "Trying to impersonate {!r}.".format(
                    self.PROTOCOL_VERSION, imp_user
                )
            )

        metadata = {}
        records = []

        # Ignoring database and bookmarks because there is no multi-db support.
        # The bookmarks are only relevant for making sure a previously created
        # db exists before querying a routing table for it.
        self.run(
            "CALL dbms.cluster.routing.getRoutingTable($context)",  # This is an internal procedure call. Only available if the Neo4j 3.5 is setup with clustering.
            {"context": self.routing_context},
            mode="r",                                               # Bolt Protocol Version(3, 0) supports mode="r"
            dehydration_hooks=dehydration_hooks,
            hydration_hooks=hydration_hooks,
            on_success=metadata.update
        )
        self.pull(dehydration_hooks=None, hydration_hooks=None,
                  on_success=metadata.update, on_records=records.extend)
        await self.send_all()
        await self.fetch_all()
        routing_info = [dict(zip(metadata.get("fields", ()), values)) for values in records]
        return routing_info

    def run(self, query, parameters=None, mode=None, bookmarks=None,
            metadata=None, timeout=None, db=None, imp_user=None,
            notifications_min_severity=None,
            notifications_disabled_categories=None, dehydration_hooks=None,
            hydration_hooks=None, **handlers):
        if db is not None:
            raise ConfigurationError(
                "Database name parameter for selecting database is not "
                "supported in Bolt Protocol {!r}. Database name {!r}.".format(
                    self.PROTOCOL_VERSION, db
                )
            )
        if imp_user is not None:
            raise ConfigurationError(
                "Impersonation is not supported in Bolt Protocol {!r}. "
                "Trying to impersonate {!r}.".format(
                    self.PROTOCOL_VERSION, imp_user
                )
            )
        if (
            notifications_min_severity is not None
            or notifications_disabled_categories is not None
        ):
            self.assert_notification_filtering_support()
        if not parameters:
            parameters = {}
        extra = {}
        if mode in (READ_ACCESS, "r"):
            extra["mode"] = "r"  # It will default to mode "w" if nothing is specified
        if bookmarks:
            try:
                extra["bookmarks"] = list(bookmarks)
            except TypeError:
                raise TypeError("Bookmarks must be provided within an iterable")
        if metadata:
            try:
                extra["tx_metadata"] = dict(metadata)
            except TypeError:
                raise TypeError("Metadata must be coercible to a dict")
        if timeout is not None:
            extra["tx_timeout"] = tx_timeout_as_ms(timeout)
        fields = (query, parameters, extra)
        log.debug("[#%04X]  C: RUN %s", self.local_port, " ".join(map(repr, fields)))
        self._append(b"\x10", fields,
                     Response(self, "run", hydration_hooks, **handlers),
                     dehydration_hooks=dehydration_hooks)

    def discard(self, n=-1, qid=-1, dehydration_hooks=None,
                hydration_hooks=None, **handlers):
        # Just ignore n and qid, it is not supported in the Bolt 3 Protocol.
        log.debug("[#%04X]  C: DISCARD_ALL", self.local_port)
        self._append(b"\x2F", (),
                     Response(self, "discard", hydration_hooks, **handlers),
                     dehydration_hooks=dehydration_hooks)

    def pull(self, n=-1, qid=-1, dehydration_hooks=None, hydration_hooks=None,
             **handlers):
        # Just ignore n and qid, it is not supported in the Bolt 3 Protocol.
        log.debug("[#%04X]  C: PULL_ALL", self.local_port)
        self._append(b"\x3F", (),
                     Response(self, "pull", hydration_hooks, **handlers),
                     dehydration_hooks=dehydration_hooks)

    def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,
              db=None, imp_user=None, notifications_min_severity=None,
              notifications_disabled_categories=None, dehydration_hooks=None,
              hydration_hooks=None, **handlers):
        if db is not None:
            raise ConfigurationError(
                "Database name parameter for selecting database is not "
                "supported in Bolt Protocol {!r}. Database name {!r}.".format(
                    self.PROTOCOL_VERSION, db
                )
            )
        if imp_user is not None:
            raise ConfigurationError(
                "Impersonation is not supported in Bolt Protocol {!r}. "
                "Trying to impersonate {!r}.".format(
                    self.PROTOCOL_VERSION, imp_user
                )
            )
        if (
            notifications_min_severity is not None
            or notifications_disabled_categories is not None
        ):
            self.assert_notification_filtering_support()
        extra = {}
        if mode in (READ_ACCESS, "r"):
            extra["mode"] = "r"  # It will default to mode "w" if nothing is specified
        if bookmarks:
            try:
                extra["bookmarks"] = list(bookmarks)
            except TypeError:
                raise TypeError("Bookmarks must be provided within an iterable")
        if metadata:
            try:
                extra["tx_metadata"] = dict(metadata)
            except TypeError:
                raise TypeError("Metadata must be coercible to a dict")
        if timeout is not None:
            extra["tx_timeout"] = tx_timeout_as_ms(timeout)
        log.debug("[#%04X]  C: BEGIN %r", self.local_port, extra)
        self._append(b"\x11", (extra,),
                     Response(self, "begin", hydration_hooks, **handlers),
                     dehydration_hooks=dehydration_hooks)

    def commit(self, dehydration_hooks=None, hydration_hooks=None, **handlers):
        log.debug("[#%04X]  C: COMMIT", self.local_port)
        self._append(b"\x12", (),
                     CommitResponse(self, "commit", hydration_hooks,
                                    **handlers),
                     dehydration_hooks=dehydration_hooks)

    def rollback(self, dehydration_hooks=None, hydration_hooks=None,
                 **handlers):
        log.debug("[#%04X]  C: ROLLBACK", self.local_port)
        self._append(b"\x13", (),
                     Response(self, "rollback", hydration_hooks, **handlers),
                     dehydration_hooks=dehydration_hooks)

    async def reset(self, dehydration_hooks=None, hydration_hooks=None):
        """ Add a RESET message to the outgoing queue, send
        it and consume all remaining messages.
        """

        def fail(metadata):
            raise BoltProtocolError("RESET failed %r" % metadata, address=self.unresolved_address)

        log.debug("[#%04X]  C: RESET", self.local_port)
        self._append(b"\x0F",
                     response=Response(self, "reset", hydration_hooks,
                                       on_failure=fail),
                     dehydration_hooks=dehydration_hooks)
        await self.send_all()
        await self.fetch_all()

    def goodbye(self, dehydration_hooks=None, hydration_hooks=None):
        log.debug("[#%04X]  C: GOODBYE", self.local_port)
        self._append(b"\x02", (), dehydration_hooks=dehydration_hooks)

    async def _process_message(self, tag, fields):
        """ Process at most one message from the server, if available.

        :returns: 2-tuple of number of detail messages and number of summary
                 messages fetched
        """
        details = []
        summary_signature = summary_metadata = None
        if tag == b"\x71":  # RECORD
            details = fields
        elif fields:
            summary_signature = tag
            summary_metadata = fields[0]
        else:
            summary_signature = tag

        if details:
            log.debug("[#%04X]  S: RECORD * %d", self.local_port, len(details))  # Do not log any data
            await self.responses[0].on_records(details)

        if summary_signature is None:
            return len(details), 0

        response = self.responses.popleft()
        response.complete = True
        if summary_signature == b"\x70":
            log.debug("[#%04X]  S: SUCCESS %r", self.local_port, summary_metadata)
            self._server_state_manager.transition(response.message,
                                                  summary_metadata)
            await response.on_success(summary_metadata or {})
        elif summary_signature == b"\x7E":
            log.debug("[#%04X]  S: IGNORED", self.local_port)
            await response.on_ignored(summary_metadata or {})
        elif summary_signature == b"\x7F":
            log.debug("[#%04X]  S: FAILURE %r", self.local_port, summary_metadata)
            self._server_state_manager.state = BoltStates.FAILED
            try:
                await response.on_failure(summary_metadata or {})
            except (ServiceUnavailable, DatabaseUnavailable):
                if self.pool:
                    await self.pool.deactivate(address=self.unresolved_address)
                raise
            except (NotALeader, ForbiddenOnReadOnlyDatabase):
                if self.pool:
                    await self.pool.on_write_failure(
                        address=self.unresolved_address,
                        database=self.last_database,
                    )
                raise
            except Neo4jError as e:
                await self.pool.on_neo4j_error(e, self)
                raise
        else:
            raise BoltProtocolError("Unexpected response message with signature %02X" % summary_signature, address=self.unresolved_address)

        return len(details), 1
class add():
    host=""
    port=0
    def __int__(self):
        self.host="127.0.0.1"
        self.port = 7687
if __name__ == "__main__":
    isT=True
    address = ("127.0.0.1", 7687)
    address_dict = add()
    max_connection_lifetime = 0
    temp_class = AsyncBolt3(address_dict, AsyncFakeSocket(address),
                            max_connection_lifetime)
    try:
        temp_class.begin()
        if temp_class.responses.popleft().message != "begin":
            isT=False
    except:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")



----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/time/_arithmetic_round_half_to_even_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from typing import (
    Tuple,
    TypeVar,
)


__all__ = [
    "nano_add",
    "nano_div",
    "nano_divmod",
    "symmetric_divmod",
    "round_half_to_even",
]


def nano_add(x, y):
    """

        >>> 0.7 + 0.2
        0.8999999999999999
        >>> -0.7 + 0.2
        -0.49999999999999994
        >>> nano_add(0.7, 0.2)
        0.9
        >>> nano_add(-0.7, 0.2)
        -0.5

    :param x:
    :param y:
    :returns:
    """
    return (int(1000000000 * x) + int(1000000000 * y)) / 1000000000


def nano_div(x, y):
    """

        >>> 0.7 / 0.2
        3.4999999999999996
        >>> -0.7 / 0.2
        -3.4999999999999996
        >>> nano_div(0.7, 0.2)
        3.5
        >>> nano_div(-0.7, 0.2)
        -3.5

    :param x:
    :param y:
    :returns:
    """
    return float(1000000000 * x) / int(1000000000 * y)


def nano_divmod(x, y):
    """

        >>> divmod(0.7, 0.2)
        (3.0, 0.09999999999999992)
        >>> nano_divmod(0.7, 0.2)
        (3, 0.1)

    :param x:
    :param y:
    :returns:
    """
    number = type(x)
    nx = int(1000000000 * x)
    ny = int(1000000000 * y)
    q, r = divmod(nx, ny)
    return int(q), number(r / 1000000000)


_TDividend = TypeVar("_TDividend", int, float)


def symmetric_divmod(
    dividend: _TDividend, divisor: float
) -> Tuple[int, _TDividend]:
    number = type(dividend)
    if dividend >= 0:
        quotient, remainder = divmod(dividend, divisor)
        return int(quotient), number(remainder)
    else:
        quotient, remainder = divmod(-dividend, divisor)
        return -int(quotient), -number(remainder)


def round_half_to_even(n):
    """

        >>> round_half_to_even(3)
        3
        >>> round_half_to_even(3.2)
        3
        >>> round_half_to_even(3.5)
        4
        >>> round_half_to_even(3.7)
        4
        >>> round_half_to_even(4)
        4
        >>> round_half_to_even(4.2)
        4
        >>> round_half_to_even(4.5)
        4
        >>> round_half_to_even(4.7)
        5

    :param n:
    :returns:
    """
    ten_n = 10 * n
    if ten_n == int(ten_n) and ten_n % 10 == 5:
        up = int(n + 0.5)
        down = int(n - 0.5)
        return up if up % 2 == 0 else down
    else:
        return int(round(n))

if __name__ == "__main__":
    isT=True
    try:
        if round_half_to_even(3)!=3 or round_half_to_even(3.2)!=3 or round_half_to_even(3.5)!=4 \
        or round_half_to_even(3.7)!=4\
        or round_half_to_even(4)!=4\
        or round_half_to_even(4.2)!=4\
        or round_half_to_even(4.5)!=4\
        or round_half_to_even(4.7)!=5:
            isT=False
    except:
        isT=False
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver/data_passk_platform1/62e60723d76274f8a4026b75/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver/data_passk_platform1/62e60723d76274f8a4026b75/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     print(args0)
    #     res0 = round_half_to_even(int(args0))
    #     print(res0)
    #
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    # if not isT:
    #     raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_spatial/__init___point_type_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


"""
This module defines spatial data types.
"""

from __future__ import annotations

import typing as t
from threading import Lock


# SRID to subclass mappings
srid_table: t.Dict[int, t.Tuple[t.Type[Point], int]] = {}
srid_table_lock = Lock()


class Point(t.Tuple[float, ...]):
    """Base-class for spatial data.

    A point within a geometric space. This type is generally used via its
    subclasses and should not be instantiated directly unless there is no
    subclass defined for the required SRID.

    :param iterable:
        An iterable of coordinates.
        All items will be converted to :class:`float`.
    :type iterable: Iterable[float]
    """

    #: The SRID (spatial reference identifier) of the spatial data.
    #: A number that identifies the coordinate system the spatial type is to
    #: be interpreted in.
    srid: t.Optional[int]

    if t.TYPE_CHECKING:
        @property
        def x(self) -> float: ...

        @property
        def y(self) -> float: ...

        @property
        def z(self) -> float: ...

    def __new__(cls, iterable: t.Iterable[float]) -> Point:
        # mypy issue https://github.com/python/mypy/issues/14890
        return tuple.__new__(  # type: ignore[type-var]
            cls, map(float, iterable)
        )

    def __repr__(self) -> str:
        return "POINT(%s)" % " ".join(map(str, self))

    def __eq__(self, other: object) -> bool:
        try:
            return (type(self) is type(other)
                    and tuple(self) == tuple(t.cast(Point, other)))
        except (AttributeError, TypeError):
            return False

    def __ne__(self, other: object) -> bool:
        return not self.__eq__(other)

    def __hash__(self):
        return hash(type(self)) ^ hash(tuple(self))


def point_type(
    name: str,
    fields: t.Tuple[str, str, str],
    srid_map: t.Dict[int, int]
) -> t.Type[Point]:
    """ Dynamically create a Point subclass.
    """

    def srid(self):
        try:
            return srid_map[len(self)]
        except KeyError:
            return None

    attributes = {"srid": property(srid)}

    for index, subclass_field in enumerate(fields):

        def accessor(self, i=index, f=subclass_field):
            try:
                return self[i]
            except IndexError:
                raise AttributeError(f)

        for field_alias in {subclass_field, "xyz"[index]}:
            #print(field_alias)
            attributes[field_alias] = property(accessor)
    cls = t.cast(t.Type[Point], type(name, (Point,), attributes))
    #print(attributes)
    with srid_table_lock:
        for dim, srid_ in srid_map.items():
            srid_table[srid_] = (cls, dim)
    #print(srid_table)

    return cls


# Point subclass definitions
if t.TYPE_CHECKING:
    class CartesianPoint(Point):
        ...
else:
    CartesianPoint = point_type("CartesianPoint", ("x", "y", "z"),
                                {2: 7203, 3: 9157})

if t.TYPE_CHECKING:
    class WGS84Point(Point):
        @property
        def longitude(self) -> float: ...

        @property
        def latitude(self) -> float: ...

        @property
        def height(self) -> float: ...
else:
    WGS84Point = point_type("WGS84Point", ("longitude", "latitude", "height"),
                            {2: 4326, 3: 4979})

if __name__ == "__main__":
    isT=True
    from unittest.mock import Mock
    try:
        CartesianPoint = point_type("CartesianPoint", ("x", "y", "z"),
                                   {2: 7203, 3: 9157})
        CartesianPointdict = CartesianPoint.__dict__
        if 'x' not in CartesianPointdict or 'y' not in CartesianPointdict or 'z' not in CartesianPointdict:
            isT=False
        WGS84Point = point_type("WGS84Point", ("longitude", "latitude", "height"),
                                {2: 4326, 3: 4979})
        WGS84Pointdict = WGS84Point.__dict__
        if "longitude" not in WGS84Pointdict or  "latitude" not in WGS84Pointdict or  "height" not in WGS84Pointdict:
            isT=False
        if 7203 not in srid_table or 9157 not in srid_table or 4326 not in srid_table or 4979 not in srid_table:
            isT=False
    except:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/neo4j---neo4j-python-driver/data_passk_platform/62e60707d76274f8a4026b69/"):
    #     f = open("/home/travis/builds/repos/neo4j---neo4j-python-driver/data_passk_platform/62e60707d76274f8a4026b69/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"],bytes):
    #         args2=dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2=content["input"]["args"][2]["bytes"]
    #     res0 = point_type(args0,args1,args2)
    #     # print(type(res0))
    #     # print(res0)
    #     # print(content["output"][0])
    #     # print(str(res0)==str(content["output"][0]))
    #     if not ( str(res0)== str(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_meta_deprecated_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from __future__ import annotations

import asyncio
import platform
import sys
import tracemalloc
import typing as t
from functools import wraps
from inspect import isclass
from warnings import warn


_FuncT = t.TypeVar("_FuncT", bound=t.Callable)


# Can be automatically overridden in builds
package = "neo4j"
version = "5.11.dev0"
deprecated_package = False


def _compute_bolt_agent() -> t.Dict[str, str]:
    def format_version_info(version_info):
        return "{}.{}.{}-{}-{}".format(*version_info)

    return {
        "product": f"neo4j-python/{version}",
        "platform":
            f"{platform.system() or 'Unknown'} "
            f"{platform.release() or 'unknown'}; "
            f"{platform.machine() or 'unknown'}",
        "language": f"Python/{format_version_info(sys.version_info)}",
        "language_details":
            f"{platform.python_implementation()}; "
            f"{format_version_info(sys.implementation.version)} "
            f"({', '.join(platform.python_build())}) "
            f"[{platform.python_compiler()}]"
    }


BOLT_AGENT_DICT = _compute_bolt_agent()


def _compute_user_agent() -> str:
    template = "neo4j-python/{} Python/{}.{}.{}-{}-{} ({})"
    fields = (version,) + tuple(sys.version_info) + (sys.platform,)
    return template.format(*fields)


USER_AGENT = _compute_user_agent()


# TODO: 6.0 - remove this function
def get_user_agent():
    """ Obtain the default user agent string sent to the server after
    a successful handshake.
    """
    return USER_AGENT


def _id(x):
    return x


def copy_signature(_: _FuncT) -> t.Callable[[t.Callable], _FuncT]:
    return _id


def deprecation_warn(message, stack_level=1):
    warn(message, category=DeprecationWarning, stacklevel=stack_level + 1)


def deprecated(message: str) -> t.Callable[[_FuncT], _FuncT]:
    """ Decorator for deprecating functions and methods.

    ::

        @deprecated("'foo' has been deprecated in favour of 'bar'")
        def foo(x):
            pass

    """
    return _make_warning_decorator(message, deprecation_warn)


def deprecated_property(message: str):
    def decorator(f):
        return property(deprecated(message)(f))
    return t.cast(property, decorator)


class ExperimentalWarning(Warning):
    """ Base class for warnings about experimental features.

    .. deprecated:: 5.8
        we now use "preview" instead of "experimental".
    """


def experimental_warn(message, stack_level=1):
    warn(message, category=ExperimentalWarning, stacklevel=stack_level + 1)


def experimental(message) -> t.Callable[[_FuncT], _FuncT]:
    """ Decorator for tagging experimental functions and methods.

    ::

        @experimental("'foo' is an experimental function and may be "
                      "removed in a future release")
        def foo(x):
            pass

    .. deprecated:: 5.8
        we now use "preview" instead of "experimental".
    """
    def decorator(f):
        if asyncio.iscoroutinefunction(f):
            @wraps(f)
            async def inner(*args, **kwargs):
                experimental_warn(message, stack_level=2)
                return await f(*args, **kwargs)

            return inner
        else:
            @wraps(f)
            def inner(*args, **kwargs):
                experimental_warn(message, stack_level=2)
                return f(*args, **kwargs)

            return inner

    return _make_warning_decorator(message, experimental_warn)


class PreviewWarning(Warning):
    """ Base class for warnings about experimental features.
    """


def preview_warn(message, stack_level=1):
    message += (
        " It might be changed without following the deprecation policy. "
        "See also "
        "https://github.com/neo4j/neo4j-python-driver/wiki/preview-features."
    )
    warn(message, category=PreviewWarning, stacklevel=stack_level + 1)


def preview(message) -> t.Callable[[_FuncT], _FuncT]:
    """
    Decorator for tagging preview functions and methods.

        @preview("foo is a preview.")
        def foo(x):
            pass
    """

    return _make_warning_decorator(message, preview_warn)


if t.TYPE_CHECKING:
    class _WarningFunc(t.Protocol):
        def __call__(self, message: str, stack_level: int = 1) -> None:
            ...
else:
    _WarningFunc = object


def _make_warning_decorator(
    message: str,
    warning_func: _WarningFunc,
) -> t.Callable[[_FuncT], _FuncT]:
    def decorator(f):
        if asyncio.iscoroutinefunction(f):
            @wraps(f)
            async def inner(*args, **kwargs):
                warning_func(message, stack_level=2)
                return await f(*args, **kwargs)

            return inner
        if isclass(f):
            if hasattr(f, "__init__"):
                original_init = f.__init__
                @wraps(original_init)
                def inner(*args, **kwargs):
                    warning_func(message, stack_level=2)
                    return original_init(*args, **kwargs)

                f.__init__ = inner
                return f
            raise TypeError(
                "Cannot decorate class without __init__"
            )
        else:
            @wraps(f)
            def inner(*args, **kwargs):
                warning_func(message, stack_level=2)
                return f(*args, **kwargs)

            return inner

    return decorator


def unclosed_resource_warn(obj):
    msg = f"Unclosed {obj!r}."
    trace = tracemalloc.get_object_traceback(obj)
    if trace:
        msg += "\nObject allocated at (most recent call last):\n"
        msg += "\n".join(trace.format())
    else:
        msg += "\nEnable tracemalloc to get the object allocation traceback."
    warn(msg, ResourceWarning, stacklevel=2, source=obj)
#_meta_deprecated_passk_validte.py

if __name__ == "__main__":
    import dill
    import os
    isT=True

    @deprecated("This is a test method")
    def deprecated_test():
        # print("This is a test method")
        pass


    import warnings
    with warnings.catch_warnings(record=True) as w:
        deprecated_test()
        if len(w) > 0:
            if str(w[0].message)!="This is a test method":
                if not isT:
                    raise Exception("Result not True!!!")

# import sys
# sys.path.append("/home/travis/builds/repos/pexip---os-python-cachetools/")


----------------------------
/home/travis/builds/repos/pre-commit---pre-commit/pre_commit/languages/r__inline_r_setup_passk_validte.py
from __future__ import annotations

import contextlib
import os
import shlex
import shutil
import tempfile
import textwrap
from typing import Generator
from typing import Sequence

from pre_commit import lang_base
from pre_commit.envcontext import envcontext
from pre_commit.envcontext import PatchesT
from pre_commit.envcontext import UNSET
from pre_commit.prefix import Prefix
from pre_commit.util import cmd_output_b
from pre_commit.util import win_exe

ENVIRONMENT_DIR = 'renv'
RSCRIPT_OPTS = ('--no-save', '--no-restore', '--no-site-file', '--no-environ')
get_default_version = lang_base.basic_get_default_version
health_check = lang_base.basic_health_check


@contextlib.contextmanager
def _r_code_in_tempfile(code: str) -> Generator[str, None, None]:
    """
    To avoid quoting and escaping issues, avoid `Rscript [options] -e {expr}`
    but use `Rscript [options] path/to/file_with_expr.R`
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        fname = os.path.join(tmpdir, 'script.R')
        with open(fname, 'w') as f:
            f.write(_inline_r_setup(textwrap.dedent(code)))
        yield fname


def get_env_patch(venv: str) -> PatchesT:
    return (
        ('R_PROFILE_USER', os.path.join(venv, 'activate.R')),
        ('RENV_PROJECT', UNSET),
    )


@contextlib.contextmanager
def in_env(prefix: Prefix, version: str) -> Generator[None, None, None]:
    envdir = lang_base.environment_dir(prefix, ENVIRONMENT_DIR, version)
    with envcontext(get_env_patch(envdir)):
        yield


def _prefix_if_file_entry(
        entry: list[str],
        prefix: Prefix,
        *,
        is_local: bool,
) -> Sequence[str]:
    if entry[1] == '-e' or is_local:
        return entry[1:]
    else:
        return (prefix.path(entry[1]),)


def _rscript_exec() -> str:
    r_home = os.environ.get('R_HOME')
    if r_home is None:
        return 'Rscript'
    else:
        return os.path.join(r_home, 'bin', win_exe('Rscript'))


def _entry_validate(entry: list[str]) -> None:
    """
    Allowed entries:
    # Rscript -e expr
    # Rscript path/to/file
    """
    if entry[0] != 'Rscript':
        raise ValueError('entry must start with `Rscript`.')

    if entry[1] == '-e':
        if len(entry) > 3:
            raise ValueError('You can supply at most one expression.')
    elif len(entry) > 2:
        raise ValueError(
            'The only valid syntax is `Rscript -e {expr}`'
            'or `Rscript path/to/hook/script`',
        )


def _cmd_from_hook(
        prefix: Prefix,
        entry: str,
        args: Sequence[str],
        *,
        is_local: bool,
) -> tuple[str, ...]:
    cmd = shlex.split(entry)
    _entry_validate(cmd)

    cmd_part = _prefix_if_file_entry(cmd, prefix, is_local=is_local)
    return (cmd[0], *RSCRIPT_OPTS, *cmd_part, *args)


def install_environment(
        prefix: Prefix,
        version: str,
        additional_dependencies: Sequence[str],
) -> None:
    lang_base.assert_version_default('r', version)

    env_dir = lang_base.environment_dir(prefix, ENVIRONMENT_DIR, version)
    os.makedirs(env_dir, exist_ok=True)
    shutil.copy(prefix.path('renv.lock'), env_dir)
    shutil.copytree(prefix.path('renv'), os.path.join(env_dir, 'renv'))

    r_code_inst_environment = f"""\
        prefix_dir <- {prefix.prefix_dir!r}
        options(
            repos = c(CRAN = "https://cran.rstudio.com"),
            renv.consent = TRUE
        )
        source("renv/activate.R")
        renv::restore()
        activate_statement <- paste0(
          'suppressWarnings({{',
          'old <- setwd("', getwd(), '"); ',
          'source("renv/activate.R"); ',
          'setwd(old); ',
          'renv::load("', getwd(), '");}})'
        )
        writeLines(activate_statement, 'activate.R')
        is_package <- tryCatch(
          {{
              path_desc <- file.path(prefix_dir, 'DESCRIPTION')
              suppressWarnings(desc <- read.dcf(path_desc))
              "Package" %in% colnames(desc)
          }},
          error = function(...) FALSE
        )
        if (is_package) {{
            renv::install(prefix_dir)
        }}
        """

    with _r_code_in_tempfile(r_code_inst_environment) as f:
        cmd_output_b(_rscript_exec(), '--vanilla', f, cwd=env_dir)

    if additional_dependencies:
        r_code_inst_add = 'renv::install(commandArgs(trailingOnly = TRUE))'
        with in_env(prefix, version):
            with _r_code_in_tempfile(r_code_inst_add) as f:
                cmd_output_b(
                    _rscript_exec(), *RSCRIPT_OPTS,
                    f,
                    *additional_dependencies,
                    cwd=env_dir,
                )


def _inline_r_setup(code: str) -> str:
    """
    Some behaviour of R cannot be configured via env variables, but can
    only be configured via R options once R has started. These are set here.
    """
    with_option = [
        textwrap.dedent("""\
        options(
            install.packages.compile.from.source = "never",
            pkgType = "binary"
        )
        """),
        code,
    ]
    return '\n'.join(with_option)


def run_hook(
        prefix: Prefix,
        entry: str,
        args: Sequence[str],
        file_args: Sequence[str],
        *,
        is_local: bool,
        require_serial: bool,
        color: bool,
) -> tuple[int, bytes]:
    cmd = _cmd_from_hook(prefix, entry, args, is_local=is_local)
    return lang_base.run_xargs(
        cmd,
        file_args,
        require_serial=require_serial,
        color=color,
    )

if __name__ == "__main__":
    import dill
    import os

    isT = True
    test_input1 = '''\
            prefix_dir <- '/tmp/pytest-of-travis/pytest-6/test_r_hook0/0/.pre-commit/repo4jk83aut'
            options(
                repos = c(CRAN = "https://cran.rstudio.com"),
                renv.consent = TRUE
            )
            source("renv/activate.R")
            renv::restore()
            activate_statement <- paste0(
              'suppressWarnings({',
              'old <- setwd("', getwd(), '"); ',
              'source("renv/activate.R"); ',
              'setwd(old); ',
              'renv::load("', getwd(), '");})'
            )
            writeLines(activate_statement, 'activate.R')
            is_package <- tryCatch(
              {
                  path_desc <- file.path(prefix_dir, 'DESCRIPTION')
                  suppressWarnings(desc <- read.dcf(path_desc))
                  "Package" %in% colnames(desc)
              },
              error = function(...) FALSE
            )
            if (is_package) {
                renv::install(prefix_dir)
            }
            '''
    #print(test_input1)
    option_prefix =  textwrap.dedent('''\
    options(
        install.packages.compile.from.source = "never",
        pkgType = "binary"
    )
    ''')
    test_output1 = option_prefix + '\n' + test_input1
    if _inline_r_setup(test_input1) != test_output1:
        isT=False

    test_input2 = '''\
            prefix_dir <- '/tmp/pytest-of-travis/pytest-6/test_r_inline_hook0/0/.pre-commit/repo_1vatyqa'
            options(
                repos = c(CRAN = "https://cran.rstudio.com"),
                renv.consent = TRUE
            )
            source("renv/activate.R")
            renv::restore()
            activate_statement <- paste0(
              'suppressWarnings({',
              'old <- setwd("', getwd(), '"); ',
              'source("renv/activate.R"); ',
              'setwd(old); ',
              'renv::load("', getwd(), '");})'
            )
            writeLines(activate_statement, 'activate.R')
            is_package <- tryCatch(
              {
                  path_desc <- file.path(prefix_dir, 'DESCRIPTION')
                  suppressWarnings(desc <- read.dcf(path_desc))
                  "Package" %in% colnames(desc)
              },
              error = function(...) FALSE
            )
            if (is_package) {
                renv::install(prefix_dir)
            }
            '''
    test_output2 = option_prefix + '\n' + test_input2
    if _inline_r_setup(test_input2) != test_output2:
        isT=False

    test_input3 = '''\
            prefix_dir <- '/tmp/pytest-of-travis/pytest-6/test_r_with_additional_depende0/0/.pre-commit/repo3rd48vq5'
            options(
                repos = c(CRAN = "https://cran.rstudio.com"),
                renv.consent = TRUE
            )
            source("renv/activate.R")
            renv::restore()
            activate_statement <- paste0(
              'suppressWarnings({',
              'old <- setwd("', getwd(), '"); ',
              'source("renv/activate.R"); ',
              'setwd(old); ',
              'renv::load("', getwd(), '");})'
            )
            writeLines(activate_statement, 'activate.R')
            is_package <- tryCatch(
              {
                  path_desc <- file.path(prefix_dir, 'DESCRIPTION')
                  suppressWarnings(desc <- read.dcf(path_desc))
                  "Package" %in% colnames(desc)
              },
              error = function(...) FALSE
            )
            if (is_package) {
                renv::install(prefix_dir)
            }
            '''
    test_output3 = option_prefix + '\n' + test_input3
    if _inline_r_setup(test_input3) != test_output3:
        isT=False

    test_input4 = '''\
            prefix_dir <- '/tmp/pytest-of-travis/pytest-6/test_r_local_with_additional_d0/0/.pre-commit/repo6fyn04yc'
            options(
                repos = c(CRAN = "https://cran.rstudio.com"),
                renv.consent = TRUE
            )
            source("renv/activate.R")
            renv::restore()
            activate_statement <- paste0(
              'suppressWarnings({',
              'old <- setwd("', getwd(), '"); ',
              'source("renv/activate.R"); ',
              'setwd(old); ',
              'renv::load("', getwd(), '");})'
            )
            writeLines(activate_statement, 'activate.R')
            is_package <- tryCatch(
              {
                  path_desc <- file.path(prefix_dir, 'DESCRIPTION')
                  suppressWarnings(desc <- read.dcf(path_desc))
                  "Package" %in% colnames(desc)
              },
              error = function(...) FALSE
            )
            if (is_package) {
                renv::install(prefix_dir)
            }
            '''
    test_output4 = option_prefix + '\n' + test_input4
    if _inline_r_setup(test_input4) != test_output4:
        isT=False

    
    # for l in os.listdir(
    #         "D:/fse/python_test/repos/pre-commit---pre-commit/data_passk_platform1/62e4fc3c85ea98643089041e/"):
    #     f = open("D:/fse/python_test/repos/pre-commit---pre-commit/data_passk_platform1/62e4fc3c85ea98643089041e/" + l,
    #              "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"], bytes):
    #         args0 = dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0 = content["input"]["args"][0]["bytes"]
    #     args0 = '\n'.join(args0[1:-1].split('\\n')).replace('\\\'','\'')
    #     print(args0)
    #     res0 = _inline_r_setup(args0)
        # if not ( dill.dumps(res0)==dill.dumps(content["output"][0]['bytes'])):
        #     isT=False
        #     break

    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pre-commit---pre-commit/pre_commit/xargs_xargs_passk_validte.py
from __future__ import annotations

import concurrent.futures
import contextlib
import math
import multiprocessing
import os
import subprocess
import sys
from typing import Any
from typing import Callable
from typing import Generator
from typing import Iterable
from typing import MutableMapping
from typing import Sequence
from typing import TypeVar

from pre_commit import parse_shebang
from pre_commit.util import cmd_output_b
from pre_commit.util import cmd_output_p

TArg = TypeVar('TArg')
TRet = TypeVar('TRet')


def cpu_count() -> int:
    try:
        # On systems that support it, this will return a more accurate count of
        # usable CPUs for the current process, which will take into account
        # cgroup limits
        return len(os.sched_getaffinity(0))
    except AttributeError:
        pass

    try:
        return multiprocessing.cpu_count()
    except NotImplementedError:
        return 1


def _environ_size(_env: MutableMapping[str, str] | None = None) -> int:
    environ = _env if _env is not None else getattr(os, 'environb', os.environ)
    size = 8 * len(environ)  # number of pointers in `envp`
    for k, v in environ.items():
        size += len(k) + len(v) + 2  # c strings in `envp`
    return size


def _get_platform_max_length() -> int:  # pragma: no cover (platform specific)
    if os.name == 'posix':
        maximum = os.sysconf('SC_ARG_MAX') - 2048 - _environ_size()
        maximum = max(min(maximum, 2 ** 17), 2 ** 12)
        return maximum
    elif os.name == 'nt':
        return 2 ** 15 - 2048  # UNICODE_STRING max - headroom
    else:
        # posix minimum
        return 2 ** 12


def _command_length(*cmd: str) -> int:
    full_cmd = ' '.join(cmd)

    # win32 uses the amount of characters, more details at:
    # https://github.com/pre-commit/pre-commit/pull/839
    if sys.platform == 'win32':
        return len(full_cmd.encode('utf-16le')) // 2
    else:
        return len(full_cmd.encode(sys.getfilesystemencoding()))


class ArgumentTooLongError(RuntimeError):
    pass


def partition(
        cmd: Sequence[str],
        varargs: Sequence[str],
        target_concurrency: int,
        _max_length: int | None = None,
) -> tuple[tuple[str, ...], ...]:
    _max_length = _max_length or _get_platform_max_length()

    # Generally, we try to partition evenly into at least `target_concurrency`
    # partitions, but we don't want a bunch of tiny partitions.
    max_args = max(4, math.ceil(len(varargs) / target_concurrency))

    cmd = tuple(cmd)
    ret = []

    ret_cmd: list[str] = []
    # Reversed so arguments are in order
    varargs = list(reversed(varargs))

    total_length = _command_length(*cmd) + 1
    while varargs:
        arg = varargs.pop()

        arg_length = _command_length(arg) + 1
        if (
                total_length + arg_length <= _max_length and
                len(ret_cmd) < max_args
        ):
            ret_cmd.append(arg)
            total_length += arg_length
        elif not ret_cmd:
            raise ArgumentTooLongError(arg)
        else:
            # We've exceeded the length, yield a command
            ret.append(cmd + tuple(ret_cmd))
            ret_cmd = []
            total_length = _command_length(*cmd) + 1
            varargs.append(arg)

    ret.append(cmd + tuple(ret_cmd))

    return tuple(ret)


@contextlib.contextmanager
def _thread_mapper(maxsize: int) -> Generator[
    Callable[[Callable[[TArg], TRet], Iterable[TArg]], Iterable[TRet]],
    None, None,
]:
    if maxsize == 1:
        yield map
    else:
        with concurrent.futures.ThreadPoolExecutor(maxsize) as ex:
            yield ex.map


def xargs(
        cmd: tuple[str, ...],
        varargs: Sequence[str],
        *,
        color: bool = False,
        target_concurrency: int = 1,
        _max_length: int = _get_platform_max_length(),
        **kwargs: Any,
) -> tuple[int, bytes]:
    """A simplified implementation of xargs.

    color: Make a pty if on a platform that supports it
    target_concurrency: Target number of partitions to run concurrently
    """
    cmd_fn = cmd_output_p if color else cmd_output_b
    retcode = 0
    stdout = b''

    try:
        cmd = parse_shebang.normalize_cmd(cmd)
    except parse_shebang.ExecutableNotFoundError as e:
        return e.to_output()[:2]

    # on windows, batch files have a separate length limit than windows itself
    if (
            sys.platform == 'win32' and
            cmd[0].lower().endswith(('.bat', '.cmd'))
    ):  # pragma: win32 cover
        # this is implementation details but the command gets translated into
        # full/path/to/cmd.exe /c *cmd
        cmd_exe = parse_shebang.find_executable('cmd.exe')
        # 1024 is additionally subtracted to give headroom for further
        # expansion inside the batch file
        _max_length = 8192 - len(cmd_exe) - len(' /c ') - 1024

    partitions = partition(cmd, varargs, target_concurrency, _max_length)

    def run_cmd_partition(
            run_cmd: tuple[str, ...],
    ) -> tuple[int, bytes, bytes | None]:
        return cmd_fn(
            *run_cmd, check=False, stderr=subprocess.STDOUT, **kwargs,
        )

    threads = min(len(partitions), target_concurrency)
    with _thread_mapper(threads) as thread_map:
        results = thread_map(run_cmd_partition, partitions)

        for proc_retcode, proc_out, _ in results:
            if abs(proc_retcode) > abs(retcode):
                retcode = proc_retcode
            stdout += proc_out

    return retcode, stdout

if __name__ == "__main__":
    import os

    isT = True
    args1_1=('ruby_hook','python3-hook')
    args1_2=[]
    args2_1 = ('foo', 'python3-hook')
    args2_2 = ['/dev/null']
    args3_1 = ('pip list', 'python3-hook')
    args3_2 = ['bar']
    ist1=xargs(args1_1, args1_2)==(1, b'Executable `ruby_hook` not found')
    ist2=xargs(args2_1, args2_2)==(1, b'Executable `foo` not found')
    ist3=xargs(args3_1, args3_2)==(1, b'Executable `pip list` not found')
    if not ist1 or not ist2 or not ist3:
        isT=False
    # if
    # print(xargs(args1_1,args1_2))
    # print(xargs(args2_1, args2_2))
    # print(xargs(args3_1, args3_2))
    # for l in os.listdir(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos\pre-commit---pre-commit\\data_passk_platform1/62e4fbda85ea986430890405/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\pre-commit---pre-commit\\data_passk_platform1/62e4fbda85ea986430890405/" + l,
    #              "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"], bytes):
    #         args0 = dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0 = content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     print(args0,args1)
    #     res0 = xargs(args0, args1)
        # print(res0)
        # print(content["output"][0])
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pre-commit---pre-commit/pre_commit/languages/helpers__shuffled_passk_validte.py
from __future__ import annotations

import contextlib
import os
import random
import re
import shlex
from typing import Any
from typing import ContextManager
from typing import Generator
from typing import NoReturn
from typing import Protocol
from typing import Sequence

import pre_commit.constants as C
from pre_commit import parse_shebang
from pre_commit import xargs
from pre_commit.prefix import Prefix
from pre_commit.util import cmd_output_b

FIXED_RANDOM_SEED = 1542676187

SHIMS_RE = re.compile(r'[/\\]shims[/\\]')


class Language(Protocol):
    # Use `None` for no installation / environment
    @property
    def ENVIRONMENT_DIR(self) -> str | None: ...
    # return a value to replace `'default` for `language_version`
    def get_default_version(self) -> str: ...
    # return whether the environment is healthy (or should be rebuilt)
    def health_check(self, prefix: Prefix, version: str) -> str | None: ...

    # install a repository for the given language and language_version
    def install_environment(
            self,
            prefix: Prefix,
            version: str,
            additional_dependencies: Sequence[str],
    ) -> None:
        ...

    # modify the environment for hook execution
    def in_env(self, prefix: Prefix, version: str) -> ContextManager[None]: ...

    # execute a hook and return the exit code and output
    def run_hook(
            self,
            prefix: Prefix,
            entry: str,
            args: Sequence[str],
            file_args: Sequence[str],
            *,
            is_local: bool,
            require_serial: bool,
            color: bool,
    ) -> tuple[int, bytes]:
        ...


def exe_exists(exe: str) -> bool:
    found = parse_shebang.find_executable(exe)
    if found is None:  # exe exists
        return False

    homedir = os.path.expanduser('~')
    try:
        common: str | None = os.path.commonpath((found, homedir))
    except ValueError:  # on windows, different drives raises ValueError
        common = None

    return (
        # it is not in a /shims/ directory
        not SHIMS_RE.search(found) and
        (
            # the homedir is / (docker, service user, etc.)
            os.path.dirname(homedir) == homedir or
            # the exe is not contained in the home directory
            common != homedir
        )
    )


def setup_cmd(prefix: Prefix, cmd: tuple[str, ...], **kwargs: Any) -> None:
    cmd_output_b(*cmd, cwd=prefix.prefix_dir, **kwargs)


def environment_dir(prefix: Prefix, d: str, language_version: str) -> str:
    return prefix.path(f'{d}-{language_version}')


def assert_version_default(binary: str, version: str) -> None:
    if version != C.DEFAULT:
        raise AssertionError(
            f'for now, pre-commit requires system-installed {binary} -- '
            f'you selected `language_version: {version}`',
        )


def assert_no_additional_deps(
        lang: str,
        additional_deps: Sequence[str],
) -> None:
    if additional_deps:
        raise AssertionError(
            f'for now, pre-commit does not support '
            f'additional_dependencies for {lang} -- '
            f'you selected `additional_dependencies: {additional_deps}`',
        )


def basic_get_default_version() -> str:
    return C.DEFAULT


def basic_health_check(prefix: Prefix, language_version: str) -> str | None:
    return None


def no_install(
        prefix: Prefix,
        version: str,
        additional_dependencies: Sequence[str],
) -> NoReturn:
    raise AssertionError('This language is not installable')


@contextlib.contextmanager
def no_env(prefix: Prefix, version: str) -> Generator[None, None, None]:
    yield


def target_concurrency() -> int:
    if 'PRE_COMMIT_NO_CONCURRENCY' in os.environ:
        return 1
    else:
        # Travis appears to have a bunch of CPUs, but we can't use them all.
        if 'TRAVIS' in os.environ:
            return 2
        else:
            return xargs.cpu_count()


def _shuffled(seq: Sequence[str]) -> list[str]:
    """Deterministically shuffle"""
    fixed_random = random.Random()
    fixed_random.seed(FIXED_RANDOM_SEED, version=1)

    seq = list(seq)
    fixed_random.shuffle(seq)
    return seq


def run_xargs(
        cmd: tuple[str, ...],
        file_args: Sequence[str],
        *,
        require_serial: bool,
        color: bool,
) -> tuple[int, bytes]:
    if require_serial:
        jobs = 1
    else:
        # Shuffle the files so that they more evenly fill out the xargs
        # partitions, but do it deterministically in case a hook cares about
        # ordering.
        file_args = _shuffled(file_args)
        jobs = target_concurrency()
    return xargs.xargs(cmd, file_args, target_concurrency=jobs, color=color)


def hook_cmd(entry: str, args: Sequence[str]) -> tuple[str, ...]:
    return (*shlex.split(entry), *args)


def basic_run_hook(
        prefix: Prefix,
        entry: str,
        args: Sequence[str],
        file_args: Sequence[str],
        *,
        is_local: bool,
        require_serial: bool,
        color: bool,
) -> tuple[int, bytes]:
    return run_xargs(
        hook_cmd(entry, args),
        file_args,
        require_serial=require_serial,
        color=color,
    )



if __name__ == "__main__":
    import os
    seq=(1,1.3,True,'hello',(1,1.2,True,'xxx'))
    out_list=_shuffled(seq)
    if out_list[0]!=1.3 or out_list[1]!=1 or out_list[2]!=True or out_list[3]!="hello":
        raise Exception("Result not True!!!")



----------------------------
/home/travis/builds/repos/pre-commit---pre-commit/pre_commit/util_parse_version_passk_validte.py
from __future__ import annotations

import contextlib
import errno
import importlib.resources
import os.path
import shutil
import stat
import subprocess
import sys
from types import TracebackType
from typing import Any
from typing import Callable
from typing import Generator

from pre_commit import parse_shebang


def force_bytes(exc: Any) -> bytes:
    with contextlib.suppress(TypeError):
        return bytes(exc)
    with contextlib.suppress(Exception):
        return str(exc).encode()
    return f'<unprintable {type(exc).__name__} object>'.encode()


@contextlib.contextmanager
def clean_path_on_failure(path: str) -> Generator[None, None, None]:
    """Cleans up the directory on an exceptional failure."""
    try:
        yield
    except BaseException:
        if os.path.exists(path):
            rmtree(path)
        raise


def resource_text(filename: str) -> str:
    return importlib.resources.read_text('pre_commit.resources', filename)


def make_executable(filename: str) -> None:
    original_mode = os.stat(filename).st_mode
    new_mode = original_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH
    os.chmod(filename, new_mode)


class CalledProcessError(RuntimeError):
    def __init__(
            self,
            returncode: int,
            cmd: tuple[str, ...],
            stdout: bytes,
            stderr: bytes | None,
    ) -> None:
        super().__init__(returncode, cmd, stdout, stderr)
        self.returncode = returncode
        self.cmd = cmd
        self.stdout = stdout
        self.stderr = stderr

    def __bytes__(self) -> bytes:
        def _indent_or_none(part: bytes | None) -> bytes:
            if part:
                return b'\n    ' + part.replace(b'\n', b'\n    ').rstrip()
            else:
                return b' (none)'

        return b''.join((
            f'command: {self.cmd!r}\n'.encode(),
            f'return code: {self.returncode}\n'.encode(),
            b'stdout:', _indent_or_none(self.stdout), b'\n',
            b'stderr:', _indent_or_none(self.stderr),
        ))

    def __str__(self) -> str:
        return self.__bytes__().decode()


def _setdefault_kwargs(kwargs: dict[str, Any]) -> None:
    for arg in ('stdin', 'stdout', 'stderr'):
        kwargs.setdefault(arg, subprocess.PIPE)


def _oserror_to_output(e: OSError) -> tuple[int, bytes, None]:
    return 1, force_bytes(e).rstrip(b'\n') + b'\n', None


def cmd_output_b(
        *cmd: str,
        check: bool = True,
        **kwargs: Any,
) -> tuple[int, bytes, bytes | None]:
    _setdefault_kwargs(kwargs)

    try:
        cmd = parse_shebang.normalize_cmd(cmd, env=kwargs.get('env'))
    except parse_shebang.ExecutableNotFoundError as e:
        returncode, stdout_b, stderr_b = e.to_output()
    else:
        try:
            proc = subprocess.Popen(cmd, **kwargs)
        except OSError as e:
            returncode, stdout_b, stderr_b = _oserror_to_output(e)
        else:
            stdout_b, stderr_b = proc.communicate()
            returncode = proc.returncode

    if check and returncode:
        raise CalledProcessError(returncode, cmd, stdout_b, stderr_b)

    return returncode, stdout_b, stderr_b


def cmd_output(*cmd: str, **kwargs: Any) -> tuple[int, str, str | None]:
    returncode, stdout_b, stderr_b = cmd_output_b(*cmd, **kwargs)
    stdout = stdout_b.decode() if stdout_b is not None else None
    stderr = stderr_b.decode() if stderr_b is not None else None
    return returncode, stdout, stderr


if sys.platform != 'win32':  # pragma: win32 no cover
    from os import openpty
    import termios

    class Pty:
        def __init__(self) -> None:
            self.r: int | None = None
            self.w: int | None = None

        def __enter__(self) -> Pty:
            self.r, self.w = openpty()

            # tty flags normally change \n to \r\n
            attrs = termios.tcgetattr(self.w)
            assert isinstance(attrs[1], int)
            attrs[1] &= ~(termios.ONLCR | termios.OPOST)
            termios.tcsetattr(self.w, termios.TCSANOW, attrs)

            return self

        def close_w(self) -> None:
            if self.w is not None:
                os.close(self.w)
                self.w = None

        def close_r(self) -> None:
            assert self.r is not None
            os.close(self.r)
            self.r = None

        def __exit__(
                self,
                exc_type: type[BaseException] | None,
                exc_value: BaseException | None,
                traceback: TracebackType | None,
        ) -> None:
            self.close_w()
            self.close_r()

    def cmd_output_p(
            *cmd: str,
            check: bool = True,
            **kwargs: Any,
    ) -> tuple[int, bytes, bytes | None]:
        assert check is False
        assert kwargs['stderr'] == subprocess.STDOUT, kwargs['stderr']
        _setdefault_kwargs(kwargs)

        try:
            cmd = parse_shebang.normalize_cmd(cmd)
        except parse_shebang.ExecutableNotFoundError as e:
            return e.to_output()

        with open(os.devnull) as devnull, Pty() as pty:
            assert pty.r is not None
            kwargs.update({'stdin': devnull, 'stdout': pty.w, 'stderr': pty.w})
            try:
                proc = subprocess.Popen(cmd, **kwargs)
            except OSError as e:
                return _oserror_to_output(e)

            pty.close_w()

            buf = b''
            while True:
                try:
                    bts = os.read(pty.r, 4096)
                except OSError as e:
                    if e.errno == errno.EIO:
                        bts = b''
                    else:
                        raise
                else:
                    buf += bts
                if not bts:
                    break

        return proc.wait(), buf, None
else:  # pragma: no cover
    cmd_output_p = cmd_output_b


def rmtree(path: str) -> None:
    """On windows, rmtree fails for readonly dirs."""
    def handle_remove_readonly(
            func: Callable[..., Any],
            path: str,
            exc: tuple[type[OSError], OSError, TracebackType],
    ) -> None:
        excvalue = exc[1]
        if (
                func in (os.rmdir, os.remove, os.unlink) and
                excvalue.errno in {errno.EACCES, errno.EPERM}
        ):
            for p in (path, os.path.dirname(path)):
                os.chmod(p, os.stat(p).st_mode | stat.S_IWUSR)
            func(path)
        else:
            raise
    shutil.rmtree(path, ignore_errors=False, onerror=handle_remove_readonly)


def win_exe(s: str) -> str:
    return s if sys.platform != 'win32' else f'{s}.exe'

def parse_version(s: str) -> tuple[int, ...]:
    """poor man's version comparison"""
    return tuple(int(p) for p in s.split('.'))

if __name__ == "__main__":
    import os

    isT = True
    ist1=parse_version('2.20.0')==(2, 20, 0)
    ist2=parse_version('1.2.9')==(1, 2, 9)
    ist3=parse_version('0.0')==(0, 0)
    if not ist1 or not ist2 or not ist3:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pre-commit---pre-commit/pre_commit/parse_shebang_normalize_cmd_passk_validte.py
from __future__ import annotations

import os.path
from typing import Mapping
from typing import NoReturn

from identify.identify import parse_shebang_from_file


class ExecutableNotFoundError(OSError):
    def to_output(self) -> tuple[int, bytes, None]:
        return (1, self.args[0].encode(), None)


def parse_filename(filename: str) -> tuple[str, ...]:
    if not os.path.exists(filename):
        return ()
    else:
        return parse_shebang_from_file(filename)


def find_executable(
        exe: str, *, env: Mapping[str, str] | None = None,
) -> str | None:
    exe = os.path.normpath(exe)
    if os.sep in exe:
        return exe

    environ = env if env is not None else os.environ

    if 'PATHEXT' in environ:
        exts = environ['PATHEXT'].split(os.pathsep)
        possible_exe_names = tuple(f'{exe}{ext}' for ext in exts) + (exe,)
    else:
        possible_exe_names = (exe,)

    for path in environ.get('PATH', '').split(os.pathsep):
        for possible_exe_name in possible_exe_names:
            joined = os.path.join(path, possible_exe_name)
            if os.path.isfile(joined) and os.access(joined, os.X_OK):
                return joined
    else:
        return None


def normexe(orig: str, *, env: Mapping[str, str] | None = None) -> str:
    def _error(msg: str) -> NoReturn:
        raise ExecutableNotFoundError(f'Executable `{orig}` {msg}')

    if os.sep not in orig and (not os.altsep or os.altsep not in orig):
        exe = find_executable(orig, env=env)
        if exe is None:
            _error('not found')
        return exe
    elif os.path.isdir(orig):
        _error('is a directory')
    elif not os.path.isfile(orig):
        _error('not found')
    elif not os.access(orig, os.X_OK):  # pragma: win32 no cover
        _error('is not executable')
    else:
        return orig


def normalize_cmd(
        cmd: tuple[str, ...],
        *,
        env: Mapping[str, str] | None = None,
) -> tuple[str, ...]:
    """Fixes for the following issues on windows
    - https://bugs.python.org/issue8557
    - windows does not parse shebangs

    This function also makes deep-path shebangs work just fine
    """
    # Use PATH to determine the executable
    exe = normexe(cmd[0], env=env)

    # Figure out the shebang from the resulting command
    cmd = parse_filename(exe) + (exe,) + cmd[1:]

    # This could have given us back another bare executable
    exe = normexe(cmd[0], env=env)

    return (exe,) + cmd[1:]

if __name__ == "__main__":

    import os
    isT = True

    args0=('python', '--version')
    args1=('java', '--version')
    args2=('git', '--version')
    args3=('git', '--version')
    ist1=normalize_cmd(args0)==('/usr/local/bin/python', '--version')
    ist2=normalize_cmd(args1)==('/usr/bin/java', '--version')
    ist3=normalize_cmd(args2)==('/usr/bin/git', '--version')
    ist4=normalize_cmd(args3)==('/usr/bin/git', '--version')
    if not ist1 or not ist2 or not ist3 or not ist4:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/decorators_cached_passk_validte.py
import functools
import sys
sys.path.append("..")
#import cachetools
from keys import hashkey


def cached(cache, key=hashkey, lock=None):
    """Decorator to wrap a function with a memoizing callable that saves
    results in a cache.

    """
    def decorator(func):
        if cache is None:
            def wrapper(*args, **kwargs):
                return func(*args, **kwargs)
        elif lock is None:
            def wrapper(*args, **kwargs):
                k = key(*args, **kwargs)
                try:
                    return cache[k]
                except KeyError:
                    pass  # key not found
                v = func(*args, **kwargs)
                try:
                    cache[k] = v
                except ValueError:
                    pass  # value too large
                return v
        else:
            def wrapper(*args, **kwargs):
                k = key(*args, **kwargs)
                try:
                    with lock:
                        return cache[k]
                except KeyError:
                    pass  # key not found
                v = func(*args, **kwargs)
                # in case of a race, prefer the item already in the cache
                try:
                    with lock:
                        return cache.setdefault(k, v)
                except ValueError:
                    return v  # value too large
        return functools.update_wrapper(wrapper, func)
    return decorator


def cachedmethod(cache, key=hashkey, lock=None):
    """Decorator to wrap a class or instance method with a memoizing
    callable that saves results in a cache.

    """
    def decorator(method):
        if lock is None:
            def wrapper(self, *args, **kwargs):
                c = cache(self)
                if c is None:
                    return method(self, *args, **kwargs)
                k = key(*args, **kwargs)
                try:
                    return c[k]
                except KeyError:
                    pass  # key not found
                v = method(self, *args, **kwargs)
                try:
                    c[k] = v
                except ValueError:
                    pass  # value too large
                return v
        else:
            def wrapper(self, *args, **kwargs):
                c = cache(self)
                if c is None:
                    return method(self, *args, **kwargs)
                k = key(*args, **kwargs)
                try:
                    with lock(self):
                        return c[k]
                except KeyError:
                    pass  # key not found
                v = method(self, *args, **kwargs)
                # in case of a race, prefer the item already in the cache
                try:
                    with lock(self):
                        return c.setdefault(k, v)
                except ValueError:
                    return v  # value too large
        return functools.update_wrapper(wrapper, method)
    return decorator



class DecoratorTestMixin(object):

    def cache(self, minsize):
        raise NotImplementedError

    def func(self, *args, **kwargs):
        if hasattr(self, 'count'):
            self.count += 1
        else:
            self.count = 0
        return self.count
if __name__ == "__main__":
    isT=True
    from cache import Cache
    from keys import typedkey
    try:
        dec=DecoratorTestMixin()
        c = Cache(3)
        key = typedkey
        wrapper = cached(c, key=key)(dec.func)
        res1=len(c)==0
        res2=wrapper.__wrapped__==dec.func

        res3=wrapper(0)==0
        res4=len(c)==1
        res5= hashkey(0)==(0,)

        res6=wrapper(1)== 1
        res7=len(c)== 2
        res8= typedkey(0)==(0, int)
        res9=wrapper(1)==1
        res10=len(c)==2
        if not res1 or not res2 or not res3 or not res4 or not res5 or not res6 or not res7 or not res8 or not res9 or not res10:
            isT=False
    except:
        isT = False


    # import dill
    # import os
    #
    # isT = True
    # for l in os.listdir(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d27a48ba5a41d1c3f4c6/"):
    #     f = open(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d27a48ba5a41d1c3f4c6/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"], bytes):
    #         args0 = dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0 = content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     # print(args1)
    #
    #     res0 = cached(args0, args1, args2)
    #
    #     if not ( dill.dumps(args0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/func_ttl_cache_passk_validte.py
"""`functools.lru_cache` compatible memoizing function decorators."""

import collections
import functools
import math
import random
import time
import sys
sys.path.append("..")

try:
    from threading import RLock
except ImportError:  # pragma: no cover
    from dummy_threading import RLock

import keys
from fifo import FIFOCache
from lfu import LFUCache
from lru import LRUCache
from mru import MRUCache
from rr import RRCache
from ttl import TTLCache

__all__ = ('lfu_cache', 'lru_cache', 'mru_cache', 'rr_cache', 'ttl_cache')


_CacheInfo = collections.namedtuple('CacheInfo', [
    'hits', 'misses', 'maxsize', 'currsize'
])


class _UnboundCache(dict):

    @property
    def maxsize(self):
        return None

    @property
    def currsize(self):
        return len(self)


class _UnboundTTLCache(TTLCache):
    def __init__(self, ttl, timer):
        TTLCache.__init__(self, math.inf, ttl, timer)

    @property
    def maxsize(self):
        return None


def _cache(cache, typed):
    maxsize = cache.maxsize

    def decorator(func):
        key = keys.typedkey if typed else keys.hashkey
        lock = RLock()
        stats = [0, 0]

        def wrapper(*args, **kwargs):
            k = key(*args, **kwargs)
            with lock:
                try:
                    v = cache[k]
                    stats[0] += 1
                    return v
                except KeyError:
                    stats[1] += 1
            v = func(*args, **kwargs)
            # in case of a race, prefer the item already in the cache
            try:
                with lock:
                    return cache.setdefault(k, v)
            except ValueError:
                return v  # value too large

        def cache_info():
            with lock:
                hits, misses = stats
                maxsize = cache.maxsize
                currsize = cache.currsize
            return _CacheInfo(hits, misses, maxsize, currsize)

        def cache_clear():
            with lock:
                try:
                    cache.clear()
                finally:
                    stats[:] = [0, 0]

        wrapper.cache_info = cache_info
        wrapper.cache_clear = cache_clear
        wrapper.cache_parameters = lambda: {'maxsize': maxsize, 'typed': typed}
        functools.update_wrapper(wrapper, func)
        return wrapper
    return decorator


def fifo_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a First In First Out (FIFO)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(FIFOCache(128), typed)(maxsize)
    else:
        return _cache(FIFOCache(maxsize), typed)


def lfu_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Least Frequently Used (LFU)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(LFUCache(128), typed)(maxsize)
    else:
        return _cache(LFUCache(maxsize), typed)


def lru_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Least Recently Used (LRU)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(LRUCache(128), typed)(maxsize)
    else:
        return _cache(LRUCache(maxsize), typed)


def mru_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Most Recently Used (MRU)
    algorithm.
    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(MRUCache(128), typed)(maxsize)
    else:
        return _cache(MRUCache(maxsize), typed)


def rr_cache(maxsize=128, choice=random.choice, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Random Replacement (RR)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(RRCache(128, choice), typed)(maxsize)
    else:
        return _cache(RRCache(maxsize, choice), typed)


def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Least Recently Used (LRU)
    algorithm with a per-item time-to-live (TTL) value.
    """
    if maxsize is None:
        return _cache(_UnboundTTLCache(ttl, timer), typed)
    elif callable(maxsize):
        return _cache(TTLCache(128, ttl, timer), typed)(maxsize)
    else:
        return _cache(TTLCache(maxsize, ttl, timer), typed)

if __name__ == "__main__":
    isT = True
    # try:
    DDDD = staticmethod(ttl_cache)
    cached = DDDD(10, typed=False)(lambda n: n)
    res1 = cached.cache_parameters()["maxsize"] == 10 and cached.cache_parameters()["typed"] == False
    
    res2 = cached.cache_info().hits == 0 and cached.cache_info().misses == 0 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 0
    
    cached(1)
    res3 = cached.cache_info().hits == 0 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
    
    cached(1)
    res4 = cached.cache_info().hits == 1 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
    
    cached(1.0)
    res5 = cached.cache_info().hits == 2 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
    
    DDDD = staticmethod(ttl_cache)
    cached = DDDD(10, typed=True)(lambda n: n)
    res6 = cached.cache_parameters()["maxsize"] == 10 and cached.cache_parameters()["typed"] == True
    
    res7 = cached.cache_info().hits == 0 and cached.cache_info().misses == 0 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 0
    
    cached(1)
    res8 = cached.cache_info().hits == 0 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
    
    cached(1)
    res9 = cached.cache_info().hits == 1 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1

    cached(1.0)
    res10 = cached.cache_info().hits == 1 and cached.cache_info().misses == 2 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 2

    if not res1 or not res2 or not res3 or not res4 or not res5 or not res6 or not res7 or not res8 or not res9 or not res10:
        isT = False
    # except:
    #     isT = False

    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/func_mru_cache_passk_validte.py
"""`functools.lru_cache` compatible memoizing function decorators."""

import collections
import functools
import math
import random
import time
import sys
sys.path.append("..")

try:
    from threading import RLock
except ImportError:  # pragma: no cover
    from dummy_threading import RLock

import keys
from fifo import FIFOCache
from lfu import LFUCache
from lru import LRUCache
from mru import MRUCache
from rr import RRCache
from ttl import TTLCache

__all__ = ('lfu_cache', 'lru_cache', 'mru_cache', 'rr_cache', 'ttl_cache')


_CacheInfo = collections.namedtuple('CacheInfo', [
    'hits', 'misses', 'maxsize', 'currsize'
])


class _UnboundCache(dict):

    @property
    def maxsize(self):
        return None

    @property
    def currsize(self):
        return len(self)


class _UnboundTTLCache(TTLCache):
    def __init__(self, ttl, timer):
        TTLCache.__init__(self, math.inf, ttl, timer)

    @property
    def maxsize(self):
        return None


def _cache(cache, typed):
    maxsize = cache.maxsize

    def decorator(func):
        key = keys.typedkey if typed else keys.hashkey
        lock = RLock()
        stats = [0, 0]

        def wrapper(*args, **kwargs):
            k = key(*args, **kwargs)
            with lock:
                try:
                    v = cache[k]
                    stats[0] += 1
                    return v
                except KeyError:
                    stats[1] += 1
            v = func(*args, **kwargs)
            # in case of a race, prefer the item already in the cache
            try:
                with lock:
                    return cache.setdefault(k, v)
            except ValueError:
                return v  # value too large

        def cache_info():
            with lock:
                hits, misses = stats
                maxsize = cache.maxsize
                currsize = cache.currsize
            return _CacheInfo(hits, misses, maxsize, currsize)

        def cache_clear():
            with lock:
                try:
                    cache.clear()
                finally:
                    stats[:] = [0, 0]

        wrapper.cache_info = cache_info
        wrapper.cache_clear = cache_clear
        wrapper.cache_parameters = lambda: {'maxsize': maxsize, 'typed': typed}
        functools.update_wrapper(wrapper, func)
        return wrapper
    return decorator


def fifo_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a First In First Out (FIFO)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(FIFOCache(128), typed)(maxsize)
    else:
        return _cache(FIFOCache(maxsize), typed)


def lfu_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Least Frequently Used (LFU)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(LFUCache(128), typed)(maxsize)
    else:
        return _cache(LFUCache(maxsize), typed)


def lru_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Least Recently Used (LRU)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(LRUCache(128), typed)(maxsize)
    else:
        return _cache(LRUCache(maxsize), typed)


def mru_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Most Recently Used (MRU)
    algorithm.
    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(MRUCache(128), typed)(maxsize)
    else:
        return _cache(MRUCache(maxsize), typed)


def rr_cache(maxsize=128, choice=random.choice, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Random Replacement (RR)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(RRCache(128, choice), typed)(maxsize)
    else:
        return _cache(RRCache(maxsize, choice), typed)


def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Least Recently Used (LRU)
    algorithm with a per-item time-to-live (TTL) value.
    """
    if maxsize is None:
        return _cache(_UnboundTTLCache(ttl, timer), typed)
    elif callable(maxsize):
        return _cache(TTLCache(128, ttl, timer), typed)(maxsize)
    else:
        return _cache(TTLCache(maxsize, ttl, timer), typed)

if __name__ == "__main__":
    isT = True
    try:
        DDDD = staticmethod(mru_cache)
        cached = DDDD(10, False)(lambda n: n)
        res1 = cached.cache_parameters()["maxsize"] == 10 and cached.cache_parameters()["typed"] == False
        res2 = cached.cache_info().hits == 0 and cached.cache_info().misses == 0 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 0
        cached(1)
        res3 = cached.cache_info().hits == 0 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
        cached(1)
        res4 = cached.cache_info().hits == 1 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
        cached(1.0)
        res5 = cached.cache_info().hits == 2 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1

        DDDD = staticmethod(mru_cache)
        cached = DDDD(10, True)(lambda n: n)
        res6 = cached.cache_parameters()["maxsize"] == 10 and cached.cache_parameters()["typed"] == True
        res7 = cached.cache_info().hits == 0 and cached.cache_info().misses == 0 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 0
        cached(1)
        res8 = cached.cache_info().hits == 0 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
        cached(1)
        res9 = cached.cache_info().hits == 1 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
        cached(1.0)
        res10 = cached.cache_info().hits == 1 and cached.cache_info().misses == 2 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 2

        if not res1 or not res2 or not res3 or not res4 or not res5 or not res6 or not res7 or not res8 or not res9 or not res10:
            isT = False
    except:
        isT = False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d23b48ba5a41d1c3f49a/"):
    #     f = open("/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d23b48ba5a41d1c3f49a/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     DDDD = staticmethod(mru_cache)
    #     cached = DDDD(args0, args1)(lambda n: n)
    #     if not ( cached.cache_info()== content["output"][0]):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/func_lru_cache_passk_validte.py
"""`functools.lru_cache` compatible memoizing function decorators."""

import collections
import functools
import math
import random
import time

try:
    from threading import RLock
except ImportError:  # pragma: no cover
    from dummy_threading import RLock
import sys
sys.path.append("..")
import keys
from fifo import FIFOCache
from lfu import LFUCache
from lru import LRUCache
from mru import MRUCache
from rr import RRCache
from ttl import TTLCache

__all__ = ('lfu_cache', 'lru_cache', 'mru_cache', 'rr_cache', 'ttl_cache')


_CacheInfo = collections.namedtuple('CacheInfo', [
    'hits', 'misses', 'maxsize', 'currsize'
])


class _UnboundCache(dict):

    @property
    def maxsize(self):
        return None

    @property
    def currsize(self):
        return len(self)


class _UnboundTTLCache(TTLCache):
    def __init__(self, ttl, timer):
        TTLCache.__init__(self, math.inf, ttl, timer)

    @property
    def maxsize(self):
        return None


def _cache(cache, typed):
    maxsize = cache.maxsize

    def decorator(func):
        key = keys.typedkey if typed else keys.hashkey
        lock = RLock()
        stats = [0, 0]

        def wrapper(*args, **kwargs):
            k = key(*args, **kwargs)
            with lock:
                try:
                    v = cache[k]
                    stats[0] += 1
                    return v
                except KeyError:
                    stats[1] += 1
            v = func(*args, **kwargs)
            # in case of a race, prefer the item already in the cache
            try:
                with lock:
                    return cache.setdefault(k, v)
            except ValueError:
                return v  # value too large

        def cache_info():
            with lock:
                hits, misses = stats
                maxsize = cache.maxsize
                currsize = cache.currsize
            return _CacheInfo(hits, misses, maxsize, currsize)

        def cache_clear():
            with lock:
                try:
                    cache.clear()
                finally:
                    stats[:] = [0, 0]

        wrapper.cache_info = cache_info
        wrapper.cache_clear = cache_clear
        wrapper.cache_parameters = lambda: {'maxsize': maxsize, 'typed': typed}
        functools.update_wrapper(wrapper, func)
        return wrapper
    return decorator


def fifo_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a First In First Out (FIFO)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(FIFOCache(128), typed)(maxsize)
    else:
        return _cache(FIFOCache(maxsize), typed)


def lfu_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Least Frequently Used (LFU)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(LFUCache(128), typed)(maxsize)
    else:
        return _cache(LFUCache(maxsize), typed)


def lru_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Least Recently Used (LRU)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(LRUCache(128), typed)(maxsize)
    else:
        return _cache(LRUCache(maxsize), typed)


def mru_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Most Recently Used (MRU)
    algorithm.
    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(MRUCache(128), typed)(maxsize)
    else:
        return _cache(MRUCache(maxsize), typed)


def rr_cache(maxsize=128, choice=random.choice, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Random Replacement (RR)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(RRCache(128, choice), typed)(maxsize)
    else:
        return _cache(RRCache(maxsize, choice), typed)


def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Least Recently Used (LRU)
    algorithm with a per-item time-to-live (TTL) value.
    """
    if maxsize is None:
        return _cache(_UnboundTTLCache(ttl, timer), typed)
    elif callable(maxsize):
        return _cache(TTLCache(128, ttl, timer), typed)(maxsize)
    else:
        return _cache(TTLCache(maxsize, ttl, timer), typed)

if __name__ == "__main__":
    isT = True
    try:
        DDDD = staticmethod(lru_cache)
        cached = DDDD(10, False)(lambda n: n)
        res1 = cached.cache_parameters()["maxsize"] == 10 and cached.cache_parameters()["typed"] == False
        res2 = cached.cache_info().hits == 0 and cached.cache_info().misses == 0 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 0
        cached(1)
        res3 = cached.cache_info().hits == 0 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
        cached(1)
        res4 = cached.cache_info().hits == 1 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
        cached(1.0)
        res5 = cached.cache_info().hits == 2 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1

        DDDD = staticmethod(lru_cache)
        cached = DDDD(10, True)(lambda n: n)
        res6 = cached.cache_parameters()["maxsize"] == 10 and cached.cache_parameters()["typed"] == True
        res7 = cached.cache_info().hits == 0 and cached.cache_info().misses == 0 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 0
        cached(1)
        res8 = cached.cache_info().hits == 0 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
        cached(1)
        res9 = cached.cache_info().hits == 1 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
        cached(1.0)
        res10 = cached.cache_info().hits == 1 and cached.cache_info().misses == 2 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 2

        if not res1 or not res2 or not res3 or not res4 or not res5 or not res6 or not res7 or not res8 or not res9 or not res10:
            isT = False
    except:
        isT = False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d23948ba5a41d1c3f498/"):
    #     f = open("/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d23948ba5a41d1c3f498/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     DDDD = staticmethod(lru_cache)
    #     cached = DDDD(args0, args1)(lambda n: n)
    #
    #     # print(content["output"][0])
    #     # print(cached.cache_info())
    #
    #     if not (cached.cache_info() == content["output"][0]):
    #         isT = False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/func_lfu_cache_passk_validte.py
"""`functools.lru_cache` compatible memoizing function decorators."""

import collections
import functools
import math
import random
import time
from threading import RLock
# try:
#     from threading import RLock
# except ImportError:  # pragma: no cover
#     from dummy_threading import RLock
import sys
sys.path.append("..")

import keys
from fifo import FIFOCache
from lfu import LFUCache
from lru import LRUCache
from mru import MRUCache
from rr import RRCache
from ttl import TTLCache

__all__ = ('lfu_cache', 'lru_cache', 'mru_cache', 'rr_cache', 'ttl_cache')


_CacheInfo = collections.namedtuple('CacheInfo', [
    'hits', 'misses', 'maxsize', 'currsize'
])


class _UnboundCache(dict):

    @property
    def maxsize(self):
        return None

    @property
    def currsize(self):
        return len(self)


class _UnboundTTLCache(TTLCache):
    def __init__(self, ttl, timer):
        TTLCache.__init__(self, math.inf, ttl, timer)

    @property
    def maxsize(self):
        return None


def _cache(cache, typed):
    maxsize = cache.maxsize

    def decorator(func):
        key = keys.typedkey if typed else keys.hashkey
        lock = RLock()
        stats = [0, 0]

        def wrapper(*args, **kwargs):
            k = key(*args, **kwargs)
            with lock:
                try:
                    v = cache[k]
                    stats[0] += 1
                    return v
                except KeyError:
                    stats[1] += 1
            v = func(*args, **kwargs)
            # in case of a race, prefer the item already in the cache
            try:
                with lock:
                    return cache.setdefault(k, v)
            except ValueError:
                return v  # value too large

        def cache_info():
            with lock:
                hits, misses = stats
                maxsize = cache.maxsize
                currsize = cache.currsize
            return _CacheInfo(hits, misses, maxsize, currsize)

        def cache_clear():
            with lock:
                try:
                    cache.clear()
                finally:
                    stats[:] = [0, 0]

        wrapper.cache_info = cache_info
        wrapper.cache_clear = cache_clear
        wrapper.cache_parameters = lambda: {'maxsize': maxsize, 'typed': typed}
        functools.update_wrapper(wrapper, func)
        return wrapper
    return decorator


def fifo_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a First In First Out (FIFO)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(FIFOCache(128), typed)(maxsize)
    else:
        return _cache(FIFOCache(maxsize), typed)


def lfu_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Least Frequently Used (LFU)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(LFUCache(128), typed)(maxsize)
    else:
        return _cache(LFUCache(maxsize), typed)


def lru_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Least Recently Used (LRU)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(LRUCache(128), typed)(maxsize)
    else:
        return _cache(LRUCache(maxsize), typed)


def mru_cache(maxsize=128, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Most Recently Used (MRU)
    algorithm.
    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(MRUCache(128), typed)(maxsize)
    else:
        return _cache(MRUCache(maxsize), typed)


def rr_cache(maxsize=128, choice=random.choice, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Random Replacement (RR)
    algorithm.

    """
    if maxsize is None:
        return _cache(_UnboundCache(), typed)
    elif callable(maxsize):
        return _cache(RRCache(128, choice), typed)(maxsize)
    else:
        return _cache(RRCache(maxsize, choice), typed)


def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):
    """Decorator to wrap a function with a memoizing callable that saves
    up to `maxsize` results based on a Least Recently Used (LRU)
    algorithm with a per-item time-to-live (TTL) value.
    """
    if maxsize is None:
        return _cache(_UnboundTTLCache(ttl, timer), typed)
    elif callable(maxsize):
        return _cache(TTLCache(128, ttl, timer), typed)(maxsize)
    else:
        return _cache(TTLCache(maxsize, ttl, timer), typed)

if __name__ == "__main__":
    isT=True
    try:
        DDDD = staticmethod(lfu_cache)
        cached = DDDD(10, False)(lambda n: n)
        res1 = cached.cache_parameters()["maxsize"] == 10 and cached.cache_parameters()["typed"] == False
        res2=cached.cache_info().hits==0 and cached.cache_info().misses==0 and cached.cache_info().maxsize==10 and cached.cache_info().currsize==0
        cached(1)
        res3=cached.cache_info().hits==0 and cached.cache_info().misses==1 and cached.cache_info().maxsize==10 and cached.cache_info().currsize==1
        cached(1)
        res4=cached.cache_info().hits==1 and cached.cache_info().misses==1 and cached.cache_info().maxsize==10 and cached.cache_info().currsize==1
        cached(1.0)
        res5=cached.cache_info().hits==2 and cached.cache_info().misses==1 and cached.cache_info().maxsize==10 and cached.cache_info().currsize==1

        DDDD = staticmethod(lfu_cache)
        cached = DDDD(10, True)(lambda n: n)
        res6=cached.cache_parameters()["maxsize"] == 10 and cached.cache_parameters()["typed"] == True
        res7 = cached.cache_info().hits == 0 and cached.cache_info().misses == 0 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 0
        cached(1)
        res8 = cached.cache_info().hits == 0 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
        cached(1)
        res9 = cached.cache_info().hits == 1 and cached.cache_info().misses == 1 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 1
        cached(1.0)
        res10 = cached.cache_info().hits == 1 and cached.cache_info().misses == 2 and cached.cache_info().maxsize == 10 and cached.cache_info().currsize == 2

        if not res1 or not res2 or not res3 or not res4 or not res5 or not res6 or not res7 or not res8 or not res9 or not res10:
            isT = False
    except:
        isT=False


    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\pexip---os-python-cachetools\\data_passk_platform1/62b8d23748ba5a41d1c3f496/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\pexip---os-python-cachetools\\data_passk_platform1/62b8d23748ba5a41d1c3f496/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     DDDD = staticmethod(lfu_cache)
    #     cached = DDDD(10, args1)(lambda n: n)
    #     print(args0,args1)
    #     print(cached)
    #     print(cached.cache_parameters())
    #     print(cached.cache_info())
    #     cached(1)
    #     print(cached.cache_info())
    #     cached(1)
    #     print(cached.cache_info())
    #     cached(1.0)
    #     print(cached.cache_info())
    #     # print(content["output"][0])
    #     # print(cached.cache_info())
    #
    # #     if not (cached.cache_info() == content["output"][0]):
    # #         isT = False
    # #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/fifo_popitem_passk_validte.py
import collections
import sys
sys.path.append("..")

from cache import Cache


class FIFOCache(Cache):
    """First In First Out (FIFO) cache implementation."""

    def __init__(self, maxsize, getsizeof=None):
        Cache.__init__(self, maxsize, getsizeof)
        self.__order = collections.OrderedDict()

    def __setitem__(self, key, value, cache_setitem=Cache.__setitem__):
        cache_setitem(self, key, value)
        try:
            self.__order.move_to_end(key)
        except KeyError:
            self.__order[key] = None

    def __delitem__(self, key, cache_delitem=Cache.__delitem__):
        cache_delitem(self, key)
        del self.__order[key]

    def popitem(self):
        """Remove and return the `(key, value)` pair first inserted."""
        try:
            key = next(iter(self.__order))
        except StopIteration:
            raise KeyError('%s is empty' % type(self).__name__) from None
        else:
            return (key, self.pop(key))

if __name__ == "__main__":
    isT = True
    try:
        temp_class = FIFOCache(10000)
        temp_class.__setitem__(4, 10)
        temp_class.__setitem__(5, "five")
        temp_class.__setitem__(6, 600)
        res1=temp_class.popitem()==(4, 10)
        res2=temp_class.popitem()==(5, 'five')
        res3=temp_class.popitem()==(6, 600)
        if not res1 or not res2 or not res3:
            isT = False
    except:
        isT = False
    # import dill
    # import os
    #
    # for l in os.listdir(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d22f48ba5a41d1c3f488/"):
    #     f = open(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d22f48ba5a41d1c3f488/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = FIFOCache(10000)
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.popitem()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/cache_setdefault_passk_validte.py
from collections.abc import MutableMapping


class _DefaultSize(object):

    __slots__ = ()

    def __getitem__(self, _):
        return 1

    def __setitem__(self, _, value):
        assert value == 1

    def pop(self, _):
        return 1


class Cache(MutableMapping):
    """Mutable mapping to serve as a simple cache or cache base class."""

    __marker = object()

    __size = _DefaultSize()

    def __init__(self, maxsize, getsizeof=None):
        if getsizeof:
            self.getsizeof = getsizeof
        if self.getsizeof is not Cache.getsizeof:
            self.__size = dict()
        self.__data = dict()
        self.__currsize = 0
        self.__maxsize = maxsize

    def __repr__(self):
        return '%s(%r, maxsize=%r, currsize=%r)' % (
            self.__class__.__name__,
            list(self.__data.items()),
            self.__maxsize,
            self.__currsize,
        )

    def __getitem__(self, key):
        try:
            return self.__data[key]
        except KeyError:
            return self.__missing__(key)

    def __setitem__(self, key, value):
        maxsize = self.__maxsize
        size = self.getsizeof(value)
        if size > maxsize:
            raise ValueError('value too large')
        if key not in self.__data or self.__size[key] < size:
            while self.__currsize + size > maxsize:
                self.popitem()
        if key in self.__data:
            diffsize = size - self.__size[key]
        else:
            diffsize = size
        self.__data[key] = value
        self.__size[key] = size
        self.__currsize += diffsize

    def __delitem__(self, key):
        size = self.__size.pop(key)
        del self.__data[key]
        self.__currsize -= size

    def __contains__(self, key):
        return key in self.__data

    def __missing__(self, key):
        raise KeyError(key)

    def __iter__(self):
        return iter(self.__data)

    def __len__(self):
        return len(self.__data)

    def get(self, key, default=None):
        if key in self:
            return self[key]
        else:
            return default

    def pop(self, key, default=__marker):
        if key in self:
            value = self[key]
            del self[key]
        elif default is self.__marker:
            raise KeyError(key)
        else:
            value = default
        return value

    def setdefault(self, key, default=None):
        if key in self:
            value = self[key]
        else:
            self[key] = value = default
        return value

    @property
    def maxsize(self):
        """The maximum size of the cache."""
        return self.__maxsize

    @property
    def currsize(self):
        """The current size of the cache."""
        return self.__currsize

    @staticmethod
    def getsizeof(value):
        """Return the size of a cache element's value."""
        return 1

if __name__ == "__main__":
    isT = True
    try:
        cache = Cache(10000)
        cache.__setitem__(4, 10)
        cache.__setitem__(5, "five")
        cache.__setitem__(6, 600)
        res1 = cache.setdefault(4, 1)
        res2 = cache.setdefault(5, 2)
        res3 = cache.setdefault(7, 2)
        res4 = cache.setdefault(10, 7)
        res5 = cache.setdefault(6, 11)
        res6 = cache.setdefault(6, 20)
        if res1 != 10 or res2 != "five" or res3 != 2 or res4 != 7 or res5 != 600 or res6 != 600:
            isT = False
    except:
        isT = False
    # for l in os.listdir(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos\pexip---os-python-cachetools\\data_passk_platform1/62b8d22a48ba5a41d1c3f47e/"):
    #     f = open(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos\pexip---os-python-cachetools\\data_passk_platform1/62b8d22a48ba5a41d1c3f47e/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     print(args1,args2)
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     # temp_class = Cache(10000)
    #     # temp_class.__dict__.update(object_class)
    #     # res0 = temp_class.setdefault(args1, args2)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/cache_get_passk_validte.py
from collections.abc import MutableMapping


class _DefaultSize(object):

    __slots__ = ()

    def __getitem__(self, _):
        return 1

    def __setitem__(self, _, value):
        assert value == 1

    def pop(self, _):
        return 1


class Cache(MutableMapping):
    """Mutable mapping to serve as a simple cache or cache base class."""

    __marker = object()

    __size = _DefaultSize()

    def __init__(self, maxsize, getsizeof=None):
        if getsizeof:
            self.getsizeof = getsizeof
        if self.getsizeof is not Cache.getsizeof:
            self.__size = dict()
        self.__data = dict()
        self.__currsize = 0
        self.__maxsize = maxsize

    def __repr__(self):
        return '%s(%r, maxsize=%r, currsize=%r)' % (
            self.__class__.__name__,
            list(self.__data.items()),
            self.__maxsize,
            self.__currsize,
        )

    def __getitem__(self, key):
        try:
            return self.__data[key]
        except KeyError:
            return self.__missing__(key)

    def __setitem__(self, key, value):
        maxsize = self.__maxsize
        size = self.getsizeof(value)
        if size > maxsize:
            raise ValueError('value too large')
        if key not in self.__data or self.__size[key] < size:
            while self.__currsize + size > maxsize:
                self.popitem()
        if key in self.__data:
            diffsize = size - self.__size[key]
        else:
            diffsize = size
        self.__data[key] = value
        self.__size[key] = size
        self.__currsize += diffsize

    def __delitem__(self, key):
        size = self.__size.pop(key)
        del self.__data[key]
        self.__currsize -= size

    def __contains__(self, key):
        return key in self.__data

    def __missing__(self, key):
        raise KeyError(key)

    def __iter__(self):
        return iter(self.__data)

    def __len__(self):
        return len(self.__data)

    def get(self, key, default=None):
        if key in self:
            return self[key]
        else:
            return default

    def pop(self, key, default=__marker):
        if key in self:
            value = self[key]
            del self[key]
        elif default is self.__marker:
            raise KeyError(key)
        else:
            value = default
        return value

    def setdefault(self, key, default=None):
        if key in self:
            value = self[key]
        else:
            self[key] = value = default
        return value

    @property
    def maxsize(self):
        """The maximum size of the cache."""
        return self.__maxsize

    @property
    def currsize(self):
        """The current size of the cache."""
        return self.__currsize

    @staticmethod
    def getsizeof(value):
        """Return the size of a cache element's value."""
        return 1

if __name__ == "__main__":
    isT = True
    try:
        cache=Cache(10000)
        cache.__setitem__(4, 10)
        cache.__setitem__(5, "five")
        cache.__setitem__(6, 600)
        res1=cache.get(4,None)
        res2=cache.get(5, None)
        res3=cache.get(5, 25)
        res4=cache.get(6, 10)
        res5=cache.get(7, 11)
        res6=cache.get(8, 20)
        if res1!=10 or res2 !="five" or res3!="five" or res4!=600 or res5!=11 or res6!=20:
            isT=False
    except:
        isT=False
    # for l in os.listdir(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos\pexip---os-python-cachetools\\data_passk_platform1/62b8d22948ba5a41d1c3f47c/"):
    #     f = open(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos\pexip---os-python-cachetools/data_passk_platform1/62b8d22948ba5a41d1c3f47c/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     print(args1)
    #     print(args2)
    #     print("---------")
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     # temp_class = Cache(10000)
    #     # temp_class.__dict__.update(object_class)
    #     # res0 = temp_class.get(args1, args2)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/decorators_cachedmethod_passk_validte.py
import functools
import operator

import cachetools
from cachetools.keys import hashkey


def cached(cache, key=hashkey, lock=None):
    """Decorator to wrap a function with a memoizing callable that saves
    results in a cache.

    """
    def decorator(func):
        if cache is None:
            def wrapper(*args, **kwargs):
                return func(*args, **kwargs)
        elif lock is None:
            def wrapper(*args, **kwargs):
                k = key(*args, **kwargs)
                try:
                    return cache[k]
                except KeyError:
                    pass  # key not found
                v = func(*args, **kwargs)
                try:
                    cache[k] = v
                except ValueError:
                    pass  # value too large
                return v
        else:
            def wrapper(*args, **kwargs):
                k = key(*args, **kwargs)
                try:
                    with lock:
                        return cache[k]
                except KeyError:
                    pass  # key not found
                v = func(*args, **kwargs)
                # in case of a race, prefer the item already in the cache
                try:
                    with lock:
                        return cache.setdefault(k, v)
                except ValueError:
                    return v  # value too large
        return functools.update_wrapper(wrapper, func)
    return decorator


def cachedmethod(cache, key=hashkey, lock=None):
    """Decorator to wrap a class or instance method with a memoizing
    callable that saves results in a cache.

    """
    def decorator(method):
        if lock is None:
            def wrapper(self, *args, **kwargs):
                c = cache(self)
                if c is None:
                    return method(self, *args, **kwargs)
                k = key(*args, **kwargs)
                try:
                    return c[k]
                except KeyError:
                    pass  # key not found
                v = method(self, *args, **kwargs)
                try:
                    c[k] = v
                except ValueError:
                    pass  # value too large
                return v
        else:
            def wrapper(self, *args, **kwargs):
                c = cache(self)
                if c is None:
                    return method(self, *args, **kwargs)
                k = key(*args, **kwargs)
                try:
                    with lock(self):
                        return c[k]
                except KeyError:
                    pass  # key not found
                v = method(self, *args, **kwargs)
                # in case of a race, prefer the item already in the cache
                try:
                    with lock(self):
                        return c.setdefault(k, v)
                except ValueError:
                    return v  # value too large
        return functools.update_wrapper(wrapper, method)
    return decorator
class Cached(object):

    def __init__(self, cache, count=0):
        self.cache = cache
        self.count = count

    @cachedmethod(operator.attrgetter('cache'))
    def get(self, value):
        count = self.count
        self.count += 1
        return count

    @cachedmethod(operator.attrgetter('cache'), key=cachetools.keys.typedkey)
    def get_typed(self, value):
        count = self.count
        self.count += 1
        return count

    # https://github.com/tkem/cachetools/issues/107
    def __hash__(self):
        raise TypeError('unhashable type')
if __name__ == "__main__":
    isT = True
    try:
        cached = Cached({})

        res1=cached.get(0)== 0
        res2=cached.get(1) ==1
        res3=cached.get(1) ==1
        res4=cached.get(1.0) ==1
        res5=cached.get(1.0) ==1

        cached.cache.clear()
        res6=cached.get(1)== 2
        if not res1 or not res2 or not res3 or not res4 or not res5 or not res6:
            isT = False
    except:
        isT = False
    # for l in os.listdir(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d22548ba5a41d1c3f472/"):
    #     f = open(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d22548ba5a41d1c3f472/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"], bytes):
    #         args0 = dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0 = content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     res0 = cachedmethod(args0, args1, args2)
    #
    #     if not (dill.dumps(args0) == dill.dumps(content["output"][0])):
    #         isT = False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/champax---pysolbase/pysolbase/SolBase_extostr_passk_validte.py
"""
# -*- coding: utf-8 -*-
# ===============================================================================
#
# Copyright (C) 2013/2017 Laurent Labatut / Laurent Champagnac
#
#
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA
# ===============================================================================
"""
import ast
import logging
import os
import platform
import sys
sys.path.append("..")

import time
import traceback
from logging.config import dictConfig
from logging.handlers import WatchedFileHandler, TimedRotatingFileHandler, SysLogHandler
from threading import Lock

import gevent
import pytz
from datetime import datetime

from gevent import monkey, config
from yaml import load, SafeLoader

import inspect
from ContextFilter import ContextFilter

logger = logging.getLogger(__name__)
lifecyclelogger = logging.getLogger("lifecycle")

class CrashMe(object):
    """
    Test
    """

    _lineException = -1

    @classmethod
    def crash(cls):
        """
        Crash
        """
        cls._lineException = inspect.currentframe().f_lineno + 1
        raise Exception("CrashException")

class SolBase(object):
    """
    Base utilities & helpers.
    """

    # ===============================
    # STATIC STUFF
    # ===============================

    # Component name (mainly for rsyslog)
    _compo_name = "CompoNotSet"

    # Global init stuff
    _voodoo_initialized = False
    _voodoo_lock = Lock()

    # Logging stuff
    _logging_initialized = False
    _logging_lock = Lock()

    # Fork stuff
    _master_process = True

    # ===============================
    # DATE & MS
    # ===============================

    @classmethod
    def mscurrent(cls):
        """
        Return current millis since epoch
        :return float
        :rtype float
        """
        return time.time() * 1000.0

    @classmethod
    def securrent(cls):
        """
        Return current seconds since epoch
        :return float
        :rtype float
        """
        return time.time()

    @classmethod
    def msdiff(cls, ms_start, ms_end=None):
        """
        Get difference in millis between current millis and provided millis.
        :param ms_start: Start millis
        :type ms_start: float
        :param ms_end: End millis (will use current if not provided)
        :type ms_end: float
        :return float
        :rtype float
        """

        if not ms_end:
            ms_end = cls.mscurrent()

        return ms_end - ms_start

    @classmethod
    def datecurrent(cls, erase_mode=0):
        """
        Return current date (UTC)
        :param erase_mode: Erase mode (0=nothing, 1=remove microseconds but keep millis, 2=remove millis completely)
        :return datetime.datetime
        :rtype datetime.datetime
        """

        if erase_mode == 0:
            return datetime.utcnow()
        elif erase_mode == 1:
            # Force precision loss (keep millis, kick micro)
            dt = datetime.utcnow()
            return dt.replace(microsecond=int((dt.microsecond * 0.001) * 1000))
        elif erase_mode == 2:
            return datetime.utcnow().replace(microsecond=0)

    @classmethod
    def datediff(cls, dt_start, dt_end=None):
        """
        Get difference in millis between two datetime
        :param dt_start: Start datetime
        :type dt_start: datetime.datetime
        :param dt_end: End datetime (will use current utc if not provided)
        :type dt_end: datetime.datetime
        :return float
        :rtype float
        """

        # Fix
        if not dt_end:
            dt_end = cls.datecurrent()

        # Get delta
        delta = dt_end - dt_start
        return ((delta.days * 86400 + delta.seconds) * 1000) + (delta.microseconds * 0.001)

    # ==========================================
    # EPOCH / DT
    # ==========================================

    DT_EPOCH = datetime.utcfromtimestamp(0)

    @classmethod
    def dt_to_epoch(cls, dt):
        """
        Convert a datetime (UTC required) to a unix time since epoch, as seconds, as integer.
        Note that millis precision is lost.
        :param dt: datetime
        :type dt: datetime
        :return int
        :rtype int
        """

        return int((dt - cls.DT_EPOCH).total_seconds())

    @classmethod
    def epoch_to_dt(cls, epoch):
        """
        Convert an epoch float or int to datetime (UTC)
        :param epoch: float,int
        :type epoch: float,int
        :return datetime
        :rtype datetime
        """

        return datetime.utcfromtimestamp(epoch)

    @classmethod
    def dt_is_naive(cls, dt):
        """
        Return true if dt is naive
        :param dt: datetime.datetime
        :type dt: datetime.datetime
        :return bool
        :rtype bool
        """

        # Naive : no tzinfo
        if not dt.tzinfo:
            return True

        # Aware
        return False

    @classmethod
    def dt_ensure_utc_aware(cls, dt):
        """
        Switch dt to utc time zone. If dt is naive, assume utc, otherwise, convert it to utc timezone.
        Return an AWARE timezone (utc switched) datetime,
        :param dt: datetime.datetime
        :type dt:datetime.datetime
        :return datetime.datetime
        :rtype datetime.datetime
        """

        # If naive, add utc
        if cls.dt_is_naive(dt):
            return dt.replace(tzinfo=pytz.utc)
        else:
            # Not naive, go utc, keep aware
            return dt.astimezone(pytz.utc)

    @classmethod
    def dt_ensure_utc_naive(cls, dt):
        """
        Ensure dt is naive. Return dt, switched to UTC (if applicable), and naive.
        :param dt: datetime.datetime
        :type dt:datetime.datetime
        :return datetime.datetime
        :rtype datetime.datetime
        """

        dt = cls.dt_ensure_utc_aware(dt)
        return dt.replace(tzinfo=None)

    # ===============================
    # COMPO NAME (FOR RSYSLOG)
    # ===============================

    @classmethod
    def set_compo_name(cls, compo_name):
        """
        Set the component name. Useful for rsyslog.
        :param compo_name: The component name or None. If None, method do nothing.
        :type compo_name: str,None
        """

        if compo_name:
            cls._compo_name = compo_name
            lifecyclelogger.debug("compo_name now set to=%s", cls._compo_name)

    @classmethod
    def get_compo_name(cls):
        """
        Get current component name.
        :return str
        :rtype str
        """

        return cls._compo_name

    @classmethod
    def get_machine_name(cls):
        """
        Get machine name
        :return: Machine name
        :rtype: str
        """

        return platform.uname()[1]

    # ===============================
    # MISC
    # ===============================

    @classmethod
    def sleep(cls, sleep_ms):
        """
        Sleep for specified ms.
        Also used as gevent context switch in code, since it rely on gevent.sleep.
        :param sleep_ms: Millis to sleep.
        :type sleep_ms: int
        :return Nothing.
        """
        ms = sleep_ms * 0.001

        # gevent 1.3 : ms is not fully respected (100 can be 80-100)
        gevent.sleep(ms)

    # ===============================
    # EXCEPTION HELPER
    # ===============================

    @classmethod
    def extostr(cls, e, max_level=30, max_path_level=5):
        """
        Format an exception.
        :param e: Any exception instance.
        :type e: Exception
        :param max_level: Maximum call stack level (default 30)
        :type max_level: int
        :param max_path_level: Maximum path level (default 5)
        :type max_path_level: int
        :return The exception readable string
        :rtype str
        """

        # Go
        list_frame = None
        try:
            out_buffer = ""

            # Class type
            out_buffer += "e.cls:[{0}]".format(e.__class__.__name__)

            # To string
            try:
                ex_buf = str(e)
            except UnicodeEncodeError:
                ex_buf = repr(str(e))
            except Exception as e:
                logger.warning("Exception, e=%s", e)
                raise
            out_buffer += ", e.bytes:[{0}]".format(ex_buf)

            # Traceback
            si = sys.exc_info()

            # Raw frame
            # tuple : (file, lineno, method, code)
            raw_frame = traceback.extract_tb(si[2])
            raw_frame.reverse()

            # Go to last tb_next
            last_tb_next = None
            cur_tb = si[2]
            while cur_tb:
                last_tb_next = cur_tb
                cur_tb = cur_tb.tb_next

            # Skip frame up to current raw frame count
            list_frame = list()
            cur_count = -1
            skip_count = len(raw_frame)
            if last_tb_next:
                cur_frame = last_tb_next.tb_frame
            else:
                cur_frame = None
            while cur_frame:
                cur_count += 1
                if cur_count < skip_count:
                    cur_frame = cur_frame.f_back
                else:
                    # Need : tuple : (file, lineno, method, code)
                    raw_frame.append((cur_frame.f_code.co_filename, cur_frame.f_lineno, cur_frame.f_code.co_name, ""))
                    cur_frame = cur_frame.f_back

            # Build it
            cur_idx = 0
            out_buffer += ", e.cs=["
            for tu in raw_frame:
                line = tu[1]
                cur_file = tu[0]
                method = tu[2]

                # Handle max path level
                ar_token = cur_file.rsplit(os.sep, max_path_level)
                if len(ar_token) > max_path_level:
                    # Remove head
                    ar_token.pop(0)
                    # Join
                    cur_file = "..." + os.sep.join(ar_token)

                # Format
                out_buffer += "in:{0}#{1}@{2} ".format(method, cur_file, line)

                # Loop
                cur_idx += 1
                if cur_idx >= max_level:
                    out_buffer += "..."
                    break

            # Close
            out_buffer += "]"

            # Ok
            return out_buffer
        finally:
            if list_frame:
                del list_frame

    # ===============================
    # VOODOO INIT
    # ===============================

    @classmethod
    def _reset(cls):
        """
        For unittest only
        """

        cls._logging_initialized = False
        cls._voodoo_initialized = False

    @classmethod
    def voodoo_init(cls, aggressive=True, init_logging=True):
        """
        Global initialization, to call asap.
        Apply gevent stuff & default logging configuration.
        :param aggressive: bool
        :type aggressive: bool
        :param init_logging: If True, logging_init is called.
        :type init_logging: bool
        :return Nothing.
        """

        try:
            # Check
            if cls._voodoo_initialized:
                return

            # Lock
            with cls._voodoo_lock:
                # Re-check
                if cls._voodoo_initialized:
                    return

                # Fire the voodoo magic :)
                lifecyclelogger.debug("Voodoo : gevent : entering, aggressive=%s", aggressive)
                monkey.patch_all(aggressive=aggressive)
                lifecyclelogger.debug("Voodoo : gevent : entering")

                # Gevent 1.3 : by default, gevent keep tracks of spawn call stack
                # This may lead to memory leak, if a method spawn itself in loop (timer mode)
                # We disable this
                config.track_greenlet_tree = False

                # Initialize log level to INFO
                if init_logging:
                    lifecyclelogger.debug("Voodoo : logging : entering")
                    cls.logging_init()
                    lifecyclelogger.debug("Voodoo : logging : done")

                # Done
                cls._voodoo_initialized = True
        finally:
            # If whenever init_logging if set AND it is NOT initialized => we must init it
            # => we may have been called previously with init_logging=false, but monkey patch is SET and logging not initialized
            # => so it must be init now
            if init_logging and not cls._logging_initialized:
                lifecyclelogger.debug("Voodoo : logging : not yet init : entering")
                cls.logging_init()
                lifecyclelogger.debug("Voodoo : logging : not yet init : done")

    # ===============================
    # LOGGING
    # ===============================

    @classmethod
    def logging_init(cls, log_level="INFO", force_reset=False, log_callback=None,
                     log_to_file=None,
                     log_to_syslog=True,
                     log_to_syslog_facility=SysLogHandler.LOG_LOCAL0,
                     log_to_console=True,
                     log_to_file_mode="watched_file",
                     context_filter=None):
        """
        Initialize logging sub system with default settings (console, pre-formatted output)
        :param log_to_console: if True to console
        :type log_to_console: bool
        :param log_level: The log level to set. Any value in "DEBUG", "INFO", "WARN", "ERROR", "CRITICAL"
        :type log_level: str
        :param force_reset: If true, logging system is reset.
        :type force_reset: bool
        :param log_to_file: If specified, log to file
        :type log_to_file: str,None
        :param log_to_syslog: If specified, log to syslog
        :type log_to_syslog: bool
        :param log_to_syslog_facility: Syslog facility.
        :type log_to_syslog_facility: int
        :param log_to_file_mode: str "watched_file" for WatchedFileHandler, "time_file" for TimedRotatingFileHandler (or time_file_seconds for unittest)
        :type log_to_file_mode: str
        :param log_callback: Callback for unittest
        :param context_filter: Context filter. If None, pysolbase.ContextFilter.ContextFilter is used. If used instance has an attr "filter", it is added to all handlers and "%(kfilter)s" will be populated by all thread context key/values, using filter method call. Refer to our ContextFilter default implementation for details.
        :type context_filter: None,object
        :return Nothing.
        """

        if cls._logging_initialized and not force_reset:
            return

        with cls._logging_lock:
            if cls._logging_initialized and not force_reset:
                return

            # Reset
            cls._reset_logging(log_level=log_level)

            # Default
            logging.basicConfig(level=log_level)

            # Filter
            if context_filter:
                c_filter = context_filter
            else:
                c_filter = ContextFilter()

            # Format begin
            s_f = "%(asctime)s | %(levelname)s | %(module)s@%(funcName)s@%(lineno)d | %(message)s "

            # Browse
            if hasattr(c_filter, "filter"):
                # Push generic field
                # We expect it to be formatted like our pysolbase.ContextFilter.ContextFilter#filter method.
                s_f += "|%(kfilter)s"

            # Format end
            s_f += "| %(thread)d:%(threadName)s | %(process)d:%(processName)s"

            # Formatter
            f = logging.Formatter(s_f)

            # Console handler
            c = None
            if log_to_console:
                # This can be overriden by unittest, we use __stdout__
                c = logging.StreamHandler(sys.__stdout__)
                c.setLevel(logging.getLevelName(log_level))
                c.setFormatter(f)

            # File handler to /tmp
            cf = None
            if log_to_file:
                if log_to_file_mode == "watched_file":
                    cf = WatchedFileHandler(log_to_file, encoding="utf-8")
                    cf.setLevel(logging.getLevelName(log_level))
                    cf.setFormatter(f)
                elif log_to_file_mode == "time_file":
                    cf = TimedRotatingFileHandler(log_to_file, encoding="utf-8", utc=True, when="D", interval=1, backupCount=7)
                    cf.setLevel(logging.getLevelName(log_level))
                    cf.setFormatter(f)
                elif log_to_file_mode == "time_file_seconds":
                    # For unittest only
                    cf = TimedRotatingFileHandler(log_to_file, encoding="utf-8", utc=True, when="S", interval=1, backupCount=7)
                    cf.setLevel(logging.getLevelName(log_level))
                    cf.setFormatter(f)
                else:
                    logger.warning("Invalid log_to_file_mode=%s", log_to_file_mode)

            # Syslog handler
            syslog = None
            if log_to_syslog:
                try:
                    from SysLogger import SysLogger

                    syslog = SysLogger(log_callback=log_callback, facility=log_to_syslog_facility)
                    syslog.setLevel(logging.getLevelName(log_level))
                    syslog.setFormatter(f)
                except Exception as e:
                    # This will fail on WINDOWS (no attr AF_UNIX)
                    logger.debug("Unable to import SysLogger, e=%s", SolBase.extostr(e))
                    syslog = False

            # Initialize
            root = logging.getLogger()
            root.setLevel(logging.getLevelName(log_level))
            root.handlers = []
            if log_to_console:
                c.addFilter(c_filter)
                root.addHandler(c)
            if log_to_file and cf:
                cf.addFilter(c_filter)
                root.addHandler(cf)
            if log_to_syslog and syslog:
                syslog.addFilter(c_filter)
                root.addHandler(syslog)

            # Done
            cls._logging_initialized = True
            if force_reset:
                lifecyclelogger.info("Logging : initialized from memory, log_level=%s, force_reset=%s", log_level, force_reset)
            else:
                lifecyclelogger.debug("Logging : initialized from memory, log_level=%s, force_reset=%s", log_level, force_reset)

    @classmethod
    def _register_filter(cls, c_filter):
        """
        Register filter across the whole logging (root and all loggers)
        Notice : addFilter is protected against duplicates add
        :param c_filter: pysolbase.ContextFilter.ContextFilter
        :type c_filter: pysolbase.ContextFilter.ContextFilter
        """

        # Initialize
        root = logging.getLogger()
        for h in list(root.handlers):
            h.addFilter(c_filter)

        # Browse all loggers and set
        for name in logging.root.manager.loggerDict:
            cur_logger = logging.getLogger(name)
            for h in list(cur_logger.handlers):
                h.addFilter(c_filter)

    @classmethod
    def _reset_logging(cls, log_level):
        """
        Reset
        :param log_level: str
        :type log_level: str
        """

        # Found no way to fully reset the logging stuff while running
        # We reset root and all loggers to INFO, and kick handlers

        # Initialize
        root = logging.getLogger()
        root.setLevel(logging.getLevelName(log_level))
        for h in root.handlers:
            # noinspection PyBroadException
            try:
                h.close()
            except:
                pass
        root.handlers = []

        # Browse all loggers and set
        for name in logging.root.manager.loggerDict:
            cur_logger = logging.getLogger(name)
            cur_logger.setLevel(logging.getLevelName(log_level))
            for h in cur_logger.handlers:
                # noinspection PyBroadException
                try:
                    h.close()
                except:
                    pass
            cur_logger.handlers = []

    @classmethod
    def logging_initfromfile(cls, config_file_name, force_reset=False, context_filter=None):
        """
        Initialize logging system from a configuration file, with optional reset.
        :param config_file_name: Configuration file name
        :type config_file_name: str
        :param force_reset: If true, logging system is reset.
        :type force_reset: bool
        :param context_filter: Context filter. If None, pysolbase.ContextFilter.ContextFilter is used. If used instance has an attr "filter", it is added to all handlers and "%(kfilter)s" will be populated by all thread context key/values, using filter method call. Refer to our ContextFilter default implementation for details.
        :type context_filter: None,object
        :return Nothing.
        """

        if cls._logging_initialized and not force_reset:
            return

        with cls._logging_lock:
            if cls._logging_initialized and not force_reset:
                return

            try:
                # Filter
                if context_filter:
                    c_filter = context_filter
                else:
                    c_filter = ContextFilter()

                # Reset
                cls._reset_logging(log_level="INFO")

                # Load
                logger.debug("Logging : yaml config_file_name=%s", config_file_name)
                with open(config_file_name, 'r') as f:
                    d = load(f, Loader=SafeLoader)
                    dictConfig(d)

                # Register filter
                if c_filter:
                    cls._register_filter(c_filter)

                if force_reset:
                    lifecyclelogger.info("Logging : initialized from yaml file, config_file_name=%s", config_file_name)
                else:
                    lifecyclelogger.debug("Logging : initialized from yaml file, config_file_name=%s", config_file_name)
            except Exception:
                raise

    @classmethod
    def context_set(cls, k, v):
        """
        Set thread/greenlet context value

        This is a wrapper to pysolbase.ContextFilter.ContextFilter#set_value
        and will work only if ContextFilter is defined (which is by default)
        :param k: key name
        :type k: basestring
        :param v: value
        :type v: object
        """

        ContextFilter.set_value(k, v)

    # ===============================
    # FORK STUFF
    # ===============================

    @classmethod
    def get_master_process(cls):
        """
        Return True if we are the master process, False otherwise.
        :return bool
        :rtype bool
        """
        return cls._master_process

    @classmethod
    def set_master_process(cls, b):
        """
        Set is we are a fork master or not
        :param b: True if we are master process, False if we are a child process.
        :type b: bool
        :return Nothing
        """

        logger.debug("Switching _masterProcess to %s", b)
        cls._master_process = b

    # ===============================
    # BINARY STUFF
    # ===============================

    @classmethod
    def binary_to_unicode(cls, bin_buf, encoding="utf-8"):
        """
        Binary buffer to str, using the specified encoding
        :param bin_buf: Binary buffer
        :type bin_buf: bytes
        :param encoding: Encoding to use
        :type encoding: str
        :return str
        :rtype str
        """

        return bin_buf.decode(encoding)

    @classmethod
    def unicode_to_binary(cls, unicode_buf, encoding="utf-8"):
        """
        Unicode to binary buffer, using the specified encoding
        :param unicode_buf: String to convert.
        :type unicode_buf: str
        :param encoding: Encoding to use.
        :type encoding: str
        :return bytes
        :rtype bytes
        """

        return unicode_buf.encode(encoding)

    @classmethod
    def fix_paths_for_popen(cls):
        """
        Fix path and env for popen calls toward current project
        Mainly used for unittests, which requires current env to be propagated while testing command line invocation within same project
        """

        # Merge all
        ar_p_path = sys.path
        if os.environ.get("PYTHONPATH"):
            ar_p_path.extend(os.environ.get("PYTHONPATH").split(":"))
        if os.environ.get("PATH"):
            ar_p_path.extend(os.environ.get("PATH").split(":"))

        # Join
        new_path = ":".join(ar_p_path)

        # Re-Assign
        os.environ["PATH"] = new_path
        os.environ["PYTHONPATH"] = new_path

    # ===============================
    # CONVERSIONS
    # ===============================

    @classmethod
    def to_int(cls, v):
        """
        Convert to int
        :param v: int,str
        :type v: int,str
        :return: int
        :rtype int
        """

        if isinstance(v, int):
            return v
        else:
            return int(v)

    @classmethod
    def to_bool(cls, v):
        """
        Convert to bool
        :param v: bool,str
        :type v: bool,str
        :return: bool
        :rtype bool
        """

        if isinstance(v, bool):
            return v
        else:
            return ast.literal_eval(v)

    @classmethod
    def get_classname(cls, my_instance):
        """
        Return the class name of my_instance, or "Instance.None".
        :param cls: Our class.
        :param my_instance: Instance to use.
        :return: Return the class name of my_instance, or "Instance.None" in case of error/None value.
        """
        if my_instance is None:
            return "Instance.None"
        else:
            return my_instance.__class__.__name__

    @classmethod
    def get_pathseparator(cls):
        """
        Return the path separator.
        https://docs.python.org/library/os.html#os.sep
        :param cls: Our class
        :return: The path separator (string)
        """
        return os.sep

    @classmethod
    def is_bool(cls, my_bool):
        """
        Return true if the provided my_bool is a boolean.
        :param cls: Our class.
        :param my_bool: A boolean..
        :return: Return true if the provided my_bool is a boolean. False otherwise.
        """
        if my_bool is None:
            return False
        else:
            return isinstance(my_bool, bool)

    @classmethod
    def is_int(cls, my_int):
        """
        Return true if the provided my_int is a integer.
        :param cls: Our class.
        :param my_int: An integer..
        :return: Return true if the provided my_int is a integer. False otherwise.
        """
        if my_int is None:
            return False
        # Caution, boolean is an integer...
        elif SolBase.is_bool(my_int):
            return False
        else:
            return isinstance(my_int, int)

    @classmethod
    def get_current_pid_as_string(cls):
        """
        Return the current pids as string.
        :param cls: Our class.
        :return: A String
        """
        try:
            return "pid={0}, ppid={1}".format(os.getpid(), os.getppid())
        except AttributeError:
            return "pid={0}".format(os.getpid())

    # =====================================================
    # HELPER FOR SOCKET CLOSING
    # =====================================================

    @classmethod
    def safe_close_socket(cls, soc_to_close):
        """
        Safe close a socket
        :param soc_to_close: socket
        :type soc_to_close: socket.socket
        """

        if soc_to_close is None:
            return

        try:
            soc_to_close.shutdown(2)
        except Exception as e:
            logger.debug("Socket shutdown ex=%s", SolBase.extostr(e))

        try:
            soc_to_close.close()
        except Exception as e:
            logger.debug("Socket close ex=%s", SolBase.extostr(e))

        try:
            del soc_to_close
        except Exception as e:
            logger.debug("Socket del ex=%s", SolBase.extostr(e))

if __name__ == "__main__":
    isT=True

    local_line_exception = inspect.currentframe().f_lineno + 2
    try:
        CrashMe.crash()
    except Exception as e:
        # Convert
        buf = SolBase.extostr(e)

        # Log
        print("e=%s" % buf)

        if buf.find("e.cls:[Exception]")< 0:
            isT=False
        if buf.find("e.bytes:[CrashException]")< 0:
            isT=False
        seek_buf = os.path.join("pysolbase", "SolBase_extostr_passk_validte.py@") + str(CrashMe._lineException) + " "
        if buf.find(seek_buf) < 0:
            isT=False
        seek_buf = os.path.join("pysolbase", "SolBase_extostr_passk_validte.py@") + str(local_line_exception) + " "
        if buf.find(seek_buf) < 0:
            isT=False
    # import dill
    # import os
    #
    # isT = True
    # for l in os.listdir("/home/travis/builds/repos/champax---pysolbase/data_passk_platform/62b8c517e0d34b282c18122e/"):
    #     f = open("/home/travis/builds/repos/champax---pysolbase/data_passk_platform/62b8c517e0d34b282c18122e/" + l,
    #              "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     if isinstance(content["input"]["args"][3]["bytes"], bytes):
    #         args3 = dill.loads(content["input"]["args"][3]["bytes"])
    #     else:
    #         args3 = content["input"]["args"][3]["bytes"]
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     # temp_class = SolBase()
    #     # temp_class.__dict__.update(object_class)
    #     res0 = object_class.extostr(args1, args2, args3)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/champax---pysolbase/pysolbase/FileUtility_append_text_to_file_passk_validte.py
"""
# -*- coding: utf-8 -*-
# ===============================================================================
#
# Copyright (C) 2013/2017 Laurent Labatut / Laurent Champagnac
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA
# ===============================================================================
"""

# Import
import logging

import os
import codecs
import sys
sys.path.append("..")

from SolBase import SolBase

logger = logging.getLogger(__name__)


class FileUtility(object):
    """
    File utility
    """

    @staticmethod
    def is_path_exist(path_name):
        """
        Check if a path (file or dir) name exist.
        :param path_name: Path name.
        :type path_name text_type
        :return: Return true (exist), false (do not exist, or invalid file name)
        :rtype bool
        """

        # Check
        if path_name is None:
            logger.warning("is_path_exist : file_name is None")
            return False
        elif not isinstance(path_name, str):
            logger.warning("is_path_exist : path_name not a text_type, className=%s", SolBase.get_classname(path_name))
            return False

        # Go
        return os.path.exists(path_name)

    @staticmethod
    def is_file_exist(file_name):
        """
        Check if file name exist.
        :param file_name: File name.
        :type file_name: str
        :return: Return true (exist), false (do not exist, or invalid file name)
        :rtype bool
        """

        # Check
        if file_name is None:
            logger.warning("is_file_exist : file_name is None")
            return False
        elif not isinstance(file_name, str):
            logger.warning("is_file_exist : file_name not a text_type, className=%s", SolBase.get_classname(file_name))
            return False

        # Go
        return os.path.isfile(file_name)

    @staticmethod
    def is_dir_exist(dir_name):
        """
        Check if dir name exist.
        :param dir_name: Directory name.
        :type dir_name: str
        :return: Return true (exist), false (do not exist, or invalid file name)
        :rtype bool
        """

        # Check
        if dir_name is None:
            logger.warning("is_dir_exist : file_name is None")
            return False
        elif not isinstance(dir_name, str):
            logger.warning("is_dir_exist : file_name not a text_type, className=%s", SolBase.get_classname(dir_name))
            return False

        # Go
        return os.path.isdir(dir_name)

    @staticmethod
    def get_file_size(file_name):
        """
        Return a file size in bytes.
        :param file_name: File name.
        :type file_name: str
        :return: An integer, gt-eq 0 if file exist, lt 0 if error.
        :rtype int
        """
        if not FileUtility.is_file_exist(file_name):
            return -1
        else:
            return os.path.getsize(file_name)

    @classmethod
    def get_current_dir(cls):
        """
        Return the current directory.
        :return: A String
        :rtype text_type
        """

        return os.getcwd()

    @staticmethod
    def file_to_binary(file_name):
        """
        Load a file toward a binary buffer.
        :param file_name: File name.
        :type file_name: str
        :return: Return the binary buffer or None in case of error.
        :rtype: bytes,None
        """

        # Check
        if not FileUtility.is_file_exist(file_name):
            logger.warning("file_to_binary : file_name not exist, file_name=%s", file_name)
            return None

        # Go
        rd = None
        try:
            # Open (binary : open return a io.BufferedReader)
            rd = open(file_name, "rb")

            # Read everything
            return rd.read()
        except IOError as e:
            # Exception...
            logger.warning("IOError, ex=%s", SolBase.extostr(e))
            return None
        except Exception as e:
            logger.warning("Exception, ex=%s", SolBase.extostr(e))
            return None
        finally:
            # Close if not None...
            if rd:
                rd.close()

    @staticmethod
    def file_to_textbuffer(file_name, encoding):
        """
        Load a file toward a text buffer (UTF-8), using the specify encoding while reading.
        CAUTION : This will read the whole file IN MEMORY.
        :param file_name: File name.
        :type file_name: str
        :param encoding: Encoding to use.
        :type encoding: str
        :return: A text buffer or None in case of error.
        :rtype str
        """

        # Check
        if not FileUtility.is_file_exist(file_name):
            logger.warning("file_to_textbuffer : file_name not exist, file_name=%s", file_name)
            return None

        # Go
        rd = None
        try:
            # Open (text : open return a io.BufferedReader)
            rd = codecs.open(file_name, "r", encoding, "strict", -1)

            # Read everything
            return rd.read()
        except IOError as e:
            # Exception...
            logger.warning("file_to_binary : IOError, ex=%s", SolBase.extostr(e))
            return None
        except Exception as e:
            logger.warning("file_to_binary : Exception, ex=%s", SolBase.extostr(e))
            return None
        finally:
            # Close if not None...
            if rd:
                rd.close()

    @staticmethod
    def append_binary_to_file(file_name, bin_buf):
        """
        Write to the specified filename, the provided binary buffer.
        Create the file if required.
        :param file_name:  File name.
        :type file_name: str
        :param bin_buf: Binary buffer to write.
        :type bin_buf: bytes
        :return: The number of bytes written or lt 0 if error.
        :rtype int
        """

        # Go
        rd = None
        try:
            # Open (text : open return a io.BufferedReader)
            rd = open(file_name, "ab+")

            # Read everything
            return rd.write(bin_buf)
        except IOError as e:
            # Exception...
            logger.warning("append_binary_to_file : IOError, ex=%s", SolBase.extostr(e))
            return -1
        except Exception as e:
            logger.warning("append_binary_to_file : Exception, ex=%s", SolBase.extostr(e))
            return -1
        finally:
            # Close if not None...
            if rd:
                rd.close()

    @staticmethod
    def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):
        """
        Write to the specified filename, the provided binary buffer
        Create the file if required.
        :param file_name:  File name.
        :type file_name: str
        :param text_buffer: Text buffer to write.
        :type text_buffer: str
        :param encoding: The encoding to use.
        :type encoding: str
        :param overwrite: If true, file is overwritten.
        :type overwrite: bool
        :return: The number of bytes written or lt 0 if error.
        :rtype int
        """

        # Go
        rd = None
        try:
            # Open (text : open return a io.BufferedReader)
            if not overwrite:
                rd = codecs.open(file_name, "a+", encoding, "strict", -1)
            else:
                rd = codecs.open(file_name, "w", encoding, "strict", -1)

            # Read everything
            # CAUTION : 2.7 return None :(
            return rd.write(text_buffer)
        except IOError as e:
            # Exception...
            logger.warning("append_text_to_file : IOError, ex=%s", SolBase.extostr(e))
            return -1
        except Exception as e:
            logger.warning("append_text_to_file : Exception, ex=%s", SolBase.extostr(e))
            return -1
        finally:
            # Close if not None...
            if rd:
                rd.close()

if __name__ == "__main__":
    isT=True

    log_file = "./pythonsol_unittest.log"
    if FileUtility.is_file_exist(log_file):
        os.remove(log_file)

    FileUtility.append_text_to_file(log_file, "TOTO\n", "utf-8", overwrite=False)
    buf = FileUtility.file_to_textbuffer(log_file, "utf-8")

    if not buf:
        isT=False
    if buf.find("TOTO")< 0:
        isT=False

    # except:
    #     print(4)
    #     isT=False
    # import dill
    # import os
    #
    # isT = True
    # for l in os.listdir("/home/travis/builds/repos/champax---pysolbase/data_passk_platform/62b8bbbfe0d34b282c181210/"):
    #     f = open("/home/travis/builds/repos/champax---pysolbase/data_passk_platform/62b8bbbfe0d34b282c181210/" + l,
    #              "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"], bytes):
    #         args0 = dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0 = content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     if isinstance(content["input"]["args"][3]["bytes"], bytes):
    #         args3 = dill.loads(content["input"]["args"][3]["bytes"])
    #     else:
    #         args3 = content["input"]["args"][3]["bytes"]
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     # print(type(object_class))
    #     temp_class = FileUtility()
    #     # temp_class.__dict__.update(object_class)
    #     res0 = temp_class.append_text_to_file(args0, args1, args2,args3)
    #     # print(res0)
    #     # print(content["output"][0])
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/champax---pysolbase/pysolbase/FileUtility_file_to_textbuffer_passk_validte.py
"""
# -*- coding: utf-8 -*-
# ===============================================================================
#
# Copyright (C) 2013/2017 Laurent Labatut / Laurent Champagnac
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA
# ===============================================================================
"""

# Import
import logging

import os
import codecs
import sys
sys.path.append("..")

from SolBase import SolBase

logger = logging.getLogger(__name__)


class FileUtility(object):
    """
    File utility
    """

    @staticmethod
    def is_path_exist(path_name):
        """
        Check if a path (file or dir) name exist.
        :param path_name: Path name.
        :type path_name text_type
        :return: Return true (exist), false (do not exist, or invalid file name)
        :rtype bool
        """

        # Check
        if path_name is None:
            logger.warning("is_path_exist : file_name is None")
            return False
        elif not isinstance(path_name, str):
            logger.warning("is_path_exist : path_name not a text_type, className=%s", SolBase.get_classname(path_name))
            return False

        # Go
        return os.path.exists(path_name)

    @staticmethod
    def is_file_exist(file_name):
        """
        Check if file name exist.
        :param file_name: File name.
        :type file_name: str
        :return: Return true (exist), false (do not exist, or invalid file name)
        :rtype bool
        """

        # Check
        if file_name is None:
            logger.warning("is_file_exist : file_name is None")
            return False
        elif not isinstance(file_name, str):
            logger.warning("is_file_exist : file_name not a text_type, className=%s", SolBase.get_classname(file_name))
            return False

        # Go
        return os.path.isfile(file_name)

    @staticmethod
    def is_dir_exist(dir_name):
        """
        Check if dir name exist.
        :param dir_name: Directory name.
        :type dir_name: str
        :return: Return true (exist), false (do not exist, or invalid file name)
        :rtype bool
        """

        # Check
        if dir_name is None:
            logger.warning("is_dir_exist : file_name is None")
            return False
        elif not isinstance(dir_name, str):
            logger.warning("is_dir_exist : file_name not a text_type, className=%s", SolBase.get_classname(dir_name))
            return False

        # Go
        return os.path.isdir(dir_name)

    @staticmethod
    def get_file_size(file_name):
        """
        Return a file size in bytes.
        :param file_name: File name.
        :type file_name: str
        :return: An integer, gt-eq 0 if file exist, lt 0 if error.
        :rtype int
        """
        if not FileUtility.is_file_exist(file_name):
            return -1
        else:
            return os.path.getsize(file_name)

    @classmethod
    def get_current_dir(cls):
        """
        Return the current directory.
        :return: A String
        :rtype text_type
        """

        return os.getcwd()

    @staticmethod
    def file_to_binary(file_name):
        """
        Load a file toward a binary buffer.
        :param file_name: File name.
        :type file_name: str
        :return: Return the binary buffer or None in case of error.
        :rtype: bytes,None
        """

        # Check
        if not FileUtility.is_file_exist(file_name):
            logger.warning("file_to_binary : file_name not exist, file_name=%s", file_name)
            return None

        # Go
        rd = None
        try:
            # Open (binary : open return a io.BufferedReader)
            rd = open(file_name, "rb")

            # Read everything
            return rd.read()
        except IOError as e:
            # Exception...
            logger.warning("IOError, ex=%s", SolBase.extostr(e))
            return None
        except Exception as e:
            logger.warning("Exception, ex=%s", SolBase.extostr(e))
            return None
        finally:
            # Close if not None...
            if rd:
                rd.close()

    @staticmethod
    def file_to_textbuffer(file_name, encoding):
        """
        Load a file toward a text buffer (UTF-8), using the specify encoding while reading.
        CAUTION : This will read the whole file IN MEMORY.
        :param file_name: File name.
        :type file_name: str
        :param encoding: Encoding to use.
        :type encoding: str
        :return: A text buffer or None in case of error.
        :rtype str
        """

        # Check
        if not FileUtility.is_file_exist(file_name):
            logger.warning("file_to_textbuffer : file_name not exist, file_name=%s", file_name)
            return None

        # Go
        rd = None
        try:
            # Open (text : open return a io.BufferedReader)
            rd = codecs.open(file_name, "r", encoding, "strict", -1)

            # Read everything
            return rd.read()
        except IOError as e:
            # Exception...
            logger.warning("file_to_binary : IOError, ex=%s", SolBase.extostr(e))
            return None
        except Exception as e:
            logger.warning("file_to_binary : Exception, ex=%s", SolBase.extostr(e))
            return None
        finally:
            # Close if not None...
            if rd:
                rd.close()

    @staticmethod
    def append_binary_to_file(file_name, bin_buf):
        """
        Write to the specified filename, the provided binary buffer.
        Create the file if required.
        :param file_name:  File name.
        :type file_name: str
        :param bin_buf: Binary buffer to write.
        :type bin_buf: bytes
        :return: The number of bytes written or lt 0 if error.
        :rtype int
        """

        # Go
        rd = None
        try:
            # Open (text : open return a io.BufferedReader)
            rd = open(file_name, "ab+")

            # Read everything
            return rd.write(bin_buf)
        except IOError as e:
            # Exception...
            logger.warning("append_binary_to_file : IOError, ex=%s", SolBase.extostr(e))
            return -1
        except Exception as e:
            logger.warning("append_binary_to_file : Exception, ex=%s", SolBase.extostr(e))
            return -1
        finally:
            # Close if not None...
            if rd:
                rd.close()

    @staticmethod
    def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):
        """
        Write to the specified filename, the provided binary buffer
        Create the file if required.
        :param file_name:  File name.
        :type file_name: str
        :param text_buffer: Text buffer to write.
        :type text_buffer: str
        :param encoding: The encoding to use.
        :type encoding: str
        :param overwrite: If true, file is overwritten.
        :type overwrite: bool
        :return: The number of bytes written or lt 0 if error.
        :rtype int
        """

        # Go
        rd = None
        try:
            # Open (text : open return a io.BufferedReader)
            if not overwrite:
                rd = codecs.open(file_name, "a+", encoding, "strict", -1)
            else:
                rd = codecs.open(file_name, "w", encoding, "strict", -1)

            # Read everything
            # CAUTION : 2.7 return None :(
            return rd.write(text_buffer)
        except IOError as e:
            # Exception...
            logger.warning("append_text_to_file : IOError, ex=%s", SolBase.extostr(e))
            return -1
        except Exception as e:
            logger.warning("append_text_to_file : Exception, ex=%s", SolBase.extostr(e))
            return -1
        finally:
            # Close if not None...
            if rd:
                rd.close()

if __name__ == "__main__":
    isT = True
    log_file = './pythonsol_unittest.log'

    if FileUtility.is_file_exist(log_file):
        os.remove(log_file)

    # Init
    SolBase.logging_init(log_level="INFO",
                         log_to_file=log_file,
                         log_to_console=True,
                         log_to_syslog=False,
                         force_reset=True)
    SolBase.set_compo_name("COMPO_XXX")

    # Emit a log
    logger.info("TEST LOG 888")

    # Emit a log (str)
    logger.info(u"BUF \u001B\u0BD9\U0001A10D\u1501\xc3 FUB")

    # Check the file
    buf = FileUtility.file_to_textbuffer(log_file, "utf-8")

    if buf is None:
        isT=False
    if buf.find("TEST LOG 888") < 0:
        isT=False

    if buf.find("BUF ")< 0 or\
    buf.find(" FUB")< 0 or\
    buf.find(u"BUF \u001B\u0BD9\U0001A10D\u1501\xc3 FUB") < 0:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/champax---pysolbase/pysolbase/FileUtility_is_file_exist_passk_validte.py
"""
# -*- coding: utf-8 -*-
# ===============================================================================
#
# Copyright (C) 2013/2017 Laurent Labatut / Laurent Champagnac
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA
# ===============================================================================
"""

# Import
import logging

import os
import codecs
import sys
sys.path.append("..")

from SolBase import SolBase

logger = logging.getLogger(__name__)


class FileUtility(object):
    """
    File utility
    """

    @staticmethod
    def is_path_exist(path_name):
        """
        Check if a path (file or dir) name exist.
        :param path_name: Path name.
        :type path_name text_type
        :return: Return true (exist), false (do not exist, or invalid file name)
        :rtype bool
        """

        # Check
        if path_name is None:
            logger.warning("is_path_exist : file_name is None")
            return False
        elif not isinstance(path_name, str):
            logger.warning("is_path_exist : path_name not a text_type, className=%s", SolBase.get_classname(path_name))
            return False

        # Go
        return os.path.exists(path_name)

    @staticmethod
    def is_file_exist(file_name):
        """
        Check if file name exist.
        :param file_name: File name.
        :type file_name: str
        :return: Return true (exist), false (do not exist, or invalid file name)
        :rtype bool
        """

        # Check
        if file_name is None:
            logger.warning("is_file_exist : file_name is None")
            return False
        elif not isinstance(file_name, str):
            logger.warning("is_file_exist : file_name not a text_type, className=%s", SolBase.get_classname(file_name))
            return False

        # Go
        return os.path.isfile(file_name)

    @staticmethod
    def is_dir_exist(dir_name):
        """
        Check if dir name exist.
        :param dir_name: Directory name.
        :type dir_name: str
        :return: Return true (exist), false (do not exist, or invalid file name)
        :rtype bool
        """

        # Check
        if dir_name is None:
            logger.warning("is_dir_exist : file_name is None")
            return False
        elif not isinstance(dir_name, str):
            logger.warning("is_dir_exist : file_name not a text_type, className=%s", SolBase.get_classname(dir_name))
            return False

        # Go
        return os.path.isdir(dir_name)

    @staticmethod
    def get_file_size(file_name):
        """
        Return a file size in bytes.
        :param file_name: File name.
        :type file_name: str
        :return: An integer, gt-eq 0 if file exist, lt 0 if error.
        :rtype int
        """
        if not FileUtility.is_file_exist(file_name):
            return -1
        else:
            return os.path.getsize(file_name)

    @classmethod
    def get_current_dir(cls):
        """
        Return the current directory.
        :return: A String
        :rtype text_type
        """

        return os.getcwd()

    @staticmethod
    def file_to_binary(file_name):
        """
        Load a file toward a binary buffer.
        :param file_name: File name.
        :type file_name: str
        :return: Return the binary buffer or None in case of error.
        :rtype: bytes,None
        """

        # Check
        if not FileUtility.is_file_exist(file_name):
            logger.warning("file_to_binary : file_name not exist, file_name=%s", file_name)
            return None

        # Go
        rd = None
        try:
            # Open (binary : open return a io.BufferedReader)
            rd = open(file_name, "rb")

            # Read everything
            return rd.read()
        except IOError as e:
            # Exception...
            logger.warning("IOError, ex=%s", SolBase.extostr(e))
            return None
        except Exception as e:
            logger.warning("Exception, ex=%s", SolBase.extostr(e))
            return None
        finally:
            # Close if not None...
            if rd:
                rd.close()

    @staticmethod
    def file_to_textbuffer(file_name, encoding):
        """
        Load a file toward a text buffer (UTF-8), using the specify encoding while reading.
        CAUTION : This will read the whole file IN MEMORY.
        :param file_name: File name.
        :type file_name: str
        :param encoding: Encoding to use.
        :type encoding: str
        :return: A text buffer or None in case of error.
        :rtype str
        """

        # Check
        if not FileUtility.is_file_exist(file_name):
            logger.warning("file_to_textbuffer : file_name not exist, file_name=%s", file_name)
            return None

        # Go
        rd = None
        try:
            # Open (text : open return a io.BufferedReader)
            rd = codecs.open(file_name, "r", encoding, "strict", -1)

            # Read everything
            return rd.read()
        except IOError as e:
            # Exception...
            logger.warning("file_to_binary : IOError, ex=%s", SolBase.extostr(e))
            return None
        except Exception as e:
            logger.warning("file_to_binary : Exception, ex=%s", SolBase.extostr(e))
            return None
        finally:
            # Close if not None...
            if rd:
                rd.close()

    @staticmethod
    def append_binary_to_file(file_name, bin_buf):
        """
        Write to the specified filename, the provided binary buffer.
        Create the file if required.
        :param file_name:  File name.
        :type file_name: str
        :param bin_buf: Binary buffer to write.
        :type bin_buf: bytes
        :return: The number of bytes written or lt 0 if error.
        :rtype int
        """

        # Go
        rd = None
        try:
            # Open (text : open return a io.BufferedReader)
            rd = open(file_name, "ab+")

            # Read everything
            return rd.write(bin_buf)
        except IOError as e:
            # Exception...
            logger.warning("append_binary_to_file : IOError, ex=%s", SolBase.extostr(e))
            return -1
        except Exception as e:
            logger.warning("append_binary_to_file : Exception, ex=%s", SolBase.extostr(e))
            return -1
        finally:
            # Close if not None...
            if rd:
                rd.close()

    @staticmethod
    def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):
        """
        Write to the specified filename, the provided binary buffer
        Create the file if required.
        :param file_name:  File name.
        :type file_name: str
        :param text_buffer: Text buffer to write.
        :type text_buffer: str
        :param encoding: The encoding to use.
        :type encoding: str
        :param overwrite: If true, file is overwritten.
        :type overwrite: bool
        :return: The number of bytes written or lt 0 if error.
        :rtype int
        """

        # Go
        rd = None
        try:
            # Open (text : open return a io.BufferedReader)
            if not overwrite:
                rd = codecs.open(file_name, "a+", encoding, "strict", -1)
            else:
                rd = codecs.open(file_name, "w", encoding, "strict", -1)

            # Read everything
            # CAUTION : 2.7 return None :(
            return rd.write(text_buffer)
        except IOError as e:
            # Exception...
            logger.warning("append_text_to_file : IOError, ex=%s", SolBase.extostr(e))
            return -1
        except Exception as e:
            logger.warning("append_text_to_file : Exception, ex=%s", SolBase.extostr(e))
            return -1
        finally:
            # Close if not None...
            if rd:
                rd.close()

if __name__ == "__main__":
    isT=True
    log_file = "./pythonsol_unittest.log"

    # Clean
    if FileUtility.is_file_exist(log_file):
        os.remove(log_file)

    buf = FileUtility.file_to_textbuffer(log_file, "utf-8")
    if buf:
        isT=False

    logging.basicConfig(filename=log_file)
    logging.info('TEST LOG 888')

    if not FileUtility.is_file_exist(log_file):
        isT=False
# import dill
    # import os
    #
    # isT = True
    # for l in os.listdir("/home/travis/builds/repos/champax---pysolbase/data_passk_platform/62b8bbbce0d34b282c18120d/"):
    #     f = open("/home/travis/builds/repos/champax---pysolbase/data_passk_platform/62b8bbbce0d34b282c18120d/" + l,
    #              "rb")
    #     content = dill.load(f)
    #     f.close()
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = FileUtility()
    #     # temp_class.__dict__.update(object_class)
    #     if isinstance(content["input"]["args"][0]["bytes"], bytes):
    #         args0 = dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0 = content["input"]["args"][0]["bytes"]
    #     res0 = temp_class.is_file_exist(args0)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/champax---pysolbase/pysolbase/SolBase__reset_logging_passk_validte.py
"""
# -*- coding: utf-8 -*-
# ===============================================================================
#
# Copyright (C) 2013/2017 Laurent Labatut / Laurent Champagnac
#
#
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA
# ===============================================================================
"""
import ast
import logging
import os
import platform
import sys
sys.path.append("..")

import time
import traceback
from logging.config import dictConfig
from logging.handlers import WatchedFileHandler, TimedRotatingFileHandler, SysLogHandler
from threading import Lock

import gevent
import pytz
from datetime import datetime

from gevent import monkey, config
from yaml import load, SafeLoader

from ContextFilter import ContextFilter

logger = logging.getLogger(__name__)
lifecyclelogger = logging.getLogger("lifecycle")


class SolBase(object):
    """
    Base utilities & helpers.
    """

    # ===============================
    # STATIC STUFF
    # ===============================

    # Component name (mainly for rsyslog)
    _compo_name = "CompoNotSet"

    # Global init stuff
    _voodoo_initialized = False
    _voodoo_lock = Lock()

    # Logging stuff
    _logging_initialized = False
    _logging_lock = Lock()

    # Fork stuff
    _master_process = True

    # ===============================
    # DATE & MS
    # ===============================

    @classmethod
    def mscurrent(cls):
        """
        Return current millis since epoch
        :return float
        :rtype float
        """
        return time.time() * 1000.0

    @classmethod
    def securrent(cls):
        """
        Return current seconds since epoch
        :return float
        :rtype float
        """
        return time.time()

    @classmethod
    def msdiff(cls, ms_start, ms_end=None):
        """
        Get difference in millis between current millis and provided millis.
        :param ms_start: Start millis
        :type ms_start: float
        :param ms_end: End millis (will use current if not provided)
        :type ms_end: float
        :return float
        :rtype float
        """

        if not ms_end:
            ms_end = cls.mscurrent()

        return ms_end - ms_start

    @classmethod
    def datecurrent(cls, erase_mode=0):
        """
        Return current date (UTC)
        :param erase_mode: Erase mode (0=nothing, 1=remove microseconds but keep millis, 2=remove millis completely)
        :return datetime.datetime
        :rtype datetime.datetime
        """

        if erase_mode == 0:
            return datetime.utcnow()
        elif erase_mode == 1:
            # Force precision loss (keep millis, kick micro)
            dt = datetime.utcnow()
            return dt.replace(microsecond=int((dt.microsecond * 0.001) * 1000))
        elif erase_mode == 2:
            return datetime.utcnow().replace(microsecond=0)

    @classmethod
    def datediff(cls, dt_start, dt_end=None):
        """
        Get difference in millis between two datetime
        :param dt_start: Start datetime
        :type dt_start: datetime.datetime
        :param dt_end: End datetime (will use current utc if not provided)
        :type dt_end: datetime.datetime
        :return float
        :rtype float
        """

        # Fix
        if not dt_end:
            dt_end = cls.datecurrent()

        # Get delta
        delta = dt_end - dt_start
        return ((delta.days * 86400 + delta.seconds) * 1000) + (delta.microseconds * 0.001)

    # ==========================================
    # EPOCH / DT
    # ==========================================

    DT_EPOCH = datetime.utcfromtimestamp(0)

    @classmethod
    def dt_to_epoch(cls, dt):
        """
        Convert a datetime (UTC required) to a unix time since epoch, as seconds, as integer.
        Note that millis precision is lost.
        :param dt: datetime
        :type dt: datetime
        :return int
        :rtype int
        """

        return int((dt - cls.DT_EPOCH).total_seconds())

    @classmethod
    def epoch_to_dt(cls, epoch):
        """
        Convert an epoch float or int to datetime (UTC)
        :param epoch: float,int
        :type epoch: float,int
        :return datetime
        :rtype datetime
        """

        return datetime.utcfromtimestamp(epoch)

    @classmethod
    def dt_is_naive(cls, dt):
        """
        Return true if dt is naive
        :param dt: datetime.datetime
        :type dt: datetime.datetime
        :return bool
        :rtype bool
        """

        # Naive : no tzinfo
        if not dt.tzinfo:
            return True

        # Aware
        return False

    @classmethod
    def dt_ensure_utc_aware(cls, dt):
        """
        Switch dt to utc time zone. If dt is naive, assume utc, otherwise, convert it to utc timezone.
        Return an AWARE timezone (utc switched) datetime,
        :param dt: datetime.datetime
        :type dt:datetime.datetime
        :return datetime.datetime
        :rtype datetime.datetime
        """

        # If naive, add utc
        if cls.dt_is_naive(dt):
            return dt.replace(tzinfo=pytz.utc)
        else:
            # Not naive, go utc, keep aware
            return dt.astimezone(pytz.utc)

    @classmethod
    def dt_ensure_utc_naive(cls, dt):
        """
        Ensure dt is naive. Return dt, switched to UTC (if applicable), and naive.
        :param dt: datetime.datetime
        :type dt:datetime.datetime
        :return datetime.datetime
        :rtype datetime.datetime
        """

        dt = cls.dt_ensure_utc_aware(dt)
        return dt.replace(tzinfo=None)

    # ===============================
    # COMPO NAME (FOR RSYSLOG)
    # ===============================

    @classmethod
    def set_compo_name(cls, compo_name):
        """
        Set the component name. Useful for rsyslog.
        :param compo_name: The component name or None. If None, method do nothing.
        :type compo_name: str,None
        """

        if compo_name:
            cls._compo_name = compo_name
            lifecyclelogger.debug("compo_name now set to=%s", cls._compo_name)

    @classmethod
    def get_compo_name(cls):
        """
        Get current component name.
        :return str
        :rtype str
        """

        return cls._compo_name

    @classmethod
    def get_machine_name(cls):
        """
        Get machine name
        :return: Machine name
        :rtype: str
        """

        return platform.uname()[1]

    # ===============================
    # MISC
    # ===============================

    @classmethod
    def sleep(cls, sleep_ms):
        """
        Sleep for specified ms.
        Also used as gevent context switch in code, since it rely on gevent.sleep.
        :param sleep_ms: Millis to sleep.
        :type sleep_ms: int
        :return Nothing.
        """
        ms = sleep_ms * 0.001

        # gevent 1.3 : ms is not fully respected (100 can be 80-100)
        gevent.sleep(ms)

    # ===============================
    # EXCEPTION HELPER
    # ===============================

    @classmethod
    def extostr(cls, e, max_level=30, max_path_level=5):
        """
        Format an exception.
        :param e: Any exception instance.
        :type e: Exception
        :param max_level: Maximum call stack level (default 30)
        :type max_level: int
        :param max_path_level: Maximum path level (default 5)
        :type max_path_level: int
        :return The exception readable string
        :rtype str
        """

        # Go
        list_frame = None
        try:
            out_buffer = ""

            # Class type
            out_buffer += "e.cls:[{0}]".format(e.__class__.__name__)

            # To string
            try:
                ex_buf = str(e)
            except UnicodeEncodeError:
                ex_buf = repr(str(e))
            except Exception as e:
                logger.warning("Exception, e=%s", e)
                raise
            out_buffer += ", e.bytes:[{0}]".format(ex_buf)

            # Traceback
            si = sys.exc_info()

            # Raw frame
            # tuple : (file, lineno, method, code)
            raw_frame = traceback.extract_tb(si[2])
            raw_frame.reverse()

            # Go to last tb_next
            last_tb_next = None
            cur_tb = si[2]
            while cur_tb:
                last_tb_next = cur_tb
                cur_tb = cur_tb.tb_next

            # Skip frame up to current raw frame count
            list_frame = list()
            cur_count = -1
            skip_count = len(raw_frame)
            if last_tb_next:
                cur_frame = last_tb_next.tb_frame
            else:
                cur_frame = None
            while cur_frame:
                cur_count += 1
                if cur_count < skip_count:
                    cur_frame = cur_frame.f_back
                else:
                    # Need : tuple : (file, lineno, method, code)
                    raw_frame.append((cur_frame.f_code.co_filename, cur_frame.f_lineno, cur_frame.f_code.co_name, ""))
                    cur_frame = cur_frame.f_back

            # Build it
            cur_idx = 0
            out_buffer += ", e.cs=["
            for tu in raw_frame:
                line = tu[1]
                cur_file = tu[0]
                method = tu[2]

                # Handle max path level
                ar_token = cur_file.rsplit(os.sep, max_path_level)
                if len(ar_token) > max_path_level:
                    # Remove head
                    ar_token.pop(0)
                    # Join
                    cur_file = "..." + os.sep.join(ar_token)

                # Format
                out_buffer += "in:{0}#{1}@{2} ".format(method, cur_file, line)

                # Loop
                cur_idx += 1
                if cur_idx >= max_level:
                    out_buffer += "..."
                    break

            # Close
            out_buffer += "]"

            # Ok
            return out_buffer
        finally:
            if list_frame:
                del list_frame

    # ===============================
    # VOODOO INIT
    # ===============================

    @classmethod
    def _reset(cls):
        """
        For unittest only
        """

        cls._logging_initialized = False
        cls._voodoo_initialized = False

    @classmethod
    def voodoo_init(cls, aggressive=True, init_logging=True):
        """
        Global initialization, to call asap.
        Apply gevent stuff & default logging configuration.
        :param aggressive: bool
        :type aggressive: bool
        :param init_logging: If True, logging_init is called.
        :type init_logging: bool
        :return Nothing.
        """

        try:
            # Check
            if cls._voodoo_initialized:
                return

            # Lock
            with cls._voodoo_lock:
                # Re-check
                if cls._voodoo_initialized:
                    return

                # Fire the voodoo magic :)
                lifecyclelogger.debug("Voodoo : gevent : entering, aggressive=%s", aggressive)
                monkey.patch_all(aggressive=aggressive)
                lifecyclelogger.debug("Voodoo : gevent : entering")

                # Gevent 1.3 : by default, gevent keep tracks of spawn call stack
                # This may lead to memory leak, if a method spawn itself in loop (timer mode)
                # We disable this
                config.track_greenlet_tree = False

                # Initialize log level to INFO
                if init_logging:
                    lifecyclelogger.debug("Voodoo : logging : entering")
                    cls.logging_init()
                    lifecyclelogger.debug("Voodoo : logging : done")

                # Done
                cls._voodoo_initialized = True
        finally:
            # If whenever init_logging if set AND it is NOT initialized => we must init it
            # => we may have been called previously with init_logging=false, but monkey patch is SET and logging not initialized
            # => so it must be init now
            if init_logging and not cls._logging_initialized:
                lifecyclelogger.debug("Voodoo : logging : not yet init : entering")
                cls.logging_init()
                lifecyclelogger.debug("Voodoo : logging : not yet init : done")

    # ===============================
    # LOGGING
    # ===============================

    @classmethod
    def logging_init(cls, log_level="INFO", force_reset=False, log_callback=None,
                     log_to_file=None,
                     log_to_syslog=True,
                     log_to_syslog_facility=SysLogHandler.LOG_LOCAL0,
                     log_to_console=True,
                     log_to_file_mode="watched_file",
                     context_filter=None):
        """
        Initialize logging sub system with default settings (console, pre-formatted output)
        :param log_to_console: if True to console
        :type log_to_console: bool
        :param log_level: The log level to set. Any value in "DEBUG", "INFO", "WARN", "ERROR", "CRITICAL"
        :type log_level: str
        :param force_reset: If true, logging system is reset.
        :type force_reset: bool
        :param log_to_file: If specified, log to file
        :type log_to_file: str,None
        :param log_to_syslog: If specified, log to syslog
        :type log_to_syslog: bool
        :param log_to_syslog_facility: Syslog facility.
        :type log_to_syslog_facility: int
        :param log_to_file_mode: str "watched_file" for WatchedFileHandler, "time_file" for TimedRotatingFileHandler (or time_file_seconds for unittest)
        :type log_to_file_mode: str
        :param log_callback: Callback for unittest
        :param context_filter: Context filter. If None, pysolbase.ContextFilter.ContextFilter is used. If used instance has an attr "filter", it is added to all handlers and "%(kfilter)s" will be populated by all thread context key/values, using filter method call. Refer to our ContextFilter default implementation for details.
        :type context_filter: None,object
        :return Nothing.
        """

        if cls._logging_initialized and not force_reset:
            return

        with cls._logging_lock:
            if cls._logging_initialized and not force_reset:
                return

            # Reset
            cls._reset_logging(log_level=log_level)

            # Default
            logging.basicConfig(level=log_level)

            # Filter
            if context_filter:
                c_filter = context_filter
            else:
                c_filter = ContextFilter()

            # Format begin
            s_f = "%(asctime)s | %(levelname)s | %(module)s@%(funcName)s@%(lineno)d | %(message)s "

            # Browse
            if hasattr(c_filter, "filter"):
                # Push generic field
                # We expect it to be formatted like our pysolbase.ContextFilter.ContextFilter#filter method.
                s_f += "|%(kfilter)s"

            # Format end
            s_f += "| %(thread)d:%(threadName)s | %(process)d:%(processName)s"

            # Formatter
            f = logging.Formatter(s_f)

            # Console handler
            c = None
            if log_to_console:
                # This can be overriden by unittest, we use __stdout__
                c = logging.StreamHandler(sys.__stdout__)
                c.setLevel(logging.getLevelName(log_level))
                c.setFormatter(f)

            # File handler to /tmp
            cf = None
            if log_to_file:
                if log_to_file_mode == "watched_file":
                    cf = WatchedFileHandler(log_to_file, encoding="utf-8")
                    cf.setLevel(logging.getLevelName(log_level))
                    cf.setFormatter(f)
                elif log_to_file_mode == "time_file":
                    cf = TimedRotatingFileHandler(log_to_file, encoding="utf-8", utc=True, when="D", interval=1, backupCount=7)
                    cf.setLevel(logging.getLevelName(log_level))
                    cf.setFormatter(f)
                elif log_to_file_mode == "time_file_seconds":
                    # For unittest only
                    cf = TimedRotatingFileHandler(log_to_file, encoding="utf-8", utc=True, when="S", interval=1, backupCount=7)
                    cf.setLevel(logging.getLevelName(log_level))
                    cf.setFormatter(f)
                else:
                    logger.warning("Invalid log_to_file_mode=%s", log_to_file_mode)

            # Syslog handler
            syslog = None
            if log_to_syslog:
                try:
                    from SysLogger import SysLogger

                    syslog = SysLogger(log_callback=log_callback, facility=log_to_syslog_facility)
                    syslog.setLevel(logging.getLevelName(log_level))
                    syslog.setFormatter(f)
                except Exception as e:
                    # This will fail on WINDOWS (no attr AF_UNIX)
                    logger.debug("Unable to import SysLogger, e=%s", SolBase.extostr(e))
                    syslog = False

            # Initialize
            root = logging.getLogger()
            root.setLevel(logging.getLevelName(log_level))
            root.handlers = []
            if log_to_console:
                c.addFilter(c_filter)
                root.addHandler(c)
            if log_to_file and cf:
                cf.addFilter(c_filter)
                root.addHandler(cf)
            if log_to_syslog and syslog:
                syslog.addFilter(c_filter)
                root.addHandler(syslog)

            # Done
            cls._logging_initialized = True
            if force_reset:
                lifecyclelogger.info("Logging : initialized from memory, log_level=%s, force_reset=%s", log_level, force_reset)
            else:
                lifecyclelogger.debug("Logging : initialized from memory, log_level=%s, force_reset=%s", log_level, force_reset)

    @classmethod
    def _register_filter(cls, c_filter):
        """
        Register filter across the whole logging (root and all loggers)
        Notice : addFilter is protected against duplicates add
        :param c_filter: pysolbase.ContextFilter.ContextFilter
        :type c_filter: pysolbase.ContextFilter.ContextFilter
        """

        # Initialize
        root = logging.getLogger()
        for h in list(root.handlers):
            h.addFilter(c_filter)

        # Browse all loggers and set
        for name in logging.root.manager.loggerDict:
            cur_logger = logging.getLogger(name)
            for h in list(cur_logger.handlers):
                h.addFilter(c_filter)

    @classmethod
    def _reset_logging(cls, log_level):
        """
        Reset
        :param log_level: str
        :type log_level: str
        """

        # Found no way to fully reset the logging stuff while running
        # We reset root and all loggers to INFO, and kick handlers

        # Initialize
        root = logging.getLogger()
        root.setLevel(logging.getLevelName(log_level))
        for h in root.handlers:
            # noinspection PyBroadException
            try:
                h.close()
            except:
                pass
        root.handlers = []

        # Browse all loggers and set
        for name in logging.root.manager.loggerDict:
            cur_logger = logging.getLogger(name)
            cur_logger.setLevel(logging.getLevelName(log_level))
            for h in cur_logger.handlers:
                # noinspection PyBroadException
                try:
                    h.close()
                except:
                    pass
            cur_logger.handlers = []

    @classmethod
    def logging_initfromfile(cls, config_file_name, force_reset=False, context_filter=None):
        """
        Initialize logging system from a configuration file, with optional reset.
        :param config_file_name: Configuration file name
        :type config_file_name: str
        :param force_reset: If true, logging system is reset.
        :type force_reset: bool
        :param context_filter: Context filter. If None, pysolbase.ContextFilter.ContextFilter is used. If used instance has an attr "filter", it is added to all handlers and "%(kfilter)s" will be populated by all thread context key/values, using filter method call. Refer to our ContextFilter default implementation for details.
        :type context_filter: None,object
        :return Nothing.
        """

        if cls._logging_initialized and not force_reset:
            return

        with cls._logging_lock:
            if cls._logging_initialized and not force_reset:
                return

            try:
                # Filter
                if context_filter:
                    c_filter = context_filter
                else:
                    c_filter = ContextFilter()

                # Reset
                cls._reset_logging(log_level="INFO")

                # Load
                logger.debug("Logging : yaml config_file_name=%s", config_file_name)
                with open(config_file_name, 'r') as f:
                    d = load(f, Loader=SafeLoader)
                    dictConfig(d)

                # Register filter
                if c_filter:
                    cls._register_filter(c_filter)

                if force_reset:
                    lifecyclelogger.info("Logging : initialized from yaml file, config_file_name=%s", config_file_name)
                else:
                    lifecyclelogger.debug("Logging : initialized from yaml file, config_file_name=%s", config_file_name)
            except Exception:
                raise

    @classmethod
    def context_set(cls, k, v):
        """
        Set thread/greenlet context value

        This is a wrapper to pysolbase.ContextFilter.ContextFilter#set_value
        and will work only if ContextFilter is defined (which is by default)
        :param k: key name
        :type k: basestring
        :param v: value
        :type v: object
        """

        ContextFilter.set_value(k, v)

    # ===============================
    # FORK STUFF
    # ===============================

    @classmethod
    def get_master_process(cls):
        """
        Return True if we are the master process, False otherwise.
        :return bool
        :rtype bool
        """
        return cls._master_process

    @classmethod
    def set_master_process(cls, b):
        """
        Set is we are a fork master or not
        :param b: True if we are master process, False if we are a child process.
        :type b: bool
        :return Nothing
        """

        logger.debug("Switching _masterProcess to %s", b)
        cls._master_process = b

    # ===============================
    # BINARY STUFF
    # ===============================

    @classmethod
    def binary_to_unicode(cls, bin_buf, encoding="utf-8"):
        """
        Binary buffer to str, using the specified encoding
        :param bin_buf: Binary buffer
        :type bin_buf: bytes
        :param encoding: Encoding to use
        :type encoding: str
        :return str
        :rtype str
        """

        return bin_buf.decode(encoding)

    @classmethod
    def unicode_to_binary(cls, unicode_buf, encoding="utf-8"):
        """
        Unicode to binary buffer, using the specified encoding
        :param unicode_buf: String to convert.
        :type unicode_buf: str
        :param encoding: Encoding to use.
        :type encoding: str
        :return bytes
        :rtype bytes
        """

        return unicode_buf.encode(encoding)

    @classmethod
    def fix_paths_for_popen(cls):
        """
        Fix path and env for popen calls toward current project
        Mainly used for unittests, which requires current env to be propagated while testing command line invocation within same project
        """

        # Merge all
        ar_p_path = sys.path
        if os.environ.get("PYTHONPATH"):
            ar_p_path.extend(os.environ.get("PYTHONPATH").split(":"))
        if os.environ.get("PATH"):
            ar_p_path.extend(os.environ.get("PATH").split(":"))

        # Join
        new_path = ":".join(ar_p_path)

        # Re-Assign
        os.environ["PATH"] = new_path
        os.environ["PYTHONPATH"] = new_path

    # ===============================
    # CONVERSIONS
    # ===============================

    @classmethod
    def to_int(cls, v):
        """
        Convert to int
        :param v: int,str
        :type v: int,str
        :return: int
        :rtype int
        """

        if isinstance(v, int):
            return v
        else:
            return int(v)

    @classmethod
    def to_bool(cls, v):
        """
        Convert to bool
        :param v: bool,str
        :type v: bool,str
        :return: bool
        :rtype bool
        """

        if isinstance(v, bool):
            return v
        else:
            return ast.literal_eval(v)

    @classmethod
    def get_classname(cls, my_instance):
        """
        Return the class name of my_instance, or "Instance.None".
        :param cls: Our class.
        :param my_instance: Instance to use.
        :return: Return the class name of my_instance, or "Instance.None" in case of error/None value.
        """
        if my_instance is None:
            return "Instance.None"
        else:
            return my_instance.__class__.__name__

    @classmethod
    def get_pathseparator(cls):
        """
        Return the path separator.
        https://docs.python.org/library/os.html#os.sep
        :param cls: Our class
        :return: The path separator (string)
        """
        return os.sep

    @classmethod
    def is_bool(cls, my_bool):
        """
        Return true if the provided my_bool is a boolean.
        :param cls: Our class.
        :param my_bool: A boolean..
        :return: Return true if the provided my_bool is a boolean. False otherwise.
        """
        if my_bool is None:
            return False
        else:
            return isinstance(my_bool, bool)

    @classmethod
    def is_int(cls, my_int):
        """
        Return true if the provided my_int is a integer.
        :param cls: Our class.
        :param my_int: An integer..
        :return: Return true if the provided my_int is a integer. False otherwise.
        """
        if my_int is None:
            return False
        # Caution, boolean is an integer...
        elif SolBase.is_bool(my_int):
            return False
        else:
            return isinstance(my_int, int)

    @classmethod
    def get_current_pid_as_string(cls):
        """
        Return the current pids as string.
        :param cls: Our class.
        :return: A String
        """
        try:
            return "pid={0}, ppid={1}".format(os.getpid(), os.getppid())
        except AttributeError:
            return "pid={0}".format(os.getpid())

    # =====================================================
    # HELPER FOR SOCKET CLOSING
    # =====================================================

    @classmethod
    def safe_close_socket(cls, soc_to_close):
        """
        Safe close a socket
        :param soc_to_close: socket
        :type soc_to_close: socket.socket
        """

        if soc_to_close is None:
            return

        try:
            soc_to_close.shutdown(2)
        except Exception as e:
            logger.debug("Socket shutdown ex=%s", SolBase.extostr(e))

        try:
            soc_to_close.close()
        except Exception as e:
            logger.debug("Socket close ex=%s", SolBase.extostr(e))

        try:
            del soc_to_close
        except Exception as e:
            logger.debug("Socket del ex=%s", SolBase.extostr(e))

if __name__ == "__main__":
    from os.path import dirname, abspath
    isT=True
    cf = dirname(abspath(__file__)) + os.sep + "logging.yaml"
    SolBase.logging_init("INFO", True)
    if not logging.getLevelName(logging.getLogger().getEffectiveLevel())=="INFO":
        isT=False
    if not logging.getLevelName(logging.getLogger("zzz").getEffectiveLevel())=="INFO":
        isT=False

    SolBase.logging_initfromfile(cf, False)
    if not logging.getLevelName(logging.getLogger().getEffectiveLevel()) == "INFO":
        isT=False
    if not logging.getLevelName(logging.getLogger("zzz").getEffectiveLevel())== "INFO":
        isT=False

    SolBase.logging_initfromfile(cf, True)
    if not logging.getLevelName(logging.getLogger().getEffectiveLevel())== "DEBUG":
        isT=False
    if not logging.getLevelName(logging.getLogger("zzz").getEffectiveLevel())=="WARNING":
        isT=False

    # import dill
    # import os
    #
    # isT = True
    # for l in os.listdir("/home/travis/builds/repos/champax---pysolbase/data_passk_platform/62b8b99de0d34b282c1811f8/"):
    #     f = open("/home/travis/builds/repos/champax---pysolbase/data_passk_platform/62b8b99de0d34b282c1811f8/" + l,
    #              "rb")
    #     content = dill.load(f)
    #     f.close()
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     res0 = object_class._reset_logging()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/tests/test_declarations__getTargetClass_passk_validte.py
##############################################################################
#
# Copyright (c) 2003 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""Test the new API for making and checking interface declarations
"""
import unittest
import sys

import zope.interface.declarations
from zope.interface._compat import _skip_under_py3k
from zope.interface._compat import PYTHON3
from zope.interface.tests import OptimizationTestMixin
from zope.interface.tests import MissingSomeAttrs
from zope.interface.tests.test_interface import NameAndModuleComparisonTestsMixin

# pylint:disable=inherit-non-class,too-many-lines,protected-access
# pylint:disable=blacklisted-name,attribute-defined-outside-init

class _Py3ClassAdvice(object):

    def _run_generated_code(self, code, globs, locs,
                            fails_under_py3k=True,
                           ):
        # pylint:disable=exec-used,no-member
        import warnings
        with warnings.catch_warnings(record=True) as log:
            warnings.resetwarnings()
            if not PYTHON3:
                exec(code, globs, locs)
                self.assertEqual(len(log), 0) # no longer warn
                return True

            try:
                exec(code, globs, locs)
            except TypeError:
                return False
            else:
                if fails_under_py3k:
                    self.fail("Didn't raise TypeError")
            return None


class NamedTests(unittest.TestCase):

    def test_class(self):
        from zope.interface.declarations import named

        @named(u'foo')
        class Foo(object):
            pass

        self.assertEqual(Foo.__component_name__, u'foo') # pylint:disable=no-member

    def test_function(self):
        from zope.interface.declarations import named

        @named(u'foo')
        def doFoo(o):
            raise NotImplementedError()

        self.assertEqual(doFoo.__component_name__, u'foo')

    def test_instance(self):
        from zope.interface.declarations import named

        class Foo(object):
            pass
        foo = Foo()
        named(u'foo')(foo)

        self.assertEqual(foo.__component_name__, u'foo') # pylint:disable=no-member


class EmptyDeclarationTests(unittest.TestCase):
    # Tests that should pass for all objects that are empty
    # declarations. This includes a Declaration explicitly created
    # that way, and the empty ImmutableDeclaration.
    def _getEmpty(self):
        from zope.interface.declarations import Declaration
        return Declaration()

    def test___iter___empty(self):
        decl = self._getEmpty()
        self.assertEqual(list(decl), [])

    def test_flattened_empty(self):
        from zope.interface.interface import Interface
        decl = self._getEmpty()
        self.assertEqual(list(decl.flattened()), [Interface])

    def test___contains___empty(self):
        from zope.interface.interface import Interface
        decl = self._getEmpty()
        self.assertNotIn(Interface, decl)

    def test_extends_empty(self):
        from zope.interface.interface import Interface
        decl = self._getEmpty()
        self.assertTrue(decl.extends(Interface))
        self.assertTrue(decl.extends(Interface, strict=True))

    def test_interfaces_empty(self):
        decl = self._getEmpty()
        l = list(decl.interfaces())
        self.assertEqual(l, [])

    def test___sro___(self):
        from zope.interface.interface import Interface
        decl = self._getEmpty()
        self.assertEqual(decl.__sro__, (decl, Interface,))

    def test___iro___(self):
        from zope.interface.interface import Interface
        decl = self._getEmpty()
        self.assertEqual(decl.__iro__, (Interface,))

    def test_get(self):
        decl = self._getEmpty()
        self.assertIsNone(decl.get('attr'))
        self.assertEqual(decl.get('abc', 'def'), 'def')
        # It's a positive cache only (when it even exists)
        # so this added nothing.
        self.assertFalse(decl._v_attrs)

    def test_changed_w_existing__v_attrs(self):
        decl = self._getEmpty()
        decl._v_attrs = object()
        decl.changed(decl)
        self.assertFalse(decl._v_attrs)


class DeclarationTests(EmptyDeclarationTests):

    def _getTargetClass(self):
        from zope.interface.declarations import Declaration
        return Declaration

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_ctor_no_bases(self):
        decl = self._makeOne()
        self.assertEqual(list(decl.__bases__), [])

    def test_ctor_w_interface_in_bases(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        decl = self._makeOne(IFoo)
        self.assertEqual(list(decl.__bases__), [IFoo])

    def test_ctor_w_implements_in_bases(self):
        from zope.interface.declarations import Implements
        impl = Implements()
        decl = self._makeOne(impl)
        self.assertEqual(list(decl.__bases__), [impl])

    def test_changed_wo_existing__v_attrs(self):
        decl = self._makeOne()
        decl.changed(decl) # doesn't raise
        self.assertIsNone(decl._v_attrs)

    def test___contains__w_self(self):
        decl = self._makeOne()
        self.assertNotIn(decl, decl)

    def test___contains__w_unrelated_iface(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        decl = self._makeOne()
        self.assertNotIn(IFoo, decl)

    def test___contains__w_base_interface(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        decl = self._makeOne(IFoo)
        self.assertIn(IFoo, decl)

    def test___iter___single_base(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        decl = self._makeOne(IFoo)
        self.assertEqual(list(decl), [IFoo])

    def test___iter___multiple_bases(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        IBar = InterfaceClass('IBar')
        decl = self._makeOne(IFoo, IBar)
        self.assertEqual(list(decl), [IFoo, IBar])

    def test___iter___inheritance(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        IBar = InterfaceClass('IBar', (IFoo,))
        decl = self._makeOne(IBar)
        self.assertEqual(list(decl), [IBar]) #IBar.interfaces() omits bases

    def test___iter___w_nested_sequence_overlap(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        IBar = InterfaceClass('IBar')
        decl = self._makeOne(IBar, (IFoo, IBar))
        self.assertEqual(list(decl), [IBar, IFoo])

    def test_flattened_single_base(self):
        from zope.interface.interface import Interface
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        decl = self._makeOne(IFoo)
        self.assertEqual(list(decl.flattened()), [IFoo, Interface])

    def test_flattened_multiple_bases(self):
        from zope.interface.interface import Interface
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        IBar = InterfaceClass('IBar')
        decl = self._makeOne(IFoo, IBar)
        self.assertEqual(list(decl.flattened()), [IFoo, IBar, Interface])

    def test_flattened_inheritance(self):
        from zope.interface.interface import Interface
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        IBar = InterfaceClass('IBar', (IFoo,))
        decl = self._makeOne(IBar)
        self.assertEqual(list(decl.flattened()), [IBar, IFoo, Interface])

    def test_flattened_w_nested_sequence_overlap(self):
        from zope.interface.interface import Interface
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        IBar = InterfaceClass('IBar')
        # This is the same as calling ``Declaration(IBar, IFoo, IBar)``
        # which doesn't make much sense, but here it is. In older
        # versions of zope.interface, the __iro__ would have been
        # IFoo, IBar, Interface, which especially makes no sense.
        decl = self._makeOne(IBar, (IFoo, IBar))
        # Note that decl.__iro__ has IFoo first.
        self.assertEqual(list(decl.flattened()), [IBar, IFoo, Interface])

    def test___sub___unrelated_interface(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        IBar = InterfaceClass('IBar')
        before = self._makeOne(IFoo)
        after = before - IBar
        self.assertIsInstance(after, self._getTargetClass())
        self.assertEqual(list(after), [IFoo])

    def test___sub___related_interface(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        before = self._makeOne(IFoo)
        after = before - IFoo
        self.assertEqual(list(after), [])

    def test___sub___related_interface_by_inheritance(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        IBar = InterfaceClass('IBar', (IFoo,))
        before = self._makeOne(IBar)
        after = before - IBar
        self.assertEqual(list(after), [])

    def test___add___unrelated_interface(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        IBar = InterfaceClass('IBar')
        before = self._makeOne(IFoo)
        after = before + IBar
        self.assertIsInstance(after, self._getTargetClass())
        self.assertEqual(list(after), [IFoo, IBar])

    def test___add___related_interface(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        IBar = InterfaceClass('IBar')
        IBaz = InterfaceClass('IBaz')
        before = self._makeOne(IFoo, IBar)
        other = self._makeOne(IBar, IBaz)
        after = before + other
        self.assertEqual(list(after), [IFoo, IBar, IBaz])


class TestImmutableDeclaration(EmptyDeclarationTests):

    def _getTargetClass(self):
        from zope.interface.declarations import _ImmutableDeclaration
        return _ImmutableDeclaration

    def _getEmpty(self):
        from zope.interface.declarations import _empty
        return _empty

    def test_pickle(self):
        import pickle
        copied = pickle.loads(pickle.dumps(self._getEmpty()))
        self.assertIs(copied, self._getEmpty())

    def test_singleton(self):
        self.assertIs(
            self._getTargetClass()(),
            self._getEmpty()
        )

    def test__bases__(self):
        self.assertEqual(self._getEmpty().__bases__, ())

    def test_change__bases__(self):
        empty = self._getEmpty()
        empty.__bases__ = ()
        self.assertEqual(self._getEmpty().__bases__, ())

        with self.assertRaises(TypeError):
            empty.__bases__ = (1,)

    def test_dependents(self):
        empty = self._getEmpty()
        deps = empty.dependents
        self.assertEqual({}, deps)
        # Doesn't change the return.
        deps[1] = 2
        self.assertEqual({}, empty.dependents)

    def test_changed(self):
        # Does nothing, has no visible side-effects
        self._getEmpty().changed(None)

    def test_extends_always_false(self):
        self.assertFalse(self._getEmpty().extends(self))
        self.assertFalse(self._getEmpty().extends(self, strict=True))
        self.assertFalse(self._getEmpty().extends(self, strict=False))

    def test_get_always_default(self):
        self.assertIsNone(self._getEmpty().get('name'))
        self.assertEqual(self._getEmpty().get('name', 42), 42)

    def test_v_attrs(self):
        decl = self._getEmpty()
        self.assertEqual(decl._v_attrs, {})

        decl._v_attrs['attr'] = 42
        self.assertEqual(decl._v_attrs, {})
        self.assertIsNone(decl.get('attr'))

        attrs = decl._v_attrs = {}
        attrs['attr'] = 42
        self.assertEqual(decl._v_attrs, {})
        self.assertIsNone(decl.get('attr'))


class TestImplements(NameAndModuleComparisonTestsMixin,
                     unittest.TestCase):

    def _getTargetClass(self):
        from zope.interface.declarations import Implements
        return Implements

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def _makeOneToCompare(self):
        from zope.interface.declarations import implementedBy
        class A(object):
            pass

        return implementedBy(A)

    def test_ctor_no_bases(self):
        impl = self._makeOne()
        self.assertEqual(impl.inherit, None)
        self.assertEqual(impl.declared, ())
        self.assertEqual(impl.__name__, '?')
        self.assertEqual(list(impl.__bases__), [])

    def test___repr__(self):
        impl = self._makeOne()
        impl.__name__ = 'Testing'
        self.assertEqual(repr(impl), '<implementedBy Testing>')

    def test___reduce__(self):
        from zope.interface.declarations import implementedBy
        impl = self._makeOne()
        self.assertEqual(impl.__reduce__(), (implementedBy, (None,)))

    def test_sort(self):
        from zope.interface.declarations import implementedBy
        class A(object):
            pass
        class B(object):
            pass
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')

        self.assertEqual(implementedBy(A), implementedBy(A))
        self.assertEqual(hash(implementedBy(A)), hash(implementedBy(A)))
        self.assertTrue(implementedBy(A) < None)
        self.assertTrue(None > implementedBy(A)) # pylint:disable=misplaced-comparison-constant
        self.assertTrue(implementedBy(A) < implementedBy(B))
        self.assertTrue(implementedBy(A) > IFoo)
        self.assertTrue(implementedBy(A) <= implementedBy(B))
        self.assertTrue(implementedBy(A) >= IFoo)
        self.assertTrue(implementedBy(A) != IFoo)

    def test_proxy_equality(self):
        # https://github.com/zopefoundation/zope.interface/issues/55
        class Proxy(object):
            def __init__(self, wrapped):
                self._wrapped = wrapped

            def __getattr__(self, name):
                raise NotImplementedError()

            def __eq__(self, other):
                return self._wrapped == other

            def __ne__(self, other):
                return self._wrapped != other

        from zope.interface.declarations import implementedBy
        class A(object):
            pass

        class B(object):
            pass

        implementedByA = implementedBy(A)
        implementedByB = implementedBy(B)
        proxy = Proxy(implementedByA)

        # The order of arguments to the operators matters,
        # test both
        self.assertTrue(implementedByA == implementedByA) # pylint:disable=comparison-with-itself
        self.assertTrue(implementedByA != implementedByB)
        self.assertTrue(implementedByB != implementedByA)

        self.assertTrue(proxy == implementedByA)
        self.assertTrue(implementedByA == proxy)
        self.assertFalse(proxy != implementedByA)
        self.assertFalse(implementedByA != proxy)

        self.assertTrue(proxy != implementedByB)
        self.assertTrue(implementedByB != proxy)

    def test_changed_deletes_super_cache(self):
        impl = self._makeOne()
        self.assertIsNone(impl._super_cache)
        self.assertNotIn('_super_cache', impl.__dict__)

        impl._super_cache = 42
        self.assertIn('_super_cache', impl.__dict__)

        impl.changed(None)
        self.assertIsNone(impl._super_cache)
        self.assertNotIn('_super_cache', impl.__dict__)

    def test_changed_does_not_add_super_cache(self):
        impl = self._makeOne()
        self.assertIsNone(impl._super_cache)
        self.assertNotIn('_super_cache', impl.__dict__)

        impl.changed(None)
        self.assertIsNone(impl._super_cache)
        self.assertNotIn('_super_cache', impl.__dict__)


class Test_implementedByFallback(unittest.TestCase):

    def _getTargetClass(self):
        # pylint:disable=no-name-in-module
        from zope.interface.declarations import implementedByFallback
        return implementedByFallback

    _getFallbackClass = _getTargetClass

    def _callFUT(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_dictless_wo_existing_Implements_wo_registrations(self):
        class Foo(object):
            __slots__ = ('__implemented__',)
        foo = Foo()
        foo.__implemented__ = None
        self.assertEqual(list(self._callFUT(foo)), [])

    def test_dictless_wo_existing_Implements_cant_assign___implemented__(self):
        class Foo(object):
            def _get_impl(self):
                raise NotImplementedError()
            def _set_impl(self, val):
                raise TypeError
            __implemented__ = property(_get_impl, _set_impl)
            def __call__(self):
                # act like a factory
                raise NotImplementedError()
        foo = Foo()
        self.assertRaises(TypeError, self._callFUT, foo)

    def test_dictless_wo_existing_Implements_w_registrations(self):
        from zope.interface import declarations
        class Foo(object):
            __slots__ = ('__implemented__',)
        foo = Foo()
        foo.__implemented__ = None
        reg = object()
        with _MonkeyDict(declarations,
                         'BuiltinImplementationSpecifications') as specs:
            specs[foo] = reg
            self.assertTrue(self._callFUT(foo) is reg)

    def test_dictless_w_existing_Implements(self):
        from zope.interface.declarations import Implements
        impl = Implements()
        class Foo(object):
            __slots__ = ('__implemented__',)
        foo = Foo()
        foo.__implemented__ = impl
        self.assertTrue(self._callFUT(foo) is impl)

    def test_dictless_w_existing_not_Implements(self):
        from zope.interface.interface import InterfaceClass
        class Foo(object):
            __slots__ = ('__implemented__',)
        foo = Foo()
        IFoo = InterfaceClass('IFoo')
        foo.__implemented__ = (IFoo,)
        self.assertEqual(list(self._callFUT(foo)), [IFoo])

    def test_w_existing_attr_as_Implements(self):
        from zope.interface.declarations import Implements
        impl = Implements()
        class Foo(object):
            __implemented__ = impl
        self.assertTrue(self._callFUT(Foo) is impl)

    def test_builtins_added_to_cache(self):
        from zope.interface import declarations
        from zope.interface.declarations import Implements
        from zope.interface._compat import _BUILTINS
        with _MonkeyDict(declarations,
                         'BuiltinImplementationSpecifications') as specs:
            self.assertEqual(list(self._callFUT(tuple)), [])
            self.assertEqual(list(self._callFUT(list)), [])
            self.assertEqual(list(self._callFUT(dict)), [])
            for typ in (tuple, list, dict):
                spec = specs[typ]
                self.assertIsInstance(spec, Implements)
                self.assertEqual(repr(spec),
                                 '<implementedBy %s.%s>'
                                 % (_BUILTINS, typ.__name__))

    def test_builtins_w_existing_cache(self):
        from zope.interface import declarations
        t_spec, l_spec, d_spec = object(), object(), object()
        with _MonkeyDict(declarations,
                         'BuiltinImplementationSpecifications') as specs:
            specs[tuple] = t_spec
            specs[list] = l_spec
            specs[dict] = d_spec
            self.assertTrue(self._callFUT(tuple) is t_spec)
            self.assertTrue(self._callFUT(list) is l_spec)
            self.assertTrue(self._callFUT(dict) is d_spec)

    def test_oldstyle_class_no_assertions(self):
        # TODO: Figure out P3 story
        class Foo:
            pass
        self.assertEqual(list(self._callFUT(Foo)), [])

    def test_no_assertions(self):
        # TODO: Figure out P3 story
        class Foo(object):
            pass
        self.assertEqual(list(self._callFUT(Foo)), [])

    def test_w_None_no_bases_not_factory(self):
        class Foo(object):
            __implemented__ = None
        foo = Foo()
        self.assertRaises(TypeError, self._callFUT, foo)

    def test_w_None_no_bases_w_factory(self):
        from zope.interface.declarations import objectSpecificationDescriptor
        class Foo(object):
            __implemented__ = None
            def __call__(self):
                raise NotImplementedError()

        foo = Foo()
        foo.__name__ = 'foo'
        spec = self._callFUT(foo)
        self.assertEqual(spec.__name__,
                         'zope.interface.tests.test_declarations.foo')
        self.assertIs(spec.inherit, foo)
        self.assertIs(foo.__implemented__, spec)
        self.assertIs(foo.__providedBy__, objectSpecificationDescriptor) # pylint:disable=no-member
        self.assertNotIn('__provides__', foo.__dict__)

    def test_w_None_no_bases_w_class(self):
        from zope.interface.declarations import ClassProvides
        class Foo(object):
            __implemented__ = None
        spec = self._callFUT(Foo)
        self.assertEqual(spec.__name__,
                         'zope.interface.tests.test_declarations.Foo')
        self.assertIs(spec.inherit, Foo)
        self.assertIs(Foo.__implemented__, spec)
        self.assertIsInstance(Foo.__providedBy__, ClassProvides) # pylint:disable=no-member
        self.assertIsInstance(Foo.__provides__, ClassProvides) # pylint:disable=no-member
        self.assertEqual(Foo.__provides__, Foo.__providedBy__) # pylint:disable=no-member

    def test_w_existing_Implements(self):
        from zope.interface.declarations import Implements
        impl = Implements()
        class Foo(object):
            __implemented__ = impl
        self.assertTrue(self._callFUT(Foo) is impl)

    def test_super_when_base_implements_interface(self):
        from zope.interface import Interface
        from zope.interface.declarations import implementer

        class IBase(Interface):
            pass

        class IDerived(IBase):
            pass

        @implementer(IBase)
        class Base(object):
            pass

        @implementer(IDerived)
        class Derived(Base):
            pass

        self.assertEqual(list(self._callFUT(Derived)), [IDerived, IBase])
        sup = super(Derived, Derived)
        self.assertEqual(list(self._callFUT(sup)), [IBase])

    def test_super_when_base_implements_interface_diamond(self):
        from zope.interface import Interface
        from zope.interface.declarations import implementer

        class IBase(Interface):
            pass

        class IDerived(IBase):
            pass

        @implementer(IBase)
        class Base(object):
            pass

        class Child1(Base):
            pass

        class Child2(Base):
            pass

        @implementer(IDerived)
        class Derived(Child1, Child2):
            pass

        self.assertEqual(list(self._callFUT(Derived)), [IDerived, IBase])
        sup = super(Derived, Derived)
        self.assertEqual(list(self._callFUT(sup)), [IBase])

    def test_super_when_parent_implements_interface_diamond(self):
        from zope.interface import Interface
        from zope.interface.declarations import implementer

        class IBase(Interface):
            pass

        class IDerived(IBase):
            pass


        class Base(object):
            pass

        class Child1(Base):
            pass

        @implementer(IBase)
        class Child2(Base):
            pass

        @implementer(IDerived)
        class Derived(Child1, Child2):
            pass

        self.assertEqual(Derived.__mro__, (Derived, Child1, Child2, Base, object))
        self.assertEqual(list(self._callFUT(Derived)), [IDerived, IBase])
        sup = super(Derived, Derived)
        fut = self._callFUT(sup)
        self.assertEqual(list(fut), [IBase])
        self.assertIsNone(fut._dependents)

    def test_super_when_base_doesnt_implement_interface(self):
        from zope.interface import Interface
        from zope.interface.declarations import implementer

        class IBase(Interface):
            pass

        class IDerived(IBase):
            pass

        class Base(object):
            pass

        @implementer(IDerived)
        class Derived(Base):
            pass

        self.assertEqual(list(self._callFUT(Derived)), [IDerived])

        sup = super(Derived, Derived)
        self.assertEqual(list(self._callFUT(sup)), [])

    def test_super_when_base_is_object(self):
        from zope.interface import Interface
        from zope.interface.declarations import implementer

        class IBase(Interface):
            pass

        class IDerived(IBase):
            pass

        @implementer(IDerived)
        class Derived(object):
            pass

        self.assertEqual(list(self._callFUT(Derived)), [IDerived])

        sup = super(Derived, Derived)
        self.assertEqual(list(self._callFUT(sup)), [])
    def test_super_multi_level_multi_inheritance(self):
        from zope.interface.declarations import implementer
        from zope.interface import Interface

        class IBase(Interface):
            pass

        class IM1(Interface):
            pass

        class IM2(Interface):
            pass

        class IDerived(IBase):
            pass

        class IUnrelated(Interface):
            pass

        @implementer(IBase)
        class Base(object):
            pass

        @implementer(IM1)
        class M1(Base):
            pass

        @implementer(IM2)
        class M2(Base):
            pass

        @implementer(IDerived, IUnrelated)
        class Derived(M1, M2):
            pass

        d = Derived
        sd = super(Derived, Derived)
        sm1 = super(M1, Derived)
        sm2 = super(M2, Derived)

        self.assertEqual(list(self._callFUT(d)),
                         [IDerived, IUnrelated, IM1, IBase, IM2])
        self.assertEqual(list(self._callFUT(sd)),
                         [IM1, IBase, IM2])
        self.assertEqual(list(self._callFUT(sm1)),
                         [IM2, IBase])
        self.assertEqual(list(self._callFUT(sm2)),
                         [IBase])


class Test_implementedBy(Test_implementedByFallback,
                         OptimizationTestMixin):
    # Repeat tests for C optimizations

    def _getTargetClass(self):
        from zope.interface.declarations import implementedBy
        return implementedBy


class _ImplementsTestMixin(object):
    FUT_SETS_PROVIDED_BY = True

    def _callFUT(self, cls, iface):
        # Declare that *cls* implements *iface*; return *cls*
        raise NotImplementedError

    def _check_implementer(self, Foo,
                           orig_spec=None,
                           spec_name=__name__ + '.Foo',
                           inherit="not given"):
        from zope.interface.declarations import ClassProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')

        returned = self._callFUT(Foo, IFoo)

        self.assertIs(returned, Foo)
        spec = Foo.__implemented__
        if orig_spec is not None:
            self.assertIs(spec, orig_spec)

        self.assertEqual(spec.__name__,
                         spec_name)
        inherit = Foo if inherit == "not given" else inherit
        self.assertIs(spec.inherit, inherit)
        self.assertIs(Foo.__implemented__, spec)
        if self.FUT_SETS_PROVIDED_BY:
            self.assertIsInstance(Foo.__providedBy__, ClassProvides)
            self.assertIsInstance(Foo.__provides__, ClassProvides)
            self.assertEqual(Foo.__provides__, Foo.__providedBy__)

        return Foo, IFoo

    def test_oldstyle_class(self):
        # This only matters on Python 2
        class Foo:
            pass
        self._check_implementer(Foo)

    def test_newstyle_class(self):
        class Foo(object):
            pass
        self._check_implementer(Foo)

class Test_classImplementsOnly(_ImplementsTestMixin, unittest.TestCase):
    FUT_SETS_PROVIDED_BY = False

    def _callFUT(self, cls, iface):
        from zope.interface.declarations import classImplementsOnly
        classImplementsOnly(cls, iface)
        return cls

    def test_w_existing_Implements(self):
        from zope.interface.declarations import Implements
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        IBar = InterfaceClass('IBar')
        impl = Implements(IFoo)
        impl.declared = (IFoo,)
        class Foo(object):
            __implemented__ = impl
        impl.inherit = Foo
        self._callFUT(Foo, IBar)
        # Same spec, now different values
        self.assertTrue(Foo.__implemented__ is impl)
        self.assertEqual(impl.inherit, None)
        self.assertEqual(impl.declared, (IBar,))

    def test_oldstyle_class(self):
        from zope.interface.declarations import Implements
        from zope.interface.interface import InterfaceClass
        IBar = InterfaceClass('IBar')
        old_spec = Implements(IBar)

        class Foo:
            __implemented__ = old_spec
        self._check_implementer(Foo, old_spec, '?', inherit=None)

    def test_newstyle_class(self):
        from zope.interface.declarations import Implements
        from zope.interface.interface import InterfaceClass
        IBar = InterfaceClass('IBar')
        old_spec = Implements(IBar)

        class Foo(object):
            __implemented__ = old_spec
        self._check_implementer(Foo, old_spec, '?', inherit=None)


    def test_redundant_with_super_still_implements(self):
        Base, IBase = self._check_implementer(
            type('Foo', (object,), {}),
            inherit=None,
        )

        class Child(Base):
            pass

        self._callFUT(Child, IBase)
        self.assertTrue(IBase.implementedBy(Child))


class Test_classImplements(_ImplementsTestMixin, unittest.TestCase):

    def _callFUT(self, cls, iface):
        from zope.interface.declarations import classImplements
        result = classImplements(cls, iface) # pylint:disable=assignment-from-no-return
        self.assertIsNone(result)
        return cls

    def __check_implementer_redundant(self, Base):
        # If we @implementer exactly what was already present, we write
        # no declared attributes on the parent (we still set everything, though)
        Base, IBase = self._check_implementer(Base)

        class Child(Base):
            pass

        returned = self._callFUT(Child, IBase)
        self.assertIn('__implemented__', returned.__dict__)
        self.assertNotIn('__providedBy__', returned.__dict__)
        self.assertIn('__provides__', returned.__dict__)

        spec = Child.__implemented__
        self.assertEqual(spec.declared, ())
        self.assertEqual(spec.inherit, Child)

        self.assertTrue(IBase.providedBy(Child()))

    def test_redundant_implementer_empty_class_declarations_newstyle(self):
        self.__check_implementer_redundant(type('Foo', (object,), {}))

    def test_redundant_implementer_empty_class_declarations_oldstyle(self):
        # This only matters on Python 2
        class Foo:
            pass
        self.__check_implementer_redundant(Foo)

    def test_redundant_implementer_Interface(self):
        from zope.interface import Interface
        from zope.interface import implementedBy
        from zope.interface import ro
        from zope.interface.tests.test_ro import C3Setting

        class Foo(object):
            pass

        with C3Setting(ro.C3.STRICT_IRO, False):
            self._callFUT(Foo, Interface)
            self.assertEqual(list(implementedBy(Foo)), [Interface])

            class Baz(Foo):
                pass

            self._callFUT(Baz, Interface)
            self.assertEqual(list(implementedBy(Baz)), [Interface])

    def _order_for_two(self, applied_first, applied_second):
        return (applied_first, applied_second)

    def test_w_existing_Implements(self):
        from zope.interface.declarations import Implements
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        IBar = InterfaceClass('IBar')
        impl = Implements(IFoo)
        impl.declared = (IFoo,)
        class Foo(object):
            __implemented__ = impl
        impl.inherit = Foo
        self._callFUT(Foo, IBar)
        # Same spec, now different values
        self.assertIs(Foo.__implemented__, impl)
        self.assertEqual(impl.inherit, Foo)
        self.assertEqual(impl.declared,
                         self._order_for_two(IFoo, IBar))

    def test_w_existing_Implements_w_bases(self):
        from zope.interface.declarations import Implements
        from zope.interface.interface import InterfaceClass
        IRoot = InterfaceClass('IRoot')
        ISecondRoot = InterfaceClass('ISecondRoot')
        IExtendsRoot = InterfaceClass('IExtendsRoot', (IRoot,))

        impl_root = Implements.named('Root', IRoot)
        impl_root.declared = (IRoot,)

        class Root1(object):
            __implemented__ = impl_root
        class Root2(object):
            __implemented__ = impl_root

        impl_extends_root = Implements.named('ExtendsRoot1', IExtendsRoot)
        impl_extends_root.declared = (IExtendsRoot,)
        class ExtendsRoot(Root1, Root2):
            __implemented__ = impl_extends_root
        impl_extends_root.inherit = ExtendsRoot

        self._callFUT(ExtendsRoot, ISecondRoot)
        # Same spec, now different values
        self.assertIs(ExtendsRoot.__implemented__, impl_extends_root)
        self.assertEqual(impl_extends_root.inherit, ExtendsRoot)
        self.assertEqual(impl_extends_root.declared,
                         self._order_for_two(IExtendsRoot, ISecondRoot,))
        self.assertEqual(impl_extends_root.__bases__,
                         self._order_for_two(IExtendsRoot, ISecondRoot) + (impl_root,))


class Test_classImplementsFirst(Test_classImplements):

    def _callFUT(self, cls, iface):
        from zope.interface.declarations import classImplementsFirst
        result = classImplementsFirst(cls, iface) # pylint:disable=assignment-from-no-return
        self.assertIsNone(result)
        return cls

    def _order_for_two(self, applied_first, applied_second):
        return (applied_second, applied_first)


class Test__implements_advice(unittest.TestCase):

    def _callFUT(self, *args, **kw):
        from zope.interface.declarations import _implements_advice
        return _implements_advice(*args, **kw)

    def test_no_existing_implements(self):
        from zope.interface.declarations import classImplements
        from zope.interface.declarations import Implements
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        class Foo(object):
            __implements_advice_data__ = ((IFoo,), classImplements)
        self._callFUT(Foo)
        self.assertNotIn('__implements_advice_data__', Foo.__dict__)
        self.assertIsInstance(Foo.__implemented__, Implements) # pylint:disable=no-member
        self.assertEqual(list(Foo.__implemented__), [IFoo]) # pylint:disable=no-member


class Test_implementer(Test_classImplements):

    def _getTargetClass(self):
        from zope.interface.declarations import implementer
        return implementer

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def _callFUT(self, cls, *ifaces):
        decorator = self._makeOne(*ifaces)
        return decorator(cls)

    def test_nonclass_cannot_assign_attr(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        decorator = self._makeOne(IFoo)
        self.assertRaises(TypeError, decorator, object())

    def test_nonclass_can_assign_attr(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        class Foo(object):
            pass
        foo = Foo()
        decorator = self._makeOne(IFoo)
        returned = decorator(foo)
        self.assertTrue(returned is foo)
        spec = foo.__implemented__ # pylint:disable=no-member
        self.assertEqual(spec.__name__, 'zope.interface.tests.test_declarations.?')
        self.assertIsNone(spec.inherit,)
        self.assertIs(foo.__implemented__, spec) # pylint:disable=no-member

    def test_does_not_leak_on_unique_classes(self):
        # Make sure nothing is hanging on to the class or Implements
        # object after they go out of scope. There was briefly a bug
        # in 5.x that caused SpecificationBase._bases (in C) to not be
        # traversed or cleared.
        # https://github.com/zopefoundation/zope.interface/issues/216
        import gc
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')

        begin_count = len(gc.get_objects())

        for _ in range(1900):
            class TestClass(object):
                pass

            self._callFUT(TestClass, IFoo)

        gc.collect()

        end_count = len(gc.get_objects())

        # How many new objects might still be around? In all currently
        # tested interpreters, there aren't any, so our counts should
        # match exactly. When the bug existed, in a steady state, the loop
        # would grow by two objects each iteration
        fudge_factor = 0
        self.assertLessEqual(end_count, begin_count + fudge_factor)



class Test_implementer_only(Test_classImplementsOnly):

    def _getTargetClass(self):
        from zope.interface.declarations import implementer_only
        return implementer_only

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def _callFUT(self, cls, iface):
        decorator = self._makeOne(iface)
        return decorator(cls)

    def test_function(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        decorator = self._makeOne(IFoo)
        def _function():
            raise NotImplementedError()
        self.assertRaises(ValueError, decorator, _function)

    def test_method(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass('IFoo')
        decorator = self._makeOne(IFoo)
        class Bar:
            def _method(self):
                raise NotImplementedError()
        self.assertRaises(ValueError, decorator, Bar._method)



# Test '_implements' by way of 'implements{,Only}', its only callers.

class Test_implementsOnly(unittest.TestCase, _Py3ClassAdvice):

    def test_simple(self):
        import warnings
        from zope.interface.declarations import implementsOnly
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        globs = {'implementsOnly': implementsOnly,
                 'IFoo': IFoo,
                }
        locs = {}
        CODE = "\n".join([
            'class Foo(object):'
            '    implementsOnly(IFoo)',
            ])
        with warnings.catch_warnings(record=True) as log:
            warnings.resetwarnings()
            try:
                exec(CODE, globs, locs)  # pylint:disable=exec-used
            except TypeError:
                self.assertTrue(PYTHON3, "Must be Python 3")
            else:
                if PYTHON3:
                    self.fail("Didn't raise TypeError")
                Foo = locs['Foo']
                spec = Foo.__implemented__
                self.assertEqual(list(spec), [IFoo])
                self.assertEqual(len(log), 0) # no longer warn

    def test_called_once_from_class_w_bases(self):
        from zope.interface.declarations import implements
        from zope.interface.declarations import implementsOnly
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        IBar = InterfaceClass("IBar")
        globs = {'implements': implements,
                 'implementsOnly': implementsOnly,
                 'IFoo': IFoo,
                 'IBar': IBar,
                }
        locs = {}
        CODE = "\n".join([
            'class Foo(object):',
            '    implements(IFoo)',
            'class Bar(Foo):'
            '    implementsOnly(IBar)',
            ])
        if self._run_generated_code(CODE, globs, locs):
            Bar = locs['Bar']
            spec = Bar.__implemented__
            self.assertEqual(list(spec), [IBar])


class Test_implements(unittest.TestCase, _Py3ClassAdvice):

    def test_called_from_function(self):
        import warnings
        from zope.interface.declarations import implements
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        globs = {'implements': implements, 'IFoo': IFoo}
        locs = {}
        CODE = "\n".join([
            'def foo():',
            '    implements(IFoo)'
            ])
        if self._run_generated_code(CODE, globs, locs, False):
            foo = locs['foo']
            with warnings.catch_warnings(record=True) as log:
                warnings.resetwarnings()
                self.assertRaises(TypeError, foo)
                self.assertEqual(len(log), 0) # no longer warn

    def test_called_twice_from_class(self):
        import warnings
        from zope.interface.declarations import implements
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        IBar = InterfaceClass("IBar")
        globs = {'implements': implements, 'IFoo': IFoo, 'IBar': IBar}
        locs = {}
        CODE = "\n".join([
            'class Foo(object):',
            '    implements(IFoo)',
            '    implements(IBar)',
            ])
        with warnings.catch_warnings(record=True) as log:
            warnings.resetwarnings()
            try:
                exec(CODE, globs, locs)  # pylint:disable=exec-used
            except TypeError:
                if not PYTHON3:
                    self.assertEqual(len(log), 0) # no longer warn
            else:
                self.fail("Didn't raise TypeError")

    def test_called_once_from_class(self):
        from zope.interface.declarations import implements
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        globs = {'implements': implements, 'IFoo': IFoo}
        locs = {}
        CODE = "\n".join([
            'class Foo(object):',
            '    implements(IFoo)',
            ])
        if self._run_generated_code(CODE, globs, locs):
            Foo = locs['Foo']
            spec = Foo.__implemented__
            self.assertEqual(list(spec), [IFoo])


class ProvidesClassTests(unittest.TestCase):

    def _getTargetClass(self):
        from zope.interface.declarations import ProvidesClass
        return ProvidesClass

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_simple_class_one_interface(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        spec = self._makeOne(Foo, IFoo)
        self.assertEqual(list(spec), [IFoo])

    def test___reduce__(self):
        from zope.interface.declarations import Provides # the function
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        spec = self._makeOne(Foo, IFoo)
        klass, args = spec.__reduce__()
        self.assertTrue(klass is Provides)
        self.assertEqual(args, (Foo, IFoo))

    def test___get___class(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        spec = self._makeOne(Foo, IFoo)
        Foo.__provides__ = spec
        self.assertTrue(Foo.__provides__ is spec)

    def test___get___instance(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        spec = self._makeOne(Foo, IFoo)
        Foo.__provides__ = spec
        def _test():
            foo = Foo()
            return foo.__provides__
        self.assertRaises(AttributeError, _test)

    def test__repr__(self):
        inst = self._makeOne(type(self))
        self.assertEqual(
            repr(inst),
            "<zope.interface.Provides for %r>"  % type(self)
        )


class Test_Provides(unittest.TestCase):

    def _callFUT(self, *args, **kw):
        from zope.interface.declarations import Provides
        return Provides(*args, **kw)

    def test_no_cached_spec(self):
        from zope.interface import declarations
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        cache = {}
        class Foo(object):
            pass
        with _Monkey(declarations, InstanceDeclarations=cache):
            spec = self._callFUT(Foo, IFoo)
        self.assertEqual(list(spec), [IFoo])
        self.assertTrue(cache[(Foo, IFoo)] is spec)

    def test_w_cached_spec(self):
        from zope.interface import declarations
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        prior = object()
        class Foo(object):
            pass
        cache = {(Foo, IFoo): prior}
        with _Monkey(declarations, InstanceDeclarations=cache):
            spec = self._callFUT(Foo, IFoo)
        self.assertTrue(spec is prior)


class Test_directlyProvides(unittest.TestCase):

    def _callFUT(self, *args, **kw):
        from zope.interface.declarations import directlyProvides
        return directlyProvides(*args, **kw)

    def test_w_normal_object(self):
        from zope.interface.declarations import ProvidesClass
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        obj = Foo()
        self._callFUT(obj, IFoo)
        self.assertIsInstance(obj.__provides__, ProvidesClass) # pylint:disable=no-member
        self.assertEqual(list(obj.__provides__), [IFoo]) # pylint:disable=no-member

    def test_w_class(self):
        from zope.interface.declarations import ClassProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        self._callFUT(Foo, IFoo)
        self.assertIsInstance(Foo.__provides__, ClassProvides) # pylint:disable=no-member
        self.assertEqual(list(Foo.__provides__), [IFoo]) # pylint:disable=no-member

    @_skip_under_py3k
    def test_w_non_descriptor_aware_metaclass(self):
        # There are no non-descriptor-aware types in Py3k
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class MetaClass(type):
            def __getattribute__(cls, name):
                # Emulate metaclass whose base is not the type object.
                if name == '__class__':
                    return cls
                # Under certain circumstances, the implementedByFallback
                # can get here for __dict__
                return type.__getattribute__(cls, name) # pragma: no cover

        class Foo(object):
            __metaclass__ = MetaClass
        obj = Foo()
        self.assertRaises(TypeError, self._callFUT, obj, IFoo)

    def test_w_classless_object(self):
        from zope.interface.declarations import ProvidesClass
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        the_dict = {}
        class Foo(object):
            def __getattribute__(self, name):
                # Emulate object w/o any class
                if name == '__class__':
                    return None
                raise NotImplementedError(name)
            def __setattr__(self, name, value):
                the_dict[name] = value
        obj = Foo()
        self._callFUT(obj, IFoo)
        self.assertIsInstance(the_dict['__provides__'], ProvidesClass)
        self.assertEqual(list(the_dict['__provides__']), [IFoo])


class Test_alsoProvides(unittest.TestCase):

    def _callFUT(self, *args, **kw):
        from zope.interface.declarations import alsoProvides
        return alsoProvides(*args, **kw)

    def test_wo_existing_provides(self):
        from zope.interface.declarations import ProvidesClass
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        obj = Foo()
        self._callFUT(obj, IFoo)
        self.assertIsInstance(obj.__provides__, ProvidesClass) # pylint:disable=no-member
        self.assertEqual(list(obj.__provides__), [IFoo]) # pylint:disable=no-member

    def test_w_existing_provides(self):
        from zope.interface.declarations import directlyProvides
        from zope.interface.declarations import ProvidesClass
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        IBar = InterfaceClass("IBar")
        class Foo(object):
            pass
        obj = Foo()
        directlyProvides(obj, IFoo)
        self._callFUT(obj, IBar)
        self.assertIsInstance(obj.__provides__, ProvidesClass) # pylint:disable=no-member
        self.assertEqual(list(obj.__provides__), [IFoo, IBar]) # pylint:disable=no-member


class Test_noLongerProvides(unittest.TestCase):

    def _callFUT(self, *args, **kw):
        from zope.interface.declarations import noLongerProvides
        return noLongerProvides(*args, **kw)

    def test_wo_existing_provides(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        obj = Foo()
        self._callFUT(obj, IFoo)
        self.assertEqual(list(obj.__provides__), []) # pylint:disable=no-member

    def test_w_existing_provides_hit(self):
        from zope.interface.declarations import directlyProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        obj = Foo()
        directlyProvides(obj, IFoo)
        self._callFUT(obj, IFoo)
        self.assertEqual(list(obj.__provides__), []) # pylint:disable=no-member

    def test_w_existing_provides_miss(self):
        from zope.interface.declarations import directlyProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        IBar = InterfaceClass("IBar")
        class Foo(object):
            pass
        obj = Foo()
        directlyProvides(obj, IFoo)
        self._callFUT(obj, IBar)
        self.assertEqual(list(obj.__provides__), [IFoo]) # pylint:disable=no-member

    def test_w_iface_implemented_by_class(self):
        from zope.interface.declarations import implementer
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        @implementer(IFoo)
        class Foo(object):
            pass
        obj = Foo()
        self.assertRaises(ValueError, self._callFUT, obj, IFoo)


class ClassProvidesBaseFallbackTests(unittest.TestCase):

    def _getTargetClass(self):
        # pylint:disable=no-name-in-module
        from zope.interface.declarations import ClassProvidesBaseFallback
        return ClassProvidesBaseFallback

    def _makeOne(self, klass, implements):
        # Don't instantiate directly:  the C version can't have attributes
        # assigned.
        class Derived(self._getTargetClass()):
            def __init__(self, k, i):
                self._cls = k
                self._implements = i
        return Derived(klass, implements)

    def test_w_same_class_via_class(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        cpbp = Foo.__provides__ = self._makeOne(Foo, IFoo)
        self.assertTrue(Foo.__provides__ is cpbp)

    def test_w_same_class_via_instance(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        foo = Foo()
        Foo.__provides__ = self._makeOne(Foo, IFoo)
        self.assertIs(foo.__provides__, IFoo)

    def test_w_different_class(self):
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        class Bar(Foo):
            pass
        bar = Bar()
        Foo.__provides__ = self._makeOne(Foo, IFoo)
        self.assertRaises(AttributeError, getattr, Bar, '__provides__')
        self.assertRaises(AttributeError, getattr, bar, '__provides__')


class ClassProvidesBaseTests(OptimizationTestMixin,
                             ClassProvidesBaseFallbackTests):
    # Repeat tests for C optimizations

    def _getTargetClass(self):
        from zope.interface.declarations import ClassProvidesBase
        return ClassProvidesBase

    def _getFallbackClass(self):
        # pylint:disable=no-name-in-module
        from zope.interface.declarations import ClassProvidesBaseFallback
        return ClassProvidesBaseFallback


class ClassProvidesTests(unittest.TestCase):

    def _getTargetClass(self):
        from zope.interface.declarations import ClassProvides
        return ClassProvides

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_w_simple_metaclass(self):
        from zope.interface.declarations import implementer
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        IBar = InterfaceClass("IBar")
        @implementer(IFoo)
        class Foo(object):
            pass
        cp = Foo.__provides__ = self._makeOne(Foo, type(Foo), IBar)
        self.assertTrue(Foo.__provides__ is cp)
        self.assertEqual(list(Foo().__provides__), [IFoo])

    def test___reduce__(self):
        from zope.interface.declarations import implementer
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        IBar = InterfaceClass("IBar")
        @implementer(IFoo)
        class Foo(object):
            pass
        cp = Foo.__provides__ = self._makeOne(Foo, type(Foo), IBar)
        self.assertEqual(cp.__reduce__(),
                         (self._getTargetClass(), (Foo, type(Foo), IBar)))

    def test__repr__(self):
        inst = self._makeOne(type(self), type)
        self.assertEqual(
            repr(inst),
            "<zope.interface.declarations.ClassProvides for %r>"  % type(self)
        )

class Test_directlyProvidedBy(unittest.TestCase):

    def _callFUT(self, *args, **kw):
        from zope.interface.declarations import directlyProvidedBy
        return directlyProvidedBy(*args, **kw)

    def test_wo_declarations_in_class_or_instance(self):
        class Foo(object):
            pass
        foo = Foo()
        self.assertEqual(list(self._callFUT(foo)), [])

    def test_w_declarations_in_class_but_not_instance(self):
        from zope.interface.declarations import implementer
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        @implementer(IFoo)
        class Foo(object):
            pass
        foo = Foo()
        self.assertEqual(list(self._callFUT(foo)), [])

    def test_w_declarations_in_instance_but_not_class(self):
        from zope.interface.declarations import directlyProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        foo = Foo()
        directlyProvides(foo, IFoo)
        self.assertEqual(list(self._callFUT(foo)), [IFoo])

    def test_w_declarations_in_instance_and_class(self):
        from zope.interface.declarations import directlyProvides
        from zope.interface.declarations import implementer
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        IBar = InterfaceClass("IBar")
        @implementer(IFoo)
        class Foo(object):
            pass
        foo = Foo()
        directlyProvides(foo, IBar)
        self.assertEqual(list(self._callFUT(foo)), [IBar])


class Test_classProvides(unittest.TestCase, _Py3ClassAdvice):
    # pylint:disable=exec-used

    def test_called_from_function(self):
        import warnings
        from zope.interface.declarations import classProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        globs = {'classProvides': classProvides, 'IFoo': IFoo}
        locs = {}
        CODE = "\n".join([
            'def foo():',
            '    classProvides(IFoo)'
            ])
        exec(CODE, globs, locs)
        foo = locs['foo']
        with warnings.catch_warnings(record=True) as log:
            warnings.resetwarnings()
            self.assertRaises(TypeError, foo)
            if not PYTHON3:
                self.assertEqual(len(log), 0) # no longer warn

    def test_called_twice_from_class(self):
        import warnings
        from zope.interface.declarations import classProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        IBar = InterfaceClass("IBar")
        globs = {'classProvides': classProvides, 'IFoo': IFoo, 'IBar': IBar}
        locs = {}
        CODE = "\n".join([
            'class Foo(object):',
            '    classProvides(IFoo)',
            '    classProvides(IBar)',
            ])
        with warnings.catch_warnings(record=True) as log:
            warnings.resetwarnings()
            try:
                exec(CODE, globs, locs)
            except TypeError:
                if not PYTHON3:
                    self.assertEqual(len(log), 0) # no longer warn
            else:
                self.fail("Didn't raise TypeError")

    def test_called_once_from_class(self):
        from zope.interface.declarations import classProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        globs = {'classProvides': classProvides, 'IFoo': IFoo}
        locs = {}
        CODE = "\n".join([
            'class Foo(object):',
            '    classProvides(IFoo)',
            ])
        if self._run_generated_code(CODE, globs, locs):
            Foo = locs['Foo']
            spec = Foo.__providedBy__
            self.assertEqual(list(spec), [IFoo])

# Test _classProvides_advice through classProvides, its only caller.


class Test_provider(unittest.TestCase):

    def _getTargetClass(self):
        from zope.interface.declarations import provider
        return provider

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_w_class(self):
        from zope.interface.declarations import ClassProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        @self._makeOne(IFoo)
        class Foo(object):
            pass
        self.assertIsInstance(Foo.__provides__, ClassProvides) # pylint:disable=no-member
        self.assertEqual(list(Foo.__provides__), [IFoo]) # pylint:disable=no-member


class Test_moduleProvides(unittest.TestCase):
    # pylint:disable=exec-used

    def test_called_from_function(self):
        from zope.interface.declarations import moduleProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        globs = {'__name__': 'zope.interface.tests.foo',
                 'moduleProvides': moduleProvides, 'IFoo': IFoo}
        locs = {}
        CODE = "\n".join([
            'def foo():',
            '    moduleProvides(IFoo)'
            ])
        exec(CODE, globs, locs)
        foo = locs['foo']
        self.assertRaises(TypeError, foo)

    def test_called_from_class(self):
        from zope.interface.declarations import moduleProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        globs = {'__name__': 'zope.interface.tests.foo',
                 'moduleProvides': moduleProvides, 'IFoo': IFoo}
        locs = {}
        CODE = "\n".join([
            'class Foo(object):',
            '    moduleProvides(IFoo)',
            ])
        with self.assertRaises(TypeError):
            exec(CODE, globs, locs)

    def test_called_once_from_module_scope(self):
        from zope.interface.declarations import moduleProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        globs = {'__name__': 'zope.interface.tests.foo',
                 'moduleProvides': moduleProvides, 'IFoo': IFoo}
        CODE = "\n".join([
            'moduleProvides(IFoo)',
            ])
        exec(CODE, globs)
        spec = globs['__provides__']
        self.assertEqual(list(spec), [IFoo])

    def test_called_twice_from_module_scope(self):
        from zope.interface.declarations import moduleProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        globs = {'__name__': 'zope.interface.tests.foo',
                 'moduleProvides': moduleProvides, 'IFoo': IFoo}

        CODE = "\n".join([
            'moduleProvides(IFoo)',
            'moduleProvides(IFoo)',
            ])
        with self.assertRaises(TypeError):
            exec(CODE, globs)


class Test_getObjectSpecificationFallback(unittest.TestCase):

    def _getFallbackClass(self):
        # pylint:disable=no-name-in-module
        from zope.interface.declarations import getObjectSpecificationFallback
        return getObjectSpecificationFallback

    _getTargetClass = _getFallbackClass

    def _callFUT(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_wo_existing_provides_classless(self):
        the_dict = {}
        class Foo(object):
            def __getattribute__(self, name):
                # Emulate object w/o any class
                if name == '__class__':
                    raise AttributeError(name)
                try:
                    return the_dict[name]
                except KeyError:
                    raise AttributeError(name)
            def __setattr__(self, name, value):
                raise NotImplementedError()
        foo = Foo()
        spec = self._callFUT(foo)
        self.assertEqual(list(spec), [])

    def test_existing_provides_is_spec(self):
        from zope.interface.declarations import directlyProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        def foo():
            raise NotImplementedError()
        directlyProvides(foo, IFoo)
        spec = self._callFUT(foo)
        self.assertIs(spec, foo.__provides__) # pylint:disable=no-member

    def test_existing_provides_is_not_spec(self):
        def foo():
            raise NotImplementedError()
        foo.__provides__ = object() # not a valid spec
        spec = self._callFUT(foo)
        self.assertEqual(list(spec), [])

    def test_existing_provides(self):
        from zope.interface.declarations import directlyProvides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        foo = Foo()
        directlyProvides(foo, IFoo)
        spec = self._callFUT(foo)
        self.assertEqual(list(spec), [IFoo])

    def test_wo_provides_on_class_w_implements(self):
        from zope.interface.declarations import implementer
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        @implementer(IFoo)
        class Foo(object):
            pass
        foo = Foo()
        spec = self._callFUT(foo)
        self.assertEqual(list(spec), [IFoo])

    def test_wo_provides_on_class_wo_implements(self):
        class Foo(object):
            pass
        foo = Foo()
        spec = self._callFUT(foo)
        self.assertEqual(list(spec), [])

    def test_catches_only_AttributeError_on_provides(self):
        MissingSomeAttrs.test_raises(self, self._callFUT, expected_missing='__provides__')

    def test_catches_only_AttributeError_on_class(self):
        MissingSomeAttrs.test_raises(self, self._callFUT, expected_missing='__class__',
                                     __provides__=None)

    def test_raises_AttributeError_when_provides_fails_type_check_AttributeError(self):
        # isinstance(ob.__provides__, SpecificationBase) is not
        # protected inside any kind of block.

        class Foo(object):
            __provides__ = MissingSomeAttrs(AttributeError)

        # isinstance() ignores AttributeError on __class__
        self._callFUT(Foo())

    def test_raises_AttributeError_when_provides_fails_type_check_RuntimeError(self):
        # isinstance(ob.__provides__, SpecificationBase) is not
        # protected inside any kind of block.
        class Foo(object):
            __provides__ = MissingSomeAttrs(RuntimeError)

        if PYTHON3:
            with self.assertRaises(RuntimeError) as exc:
                self._callFUT(Foo())

            self.assertEqual('__class__', exc.exception.args[0])
        else:
            # Python 2 catches everything.
            self._callFUT(Foo())


class Test_getObjectSpecification(Test_getObjectSpecificationFallback,
                                  OptimizationTestMixin):
    # Repeat tests for C optimizations

    def _getTargetClass(self):
        from zope.interface.declarations import getObjectSpecification
        return getObjectSpecification


class Test_providedByFallback(unittest.TestCase):

    def _getFallbackClass(self):
        # pylint:disable=no-name-in-module
        from zope.interface.declarations import providedByFallback
        return providedByFallback

    _getTargetClass = _getFallbackClass

    def _callFUT(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_wo_providedBy_on_class_wo_implements(self):
        class Foo(object):
            pass
        foo = Foo()
        spec = self._callFUT(foo)
        self.assertEqual(list(spec), [])

    def test_w_providedBy_valid_spec(self):
        from zope.interface.declarations import Provides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        foo = Foo()
        foo.__providedBy__ = Provides(Foo, IFoo)
        spec = self._callFUT(foo)
        self.assertEqual(list(spec), [IFoo])

    def test_w_providedBy_invalid_spec(self):
        class Foo(object):
            pass
        foo = Foo()
        foo.__providedBy__ = object()
        spec = self._callFUT(foo)
        self.assertEqual(list(spec), [])

    def test_w_providedBy_invalid_spec_class_w_implements(self):
        from zope.interface.declarations import implementer
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        @implementer(IFoo)
        class Foo(object):
            pass
        foo = Foo()
        foo.__providedBy__ = object()
        spec = self._callFUT(foo)
        self.assertEqual(list(spec), [IFoo])

    def test_w_providedBy_invalid_spec_w_provides_no_provides_on_class(self):
        class Foo(object):
            pass
        foo = Foo()
        foo.__providedBy__ = object()
        expected = foo.__provides__ = object()
        spec = self._callFUT(foo)
        self.assertTrue(spec is expected)

    def test_w_providedBy_invalid_spec_w_provides_diff_provides_on_class(self):
        class Foo(object):
            pass
        foo = Foo()
        foo.__providedBy__ = object()
        expected = foo.__provides__ = object()
        Foo.__provides__ = object()
        spec = self._callFUT(foo)
        self.assertTrue(spec is expected)

    def test_w_providedBy_invalid_spec_w_provides_same_provides_on_class(self):
        from zope.interface.declarations import implementer
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        @implementer(IFoo)
        class Foo(object):
            pass
        foo = Foo()
        foo.__providedBy__ = object()
        foo.__provides__ = Foo.__provides__ = object()
        spec = self._callFUT(foo)
        self.assertEqual(list(spec), [IFoo])

    def test_super_when_base_implements_interface(self):
        from zope.interface import Interface
        from zope.interface.declarations import implementer

        class IBase(Interface):
            pass

        class IDerived(IBase):
            pass

        @implementer(IBase)
        class Base(object):
            pass

        @implementer(IDerived)
        class Derived(Base):
            pass

        derived = Derived()
        self.assertEqual(list(self._callFUT(derived)), [IDerived, IBase])

        sup = super(Derived, derived)
        fut = self._callFUT(sup)
        self.assertIsNone(fut._dependents)
        self.assertEqual(list(fut), [IBase])

    def test_super_when_base_doesnt_implement_interface(self):
        from zope.interface import Interface
        from zope.interface.declarations import implementer

        class IBase(Interface):
            pass

        class IDerived(IBase):
            pass

        class Base(object):
            pass

        @implementer(IDerived)
        class Derived(Base):
            pass

        derived = Derived()
        self.assertEqual(list(self._callFUT(derived)), [IDerived])

        sup = super(Derived, derived)
        self.assertEqual(list(self._callFUT(sup)), [])

    def test_super_when_base_is_object(self):
        from zope.interface import Interface
        from zope.interface.declarations import implementer

        class IBase(Interface):
            pass

        class IDerived(IBase):
            pass

        @implementer(IDerived)
        class Derived(object):
            pass

        derived = Derived()
        self.assertEqual(list(self._callFUT(derived)), [IDerived])

        sup = super(Derived, derived)
        fut = self._callFUT(sup)
        self.assertIsNone(fut._dependents)
        self.assertEqual(list(fut), [])

    def test_super_when_object_directly_provides(self):
        from zope.interface import Interface
        from zope.interface.declarations import implementer
        from zope.interface.declarations import directlyProvides

        class IBase(Interface):
            pass

        class IDerived(IBase):
            pass

        @implementer(IBase)
        class Base(object):
            pass

        class Derived(Base):
            pass

        derived = Derived()
        self.assertEqual(list(self._callFUT(derived)), [IBase])

        directlyProvides(derived, IDerived)
        self.assertEqual(list(self._callFUT(derived)), [IDerived, IBase])

        sup = super(Derived, derived)
        fut = self._callFUT(sup)
        self.assertIsNone(fut._dependents)
        self.assertEqual(list(fut), [IBase])

    def test_super_multi_level_multi_inheritance(self):
        from zope.interface.declarations import implementer
        from zope.interface import Interface

        class IBase(Interface):
            pass

        class IM1(Interface):
            pass

        class IM2(Interface):
            pass

        class IDerived(IBase):
            pass

        class IUnrelated(Interface):
            pass

        @implementer(IBase)
        class Base(object):
            pass

        @implementer(IM1)
        class M1(Base):
            pass

        @implementer(IM2)
        class M2(Base):
            pass

        @implementer(IDerived, IUnrelated)
        class Derived(M1, M2):
            pass

        d = Derived()
        sd = super(Derived, d)
        sm1 = super(M1, d)
        sm2 = super(M2, d)

        self.assertEqual(list(self._callFUT(d)),
                         [IDerived, IUnrelated, IM1, IBase, IM2])
        self.assertEqual(list(self._callFUT(sd)),
                         [IM1, IBase, IM2])
        self.assertEqual(list(self._callFUT(sm1)),
                         [IM2, IBase])
        self.assertEqual(list(self._callFUT(sm2)),
                         [IBase])

    def test_catches_only_AttributeError_on_providedBy(self):
        MissingSomeAttrs.test_raises(self, self._callFUT,
                                     expected_missing='__providedBy__',
                                     __class__=object)

    def test_catches_only_AttributeError_on_class(self):
        # isinstance() tries to get the __class__, which is non-obvious,
        # so it must be protected too.
        PY3 = str is not bytes
        MissingSomeAttrs.test_raises(self, self._callFUT,
                                     expected_missing='__class__' if PY3 else '__providedBy__')



class Test_providedBy(Test_providedByFallback,
                      OptimizationTestMixin):
    # Repeat tests for C optimizations

    def _getTargetClass(self):
        from zope.interface.declarations import providedBy
        return providedBy


class ObjectSpecificationDescriptorFallbackTests(unittest.TestCase):

    def _getFallbackClass(self):
        # pylint:disable=no-name-in-module
        from zope.interface.declarations \
            import ObjectSpecificationDescriptorFallback
        return ObjectSpecificationDescriptorFallback

    _getTargetClass = _getFallbackClass

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_accessed_via_class(self):
        from zope.interface.declarations import Provides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        class Foo(object):
            pass
        Foo.__provides__ = Provides(Foo, IFoo)
        Foo.__providedBy__ = self._makeOne()
        self.assertEqual(list(Foo.__providedBy__), [IFoo])

    def test_accessed_via_inst_wo_provides(self):
        from zope.interface.declarations import implementer
        from zope.interface.declarations import Provides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        IBar = InterfaceClass("IBar")
        @implementer(IFoo)
        class Foo(object):
            pass
        Foo.__provides__ = Provides(Foo, IBar)
        Foo.__providedBy__ = self._makeOne()
        foo = Foo()
        self.assertEqual(list(foo.__providedBy__), [IFoo])

    def test_accessed_via_inst_w_provides(self):
        from zope.interface.declarations import directlyProvides
        from zope.interface.declarations import implementer
        from zope.interface.declarations import Provides
        from zope.interface.interface import InterfaceClass
        IFoo = InterfaceClass("IFoo")
        IBar = InterfaceClass("IBar")
        IBaz = InterfaceClass("IBaz")
        @implementer(IFoo)
        class Foo(object):
            pass
        Foo.__provides__ = Provides(Foo, IBar)
        Foo.__providedBy__ = self._makeOne()
        foo = Foo()
        directlyProvides(foo, IBaz)
        self.assertEqual(list(foo.__providedBy__), [IBaz, IFoo])


class ObjectSpecificationDescriptorTests(
        ObjectSpecificationDescriptorFallbackTests,
        OptimizationTestMixin):
    # Repeat tests for C optimizations

    def _getTargetClass(self):
        from zope.interface.declarations import ObjectSpecificationDescriptor
        return ObjectSpecificationDescriptor


# Test _normalizeargs through its callers.


class _Monkey(object):
    # context-manager for replacing module names in the scope of a test.
    def __init__(self, module, **kw):
        self.module = module
        self.to_restore = {key: getattr(module, key) for key in kw}
        for key, value in kw.items():
            setattr(module, key, value)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        for key, value in self.to_restore.items():
            setattr(self.module, key, value)


class _MonkeyDict(object):
    # context-manager for restoring a dict w/in a module in the scope of a test.
    def __init__(self, module, attrname, **kw):
        self.module = module
        self.target = getattr(module, attrname)
        self.to_restore = self.target.copy()
        self.target.clear()
        self.target.update(kw)

    def __enter__(self):
        return self.target

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.target.clear()
        self.target.update(self.to_restore)

if __name__ == "__main__":
    isT = True
    try:
        temp_class = DeclarationTests()
        res0 = temp_class._getTargetClass()
        if not isinstance(res0(), zope.interface.declarations.Declaration):
            isT=False
    except:
        isT=False
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b59feb7e40a82d2d1291/"):
    #     f = open("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b59feb7e40a82d2d1291/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     object_class=dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class=DeclarationTests()
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class._getTargetClass()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/ro__legacy_mergeOrderings_passk_validte.py
##############################################################################
#
# Copyright (c) 2003 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""
Compute a resolution order for an object and its bases.

.. versionchanged:: 5.0
   The resolution order is now based on the same C3 order that Python
   uses for classes. In complex instances of multiple inheritance, this
   may result in a different ordering.

   In older versions, the ordering wasn't required to be C3 compliant,
   and for backwards compatibility, it still isn't. If the ordering
   isn't C3 compliant (if it is *inconsistent*), zope.interface will
   make a best guess to try to produce a reasonable resolution order.
   Still (just as before), the results in such cases may be
   surprising.

.. rubric:: Environment Variables

Due to the change in 5.0, certain environment variables can be used to control errors
and warnings about inconsistent resolution orders. They are listed in priority order, with
variables at the bottom generally overriding variables above them.

ZOPE_INTERFACE_WARN_BAD_IRO
    If this is set to "1", then if there is at least one inconsistent resolution
    order discovered, a warning (:class:`InconsistentResolutionOrderWarning`) will
    be issued. Use the usual warning mechanisms to control this behaviour. The warning
    text will contain additional information on debugging.
ZOPE_INTERFACE_TRACK_BAD_IRO
    If this is set to "1", then zope.interface will log information about each
    inconsistent resolution order discovered, and keep those details in memory in this module
    for later inspection.
ZOPE_INTERFACE_STRICT_IRO
    If this is set to "1", any attempt to use :func:`ro` that would produce a non-C3
    ordering will fail by raising :class:`InconsistentResolutionOrderError`.

There are two environment variables that are independent.

ZOPE_INTERFACE_LOG_CHANGED_IRO
    If this is set to "1", then if the C3 resolution order is different from
    the legacy resolution order for any given object, a message explaining the differences
    will be logged. This is intended to be used for debugging complicated IROs.
ZOPE_INTERFACE_USE_LEGACY_IRO
    If this is set to "1", then the C3 resolution order will *not* be used. The
    legacy IRO will be used instead. This is a temporary measure and will be removed in the
    future. It is intended to help during the transition.
    It implies ``ZOPE_INTERFACE_LOG_CHANGED_IRO``.
"""
from __future__ import print_function
import sys
sys.path.append("/home/travis/builds/repos/pexip---os-zope")

__docformat__ = 'restructuredtext'

__all__ = [
    'ro',
    'InconsistentResolutionOrderError',
    'InconsistentResolutionOrderWarning',
]

__logger = None

def _logger():
    global __logger # pylint:disable=global-statement
    if __logger is None:
        import logging
        __logger = logging.getLogger(__name__)
    return __logger

def _legacy_mergeOrderings(orderings):
    """Merge multiple orderings so that within-ordering order is preserved

    Orderings are constrained in such a way that if an object appears
    in two or more orderings, then the suffix that begins with the
    object must be in both orderings.

    For example:

    >>> _mergeOrderings([
    ... ['x', 'y', 'z'],
    ... ['q', 'z'],
    ... [1, 3, 5],
    ... ['z']
    ... ])
    ['x', 'y', 'q', 1, 3, 5, 'z']

    """

    seen = set()
    result = []
    for ordering in reversed(orderings):
        for o in reversed(ordering):
            if o not in seen:
                seen.add(o)
                result.insert(0, o)

    return result

def _legacy_flatten(begin):
    result = [begin]
    i = 0
    for ob in iter(result):
        i += 1
        # The recursive calls can be avoided by inserting the base classes
        # into the dynamically growing list directly after the currently
        # considered object;  the iterator makes sure this will keep working
        # in the future, since it cannot rely on the length of the list
        # by definition.
        result[i:i] = ob.__bases__
    return result

def _legacy_ro(ob):
    return _legacy_mergeOrderings([_legacy_flatten(ob)])

###
# Compare base objects using identity, not equality. This matches what
# the CPython MRO algorithm does, and is *much* faster to boot: that,
# plus some other small tweaks makes the difference between 25s and 6s
# in loading 446 plone/zope interface.py modules (1925 InterfaceClass,
# 1200 Implements, 1100 ClassProvides objects)
###


class InconsistentResolutionOrderWarning(PendingDeprecationWarning):
    """
    The warning issued when an invalid IRO is requested.
    """

class InconsistentResolutionOrderError(TypeError):
    """
    The error raised when an invalid IRO is requested in strict mode.
    """

    def __init__(self, c3, base_tree_remaining):
        self.C = c3.leaf
        base_tree = c3.base_tree
        self.base_ros = {
            base: base_tree[i + 1]
            for i, base in enumerate(self.C.__bases__)
        }
        # Unfortunately, this doesn't necessarily directly match
        # up to any transformation on C.__bases__, because
        # if any were fully used up, they were removed already.
        self.base_tree_remaining = base_tree_remaining

        TypeError.__init__(self)

    def __str__(self):
        import pprint
        return "%s: For object %r.\nBase ROs:\n%s\nConflict Location:\n%s" % (
            self.__class__.__name__,
            self.C,
            pprint.pformat(self.base_ros),
            pprint.pformat(self.base_tree_remaining),
        )


class _NamedBool(int): # cannot actually inherit bool

    def __new__(cls, val, name):
        inst = super(cls, _NamedBool).__new__(cls, val)
        inst.__name__ = name
        return inst


class _ClassBoolFromEnv(object):
    """
    Non-data descriptor that reads a transformed environment variable
    as a boolean, and caches the result in the class.
    """

    def __get__(self, inst, klass):
        import os
        for cls in klass.__mro__:
            my_name = None
            for k in dir(klass):
                if k in cls.__dict__ and cls.__dict__[k] is self:
                    my_name = k
                    break
            if my_name is not None:
                break
        else: # pragma: no cover
            raise RuntimeError("Unable to find self")

        env_name = 'ZOPE_INTERFACE_' + my_name
        val = os.environ.get(env_name, '') == '1'
        val = _NamedBool(val, my_name)
        setattr(klass, my_name, val)
        setattr(klass, 'ORIG_' + my_name, self)
        return val


class _StaticMRO(object):
    # A previously resolved MRO, supplied by the caller.
    # Used in place of calculating it.

    had_inconsistency = None # We don't know...

    def __init__(self, C, mro):
        self.leaf = C
        self.__mro = tuple(mro)

    def mro(self):
        return list(self.__mro)


class C3(object):
    # Holds the shared state during computation of an MRO.

    @staticmethod
    def resolver(C, strict, base_mros):
        strict = strict if strict is not None else C3.STRICT_IRO
        factory = C3
        if strict:
            factory = _StrictC3
        elif C3.TRACK_BAD_IRO:
            factory = _TrackingC3

        memo = {}
        base_mros = base_mros or {}
        for base, mro in base_mros.items():
            assert base in C.__bases__
            memo[base] = _StaticMRO(base, mro)

        return factory(C, memo)

    __mro = None
    __legacy_ro = None
    direct_inconsistency = False

    def __init__(self, C, memo):
        self.leaf = C
        self.memo = memo
        kind = self.__class__

        base_resolvers = []
        for base in C.__bases__:
            if base not in memo:
                resolver = kind(base, memo)
                memo[base] = resolver
            base_resolvers.append(memo[base])

        self.base_tree = [
            [C]
        ] + [
            memo[base].mro() for base in C.__bases__
        ] + [
            list(C.__bases__)
        ]

        self.bases_had_inconsistency = any(base.had_inconsistency for base in base_resolvers)

        if len(C.__bases__) == 1:
            self.__mro = [C] + memo[C.__bases__[0]].mro()

    @property
    def had_inconsistency(self):
        return self.direct_inconsistency or self.bases_had_inconsistency

    @property
    def legacy_ro(self):
        if self.__legacy_ro is None:
            self.__legacy_ro = tuple(_legacy_ro(self.leaf))
        return list(self.__legacy_ro)

    TRACK_BAD_IRO = _ClassBoolFromEnv()
    STRICT_IRO = _ClassBoolFromEnv()
    WARN_BAD_IRO = _ClassBoolFromEnv()
    LOG_CHANGED_IRO = _ClassBoolFromEnv()
    USE_LEGACY_IRO = _ClassBoolFromEnv()
    BAD_IROS = ()

    def _warn_iro(self):
        if not self.WARN_BAD_IRO:
            # For the initial release, one must opt-in to see the warning.
            # In the future (2021?) seeing at least the first warning will
            # be the default
            return
        import warnings
        warnings.warn(
            "An inconsistent resolution order is being requested. "
            "(Interfaces should follow the Python class rules known as C3.) "
            "For backwards compatibility, zope.interface will allow this, "
            "making the best guess it can to produce as meaningful an order as possible. "
            "In the future this might be an error. Set the warning filter to error, or set "
            "the environment variable 'ZOPE_INTERFACE_TRACK_BAD_IRO' to '1' and examine "
            "ro.C3.BAD_IROS to debug, or set 'ZOPE_INTERFACE_STRICT_IRO' to raise exceptions.",
            InconsistentResolutionOrderWarning,
        )

    @staticmethod
    def _can_choose_base(base, base_tree_remaining):
        # From C3:
        # nothead = [s for s in nonemptyseqs if cand in s[1:]]
        for bases in base_tree_remaining:
            if not bases or bases[0] is base:
                continue

            for b in bases:
                if b is base:
                    return False
        return True

    @staticmethod
    def _nonempty_bases_ignoring(base_tree, ignoring):
        return list(filter(None, [
            [b for b in bases if b is not ignoring]
            for bases
            in base_tree
        ]))

    def _choose_next_base(self, base_tree_remaining):
        """
        Return the next base.

        The return value will either fit the C3 constraints or be our best
        guess about what to do. If we cannot guess, this may raise an exception.
        """
        base = self._find_next_C3_base(base_tree_remaining)
        if base is not None:
            return base
        return self._guess_next_base(base_tree_remaining)

    def _find_next_C3_base(self, base_tree_remaining):
        """
        Return the next base that fits the constraints, or ``None`` if there isn't one.
        """
        for bases in base_tree_remaining:
            base = bases[0]
            if self._can_choose_base(base, base_tree_remaining):
                return base
        return None

    class _UseLegacyRO(Exception):
        pass

    def _guess_next_base(self, base_tree_remaining):
        # Narf. We may have an inconsistent order (we won't know for
        # sure until we check all the bases). Python cannot create
        # classes like this:
        #
        # class B1:
        #   pass
        # class B2(B1):
        #   pass
        # class C(B1, B2): # -> TypeError; this is like saying C(B1, B2, B1).
        #  pass
        #
        # However, older versions of zope.interface were fine with this order.
        # A good example is ``providedBy(IOError())``. Because of the way
        # ``classImplements`` works, it winds up with ``__bases__`` ==
        # ``[IEnvironmentError, IIOError, IOSError, <implementedBy Exception>]``
        # (on Python 3). But ``IEnvironmentError`` is a base of both ``IIOError``
        # and ``IOSError``. Previously, we would get a resolution order of
        # ``[IIOError, IOSError, IEnvironmentError, IStandardError, IException, Interface]``
        # but the standard Python algorithm would forbid creating that order entirely.

        # Unlike Python's MRO, we attempt to resolve the issue. A few
        # heuristics have been tried. One was:
        #
        # Strip off the first (highest priority) base of each direct
        # base one at a time and seeing if we can come to an agreement
        # with the other bases. (We're trying for a partial ordering
        # here.) This often resolves cases (such as the IOSError case
        # above), and frequently produces the same ordering as the
        # legacy MRO did. If we looked at all the highest priority
        # bases and couldn't find any partial ordering, then we strip
        # them *all* out and begin the C3 step again. We take care not
        # to promote a common root over all others.
        #
        # If we only did the first part, stripped off the first
        # element of the first item, we could resolve simple cases.
        # But it tended to fail badly. If we did the whole thing, it
        # could be extremely painful from a performance perspective
        # for deep/wide things like Zope's OFS.SimpleItem.Item. Plus,
        # anytime you get ExtensionClass.Base into the mix, you're
        # likely to wind up in trouble, because it messes with the MRO
        # of classes. Sigh.
        #
        # So now, we fall back to the old linearization (fast to compute).
        self._warn_iro()
        self.direct_inconsistency = InconsistentResolutionOrderError(self, base_tree_remaining)
        raise self._UseLegacyRO

    def _merge(self):
        # Returns a merged *list*.
        result = self.__mro = []
        base_tree_remaining = self.base_tree
        base = None
        while 1:
            # Take last picked base out of the base tree wherever it is.
            # This differs slightly from the standard Python MRO and is needed
            # because we have no other step that prevents duplicates
            # from coming in (e.g., in the inconsistent fallback path)
            base_tree_remaining = self._nonempty_bases_ignoring(base_tree_remaining, base)

            if not base_tree_remaining:
                return result
            try:
                base = self._choose_next_base(base_tree_remaining)
            except self._UseLegacyRO:
                self.__mro = self.legacy_ro
                return self.legacy_ro

            result.append(base)

    def mro(self):
        if self.__mro is None:
            self.__mro = tuple(self._merge())
        return list(self.__mro)


class _StrictC3(C3):
    __slots__ = ()
    def _guess_next_base(self, base_tree_remaining):
        raise InconsistentResolutionOrderError(self, base_tree_remaining)


class _TrackingC3(C3):
    __slots__ = ()
    def _guess_next_base(self, base_tree_remaining):
        import traceback
        bad_iros = C3.BAD_IROS
        if self.leaf not in bad_iros:
            if bad_iros == ():
                import weakref
                # This is a race condition, but it doesn't matter much.
                bad_iros = C3.BAD_IROS = weakref.WeakKeyDictionary()
            bad_iros[self.leaf] = t = (
                InconsistentResolutionOrderError(self, base_tree_remaining),
                traceback.format_stack()
            )
            _logger().warning("Tracking inconsistent IRO: %s", t[0])
        return C3._guess_next_base(self, base_tree_remaining)


class _ROComparison(object):
    # Exists to compute and print a pretty string comparison
    # for differing ROs.
    # Since we're used in a logging context, and may actually never be printed,
    # this is a class so we can defer computing the diff until asked.

    # Components we use to build up the comparison report
    class Item(object):
        prefix = '  '
        def __init__(self, item):
            self.item = item
        def __str__(self):
            return "%s%s" % (
                self.prefix,
                self.item,
            )

    class Deleted(Item):
        prefix = '- '

    class Inserted(Item):
        prefix = '+ '

    Empty = str

    class ReplacedBy(object): # pragma: no cover
        prefix = '- '
        suffix = ''
        def __init__(self, chunk, total_count):
            self.chunk = chunk
            self.total_count = total_count

        def __iter__(self):
            lines = [
                self.prefix + str(item) + self.suffix
                for item in self.chunk
            ]
            while len(lines) < self.total_count:
                lines.append('')

            return iter(lines)

    class Replacing(ReplacedBy):
        prefix = "+ "
        suffix = ''


    _c3_report = None
    _legacy_report = None

    def __init__(self, c3, c3_ro, legacy_ro):
        self.c3 = c3
        self.c3_ro = c3_ro
        self.legacy_ro = legacy_ro

    def __move(self, from_, to_, chunk, operation):
        for x in chunk:
            to_.append(operation(x))
            from_.append(self.Empty())

    def _generate_report(self):
        if self._c3_report is None:
            import difflib
            # The opcodes we get describe how to turn 'a' into 'b'. So
            # the old one (legacy) needs to be first ('a')
            matcher = difflib.SequenceMatcher(None, self.legacy_ro, self.c3_ro)
            # The reports are equal length sequences. We're going for a
            # side-by-side diff.
            self._c3_report = c3_report = []
            self._legacy_report = legacy_report = []
            for opcode, leg1, leg2, c31, c32 in matcher.get_opcodes():
                c3_chunk = self.c3_ro[c31:c32]
                legacy_chunk = self.legacy_ro[leg1:leg2]

                if opcode == 'equal':
                    # Guaranteed same length
                    c3_report.extend((self.Item(x) for x in c3_chunk))
                    legacy_report.extend(self.Item(x) for x in legacy_chunk)
                if opcode == 'delete':
                    # Guaranteed same length
                    assert not c3_chunk
                    self.__move(c3_report, legacy_report, legacy_chunk, self.Deleted)
                if opcode == 'insert':
                    # Guaranteed same length
                    assert not legacy_chunk
                    self.__move(legacy_report, c3_report, c3_chunk, self.Inserted)
                if opcode == 'replace': # pragma: no cover (How do you make it output this?)
                    # Either side could be longer.
                    chunk_size = max(len(c3_chunk), len(legacy_chunk))
                    c3_report.extend(self.Replacing(c3_chunk, chunk_size))
                    legacy_report.extend(self.ReplacedBy(legacy_chunk, chunk_size))

        return self._c3_report, self._legacy_report

    @property
    def _inconsistent_label(self):
        inconsistent = []
        if self.c3.direct_inconsistency:
            inconsistent.append('direct')
        if self.c3.bases_had_inconsistency:
            inconsistent.append('bases')
        return '+'.join(inconsistent) if inconsistent else 'no'

    def __str__(self):
        c3_report, legacy_report = self._generate_report()
        assert len(c3_report) == len(legacy_report)

        left_lines = [str(x) for x in legacy_report]
        right_lines = [str(x) for x in c3_report]

        # We have the same number of lines in the report; this is not
        # necessarily the same as the number of items in either RO.
        assert len(left_lines) == len(right_lines)

        padding = ' ' * 2
        max_left = max(len(x) for x in left_lines)
        max_right = max(len(x) for x in right_lines)

        left_title = 'Legacy RO (len=%s)' % (len(self.legacy_ro),)

        right_title = 'C3 RO (len=%s; inconsistent=%s)' % (
            len(self.c3_ro),
            self._inconsistent_label,
        )
        lines = [
            (padding + left_title.ljust(max_left) + padding + right_title.ljust(max_right)),
            padding + '=' * (max_left + len(padding) + max_right)
        ]
        lines += [
            padding + left.ljust(max_left) + padding + right
            for left, right in zip(left_lines, right_lines)
        ]

        return '\n'.join(lines)


# Set to `Interface` once it is defined. This is used to
# avoid logging false positives about changed ROs.
_ROOT = None

def ro(C, strict=None, base_mros=None, log_changed_ro=None, use_legacy_ro=None):
    """
    ro(C) -> list

    Compute the precedence list (mro) according to C3.

    :return: A fresh `list` object.

    .. versionchanged:: 5.0.0
       Add the *strict*, *log_changed_ro* and *use_legacy_ro*
       keyword arguments. These are provisional and likely to be
       removed in the future. They are most useful for testing.
    """
    # The ``base_mros`` argument is for internal optimization and
    # not documented.
    resolver = C3.resolver(C, strict, base_mros)
    mro = resolver.mro()

    log_changed = log_changed_ro if log_changed_ro is not None else resolver.LOG_CHANGED_IRO
    use_legacy = use_legacy_ro if use_legacy_ro is not None else resolver.USE_LEGACY_IRO

    if log_changed or use_legacy:
        legacy_ro = resolver.legacy_ro
        assert isinstance(legacy_ro, list)
        assert isinstance(mro, list)
        changed = legacy_ro != mro
        if changed:
            # Did only Interface move? The fix for issue #8 made that
            # somewhat common. It's almost certainly not a problem, though,
            # so allow ignoring it.
            legacy_without_root = [x for x in legacy_ro if x is not _ROOT]
            mro_without_root = [x for x in mro if x is not _ROOT]
            changed = legacy_without_root != mro_without_root

        if changed:
            comparison = _ROComparison(resolver, mro, legacy_ro)
            _logger().warning(
                "Object %r has different legacy and C3 MROs:\n%s",
                C, comparison
            )
        if resolver.had_inconsistency and legacy_ro == mro:
            comparison = _ROComparison(resolver, mro, legacy_ro)
            _logger().warning(
                "Object %r had inconsistent IRO and used the legacy RO:\n%s"
                "\nInconsistency entered at:\n%s",
                C, comparison, resolver.direct_inconsistency
            )
        if use_legacy:
            return legacy_ro

    return mro


def is_consistent(C):
    """
    Check if the resolution order for *C*, as computed by :func:`ro`, is consistent
    according to C3.
    """
    return not C3.resolver(C, False, None).had_inconsistency

if __name__ == "__main__":
    # import dill
    # import os
    isT=True
    args = [['x', 'y', 'z'],
            ['q', 'z'],
            [1, 3, 5],
            ['z']]
    target = ['x', 'y', 'q', 1, 3, 5, 'z']
    if _legacy_mergeOrderings(args)!=target:
        isT=False
    # for l in os.listdir("D:\\fse\\python_test\\repos\\pexip---os-zope\\data_passk_platform1/62b8b590eb7e40a82d2d1275/"):
    #     f = open("D:\\fse\\python_test\\repos/pexip---os-zope/data_passk_platform1/62b8b590eb7e40a82d2d1275/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     print(args0)
    #     print(content["output"][0])
    #     res0 = _legacy_mergeOrderings(args0)
    #     print(res0)
    #     #print(args0)
    #     if res0!=['[', '_', 'F', 'C', 'z', 'I', 'r', 'f', 'a', ',', '<', 'p', 'm', 'd', 'B', 'y', ' ', 'u', 'l', 'i', 'n', 's', '.', 'o', 'b', 'j', 'e', 'c', 't', '>', ']']:
    #         isT=False
    #         break
        # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
        #     isT=False
        #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/declarations_directlyProvidedBy_passk_validte.py
##############################################################################
# Copyright (c) 2003 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
##############################################################################
"""Implementation of interface declarations

There are three flavors of declarations:

  - Declarations are used to simply name declared interfaces.

  - ImplementsDeclarations are used to express the interfaces that a
    class implements (that instances of the class provides).

    Implements specifications support inheriting interfaces.

  - ProvidesDeclarations are used to express interfaces directly
    provided by objects.

"""
__docformat__ = 'restructuredtext'

import sys
from types import FunctionType
from types import MethodType
from types import ModuleType
import weakref
import sys
sys.path.append("/home/travis/builds/repos/pexip---os-zope")

from src.zope.interface.advice import addClassAdvisor
from zope.interface.interface import Interface
from zope.interface.interface import InterfaceClass
from zope.interface.interface import SpecificationBase
from zope.interface.interface import Specification
from zope.interface.interface import NameAndModuleComparisonMixin
from zope.interface._compat import CLASS_TYPES as DescriptorAwareMetaClasses
from zope.interface._compat import PYTHON3
from zope.interface._compat import _use_c_impl


__all__ = [
    # None. The public APIs of this module are
    # re-exported from zope.interface directly.
]

# pylint:disable=too-many-lines

# Registry of class-implementation specifications
BuiltinImplementationSpecifications = {}

_ADVICE_ERROR = ('Class advice impossible in Python3.  '
                 'Use the @%s class decorator instead.')

_ADVICE_WARNING = ('The %s API is deprecated, and will not work in Python3  '
                   'Use the @%s class decorator instead.')

def _next_super_class(ob):
    # When ``ob`` is an instance of ``super``, return
    # the next class in the MRO that we should actually be
    # looking at. Watch out for diamond inheritance!
    self_class = ob.__self_class__
    class_that_invoked_super = ob.__thisclass__
    complete_mro = self_class.__mro__
    next_class = complete_mro[complete_mro.index(class_that_invoked_super) + 1]
    return next_class

class named(object):

    def __init__(self, name):
        self.name = name

    def __call__(self, ob):
        ob.__component_name__ = self.name
        return ob


class Declaration(Specification):
    """Interface declarations"""

    __slots__ = ()

    def __init__(self, *bases):
        Specification.__init__(self, _normalizeargs(bases))

    def __contains__(self, interface):
        """Test whether an interface is in the specification
        """

        return self.extends(interface) and interface in self.interfaces()

    def __iter__(self):
        """Return an iterator for the interfaces in the specification
        """
        return self.interfaces()

    def flattened(self):
        """Return an iterator of all included and extended interfaces
        """
        return iter(self.__iro__)

    def __sub__(self, other):
        """Remove interfaces from a specification
        """
        return Declaration(*[
            i for i in self.interfaces()
            if not [
                j
                for j in other.interfaces()
                if i.extends(j, 0) # non-strict extends
            ]
        ])

    def __add__(self, other):
        """Add two specifications or a specification and an interface
        """
        seen = {}
        result = []
        for i in self.interfaces():
            seen[i] = 1
            result.append(i)
        for i in other.interfaces():
            if i not in seen:
                seen[i] = 1
                result.append(i)

        return Declaration(*result)

    __radd__ = __add__


class _ImmutableDeclaration(Declaration):
    # A Declaration that is immutable. Used as a singleton to
    # return empty answers for things like ``implementedBy``.
    # We have to define the actual singleton after normalizeargs
    # is defined, and that in turn is defined after InterfaceClass and
    # Implements.

    __slots__ = ()

    __instance = None

    def __new__(cls):
        if _ImmutableDeclaration.__instance is None:
            _ImmutableDeclaration.__instance = object.__new__(cls)
        return _ImmutableDeclaration.__instance

    def __reduce__(self):
        return "_empty"

    @property
    def __bases__(self):
        return ()

    @__bases__.setter
    def __bases__(self, new_bases):
        # We expect the superclass constructor to set ``self.__bases__ = ()``.
        # Rather than attempt to special case that in the constructor and allow
        # setting __bases__ only at that time, it's easier to just allow setting
        # the empty tuple at any time. That makes ``x.__bases__ = x.__bases__`` a nice
        # no-op too. (Skipping the superclass constructor altogether is a recipe
        # for maintenance headaches.)
        if new_bases != ():
            raise TypeError("Cannot set non-empty bases on shared empty Declaration.")

    # As the immutable empty declaration, we cannot be changed.
    # This means there's no logical reason for us to have dependents
    # or subscriptions: we'll never notify them. So there's no need for
    # us to keep track of any of that.
    @property
    def dependents(self):
        return {}

    changed = subscribe = unsubscribe = lambda self, _ignored: None

    def interfaces(self):
        # An empty iterator
        return iter(())

    def extends(self, interface, strict=True):
        return interface is self._ROOT

    def get(self, name, default=None):
        return default

    def weakref(self, callback=None):
        # We're a singleton, we never go away. So there's no need to return
        # distinct weakref objects here; their callbacks will never
        # be called. Instead, we only need to return a callable that
        # returns ourself. The easiest one is to return _ImmutableDeclaration
        # itself; testing on Python 3.8 shows that's faster than a function that
        # returns _empty. (Remember, one goal is to avoid allocating any
        # object, and that includes a method.)
        return _ImmutableDeclaration

    @property
    def _v_attrs(self):
        # _v_attrs is not a public, documented property, but some client
        # code uses it anyway as a convenient place to cache things. To keep
        # the empty declaration truly immutable, we must ignore that. That includes
        # ignoring assignments as well.
        return {}

    @_v_attrs.setter
    def _v_attrs(self, new_attrs):
        pass


##############################################################################
#
# Implementation specifications
#
# These specify interfaces implemented by instances of classes

class Implements(NameAndModuleComparisonMixin,
                 Declaration):
    # Inherit from NameAndModuleComparisonMixin to be
    # mutually comparable with InterfaceClass objects.
    # (The two must be mutually comparable to be able to work in e.g., BTrees.)
    # Instances of this class generally don't have a __module__ other than
    # `zope.interface.declarations`, whereas they *do* have a __name__ that is the
    # fully qualified name of the object they are representing.

    # Note, though, that equality and hashing are still identity based. This
    # accounts for things like nested objects that have the same name (typically
    # only in tests) and is consistent with pickling. As far as comparisons to InterfaceClass
    # goes, we'll never have equal name and module to those, so we're still consistent there.
    # Instances of this class are essentially intended to be unique and are
    # heavily cached (note how our __reduce__ handles this) so having identity
    # based hash and eq should also work.

    # We want equality and hashing to be based on identity. However, we can't actually
    # implement __eq__/__ne__ to do this because sometimes we get wrapped in a proxy.
    # We need to let the proxy types implement these methods so they can handle unwrapping
    # and then rely on: (1) the interpreter automatically changing `implements == proxy` into
    # `proxy == implements` (which will call proxy.__eq__ to do the unwrapping) and then
    # (2) the default equality and hashing semantics being identity based.

    # class whose specification should be used as additional base
    inherit = None

    # interfaces actually declared for a class
    declared = ()

    # Weak cache of {class: <implements>} for super objects.
    # Created on demand. These are rare, as of 5.0 anyway. Using a class
    # level default doesn't take space in instances. Using _v_attrs would be
    # another place to store this without taking space unless needed.
    _super_cache = None

    __name__ = '?'

    @classmethod
    def named(cls, name, *bases):
        # Implementation method: Produce an Implements interface with
        # a fully fleshed out __name__ before calling the constructor, which
        # sets bases to the given interfaces and which may pass this object to
        # other objects (e.g., to adjust dependents). If they're sorting or comparing
        # by name, this needs to be set.
        inst = cls.__new__(cls)
        inst.__name__ = name
        inst.__init__(*bases)
        return inst

    def changed(self, originally_changed):
        try:
            del self._super_cache
        except AttributeError:
            pass
        return super(Implements, self).changed(originally_changed)

    def __repr__(self):
        return '<implementedBy %s>' % (self.__name__)

    def __reduce__(self):
        return implementedBy, (self.inherit, )


def _implements_name(ob):
    # Return the __name__ attribute to be used by its __implemented__
    # property.
    # This must be stable for the "same" object across processes
    # because it is used for sorting. It needn't be unique, though, in cases
    # like nested classes named Foo created by different functions, because
    # equality and hashing is still based on identity.
    # It might be nice to use __qualname__ on Python 3, but that would produce
    # different values between Py2 and Py3.
    return (getattr(ob, '__module__', '?') or '?') + \
        '.' + (getattr(ob, '__name__', '?') or '?')


def _implementedBy_super(sup):
    # TODO: This is now simple enough we could probably implement
    # in C if needed.

    # If the class MRO is strictly linear, we could just
    # follow the normal algorithm for the next class in the
    # search order (e.g., just return
    # ``implemented_by_next``). But when diamond inheritance
    # or mixins + interface declarations are present, we have
    # to consider the whole MRO and compute a new Implements
    # that excludes the classes being skipped over but
    # includes everything else.
    implemented_by_self = implementedBy(sup.__self_class__)
    cache = implemented_by_self._super_cache # pylint:disable=protected-access
    if cache is None:
        cache = implemented_by_self._super_cache = weakref.WeakKeyDictionary()

    key = sup.__thisclass__
    try:
        return cache[key]
    except KeyError:
        pass

    next_cls = _next_super_class(sup)
    # For ``implementedBy(cls)``:
    # .__bases__ is .declared + [implementedBy(b) for b in cls.__bases__]
    # .inherit is cls

    implemented_by_next = implementedBy(next_cls)
    mro = sup.__self_class__.__mro__
    ix_next_cls = mro.index(next_cls)
    classes_to_keep = mro[ix_next_cls:]
    new_bases = [implementedBy(c) for c in classes_to_keep]

    new = Implements.named(
        implemented_by_self.__name__ + ':' + implemented_by_next.__name__,
        *new_bases
    )
    new.inherit = implemented_by_next.inherit
    new.declared = implemented_by_next.declared
    # I don't *think* that new needs to subscribe to ``implemented_by_self``;
    # it auto-subscribed to its bases, and that should be good enough.
    cache[key] = new

    return new


@_use_c_impl
def implementedBy(cls): # pylint:disable=too-many-return-statements,too-many-branches
    """Return the interfaces implemented for a class' instances

      The value returned is an `~zope.interface.interfaces.IDeclaration`.
    """
    try:
        if isinstance(cls, super):
            # Yes, this needs to be inside the try: block. Some objects
            # like security proxies even break isinstance.
            return _implementedBy_super(cls)

        spec = cls.__dict__.get('__implemented__')
    except AttributeError:

        # we can't get the class dict. This is probably due to a
        # security proxy.  If this is the case, then probably no
        # descriptor was installed for the class.

        # We don't want to depend directly on zope.security in
        # zope.interface, but we'll try to make reasonable
        # accommodations in an indirect way.

        # We'll check to see if there's an implements:

        spec = getattr(cls, '__implemented__', None)
        if spec is None:
            # There's no spec stred in the class. Maybe its a builtin:
            spec = BuiltinImplementationSpecifications.get(cls)
            if spec is not None:
                return spec
            return _empty

        if spec.__class__ == Implements:
            # we defaulted to _empty or there was a spec. Good enough.
            # Return it.
            return spec

        # TODO: need old style __implements__ compatibility?
        # Hm, there's an __implemented__, but it's not a spec. Must be
        # an old-style declaration. Just compute a spec for it
        return Declaration(*_normalizeargs((spec, )))

    if isinstance(spec, Implements):
        return spec

    if spec is None:
        spec = BuiltinImplementationSpecifications.get(cls)
        if spec is not None:
            return spec

    # TODO: need old style __implements__ compatibility?
    spec_name = _implements_name(cls)
    if spec is not None:
        # old-style __implemented__ = foo declaration
        spec = (spec, ) # tuplefy, as it might be just an int
        spec = Implements.named(spec_name, *_normalizeargs(spec))
        spec.inherit = None    # old-style implies no inherit
        del cls.__implemented__ # get rid of the old-style declaration
    else:
        try:
            bases = cls.__bases__
        except AttributeError:
            if not callable(cls):
                raise TypeError("ImplementedBy called for non-factory", cls)
            bases = ()

        spec = Implements.named(spec_name, *[implementedBy(c) for c in bases])
        spec.inherit = cls

    try:
        cls.__implemented__ = spec
        if not hasattr(cls, '__providedBy__'):
            cls.__providedBy__ = objectSpecificationDescriptor

        if (isinstance(cls, DescriptorAwareMetaClasses)
                and '__provides__' not in cls.__dict__):
            # Make sure we get a __provides__ descriptor
            cls.__provides__ = ClassProvides(
                cls,
                getattr(cls, '__class__', type(cls)),
                )

    except TypeError:
        if not isinstance(cls, type):
            raise TypeError("ImplementedBy called for non-type", cls)
        BuiltinImplementationSpecifications[cls] = spec

    return spec


def classImplementsOnly(cls, *interfaces):
    """
    Declare the only interfaces implemented by instances of a class

    The arguments after the class are one or more interfaces or interface
    specifications (`~zope.interface.interfaces.IDeclaration` objects).

    The interfaces given (including the interfaces in the specifications)
    replace any previous declarations, *including* inherited definitions. If you
    wish to preserve inherited declarations, you can pass ``implementedBy(cls)``
    in *interfaces*. This can be used to alter the interface resolution order.
    """
    spec = implementedBy(cls)
    # Clear out everything inherited. It's important to
    # also clear the bases right now so that we don't improperly discard
    # interfaces that are already implemented by *old* bases that we're
    # about to get rid of.
    spec.declared = ()
    spec.inherit = None
    spec.__bases__ = ()
    _classImplements_ordered(spec, interfaces, ())


def classImplements(cls, *interfaces):
    """
    Declare additional interfaces implemented for instances of a class

    The arguments after the class are one or more interfaces or
    interface specifications (`~zope.interface.interfaces.IDeclaration` objects).

    The interfaces given (including the interfaces in the specifications)
    are added to any interfaces previously declared. An effort is made to
    keep a consistent C3 resolution order, but this cannot be guaranteed.

    .. versionchanged:: 5.0.0
       Each individual interface in *interfaces* may be added to either the
       beginning or end of the list of interfaces declared for *cls*,
       based on inheritance, in order to try to maintain a consistent
       resolution order. Previously, all interfaces were added to the end.
    .. versionchanged:: 5.1.0
       If *cls* is already declared to implement an interface (or derived interface)
       in *interfaces* through inheritance, the interface is ignored. Previously, it
       would redundantly be made direct base of *cls*, which often produced inconsistent
       interface resolution orders. Now, the order will be consistent, but may change.
       Also, if the ``__bases__`` of the *cls* are later changed, the *cls* will no
       longer be considered to implement such an interface (changing the ``__bases__`` of *cls*
       has never been supported).
    """
    spec = implementedBy(cls)
    interfaces = tuple(_normalizeargs(interfaces))

    before = []
    after = []

    # Take steps to try to avoid producing an invalid resolution
    # order, while still allowing for BWC (in the past, we always
    # appended)
    for iface in interfaces:
        for b in spec.declared:
            if iface.extends(b):
                before.append(iface)
                break
        else:
            after.append(iface)
    _classImplements_ordered(spec, tuple(before), tuple(after))


def classImplementsFirst(cls, iface):
    """
    Declare that instances of *cls* additionally provide *iface*.

    The second argument is an interface or interface specification.
    It is added as the highest priority (first in the IRO) interface;
    no attempt is made to keep a consistent resolution order.

    .. versionadded:: 5.0.0
    """
    spec = implementedBy(cls)
    _classImplements_ordered(spec, (iface,), ())


def _classImplements_ordered(spec, before=(), after=()):
    # Elide everything already inherited.
    # Except, if it is the root, and we don't already declare anything else
    # that would imply it, allow the root through. (TODO: When we disallow non-strict
    # IRO, this part of the check can be removed because it's not possible to re-declare
    # like that.)
    before = [
        x
        for x in before
        if not spec.isOrExtends(x) or (x is Interface and not spec.declared)
    ]
    after = [
        x
        for x in after
        if not spec.isOrExtends(x) or (x is Interface and not spec.declared)
    ]

    # eliminate duplicates
    new_declared = []
    seen = set()
    for l in before, spec.declared, after:
        for b in l:
            if b not in seen:
                new_declared.append(b)
                seen.add(b)

    spec.declared = tuple(new_declared)

    # compute the bases
    bases = new_declared # guaranteed no dupes

    if spec.inherit is not None:
        for c in spec.inherit.__bases__:
            b = implementedBy(c)
            if b not in seen:
                seen.add(b)
                bases.append(b)

    spec.__bases__ = tuple(bases)


def _implements_advice(cls):
    interfaces, do_classImplements = cls.__dict__['__implements_advice_data__']
    del cls.__implements_advice_data__
    do_classImplements(cls, *interfaces)
    return cls


class implementer(object):
    """
    Declare the interfaces implemented by instances of a class.

    This function is called as a class decorator.

    The arguments are one or more interfaces or interface
    specifications (`~zope.interface.interfaces.IDeclaration`
    objects).

    The interfaces given (including the interfaces in the
    specifications) are added to any interfaces previously declared,
    unless the interface is already implemented.

    Previous declarations include declarations for base classes unless
    implementsOnly was used.

    This function is provided for convenience. It provides a more
    convenient way to call `classImplements`. For example::

        @implementer(I1)
        class C(object):
            pass

    is equivalent to calling::

        classImplements(C, I1)

    after the class has been created.

    .. seealso:: `classImplements`
       The change history provided there applies to this function too.
    """
    __slots__ = ('interfaces',)

    def __init__(self, *interfaces):
        self.interfaces = interfaces

    def __call__(self, ob):
        if isinstance(ob, DescriptorAwareMetaClasses):
            # This is the common branch for new-style (object) and
            # on Python 2 old-style classes.
            classImplements(ob, *self.interfaces)
            return ob

        spec_name = _implements_name(ob)
        spec = Implements.named(spec_name, *self.interfaces)
        try:
            ob.__implemented__ = spec
        except AttributeError:
            raise TypeError("Can't declare implements", ob)
        return ob

class implementer_only(object):
    """Declare the only interfaces implemented by instances of a class

      This function is called as a class decorator.

      The arguments are one or more interfaces or interface
      specifications (`~zope.interface.interfaces.IDeclaration` objects).

      Previous declarations including declarations for base classes
      are overridden.

      This function is provided for convenience. It provides a more
      convenient way to call `classImplementsOnly`. For example::

        @implementer_only(I1)
        class C(object): pass

      is equivalent to calling::

        classImplementsOnly(I1)

      after the class has been created.
      """

    def __init__(self, *interfaces):
        self.interfaces = interfaces

    def __call__(self, ob):
        if isinstance(ob, (FunctionType, MethodType)):
            # XXX Does this decorator make sense for anything but classes?
            # I don't think so. There can be no inheritance of interfaces
            # on a method or function....
            raise ValueError('The implementer_only decorator is not '
                             'supported for methods or functions.')

        # Assume it's a class:
        classImplementsOnly(ob, *self.interfaces)
        return ob

def _implements(name, interfaces, do_classImplements):
    # This entire approach is invalid under Py3K.  Don't even try to fix
    # the coverage for this block there. :(
    frame = sys._getframe(2) # pylint:disable=protected-access
    locals = frame.f_locals # pylint:disable=redefined-builtin

    # Try to make sure we were called from a class def. In 2.2.0 we can't
    # check for __module__ since it doesn't seem to be added to the locals
    # until later on.
    if locals is frame.f_globals or '__module__' not in locals:
        raise TypeError(name+" can be used only from a class definition.")

    if '__implements_advice_data__' in locals:
        raise TypeError(name+" can be used only once in a class definition.")

    locals['__implements_advice_data__'] = interfaces, do_classImplements
    addClassAdvisor(_implements_advice, depth=3)

def implements(*interfaces):
    """
    Declare interfaces implemented by instances of a class.

    .. deprecated:: 5.0
        This only works for Python 2. The `implementer` decorator
        is preferred for all versions.

    This function is called in a class definition.

    The arguments are one or more interfaces or interface
    specifications (`~zope.interface.interfaces.IDeclaration`
    objects).

    The interfaces given (including the interfaces in the
    specifications) are added to any interfaces previously declared.

    Previous declarations include declarations for base classes unless
    `implementsOnly` was used.

    This function is provided for convenience. It provides a more
    convenient way to call `classImplements`. For example::

        implements(I1)

    is equivalent to calling::

        classImplements(C, I1)

    after the class has been created.
    """
    # This entire approach is invalid under Py3K.  Don't even try to fix
    # the coverage for this block there. :(
    if PYTHON3:
        raise TypeError(_ADVICE_ERROR % 'implementer')
    _implements("implements", interfaces, classImplements)

def implementsOnly(*interfaces):
    """Declare the only interfaces implemented by instances of a class

      This function is called in a class definition.

      The arguments are one or more interfaces or interface
      specifications (`~zope.interface.interfaces.IDeclaration` objects).

      Previous declarations including declarations for base classes
      are overridden.

      This function is provided for convenience. It provides a more
      convenient way to call `classImplementsOnly`. For example::

        implementsOnly(I1)

      is equivalent to calling::

        classImplementsOnly(I1)

      after the class has been created.
    """
    # This entire approach is invalid under Py3K.  Don't even try to fix
    # the coverage for this block there. :(
    if PYTHON3:
        raise TypeError(_ADVICE_ERROR % 'implementer_only')
    _implements("implementsOnly", interfaces, classImplementsOnly)

##############################################################################
#
# Instance declarations

class Provides(Declaration):  # Really named ProvidesClass
    """Implement ``__provides__``, the instance-specific specification

    When an object is pickled, we pickle the interfaces that it implements.
    """

    def __init__(self, cls, *interfaces):
        self.__args = (cls, ) + interfaces
        self._cls = cls
        Declaration.__init__(self, *(interfaces + (implementedBy(cls), )))

    def __repr__(self):
        return "<%s.%s for %s>" % (
            self.__class__.__module__,
            self.__class__.__name__,
            self._cls,
        )

    def __reduce__(self):
        return Provides, self.__args

    __module__ = 'zope.interface'

    def __get__(self, inst, cls):
        """Make sure that a class __provides__ doesn't leak to an instance
        """
        if inst is None and cls is self._cls:
            # We were accessed through a class, so we are the class'
            # provides spec. Just return this object, but only if we are
            # being called on the same class that we were defined for:
            return self

        raise AttributeError('__provides__')

ProvidesClass = Provides

# Registry of instance declarations
# This is a memory optimization to allow objects to share specifications.
InstanceDeclarations = weakref.WeakValueDictionary()

def Provides(*interfaces): # pylint:disable=function-redefined
    """Cache instance declarations

      Instance declarations are shared among instances that have the same
      declaration. The declarations are cached in a weak value dictionary.
    """
    spec = InstanceDeclarations.get(interfaces)
    if spec is None:
        spec = ProvidesClass(*interfaces)
        InstanceDeclarations[interfaces] = spec

    return spec

Provides.__safe_for_unpickling__ = True


def directlyProvides(object, *interfaces): # pylint:disable=redefined-builtin
    """Declare interfaces declared directly for an object

      The arguments after the object are one or more interfaces or interface
      specifications (`~zope.interface.interfaces.IDeclaration` objects).

      The interfaces given (including the interfaces in the specifications)
      replace interfaces previously declared for the object.
    """
    cls = getattr(object, '__class__', None)
    if cls is not None and getattr(cls, '__class__', None) is cls:
        # It's a meta class (well, at least it it could be an extension class)
        # Note that we can't get here from Py3k tests:  there is no normal
        # class which isn't descriptor aware.
        if not isinstance(object,
                          DescriptorAwareMetaClasses):
            raise TypeError("Attempt to make an interface declaration on a "
                            "non-descriptor-aware class")

    interfaces = _normalizeargs(interfaces)
    if cls is None:
        cls = type(object)

    issub = False
    for damc in DescriptorAwareMetaClasses:
        if issubclass(cls, damc):
            issub = True
            break
    if issub:
        # we have a class or type.  We'll use a special descriptor
        # that provides some extra caching
        object.__provides__ = ClassProvides(object, cls, *interfaces)
    else:
        object.__provides__ = Provides(cls, *interfaces)


def alsoProvides(object, *interfaces): # pylint:disable=redefined-builtin
    """Declare interfaces declared directly for an object

    The arguments after the object are one or more interfaces or interface
    specifications (`~zope.interface.interfaces.IDeclaration` objects).

    The interfaces given (including the interfaces in the specifications) are
    added to the interfaces previously declared for the object.
    """
    directlyProvides(object, directlyProvidedBy(object), *interfaces)


def noLongerProvides(object, interface): # pylint:disable=redefined-builtin
    """ Removes a directly provided interface from an object.
    """
    directlyProvides(object, directlyProvidedBy(object) - interface)
    if interface.providedBy(object):
        raise ValueError("Can only remove directly provided interfaces.")


@_use_c_impl
class ClassProvidesBase(SpecificationBase):

    __slots__ = (
        '_cls',
        '_implements',
    )

    def __get__(self, inst, cls):
        # member slots are set by subclass
        # pylint:disable=no-member
        if cls is self._cls:
            # We only work if called on the class we were defined for

            if inst is None:
                # We were accessed through a class, so we are the class'
                # provides spec. Just return this object as is:
                return self

            return self._implements

        raise AttributeError('__provides__')


class ClassProvides(Declaration, ClassProvidesBase):
    """Special descriptor for class ``__provides__``

    The descriptor caches the implementedBy info, so that
    we can get declarations for objects without instance-specific
    interfaces a bit quicker.
    """

    __slots__ = (
        '__args',
    )

    def __init__(self, cls, metacls, *interfaces):
        self._cls = cls
        self._implements = implementedBy(cls)
        self.__args = (cls, metacls, ) + interfaces
        Declaration.__init__(self, *(interfaces + (implementedBy(metacls), )))

    def __repr__(self):
        return "<%s.%s for %s>" % (
            self.__class__.__module__,
            self.__class__.__name__,
            self._cls,
        )

    def __reduce__(self):
        return self.__class__, self.__args

    # Copy base-class method for speed
    __get__ = ClassProvidesBase.__get__


def directlyProvidedBy(object): # pylint:disable=redefined-builtin
    """Return the interfaces directly provided by the given object

    The value returned is an `~zope.interface.interfaces.IDeclaration`.
    """
    provides = getattr(object, "__provides__", None)
    if (
            provides is None # no spec
            # We might have gotten the implements spec, as an
            # optimization. If so, it's like having only one base, that we
            # lop off to exclude class-supplied declarations:
            or isinstance(provides, Implements)
    ):
        return _empty

    # Strip off the class part of the spec:
    return Declaration(provides.__bases__[:-1])


def classProvides(*interfaces):
    """Declare interfaces provided directly by a class

      This function is called in a class definition.

      The arguments are one or more interfaces or interface specifications
      (`~zope.interface.interfaces.IDeclaration` objects).

      The given interfaces (including the interfaces in the specifications)
      are used to create the class's direct-object interface specification.
      An error will be raised if the module class has an direct interface
      specification. In other words, it is an error to call this function more
      than once in a class definition.

      Note that the given interfaces have nothing to do with the interfaces
      implemented by instances of the class.

      This function is provided for convenience. It provides a more convenient
      way to call `directlyProvides` for a class. For example::

        classProvides(I1)

      is equivalent to calling::

        directlyProvides(theclass, I1)

      after the class has been created.
    """
    # This entire approach is invalid under Py3K.  Don't even try to fix
    # the coverage for this block there. :(

    if PYTHON3:
        raise TypeError(_ADVICE_ERROR % 'provider')

    frame = sys._getframe(1) # pylint:disable=protected-access
    locals = frame.f_locals # pylint:disable=redefined-builtin

    # Try to make sure we were called from a class def
    if (locals is frame.f_globals) or ('__module__' not in locals):
        raise TypeError("classProvides can be used only from a "
                        "class definition.")

    if '__provides__' in locals:
        raise TypeError(
            "classProvides can only be used once in a class definition.")

    locals["__provides__"] = _normalizeargs(interfaces)

    addClassAdvisor(_classProvides_advice, depth=2)

def _classProvides_advice(cls):
    # This entire approach is invalid under Py3K.  Don't even try to fix
    # the coverage for this block there. :(
    interfaces = cls.__dict__['__provides__']
    del cls.__provides__
    directlyProvides(cls, *interfaces)
    return cls


class provider(object):
    """Class decorator version of classProvides"""

    def __init__(self, *interfaces):
        self.interfaces = interfaces

    def __call__(self, ob):
        directlyProvides(ob, *self.interfaces)
        return ob


def moduleProvides(*interfaces):
    """Declare interfaces provided by a module

    This function is used in a module definition.

    The arguments are one or more interfaces or interface specifications
    (`~zope.interface.interfaces.IDeclaration` objects).

    The given interfaces (including the interfaces in the specifications) are
    used to create the module's direct-object interface specification.  An
    error will be raised if the module already has an interface specification.
    In other words, it is an error to call this function more than once in a
    module definition.

    This function is provided for convenience. It provides a more convenient
    way to call directlyProvides. For example::

      moduleImplements(I1)

    is equivalent to::

      directlyProvides(sys.modules[__name__], I1)
    """
    frame = sys._getframe(1) # pylint:disable=protected-access
    locals = frame.f_locals # pylint:disable=redefined-builtin

    # Try to make sure we were called from a class def
    if (locals is not frame.f_globals) or ('__name__' not in locals):
        raise TypeError(
            "moduleProvides can only be used from a module definition.")

    if '__provides__' in locals:
        raise TypeError(
            "moduleProvides can only be used once in a module definition.")

    locals["__provides__"] = Provides(ModuleType,
                                      *_normalizeargs(interfaces))


##############################################################################
#
# Declaration querying support

# XXX:  is this a fossil?  Nobody calls it, no unit tests exercise it, no
#       doctests import it, and the package __init__ doesn't import it.
#       (Answer: Versions of zope.container prior to 4.4.0 called this.)
def ObjectSpecification(direct, cls):
    """Provide object specifications

    These combine information for the object and for it's classes.
    """
    return Provides(cls, direct) # pragma: no cover fossil

@_use_c_impl
def getObjectSpecification(ob):
    try:
        provides = ob.__provides__
    except AttributeError:
        provides = None

    if provides is not None:
        if isinstance(provides, SpecificationBase):
            return provides

    try:
        cls = ob.__class__
    except AttributeError:
        # We can't get the class, so just consider provides
        return _empty
    return implementedBy(cls)


@_use_c_impl
def providedBy(ob):
    """
    Return the interfaces provided by *ob*.

    If *ob* is a :class:`super` object, then only interfaces implemented
    by the remainder of the classes in the method resolution order are
    considered. Interfaces directly provided by the object underlying *ob*
    are not.
    """
    # Here we have either a special object, an old-style declaration
    # or a descriptor

    # Try to get __providedBy__
    try:
        if isinstance(ob, super): # Some objects raise errors on isinstance()
            return implementedBy(ob)

        r = ob.__providedBy__
    except AttributeError:
        # Not set yet. Fall back to lower-level thing that computes it
        return getObjectSpecification(ob)

    try:
        # We might have gotten a descriptor from an instance of a
        # class (like an ExtensionClass) that doesn't support
        # descriptors.  We'll make sure we got one by trying to get
        # the only attribute, which all specs have.
        r.extends
    except AttributeError:

        # The object's class doesn't understand descriptors.
        # Sigh. We need to get an object descriptor, but we have to be
        # careful.  We want to use the instance's __provides__, if
        # there is one, but only if it didn't come from the class.

        try:
            r = ob.__provides__
        except AttributeError:
            # No __provides__, so just fall back to implementedBy
            return implementedBy(ob.__class__)

        # We need to make sure we got the __provides__ from the
        # instance. We'll do this by making sure we don't get the same
        # thing from the class:

        try:
            cp = ob.__class__.__provides__
        except AttributeError:
            # The ob doesn't have a class or the class has no
            # provides, assume we're done:
            return r

        if r is cp:
            # Oops, we got the provides from the class. This means
            # the object doesn't have it's own. We should use implementedBy
            return implementedBy(ob.__class__)

    return r


@_use_c_impl
class ObjectSpecificationDescriptor(object):
    """Implement the `__providedBy__` attribute

    The `__providedBy__` attribute computes the interfaces provided by
    an object.
    """

    def __get__(self, inst, cls):
        """Get an object specification for an object
        """
        if inst is None:
            return getObjectSpecification(cls)

        provides = getattr(inst, '__provides__', None)
        if provides is not None:
            return provides

        return implementedBy(cls)


##############################################################################

def _normalizeargs(sequence, output=None):
    """Normalize declaration arguments

    Normalization arguments might contain Declarions, tuples, or single
    interfaces.

    Anything but individial interfaces or implements specs will be expanded.
    """
    if output is None:
        output = []

    cls = sequence.__class__
    if InterfaceClass in cls.__mro__ or Implements in cls.__mro__:
        output.append(sequence)
    else:
        for v in sequence:
            _normalizeargs(v, output)

    return output

_empty = _ImmutableDeclaration()

objectSpecificationDescriptor = ObjectSpecificationDescriptor()
class I1(Interface): pass
class I2(Interface): pass
class I3(Interface): pass
class I31(I3): pass
class I4(Interface): pass
class I5(Interface): pass
class Odd(object):
    pass
if __name__ == "__main__":
    isT=True
    try:
        class IA1(Interface):
            pass
        class IA2(Interface):
            pass
        class IB(Interface):
            pass
        class IC(Interface):
            pass
        class A(Odd):
            pass
        classImplements(A, IA1, IA2)


        class B(Odd):
            pass


        classImplements(B, IB)


        class C(A, B):
            pass


        classImplements(C, IC)

        ob = C()
        directlyProvides(ob, I1, I2)
        res1=I1 in providedBy(ob)
        res2=I2 in providedBy(ob)
        res3=IA1 in providedBy(ob)
        res4=IA2 in providedBy(ob)
        res5=IB in providedBy(ob)
        res6=IC in providedBy(ob)

        directlyProvides(ob, directlyProvidedBy(ob) - I2)
        res7=I1 in providedBy(ob)
        res8=I2 not in providedBy(ob)
        directlyProvides(ob, directlyProvidedBy(ob), I2)
        res9=I2 in providedBy(ob)
        if not res1 or not res2 or not res3 or not res4 or not res5 or not res6 or not res7 or not res8 or not res9:
            isT=False
    except:
        isT=False
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b58deb7e40a82d2d1269/"):
    #     f = open("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b58deb7e40a82d2d1269/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = directlyProvidedBy(args0)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/advice_minimalBases_passk_validte.py
##############################################################################
#
# Copyright (c) 2003 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""Class advice.

This module was adapted from 'protocols.advice', part of the Python
Enterprise Application Kit (PEAK).  Please notify the PEAK authors
(pje@telecommunity.com and tsarna@sarna.org) if bugs are found or
Zope-specific changes are required, so that the PEAK version of this module
can be kept in sync.

PEAK is a Python application framework that interoperates with (but does
not require) Zope 3 and Twisted.  It provides tools for manipulating UML
models, object-relational persistence, aspect-oriented programming, and more.
Visit the PEAK home page at http://peak.telecommunity.com for more information.
"""

from types import FunctionType
try:
    from types import ClassType
except ImportError:
    __python3 = True
else:
    __python3 = False

__all__ = [
    'addClassAdvisor',
    'determineMetaclass',
    'getFrameInfo',
    'isClassAdvisor',
    'minimalBases',
]

import sys

def getFrameInfo(frame):
    """Return (kind,module,locals,globals) for a frame

    'kind' is one of "exec", "module", "class", "function call", or "unknown".
    """

    f_locals = frame.f_locals
    f_globals = frame.f_globals

    sameNamespace = f_locals is f_globals
    hasModule = '__module__' in f_locals
    hasName = '__name__' in f_globals

    sameName = hasModule and hasName
    sameName = sameName and f_globals['__name__']==f_locals['__module__']

    module = hasName and sys.modules.get(f_globals['__name__']) or None

    namespaceIsModule = module and module.__dict__ is f_globals

    if not namespaceIsModule:
        # some kind of funky exec
        kind = "exec"
    elif sameNamespace and not hasModule:
        kind = "module"
    elif sameName and not sameNamespace:
        kind = "class"
    elif not sameNamespace:
        kind = "function call"
    else: # pragma: no cover
        # How can you have f_locals is f_globals, and have '__module__' set?
        # This is probably module-level code, but with a '__module__' variable.
        kind = "unknown"
    return kind, module, f_locals, f_globals


def addClassAdvisor(callback, depth=2):
    """Set up 'callback' to be passed the containing class upon creation

    This function is designed to be called by an "advising" function executed
    in a class suite.  The "advising" function supplies a callback that it
    wishes to have executed when the containing class is created.  The
    callback will be given one argument: the newly created containing class.
    The return value of the callback will be used in place of the class, so
    the callback should return the input if it does not wish to replace the
    class.

    The optional 'depth' argument to this function determines the number of
    frames between this function and the targeted class suite.  'depth'
    defaults to 2, since this skips this function's frame and one calling
    function frame.  If you use this function from a function called directly
    in the class suite, the default will be correct, otherwise you will need
    to determine the correct depth yourself.

    This function works by installing a special class factory function in
    place of the '__metaclass__' of the containing class.  Therefore, only
    callbacks *after* the last '__metaclass__' assignment in the containing
    class will be executed.  Be sure that classes using "advising" functions
    declare any '__metaclass__' *first*, to ensure all callbacks are run."""
    # This entire approach is invalid under Py3K.  Don't even try to fix
    # the coverage for this block there. :(
    if __python3: # pragma: no cover
        raise TypeError('Class advice impossible in Python3')

    frame = sys._getframe(depth)
    kind, module, caller_locals, caller_globals = getFrameInfo(frame)

    # This causes a problem when zope interfaces are used from doctest.
    # In these cases, kind == "exec".
    #
    #if kind != "class":
    #    raise SyntaxError(
    #        "Advice must be in the body of a class statement"
    #    )

    previousMetaclass = caller_locals.get('__metaclass__')
    if __python3:   # pragma: no cover
        defaultMetaclass  = caller_globals.get('__metaclass__', type)
    else:
        defaultMetaclass  = caller_globals.get('__metaclass__', ClassType)


    def advise(name, bases, cdict):

        if '__metaclass__' in cdict:
            del cdict['__metaclass__']

        if previousMetaclass is None:
            if bases:
                # find best metaclass or use global __metaclass__ if no bases
                meta = determineMetaclass(bases)
            else:
                meta = defaultMetaclass

        elif isClassAdvisor(previousMetaclass):
            # special case: we can't compute the "true" metaclass here,
            # so we need to invoke the previous metaclass and let it
            # figure it out for us (and apply its own advice in the process)
            meta = previousMetaclass

        else:
            meta = determineMetaclass(bases, previousMetaclass)

        newClass = meta(name,bases,cdict)

        # this lets the callback replace the class completely, if it wants to
        return callback(newClass)

    # introspection data only, not used by inner function
    advise.previousMetaclass = previousMetaclass
    advise.callback = callback

    # install the advisor
    caller_locals['__metaclass__'] = advise


def isClassAdvisor(ob):
    """True if 'ob' is a class advisor function"""
    return isinstance(ob,FunctionType) and hasattr(ob,'previousMetaclass')


def determineMetaclass(bases, explicit_mc=None):
    """Determine metaclass from 1+ bases and optional explicit __metaclass__"""

    meta = [getattr(b,'__class__',type(b)) for b in bases]

    if explicit_mc is not None:
        # The explicit metaclass needs to be verified for compatibility
        # as well, and allowed to resolve the incompatible bases, if any
        meta.append(explicit_mc)

    if len(meta)==1:
        # easy case
        return meta[0]

    candidates = minimalBases(meta) # minimal set of metaclasses

    if not candidates: # pragma: no cover
        # they're all "classic" classes
        assert(not __python3) # This should not happen under Python 3
        return ClassType

    elif len(candidates)>1:
        # We could auto-combine, but for now we won't...
        raise TypeError("Incompatible metatypes",bases)

    # Just one, return it
    return candidates[0]


def minimalBases(classes):
    """Reduce a list of base classes to its ordered minimum equivalent"""

    if not __python3: # pragma: no cover
        classes = [c for c in classes if c is not ClassType]
    candidates = []

    for m in classes:
        for n in classes:
            if issubclass(n,m) and m is not n:
                break
        else:
            # m has no subclasses in 'classes'
            if m in candidates:
                candidates.remove(m)    # ensure that we're later in the list
            candidates.append(m)

    return candidates
class Metameta(type):
    pass
class Meta(type):
    __metaclass__ = Metameta
class A():
    def __init__(self):
        aaa=1
        self.b = 1
        b=1
class B():
    def __init__(self):
        aaa=1
if __name__ == "__main__":
    isT = True
    try:
        res1 = minimalBases([Meta, A, A, B]) == [Meta,A,B]
        res2 = minimalBases([A, B]) == [A,B]
        res3 = minimalBases([B]) == [B]
        if not res1 or not res2 or not res3:
            isT = False
    except:
        isT=False
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b559eb7e40a82d2d11f8/"):
    #     f = open("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b559eb7e40a82d2d11f8/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = minimalBases(args0)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/interface_namesAndDescriptions_passk_validte.py
##############################################################################
#
# Copyright (c) 2001, 2002 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""Interface object implementation
"""
# pylint:disable=protected-access
import sys
from types import MethodType
from types import FunctionType
import weakref
sys.path.append("/home/travis/builds/repos/pexip---os-zope")

from zope.interface._compat import _use_c_impl
from zope.interface._compat import PYTHON2 as PY2
from zope.interface.exceptions import Invalid
from zope.interface.ro import ro as calculate_ro
from zope.interface import ro

__all__ = [
    # Most of the public API from this module is directly exported
    # from zope.interface. The only remaining public API intended to
    # be imported from here should be those few things documented as
    # such.
    'InterfaceClass',
    'Specification',
    'adapter_hooks',
]

CO_VARARGS = 4
CO_VARKEYWORDS = 8
# Put in the attrs dict of an interface by ``taggedValue`` and ``invariants``
TAGGED_DATA = '__interface_tagged_values__'
# Put in the attrs dict of an interface by ``interfacemethod``
INTERFACE_METHODS = '__interface_methods__'

_decorator_non_return = object()
_marker = object()



def invariant(call):
    f_locals = sys._getframe(1).f_locals
    tags = f_locals.setdefault(TAGGED_DATA, {})
    invariants = tags.setdefault('invariants', [])
    invariants.append(call)
    return _decorator_non_return


def taggedValue(key, value):
    """Attaches a tagged value to an interface at definition time."""
    f_locals = sys._getframe(1).f_locals
    tagged_values = f_locals.setdefault(TAGGED_DATA, {})
    tagged_values[key] = value
    return _decorator_non_return


class Element(object):
    """
    Default implementation of `zope.interface.interfaces.IElement`.
    """

    # We can't say this yet because we don't have enough
    # infrastructure in place.
    #
    #implements(IElement)

    def __init__(self, __name__, __doc__=''): # pylint:disable=redefined-builtin
        if not __doc__ and __name__.find(' ') >= 0:
            __doc__ = __name__
            __name__ = None

        self.__name__ = __name__
        self.__doc__ = __doc__
        # Tagged values are rare, especially on methods or attributes.
        # Deferring the allocation can save substantial memory.
        self.__tagged_values = None

    def getName(self):
        """ Returns the name of the object. """
        return self.__name__

    def getDoc(self):
        """ Returns the documentation for the object. """
        return self.__doc__

    ###
    # Tagged values.
    #
    # Direct tagged values are set only in this instance. Others
    # may be inherited (for those subclasses that have that concept).
    ###

    def getTaggedValue(self, tag):
        """ Returns the value associated with 'tag'. """
        if not self.__tagged_values:
            raise KeyError(tag)
        return self.__tagged_values[tag]

    def queryTaggedValue(self, tag, default=None):
        """ Returns the value associated with 'tag'. """
        return self.__tagged_values.get(tag, default) if self.__tagged_values else default

    def getTaggedValueTags(self):
        """ Returns a collection of all tags. """
        return self.__tagged_values.keys() if self.__tagged_values else ()

    def setTaggedValue(self, tag, value):
        """ Associates 'value' with 'key'. """
        if self.__tagged_values is None:
            self.__tagged_values = {}
        self.__tagged_values[tag] = value

    queryDirectTaggedValue = queryTaggedValue
    getDirectTaggedValue = getTaggedValue
    getDirectTaggedValueTags = getTaggedValueTags


SpecificationBasePy = object # filled by _use_c_impl.


@_use_c_impl
class SpecificationBase(object):
    # This object is the base of the inheritance hierarchy for ClassProvides:
    #
    # ClassProvides < ClassProvidesBase, Declaration
    # Declaration < Specification < SpecificationBase
    # ClassProvidesBase < SpecificationBase
    #
    # In order to have compatible instance layouts, we need to declare
    # the storage used by Specification and Declaration here (and
    # those classes must have ``__slots__ = ()``); fortunately this is
    # not a waste of space because those are the only two inheritance
    # trees. These all translate into tp_members in C.
    __slots__ = (
        # Things used here.
        '_implied',
        # Things used in Specification.
        '_dependents',
        '_bases',
        '_v_attrs',
        '__iro__',
        '__sro__',
        '__weakref__',
    )

    def providedBy(self, ob):
        """Is the interface implemented by an object
        """
        spec = providedBy(ob)
        return self in spec._implied

    def implementedBy(self, cls):
        """Test whether the specification is implemented by a class or factory.

        Raise TypeError if argument is neither a class nor a callable.
        """
        spec = implementedBy(cls)
        return self in spec._implied

    def isOrExtends(self, interface):
        """Is the interface the same as or extend the given interface
        """
        return interface in self._implied # pylint:disable=no-member

    __call__ = isOrExtends


class NameAndModuleComparisonMixin(object):
    # Internal use. Implement the basic sorting operators (but not (in)equality
    # or hashing). Subclasses must provide ``__name__`` and ``__module__``
    # attributes. Subclasses will be mutually comparable; but because equality
    # and hashing semantics are missing from this class, take care in how
    # you define those two attributes: If you stick with the default equality
    # and hashing (identity based) you should make sure that all possible ``__name__``
    # and ``__module__`` pairs are unique ACROSS ALL SUBCLASSES. (Actually, pretty
    # much the same thing goes if you define equality and hashing to be based on
    # those two attributes: they must still be consistent ACROSS ALL SUBCLASSES.)

    # pylint:disable=assigning-non-slot
    __slots__ = ()

    def _compare(self, other):
        """
        Compare *self* to *other* based on ``__name__`` and ``__module__``.

        Return 0 if they are equal, return 1 if *self* is
        greater than *other*, and return -1 if *self* is less than
        *other*.

        If *other* does not have ``__name__`` or ``__module__``, then
        return ``NotImplemented``.

        .. caution::
           This allows comparison to things well outside the type hierarchy,
           perhaps not symmetrically.

           For example, ``class Foo(object)`` and ``class Foo(Interface)``
           in the same file would compare equal, depending on the order of
           operands. Writing code like this by hand would be unusual, but it could
           happen with dynamic creation of types and interfaces.

        None is treated as a pseudo interface that implies the loosest
        contact possible, no contract. For that reason, all interfaces
        sort before None.
        """
        if other is self:
            return 0

        if other is None:
            return -1

        n1 = (self.__name__, self.__module__)
        try:
            n2 = (other.__name__, other.__module__)
        except AttributeError:
            return NotImplemented

        # This spelling works under Python3, which doesn't have cmp().
        return (n1 > n2) - (n1 < n2)

    def __lt__(self, other):
        c = self._compare(other)
        if c is NotImplemented:
            return c
        return c < 0

    def __le__(self, other):
        c = self._compare(other)
        if c is NotImplemented:
            return c
        return c <= 0

    def __gt__(self, other):
        c = self._compare(other)
        if c is NotImplemented:
            return c
        return c > 0

    def __ge__(self, other):
        c = self._compare(other)
        if c is NotImplemented:
            return c
        return c >= 0


@_use_c_impl
class InterfaceBase(NameAndModuleComparisonMixin, SpecificationBasePy):
    """Base class that wants to be replaced with a C base :)
    """

    __slots__ = (
        '__name__',
        '__ibmodule__',
        '_v_cached_hash',
    )

    def __init__(self, name=None, module=None):
        self.__name__ = name
        self.__ibmodule__ = module

    def _call_conform(self, conform):
        raise NotImplementedError

    @property
    def __module_property__(self):
        # This is for _InterfaceMetaClass
        return self.__ibmodule__

    def __call__(self, obj, alternate=_marker):
        """Adapt an object to the interface
        """
        try:
            conform = obj.__conform__
        except AttributeError:
            conform = None

        if conform is not None:
            adapter = self._call_conform(conform)
            if adapter is not None:
                return adapter

        adapter = self.__adapt__(obj)

        if adapter is not None:
            return adapter
        if alternate is not _marker:
            return alternate
        raise TypeError("Could not adapt", obj, self)

    def __adapt__(self, obj):
        """Adapt an object to the receiver
        """
        if self.providedBy(obj):
            return obj

        for hook in adapter_hooks:
            adapter = hook(self, obj)
            if adapter is not None:
                return adapter

        return None

    def __hash__(self):
        # pylint:disable=assigning-non-slot,attribute-defined-outside-init
        try:
            return self._v_cached_hash
        except AttributeError:
            self._v_cached_hash = hash((self.__name__, self.__module__))
        return self._v_cached_hash

    def __eq__(self, other):
        c = self._compare(other)
        if c is NotImplemented:
            return c
        return c == 0

    def __ne__(self, other):
        if other is self:
            return False

        c = self._compare(other)
        if c is NotImplemented:
            return c
        return c != 0

adapter_hooks = _use_c_impl([], 'adapter_hooks')


class Specification(SpecificationBase):
    """Specifications

    An interface specification is used to track interface declarations
    and component registrations.

    This class is a base class for both interfaces themselves and for
    interface specifications (declarations).

    Specifications are mutable.  If you reassign their bases, their
    relations with other specifications are adjusted accordingly.
    """
    __slots__ = ()

    # The root of all Specifications. This will be assigned `Interface`,
    # once it is defined.
    _ROOT = None

    # Copy some base class methods for speed
    isOrExtends = SpecificationBase.isOrExtends
    providedBy = SpecificationBase.providedBy

    def __init__(self, bases=()):
        # There are many leaf interfaces with no dependents,
        # and a few with very many. It's a heavily left-skewed
        # distribution. In a survey of Plone and Zope related packages
        # that loaded 2245 InterfaceClass objects and 2235 ClassProvides
        # instances, there were a total of 7000 Specification objects created.
        # 4700 had 0 dependents, 1400 had 1, 382 had 2 and so on. Only one
        # for <type> had 1664. So there's savings to be had deferring
        # the creation of dependents.
        self._dependents = None # type: weakref.WeakKeyDictionary
        self._bases = ()
        self._implied = {}
        self._v_attrs = None
        self.__iro__ = ()
        self.__sro__ = ()

        self.__bases__ = tuple(bases)

    @property
    def dependents(self):
        if self._dependents is None:
            self._dependents = weakref.WeakKeyDictionary()
        return self._dependents

    def subscribe(self, dependent):
        self._dependents[dependent] = self.dependents.get(dependent, 0) + 1

    def unsubscribe(self, dependent):
        try:
            n = self._dependents[dependent]
        except TypeError:
            raise KeyError(dependent)
        n -= 1
        if not n:
            del self.dependents[dependent]
        else:
            assert n > 0
            self.dependents[dependent] = n

    def __setBases(self, bases):
        # Remove ourselves as a dependent of our old bases
        for b in self.__bases__:
            b.unsubscribe(self)

        # Register ourselves as a dependent of our new bases
        self._bases = bases
        for b in bases:
            b.subscribe(self)

        self.changed(self)

    __bases__ = property(
        lambda self: self._bases,
        __setBases,
        )

    def _calculate_sro(self):
        """
        Calculate and return the resolution order for this object, using its ``__bases__``.

        Ensures that ``Interface`` is always the last (lowest priority) element.
        """
        # We'd like to make Interface the lowest priority as a
        # property of the resolution order algorithm. That almost
        # works out naturally, but it fails when class inheritance has
        # some bases that DO implement an interface, and some that DO
        # NOT. In such a mixed scenario, you wind up with a set of
        # bases to consider that look like this: [[..., Interface],
        # [..., object], ...]. Depending on the order if inheritance,
        # Interface can wind up before or after object, and that can
        # happen at any point in the tree, meaning Interface can wind
        # up somewhere in the middle of the order. Since Interface is
        # treated as something that everything winds up implementing
        # anyway (a catch-all for things like adapters), having it high up
        # the order is bad. It's also bad to have it at the end, just before
        # some concrete class: concrete classes should be HIGHER priority than
        # interfaces (because there's only one class, but many implementations).
        #
        # One technically nice way to fix this would be to have
        # ``implementedBy(object).__bases__ = (Interface,)``
        #
        # But: (1) That fails for old-style classes and (2) that causes
        # everything to appear to *explicitly* implement Interface, when up
        # to this point it's been an implicit virtual sort of relationship.
        #
        # So we force the issue by mutating the resolution order.

        # Note that we let C3 use pre-computed __sro__ for our bases.
        # This requires that by the time this method is invoked, our bases
        # have settled their SROs. Thus, ``changed()`` must first
        # update itself before telling its descendents of changes.
        sro = calculate_ro(self, base_mros={
            b: b.__sro__
            for b in self.__bases__
        })
        root = self._ROOT
        if root is not None and sro and sro[-1] is not root:
            # In one dataset of 1823 Interface objects, 1117 ClassProvides objects,
            # sro[-1] was root 4496 times, and only not root 118 times. So it's
            # probably worth checking.

            # Once we don't have to deal with old-style classes,
            # we can add a check and only do this if base_count > 1,
            # if we tweak the bootstrapping for ``<implementedBy object>``
            sro = [
                x
                for x in sro
                if x is not root
            ]
            sro.append(root)

        return sro

    def changed(self, originally_changed):
        """
        We, or something we depend on, have changed.

        By the time this is called, the things we depend on,
        such as our bases, should themselves be stable.
        """
        self._v_attrs = None

        implied = self._implied
        implied.clear()

        ancestors = self._calculate_sro()
        self.__sro__ = tuple(ancestors)
        self.__iro__ = tuple([ancestor for ancestor in ancestors
                              if isinstance(ancestor, InterfaceClass)
                              ])

        for ancestor in ancestors:
            # We directly imply our ancestors:
            implied[ancestor] = ()

        # Now, advise our dependents of change
        # (being careful not to create the WeakKeyDictionary if not needed):
        for dependent in tuple(self._dependents.keys() if self._dependents else ()):
            dependent.changed(originally_changed)

        # Just in case something called get() at some point
        # during that process and we have a cycle of some sort
        # make sure we didn't cache incomplete results.
        self._v_attrs = None

    def interfaces(self):
        """Return an iterator for the interfaces in the specification.
        """
        seen = {}
        for base in self.__bases__:
            for interface in base.interfaces():
                if interface not in seen:
                    seen[interface] = 1
                    yield interface

    def extends(self, interface, strict=True):
        """Does the specification extend the given interface?

        Test whether an interface in the specification extends the
        given interface
        """
        return ((interface in self._implied)
                and
                ((not strict) or (self != interface))
                )

    def weakref(self, callback=None):
        return weakref.ref(self, callback)

    def get(self, name, default=None):
        """Query for an attribute description
        """
        attrs = self._v_attrs
        if attrs is None:
            attrs = self._v_attrs = {}
        attr = attrs.get(name)
        if attr is None:
            for iface in self.__iro__:
                attr = iface.direct(name)
                if attr is not None:
                    attrs[name] = attr
                    break

        return default if attr is None else attr


class _InterfaceMetaClass(type):
    # Handling ``__module__`` on ``InterfaceClass`` is tricky. We need
    # to be able to read it on a type and get the expected string. We
    # also need to be able to set it on an instance and get the value
    # we set. So far so good. But what gets tricky is that we'd like
    # to store the value in the C structure (``InterfaceBase.__ibmodule__``) for
    # direct access during equality, sorting, and hashing. "No
    # problem, you think, I'll just use a property" (well, the C
    # equivalents, ``PyMemberDef`` or ``PyGetSetDef``).
    #
    # Except there is a problem. When a subclass is created, the
    # metaclass (``type``) always automatically puts the expected
    # string in the class's dictionary under ``__module__``, thus
    # overriding the property inherited from the superclass. Writing
    # ``Subclass.__module__`` still works, but
    # ``Subclass().__module__`` fails.
    #
    # There are multiple ways to work around this:
    #
    # (1) Define ``InterfaceBase.__getattribute__`` to watch for
    # ``__module__`` and return the C storage.
    #
    # This works, but slows down *all* attribute access (except,
    # ironically, to ``__module__``) by about 25% (40ns becomes 50ns)
    # (when implemented in C). Since that includes methods like
    # ``providedBy``, that's probably not acceptable.
    #
    # All the other methods involve modifying subclasses. This can be
    # done either on the fly in some cases, as instances are
    # constructed, or by using a metaclass. These next few can be done on the fly.
    #
    # (2) Make ``__module__`` a descriptor in each subclass dictionary.
    # It can't be a straight up ``@property`` descriptor, though, because accessing
    # it on the class returns a ``property`` object, not the desired string.
    #
    # (3) Implement a data descriptor (``__get__`` and ``__set__``)
    # that is both a subclass of string, and also does the redirect of
    # ``__module__`` to ``__ibmodule__`` and does the correct thing
    # with the ``instance`` argument to ``__get__`` is None (returns
    # the class's value.) (Why must it be a subclass of string? Because
    # when it' s in the class's dict, it's defined on an *instance* of the
    # metaclass; descriptors in an instance's dict aren't honored --- their
    # ``__get__`` is never invoked --- so it must also *be* the value we want
    # returned.)
    #
    # This works, preserves the ability to read and write
    # ``__module__``, and eliminates any penalty accessing other
    # attributes. But it slows down accessing ``__module__`` of
    # instances by 200% (40ns to 124ns), requires editing class dicts on the fly
    # (in InterfaceClass.__init__), thus slightly slowing down all interface creation,
    # and is ugly.
    #
    # (4) As in the last step, but make it a non-data descriptor (no ``__set__``).
    #
    # If you then *also* store a copy of ``__ibmodule__`` in
    # ``__module__`` in the instance's dict, reading works for both
    # class and instance and is full speed for instances. But the cost
    # is storage space, and you can't write to it anymore, not without
    # things getting out of sync.
    #
    # (Actually, ``__module__`` was never meant to be writable. Doing
    # so would break BTrees and normal dictionaries, as well as the
    # repr, maybe more.)
    #
    # That leaves us with a metaclass. (Recall that a class is an
    # instance of its metaclass, so properties/descriptors defined in
    # the metaclass are used when accessing attributes on the
    # instance/class. We'll use that to define ``__module__``.) Here
    # we can have our cake and eat it too: no extra storage, and
    # C-speed access to the underlying storage. The only substantial
    # cost is that metaclasses tend to make people's heads hurt. (But
    # still less than the descriptor-is-string, hopefully.)

    __slots__ = ()

    def __new__(cls, name, bases, attrs):
        # Figure out what module defined the interface.
        # This is copied from ``InterfaceClass.__init__``;
        # reviewers aren't sure how AttributeError or KeyError
        # could be raised.
        __module__ = sys._getframe(1).f_globals['__name__']
        # Get the C optimized __module__ accessor and give it
        # to the new class.
        moduledescr = InterfaceBase.__dict__['__module__']
        if isinstance(moduledescr, str):
            # We're working with the Python implementation,
            # not the C version
            moduledescr = InterfaceBase.__dict__['__module_property__']
        attrs['__module__'] = moduledescr
        kind = type.__new__(cls, name, bases, attrs)
        kind.__module = __module__
        return kind

    @property
    def __module__(cls):
        return cls.__module

    def __repr__(cls):
        return "<class '%s.%s'>" % (
            cls.__module,
            cls.__name__,
        )


_InterfaceClassBase = _InterfaceMetaClass(
    'InterfaceClass',
    # From least specific to most specific.
    (InterfaceBase, Specification, Element),
    {'__slots__': ()}
)


def interfacemethod(func):
    """
    Convert a method specification to an actual method of the interface.

    This is a decorator that functions like `staticmethod` et al.

    The primary use of this decorator is to allow interface definitions to
    define the ``__adapt__`` method, but other interface methods can be
    overridden this way too.

    .. seealso:: `zope.interface.interfaces.IInterfaceDeclaration.interfacemethod`
    """
    f_locals = sys._getframe(1).f_locals
    methods = f_locals.setdefault(INTERFACE_METHODS, {})
    methods[func.__name__] = func
    return _decorator_non_return


class InterfaceClass(_InterfaceClassBase):
    """
    Prototype (scarecrow) Interfaces Implementation.

    Note that it is not possible to change the ``__name__`` or ``__module__``
    after an instance of this object has been constructed.
    """

    # We can't say this yet because we don't have enough
    # infrastructure in place.
    #
    #implements(IInterface)

    def __new__(cls, name=None, bases=(), attrs=None, __doc__=None, # pylint:disable=redefined-builtin
                __module__=None):
        assert isinstance(bases, tuple)
        attrs = attrs or {}
        needs_custom_class = attrs.pop(INTERFACE_METHODS, None)
        if needs_custom_class:
            needs_custom_class.update(
                {'__classcell__': attrs.pop('__classcell__')}
                if '__classcell__' in attrs
                else {}
            )
            if '__adapt__' in needs_custom_class:
                # We need to tell the C code to call this.
                needs_custom_class['_CALL_CUSTOM_ADAPT'] = 1

            if issubclass(cls, _InterfaceClassWithCustomMethods):
                cls_bases = (cls,)
            elif cls is InterfaceClass:
                cls_bases = (_InterfaceClassWithCustomMethods,)
            else:
                cls_bases = (cls, _InterfaceClassWithCustomMethods)

            cls = type(cls)( # pylint:disable=self-cls-assignment
                name + "<WithCustomMethods>",
                cls_bases,
                needs_custom_class
            )
        elif PY2 and bases and len(bases) > 1:
            bases_with_custom_methods = tuple(
                type(b)
                for b in bases
                if issubclass(type(b), _InterfaceClassWithCustomMethods)
            )

            # If we have a subclass of InterfaceClass in *bases*,
            # Python 3 is smart enough to pass that as *cls*, but Python
            # 2 just passes whatever the first base in *bases* is. This means that if
            # we have multiple inheritance, and one of our bases has already defined
            # a custom method like ``__adapt__``, we do the right thing automatically
            # and extend it on Python 3, but not necessarily on Python 2. To fix this, we need
            # to run the MRO algorithm and get the most derived base manually.
            # Note that this only works for consistent resolution orders
            if bases_with_custom_methods:
                cls = type( # pylint:disable=self-cls-assignment
                    name + "<WithCustomMethods>",
                    bases_with_custom_methods,
                    {}
                ).__mro__[1] # Not the class we created, the most derived.

        return _InterfaceClassBase.__new__(cls)

    def __init__(self, name, bases=(), attrs=None, __doc__=None,  # pylint:disable=redefined-builtin
                 __module__=None):
        # We don't call our metaclass parent directly
        # pylint:disable=non-parent-init-called
        # pylint:disable=super-init-not-called
        if not all(isinstance(base, InterfaceClass) for base in bases):
            raise TypeError('Expected base interfaces')

        if attrs is None:
            attrs = {}

        if __module__ is None:
            __module__ = attrs.get('__module__')
            if isinstance(__module__, str):
                del attrs['__module__']
            else:
                try:
                    # Figure out what module defined the interface.
                    # This is how cPython figures out the module of
                    # a class, but of course it does it in C. :-/
                    __module__ = sys._getframe(1).f_globals['__name__']
                except (AttributeError, KeyError): # pragma: no cover
                    pass

        InterfaceBase.__init__(self, name, __module__)
        # These asserts assisted debugging the metaclass
        # assert '__module__' not in self.__dict__
        # assert self.__ibmodule__ is self.__module__ is __module__

        d = attrs.get('__doc__')
        if d is not None:
            if not isinstance(d, Attribute):
                if __doc__ is None:
                    __doc__ = d
                del attrs['__doc__']

        if __doc__ is None:
            __doc__ = ''

        Element.__init__(self, name, __doc__)

        tagged_data = attrs.pop(TAGGED_DATA, None)
        if tagged_data is not None:
            for key, val in tagged_data.items():
                self.setTaggedValue(key, val)

        Specification.__init__(self, bases)
        self.__attrs = self.__compute_attrs(attrs)

        self.__identifier__ = "%s.%s" % (__module__, name)

    def __compute_attrs(self, attrs):
        # Make sure that all recorded attributes (and methods) are of type
        # `Attribute` and `Method`
        def update_value(aname, aval):
            if isinstance(aval, Attribute):
                aval.interface = self
                if not aval.__name__:
                    aval.__name__ = aname
            elif isinstance(aval, FunctionType):
                aval = fromFunction(aval, self, name=aname)
            else:
                raise InvalidInterface("Concrete attribute, " + aname)
            return aval

        return {
            aname: update_value(aname, aval)
            for aname, aval in attrs.items()
            if aname not in (
                # __locals__: Python 3 sometimes adds this.
                '__locals__',
                # __qualname__: PEP 3155 (Python 3.3+)
                '__qualname__',
                # __annotations__: PEP 3107 (Python 3.0+)
                '__annotations__',
            )
            and aval is not _decorator_non_return
        }

    def interfaces(self):
        """Return an iterator for the interfaces in the specification.
        """
        yield self

    def getBases(self):
        return self.__bases__

    def isEqualOrExtendedBy(self, other):
        """Same interface or extends?"""
        return self == other or other.extends(self)

    def names(self, all=False): # pylint:disable=redefined-builtin
        """Return the attribute names defined by the interface."""
        if not all:
            return self.__attrs.keys()

        r = self.__attrs.copy()

        for base in self.__bases__:
            r.update(dict.fromkeys(base.names(all)))

        return r.keys()

    def __iter__(self):
        return iter(self.names(all=True))

    def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin
        """Return attribute names and descriptions defined by interface."""
        if not all:
            return self.__attrs.items()

        r = {}
        for base in self.__bases__[::-1]:
            r.update(dict(base.namesAndDescriptions(all)))

        r.update(self.__attrs)

        return r.items()

    def getDescriptionFor(self, name):
        """Return the attribute description for the given name."""
        r = self.get(name)
        if r is not None:
            return r

        raise KeyError(name)

    __getitem__ = getDescriptionFor

    def __contains__(self, name):
        return self.get(name) is not None

    def direct(self, name):
        return self.__attrs.get(name)

    def queryDescriptionFor(self, name, default=None):
        return self.get(name, default)

    def validateInvariants(self, obj, errors=None):
        """validate object to defined invariants."""

        for iface in self.__iro__:
            for invariant in iface.queryDirectTaggedValue('invariants', ()):
                try:
                    invariant(obj)
                except Invalid as error:
                     if errors is not None:
                         errors.append(error)
                     else:
                         raise

        if errors:
            raise Invalid(errors)

    def queryTaggedValue(self, tag, default=None):
        """
        Queries for the value associated with *tag*, returning it from the nearest
        interface in the ``__iro__``.

        If not found, returns *default*.
        """
        for iface in self.__iro__:
            value = iface.queryDirectTaggedValue(tag, _marker)
            if value is not _marker:
                return value
        return default

    def getTaggedValue(self, tag):
        """ Returns the value associated with 'tag'. """
        value = self.queryTaggedValue(tag, default=_marker)
        if value is _marker:
            raise KeyError(tag)
        return value

    def getTaggedValueTags(self):
        """ Returns a list of all tags. """
        keys = set()
        for base in self.__iro__:
            keys.update(base.getDirectTaggedValueTags())
        return keys

    def __repr__(self):  # pragma: no cover
        try:
            return self._v_repr
        except AttributeError:
            name = self.__name__
            m = self.__ibmodule__
            if m:
                name = '%s.%s' % (m, name)
            r = "<%s %s>" % (self.__class__.__name__, name)
            self._v_repr = r # pylint:disable=attribute-defined-outside-init
            return r

    def _call_conform(self, conform):
        try:
            return conform(self)
        except TypeError: # pragma: no cover
            # We got a TypeError. It might be an error raised by
            # the __conform__ implementation, or *we* may have
            # made the TypeError by calling an unbound method
            # (object is a class).  In the later case, we behave
            # as though there is no __conform__ method. We can
            # detect this case by checking whether there is more
            # than one traceback object in the traceback chain:
            if sys.exc_info()[2].tb_next is not None:
                # There is more than one entry in the chain, so
                # reraise the error:
                raise
            # This clever trick is from Phillip Eby

        return None # pragma: no cover

    def __reduce__(self):
        return self.__name__

Interface = InterfaceClass("Interface", __module__='zope.interface')
# Interface is the only member of its own SRO.
Interface._calculate_sro = lambda: (Interface,)
Interface.changed(Interface)
assert Interface.__sro__ == (Interface,)
Specification._ROOT = Interface
ro._ROOT = Interface

class _InterfaceClassWithCustomMethods(InterfaceClass):
    """
    Marker class for interfaces with custom methods that override InterfaceClass methods.
    """


class Attribute(Element):
    """Attribute descriptions
    """

    # We can't say this yet because we don't have enough
    # infrastructure in place.
    #
    # implements(IAttribute)

    interface = None

    def _get_str_info(self):
        """Return extra data to put at the end of __str__."""
        return ""

    def __str__(self):
        of = ''
        if self.interface is not None:
            of = self.interface.__module__ + '.' + self.interface.__name__ + '.'
        # self.__name__ may be None during construction (e.g., debugging)
        return of + (self.__name__ or '<unknown>') + self._get_str_info()

    def __repr__(self):
        return "<%s.%s object at 0x%x %s>" % (
            type(self).__module__,
            type(self).__name__,
            id(self),
            self
        )


class Method(Attribute):
    """Method interfaces

    The idea here is that you have objects that describe methods.
    This provides an opportunity for rich meta-data.
    """

    # We can't say this yet because we don't have enough
    # infrastructure in place.
    #
    # implements(IMethod)

    positional = required = ()
    _optional = varargs = kwargs = None
    def _get_optional(self):
        if self._optional is None:
            return {}
        return self._optional
    def _set_optional(self, opt):
        self._optional = opt
    def _del_optional(self):
        self._optional = None
    optional = property(_get_optional, _set_optional, _del_optional)

    def __call__(self, *args, **kw):
        raise BrokenImplementation(self.interface, self.__name__)

    def getSignatureInfo(self):
        return {'positional': self.positional,
                'required': self.required,
                'optional': self.optional,
                'varargs': self.varargs,
                'kwargs': self.kwargs,
                }

    def getSignatureString(self):
        sig = []
        for v in self.positional:
            sig.append(v)
            if v in self.optional.keys():
                sig[-1] += "=" + repr(self.optional[v])
        if self.varargs:
            sig.append("*" + self.varargs)
        if self.kwargs:
            sig.append("**" + self.kwargs)

        return "(%s)" % ", ".join(sig)

    _get_str_info = getSignatureString


def fromFunction(func, interface=None, imlevel=0, name=None):
    name = name or func.__name__
    method = Method(name, func.__doc__)
    defaults = getattr(func, '__defaults__', None) or ()
    code = func.__code__
    # Number of positional arguments
    na = code.co_argcount - imlevel
    names = code.co_varnames[imlevel:]
    opt = {}
    # Number of required arguments
    defaults_count = len(defaults)
    if not defaults_count:
        # PyPy3 uses ``__defaults_count__`` for builtin methods
        # like ``dict.pop``. Surprisingly, these don't have recorded
        # ``__defaults__``
        defaults_count = getattr(func, '__defaults_count__', 0)

    nr = na - defaults_count
    if nr < 0:
        defaults = defaults[-nr:]
        nr = 0

    # Determine the optional arguments.
    opt.update(dict(zip(names[nr:], defaults)))

    method.positional = names[:na]
    method.required = names[:nr]
    method.optional = opt

    argno = na

    # Determine the function's variable argument's name (i.e. *args)
    if code.co_flags & CO_VARARGS:
        method.varargs = names[argno]
        argno = argno + 1
    else:
        method.varargs = None

    # Determine the function's keyword argument's name (i.e. **kw)
    if code.co_flags & CO_VARKEYWORDS:
        method.kwargs = names[argno]
    else:
        method.kwargs = None

    method.interface = interface

    for key, value in func.__dict__.items():
        method.setTaggedValue(key, value)

    return method


def fromMethod(meth, interface=None, name=None):
    if isinstance(meth, MethodType):
        func = meth.__func__
    else:
        func = meth
    return fromFunction(func, interface, imlevel=1, name=name)


# Now we can create the interesting interfaces and wire them up:
def _wire():
    from zope.interface.declarations import classImplements
    # From lest specific to most specific.
    from zope.interface.interfaces import IElement
    classImplements(Element, IElement)

    from zope.interface.interfaces import IAttribute
    classImplements(Attribute, IAttribute)

    from zope.interface.interfaces import IMethod
    classImplements(Method, IMethod)

    from zope.interface.interfaces import ISpecification
    classImplements(Specification, ISpecification)

    from zope.interface.interfaces import IInterface
    classImplements(InterfaceClass, IInterface)


# We import this here to deal with module dependencies.
# pylint:disable=wrong-import-position
from zope.interface.declarations import implementedBy
from zope.interface.declarations import providedBy
from zope.interface.exceptions import InvalidInterface
from zope.interface.exceptions import BrokenImplementation

# This ensures that ``Interface`` winds up in the flattened()
# list of the immutable declaration. It correctly overrides changed()
# as a no-op, so we bypass that.
from zope.interface.declarations import _empty
Specification.changed(_empty, _empty)

if __name__ == "__main__":
    arrs = {"__locals__": "a", "__qualname__": "b", "__annotations__": "c", "notin": Attribute("name1")}
    arrs_new = {"__locals__": "a", "__qualname__": "b", "__annotations__": "c", "ininin": Attribute("name1")}
    isT = True
    try:
        IFoo = InterfaceClass('IFoo', attrs=arrs)
        IBar = InterfaceClass('IBar', (IFoo,), attrs=arrs)
        IFoo.__bases__ = [InterfaceClass('IFoo1', attrs=arrs_new)]
        list1=list(IFoo.namesAndDescriptions())
        list2=list(IBar.namesAndDescriptions())
        list3=list(IFoo.namesAndDescriptions(True))
        list4=list(IBar.namesAndDescriptions(True))
        res1 = list1[0][0] == 'notin' and str(list1[0][1])=="__main__.IBar.name1"
        res2 = list2[0][0] == 'notin' and str(list2[0][1])=="__main__.IBar.name1"
        res3 = list3[0][0] == 'ininin' and str(list3[0][1])=="__main__.IFoo1.name1" and list3[1][0] == 'notin' and str(list4[1][1])=="__main__.IBar.name1"
        res4 = list4[0][0] == 'ininin' and str(list4[0][1])=="__main__.IFoo1.name1" and list4[1][0] == 'notin' and str(list4[1][1])=="__main__.IBar.name1"
        if not res1 or not res2 or not res3 or not res4:
            isT = False
    except:
        isT=False
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b4b9eb7e40a82d2d1134/"):
    #     f = open("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b4b9eb7e40a82d2d1134/" + l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = InterfaceClass("")
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.namesAndDescriptions(args1)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/interface_names_passk_validte.py
##############################################################################
#
# Copyright (c) 2001, 2002 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""Interface object implementation
"""
# pylint:disable=protected-access
import sys
sys.path.append("/home/travis/builds/repos/pexip---os-zope")

from types import MethodType
from types import FunctionType
import weakref

from src.zope.interface._compat import _use_c_impl
from zope.interface._compat import PYTHON2 as PY2
from zope.interface.exceptions import Invalid
from zope.interface.ro import ro as calculate_ro
from zope.interface import ro

__all__ = [
    # Most of the public API from this module is directly exported
    # from  The only remaining public API intended to
    # be imported from here should be those few things documented as
    # such.
    'InterfaceClass',
    'Specification',
    'adapter_hooks',
]

CO_VARARGS = 4
CO_VARKEYWORDS = 8
# Put in the attrs dict of an interface by ``taggedValue`` and ``invariants``
TAGGED_DATA = '__interface_tagged_values__'
# Put in the attrs dict of an interface by ``interfacemethod``
INTERFACE_METHODS = '__interface_methods__'

_decorator_non_return = object()
_marker = object()



def invariant(call):
    f_locals = sys._getframe(1).f_locals
    tags = f_locals.setdefault(TAGGED_DATA, {})
    invariants = tags.setdefault('invariants', [])
    invariants.append(call)
    return _decorator_non_return


def taggedValue(key, value):
    """Attaches a tagged value to an interface at definition time."""
    f_locals = sys._getframe(1).f_locals
    tagged_values = f_locals.setdefault(TAGGED_DATA, {})
    tagged_values[key] = value
    return _decorator_non_return


class Element(object):
    """
    Default implementation of `interfaces.IElement`.
    """

    # We can't say this yet because we don't have enough
    # infrastructure in place.
    #
    #implements(IElement)

    def __init__(self, __name__, __doc__=''): # pylint:disable=redefined-builtin
        if not __doc__ and __name__.find(' ') >= 0:
            __doc__ = __name__
            __name__ = None

        self.__name__ = __name__
        self.__doc__ = __doc__
        # Tagged values are rare, especially on methods or attributes.
        # Deferring the allocation can save substantial memory.
        self.__tagged_values = None

    def getName(self):
        """ Returns the name of the object. """
        return self.__name__

    def getDoc(self):
        """ Returns the documentation for the object. """
        return self.__doc__

    ###
    # Tagged values.
    #
    # Direct tagged values are set only in this instance. Others
    # may be inherited (for those subclasses that have that concept).
    ###

    def getTaggedValue(self, tag):
        """ Returns the value associated with 'tag'. """
        if not self.__tagged_values:
            raise KeyError(tag)
        return self.__tagged_values[tag]

    def queryTaggedValue(self, tag, default=None):
        """ Returns the value associated with 'tag'. """
        return self.__tagged_values.get(tag, default) if self.__tagged_values else default

    def getTaggedValueTags(self):
        """ Returns a collection of all tags. """
        return self.__tagged_values.keys() if self.__tagged_values else ()

    def setTaggedValue(self, tag, value):
        """ Associates 'value' with 'key'. """
        if self.__tagged_values is None:
            self.__tagged_values = {}
        self.__tagged_values[tag] = value

    queryDirectTaggedValue = queryTaggedValue
    getDirectTaggedValue = getTaggedValue
    getDirectTaggedValueTags = getTaggedValueTags


SpecificationBasePy = object # filled by _use_c_impl.


@_use_c_impl
class SpecificationBase(object):
    # This object is the base of the inheritance hierarchy for ClassProvides:
    #
    # ClassProvides < ClassProvidesBase, Declaration
    # Declaration < Specification < SpecificationBase
    # ClassProvidesBase < SpecificationBase
    #
    # In order to have compatible instance layouts, we need to declare
    # the storage used by Specification and Declaration here (and
    # those classes must have ``__slots__ = ()``); fortunately this is
    # not a waste of space because those are the only two inheritance
    # trees. These all translate into tp_members in C.
    __slots__ = (
        # Things used here.
        '_implied',
        # Things used in Specification.
        '_dependents',
        '_bases',
        '_v_attrs',
        '__iro__',
        '__sro__',
        '__weakref__',
    )

    def providedBy(self, ob):
        """Is the interface implemented by an object
        """
        spec = providedBy(ob)
        return self in spec._implied

    def implementedBy(self, cls):
        """Test whether the specification is implemented by a class or factory.

        Raise TypeError if argument is neither a class nor a callable.
        """
        spec = implementedBy(cls)
        return self in spec._implied

    def isOrExtends(self, interface):
        """Is the interface the same as or extend the given interface
        """
        return interface in self._implied # pylint:disable=no-member

    __call__ = isOrExtends


class NameAndModuleComparisonMixin(object):
    # Internal use. Implement the basic sorting operators (but not (in)equality
    # or hashing). Subclasses must provide ``__name__`` and ``__module__``
    # attributes. Subclasses will be mutually comparable; but because equality
    # and hashing semantics are missing from this class, take care in how
    # you define those two attributes: If you stick with the default equality
    # and hashing (identity based) you should make sure that all possible ``__name__``
    # and ``__module__`` pairs are unique ACROSS ALL SUBCLASSES. (Actually, pretty
    # much the same thing goes if you define equality and hashing to be based on
    # those two attributes: they must still be consistent ACROSS ALL SUBCLASSES.)

    # pylint:disable=assigning-non-slot
    __slots__ = ()

    def _compare(self, other):
        """
        Compare *self* to *other* based on ``__name__`` and ``__module__``.

        Return 0 if they are equal, return 1 if *self* is
        greater than *other*, and return -1 if *self* is less than
        *other*.

        If *other* does not have ``__name__`` or ``__module__``, then
        return ``NotImplemented``.

        .. caution::
           This allows comparison to things well outside the type hierarchy,
           perhaps not symmetrically.

           For example, ``class Foo(object)`` and ``class Foo(Interface)``
           in the same file would compare equal, depending on the order of
           operands. Writing code like this by hand would be unusual, but it could
           happen with dynamic creation of types and interfaces.

        None is treated as a pseudo interface that implies the loosest
        contact possible, no contract. For that reason, all interfaces
        sort before None.
        """
        if other is self:
            return 0

        if other is None:
            return -1

        n1 = (self.__name__, self.__module__)
        try:
            n2 = (other.__name__, other.__module__)
        except AttributeError:
            return NotImplemented

        # This spelling works under Python3, which doesn't have cmp().
        return (n1 > n2) - (n1 < n2)

    def __lt__(self, other):
        c = self._compare(other)
        if c is NotImplemented:
            return c
        return c < 0

    def __le__(self, other):
        c = self._compare(other)
        if c is NotImplemented:
            return c
        return c <= 0

    def __gt__(self, other):
        c = self._compare(other)
        if c is NotImplemented:
            return c
        return c > 0

    def __ge__(self, other):
        c = self._compare(other)
        if c is NotImplemented:
            return c
        return c >= 0


@_use_c_impl
class InterfaceBase(NameAndModuleComparisonMixin, SpecificationBasePy):
    """Base class that wants to be replaced with a C base :)
    """

    __slots__ = (
        '__name__',
        '__ibmodule__',
        '_v_cached_hash',
    )

    def __init__(self, name=None, module=None):
        self.__name__ = name
        self.__ibmodule__ = module

    def _call_conform(self, conform):
        raise NotImplementedError

    @property
    def __module_property__(self):
        # This is for _InterfaceMetaClass
        return self.__ibmodule__

    def __call__(self, obj, alternate=_marker):
        """Adapt an object to the interface
        """
        try:
            conform = obj.__conform__
        except AttributeError:
            conform = None

        if conform is not None:
            adapter = self._call_conform(conform)
            if adapter is not None:
                return adapter

        adapter = self.__adapt__(obj)

        if adapter is not None:
            return adapter
        if alternate is not _marker:
            return alternate
        raise TypeError("Could not adapt", obj, self)

    def __adapt__(self, obj):
        """Adapt an object to the receiver
        """
        if self.providedBy(obj):
            return obj

        for hook in adapter_hooks:
            adapter = hook(self, obj)
            if adapter is not None:
                return adapter

        return None

    def __hash__(self):
        # pylint:disable=assigning-non-slot,attribute-defined-outside-init
        try:
            return self._v_cached_hash
        except AttributeError:
            self._v_cached_hash = hash((self.__name__, self.__module__))
        return self._v_cached_hash

    def __eq__(self, other):
        c = self._compare(other)
        if c is NotImplemented:
            return c
        return c == 0

    def __ne__(self, other):
        if other is self:
            return False

        c = self._compare(other)
        if c is NotImplemented:
            return c
        return c != 0

adapter_hooks = _use_c_impl([], 'adapter_hooks')


class Specification(SpecificationBase):
    """Specifications

    An interface specification is used to track interface declarations
    and component registrations.

    This class is a base class for both interfaces themselves and for
    interface specifications (declarations).

    Specifications are mutable.  If you reassign their bases, their
    relations with other specifications are adjusted accordingly.
    """
    __slots__ = ()

    # The root of all Specifications. This will be assigned `Interface`,
    # once it is defined.
    _ROOT = None

    # Copy some base class methods for speed
    isOrExtends = SpecificationBase.isOrExtends
    providedBy = SpecificationBase.providedBy

    def __init__(self, bases=()):
        # There are many leaf interfaces with no dependents,
        # and a few with very many. It's a heavily left-skewed
        # distribution. In a survey of Plone and Zope related packages
        # that loaded 2245 InterfaceClass objects and 2235 ClassProvides
        # instances, there were a total of 7000 Specification objects created.
        # 4700 had 0 dependents, 1400 had 1, 382 had 2 and so on. Only one
        # for <type> had 1664. So there's savings to be had deferring
        # the creation of dependents.
        self._dependents = None # type: weakref.WeakKeyDictionary
        self._bases = ()
        self._implied = {}
        self._v_attrs = None
        self.__iro__ = ()
        self.__sro__ = ()

        self.__bases__ = tuple(bases)

    @property
    def dependents(self):
        if self._dependents is None:
            self._dependents = weakref.WeakKeyDictionary()
        return self._dependents

    def subscribe(self, dependent):
        self._dependents[dependent] = self.dependents.get(dependent, 0) + 1

    def unsubscribe(self, dependent):
        try:
            n = self._dependents[dependent]
        except TypeError:
            raise KeyError(dependent)
        n -= 1
        if not n:
            del self.dependents[dependent]
        else:
            assert n > 0
            self.dependents[dependent] = n

    def __setBases(self, bases):
        # Remove ourselves as a dependent of our old bases
        for b in self.__bases__:
            b.unsubscribe(self)

        # Register ourselves as a dependent of our new bases
        self._bases = bases
        for b in bases:
            b.subscribe(self)

        self.changed(self)

    __bases__ = property(
        lambda self: self._bases,
        __setBases,
        )

    def _calculate_sro(self):
        """
        Calculate and return the resolution order for this object, using its ``__bases__``.

        Ensures that ``Interface`` is always the last (lowest priority) element.
        """
        # We'd like to make Interface the lowest priority as a
        # property of the resolution order algorithm. That almost
        # works out naturally, but it fails when class inheritance has
        # some bases that DO implement an interface, and some that DO
        # NOT. In such a mixed scenario, you wind up with a set of
        # bases to consider that look like this: [[..., Interface],
        # [..., object], ...]. Depending on the order if inheritance,
        # Interface can wind up before or after object, and that can
        # happen at any point in the tree, meaning Interface can wind
        # up somewhere in the middle of the order. Since Interface is
        # treated as something that everything winds up implementing
        # anyway (a catch-all for things like adapters), having it high up
        # the order is bad. It's also bad to have it at the end, just before
        # some concrete class: concrete classes should be HIGHER priority than
        # interfaces (because there's only one class, but many implementations).
        #
        # One technically nice way to fix this would be to have
        # ``implementedBy(object).__bases__ = (Interface,)``
        #
        # But: (1) That fails for old-style classes and (2) that causes
        # everything to appear to *explicitly* implement Interface, when up
        # to this point it's been an implicit virtual sort of relationship.
        #
        # So we force the issue by mutating the resolution order.

        # Note that we let C3 use pre-computed __sro__ for our bases.
        # This requires that by the time this method is invoked, our bases
        # have settled their SROs. Thus, ``changed()`` must first
        # update itself before telling its descendents of changes.
        sro = calculate_ro(self, base_mros={
            b: b.__sro__
            for b in self.__bases__
        })
        root = self._ROOT
        if root is not None and sro and sro[-1] is not root:
            # In one dataset of 1823 Interface objects, 1117 ClassProvides objects,
            # sro[-1] was root 4496 times, and only not root 118 times. So it's
            # probably worth checking.

            # Once we don't have to deal with old-style classes,
            # we can add a check and only do this if base_count > 1,
            # if we tweak the bootstrapping for ``<implementedBy object>``
            sro = [
                x
                for x in sro
                if x is not root
            ]
            sro.append(root)

        return sro

    def changed(self, originally_changed):
        """
        We, or something we depend on, have changed.

        By the time this is called, the things we depend on,
        such as our bases, should themselves be stable.
        """
        self._v_attrs = None

        implied = self._implied
        implied.clear()

        ancestors = self._calculate_sro()
        self.__sro__ = tuple(ancestors)
        self.__iro__ = tuple([ancestor for ancestor in ancestors
                              if isinstance(ancestor, InterfaceClass)
                              ])

        for ancestor in ancestors:
            # We directly imply our ancestors:
            implied[ancestor] = ()

        # Now, advise our dependents of change
        # (being careful not to create the WeakKeyDictionary if not needed):
        for dependent in tuple(self._dependents.keys() if self._dependents else ()):
            dependent.changed(originally_changed)

        # Just in case something called get() at some point
        # during that process and we have a cycle of some sort
        # make sure we didn't cache incomplete results.
        self._v_attrs = None

    def interfaces(self):
        """Return an iterator for the interfaces in the specification.
        """
        seen = {}
        for base in self.__bases__:
            for interface in base.interfaces():
                if interface not in seen:
                    seen[interface] = 1
                    yield interface

    def extends(self, interface, strict=True):
        """Does the specification extend the given interface?

        Test whether an interface in the specification extends the
        given interface
        """
        return ((interface in self._implied)
                and
                ((not strict) or (self != interface))
                )

    def weakref(self, callback=None):
        return weakref.ref(self, callback)

    def get(self, name, default=None):
        """Query for an attribute description
        """
        attrs = self._v_attrs
        if attrs is None:
            attrs = self._v_attrs = {}
        attr = attrs.get(name)
        if attr is None:
            for iface in self.__iro__:
                attr = iface.direct(name)
                if attr is not None:
                    attrs[name] = attr
                    break

        return default if attr is None else attr


class _InterfaceMetaClass(type):
    # Handling ``__module__`` on ``InterfaceClass`` is tricky. We need
    # to be able to read it on a type and get the expected string. We
    # also need to be able to set it on an instance and get the value
    # we set. So far so good. But what gets tricky is that we'd like
    # to store the value in the C structure (``InterfaceBase.__ibmodule__``) for
    # direct access during equality, sorting, and hashing. "No
    # problem, you think, I'll just use a property" (well, the C
    # equivalents, ``PyMemberDef`` or ``PyGetSetDef``).
    #
    # Except there is a problem. When a subclass is created, the
    # metaclass (``type``) always automatically puts the expected
    # string in the class's dictionary under ``__module__``, thus
    # overriding the property inherited from the superclass. Writing
    # ``Subclass.__module__`` still works, but
    # ``Subclass().__module__`` fails.
    #
    # There are multiple ways to work around this:
    #
    # (1) Define ``InterfaceBase.__getattribute__`` to watch for
    # ``__module__`` and return the C storage.
    #
    # This works, but slows down *all* attribute access (except,
    # ironically, to ``__module__``) by about 25% (40ns becomes 50ns)
    # (when implemented in C). Since that includes methods like
    # ``providedBy``, that's probably not acceptable.
    #
    # All the other methods involve modifying subclasses. This can be
    # done either on the fly in some cases, as instances are
    # constructed, or by using a metaclass. These next few can be done on the fly.
    #
    # (2) Make ``__module__`` a descriptor in each subclass dictionary.
    # It can't be a straight up ``@property`` descriptor, though, because accessing
    # it on the class returns a ``property`` object, not the desired string.
    #
    # (3) Implement a data descriptor (``__get__`` and ``__set__``)
    # that is both a subclass of string, and also does the redirect of
    # ``__module__`` to ``__ibmodule__`` and does the correct thing
    # with the ``instance`` argument to ``__get__`` is None (returns
    # the class's value.) (Why must it be a subclass of string? Because
    # when it' s in the class's dict, it's defined on an *instance* of the
    # metaclass; descriptors in an instance's dict aren't honored --- their
    # ``__get__`` is never invoked --- so it must also *be* the value we want
    # returned.)
    #
    # This works, preserves the ability to read and write
    # ``__module__``, and eliminates any penalty accessing other
    # attributes. But it slows down accessing ``__module__`` of
    # instances by 200% (40ns to 124ns), requires editing class dicts on the fly
    # (in InterfaceClass.__init__), thus slightly slowing down all interface creation,
    # and is ugly.
    #
    # (4) As in the last step, but make it a non-data descriptor (no ``__set__``).
    #
    # If you then *also* store a copy of ``__ibmodule__`` in
    # ``__module__`` in the instance's dict, reading works for both
    # class and instance and is full speed for instances. But the cost
    # is storage space, and you can't write to it anymore, not without
    # things getting out of sync.
    #
    # (Actually, ``__module__`` was never meant to be writable. Doing
    # so would break BTrees and normal dictionaries, as well as the
    # repr, maybe more.)
    #
    # That leaves us with a metaclass. (Recall that a class is an
    # instance of its metaclass, so properties/descriptors defined in
    # the metaclass are used when accessing attributes on the
    # instance/class. We'll use that to define ``__module__``.) Here
    # we can have our cake and eat it too: no extra storage, and
    # C-speed access to the underlying storage. The only substantial
    # cost is that metaclasses tend to make people's heads hurt. (But
    # still less than the descriptor-is-string, hopefully.)

    __slots__ = ()

    def __new__(cls, name, bases, attrs):
        # Figure out what module defined the interface.
        # This is copied from ``InterfaceClass.__init__``;
        # reviewers aren't sure how AttributeError or KeyError
        # could be raised.
        __module__ = sys._getframe(1).f_globals['__name__']
        # Get the C optimized __module__ accessor and give it
        # to the new class.
        moduledescr = InterfaceBase.__dict__['__module__']
        if isinstance(moduledescr, str):
            # We're working with the Python implementation,
            # not the C version
            moduledescr = InterfaceBase.__dict__['__module_property__']
        attrs['__module__'] = moduledescr
        kind = type.__new__(cls, name, bases, attrs)
        kind.__module = __module__
        return kind

    @property
    def __module__(cls):
        return cls.__module

    def __repr__(cls):
        return "<class '%s.%s'>" % (
            cls.__module,
            cls.__name__,
        )


_InterfaceClassBase = _InterfaceMetaClass(
    'InterfaceClass',
    # From least specific to most specific.
    (InterfaceBase, Specification, Element),
    {'__slots__': ()}
)


def interfacemethod(func):
    """
    Convert a method specification to an actual method of the interface.

    This is a decorator that functions like `staticmethod` et al.

    The primary use of this decorator is to allow interface definitions to
    define the ``__adapt__`` method, but other interface methods can be
    overridden this way too.

    .. seealso:: `interfaces.IInterfaceDeclaration.interfacemethod`
    """
    f_locals = sys._getframe(1).f_locals
    methods = f_locals.setdefault(INTERFACE_METHODS, {})
    methods[func.__name__] = func
    return _decorator_non_return


class InterfaceClass(_InterfaceClassBase):
    """
    Prototype (scarecrow) Interfaces Implementation.

    Note that it is not possible to change the ``__name__`` or ``__module__``
    after an instance of this object has been constructed.
    """

    # We can't say this yet because we don't have enough
    # infrastructure in place.
    #
    #implements(IInterface)

    def __new__(cls, name=None, bases=(), attrs=None, __doc__=None, # pylint:disable=redefined-builtin
                __module__=None):
        assert isinstance(bases, tuple)
        attrs = attrs or {}
        needs_custom_class = attrs.pop(INTERFACE_METHODS, None)
        if needs_custom_class:
            needs_custom_class.update(
                {'__classcell__': attrs.pop('__classcell__')}
                if '__classcell__' in attrs
                else {}
            )
            if '__adapt__' in needs_custom_class:
                # We need to tell the C code to call this.
                needs_custom_class['_CALL_CUSTOM_ADAPT'] = 1

            if issubclass(cls, _InterfaceClassWithCustomMethods):
                cls_bases = (cls,)
            elif cls is InterfaceClass:
                cls_bases = (_InterfaceClassWithCustomMethods,)
            else:
                cls_bases = (cls, _InterfaceClassWithCustomMethods)

            cls = type(cls)( # pylint:disable=self-cls-assignment
                name + "<WithCustomMethods>",
                cls_bases,
                needs_custom_class
            )
        elif PY2 and bases and len(bases) > 1:
            bases_with_custom_methods = tuple(
                type(b)
                for b in bases
                if issubclass(type(b), _InterfaceClassWithCustomMethods)
            )

            # If we have a subclass of InterfaceClass in *bases*,
            # Python 3 is smart enough to pass that as *cls*, but Python
            # 2 just passes whatever the first base in *bases* is. This means that if
            # we have multiple inheritance, and one of our bases has already defined
            # a custom method like ``__adapt__``, we do the right thing automatically
            # and extend it on Python 3, but not necessarily on Python 2. To fix this, we need
            # to run the MRO algorithm and get the most derived base manually.
            # Note that this only works for consistent resolution orders
            if bases_with_custom_methods:
                cls = type( # pylint:disable=self-cls-assignment
                    name + "<WithCustomMethods>",
                    bases_with_custom_methods,
                    {}
                ).__mro__[1] # Not the class we created, the most derived.

        return _InterfaceClassBase.__new__(cls)

    def __init__(self, name, bases=(), attrs=None, __doc__=None,  # pylint:disable=redefined-builtin
                 __module__=None):
        # We don't call our metaclass parent directly
        # pylint:disable=non-parent-init-called
        # pylint:disable=super-init-not-called
        if not all(isinstance(base, InterfaceClass) for base in bases):
            raise TypeError('Expected base interfaces')

        if attrs is None:
            attrs = {}

        if __module__ is None:
            __module__ = attrs.get('__module__')
            if isinstance(__module__, str):
                del attrs['__module__']
            else:
                try:
                    # Figure out what module defined the interface.
                    # This is how cPython figures out the module of
                    # a class, but of course it does it in C. :-/
                    __module__ = sys._getframe(1).f_globals['__name__']
                except (AttributeError, KeyError): # pragma: no cover
                    pass

        InterfaceBase.__init__(self, name, __module__)
        # These asserts assisted debugging the metaclass
        # assert '__module__' not in self.__dict__
        # assert self.__ibmodule__ is self.__module__ is __module__

        d = attrs.get('__doc__')
        if d is not None:
            if not isinstance(d, Attribute):
                if __doc__ is None:
                    __doc__ = d
                del attrs['__doc__']

        if __doc__ is None:
            __doc__ = ''

        Element.__init__(self, name, __doc__)

        tagged_data = attrs.pop(TAGGED_DATA, None)
        if tagged_data is not None:
            for key, val in tagged_data.items():
                self.setTaggedValue(key, val)

        Specification.__init__(self, bases)
        self.__attrs = self.__compute_attrs(attrs)
        self.__identifier__ = "%s.%s" % (__module__, name)

    def __compute_attrs(self, attrs):
        # Make sure that all recorded attributes (and methods) are of type
        # `Attribute` and `Method`
        def update_value(aname, aval):
            if isinstance(aval, Attribute):
                aval.interface = self
                if not aval.__name__:
                    aval.__name__ = aname
            elif isinstance(aval, FunctionType):
                aval = fromFunction(aval, self, name=aname)
            else:
                raise InvalidInterface("Concrete attribute, " + aname)
            return aval


        return {
            aname: update_value(aname, aval)
            for aname, aval in attrs.items()
            if aname not in (
                # __locals__: Python 3 sometimes adds this.
                '__locals__',
                # __qualname__: PEP 3155 (Python 3.3+)
                '__qualname__',
                # __annotations__: PEP 3107 (Python 3.0+)
                '__annotations__',
            )
            and aval is not _decorator_non_return
        }

    def interfaces(self):
        """Return an iterator for the interfaces in the specification.
        """
        yield self

    def getBases(self):
        return self.__bases__

    def isEqualOrExtendedBy(self, other):
        """Same interface or extends?"""
        return self == other or other.extends(self)

    def names(self, all=False): # pylint:disable=redefined-builtin
        """Return the attribute names defined by the interface."""
        if not all:
            return self.__attrs.keys()

        r = self.__attrs.copy()
        for base in self.__bases__:
            r.update(dict.fromkeys(base.names(all)))

        return r.keys()

    def __iter__(self):
        return iter(self.names(all=True))

    def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin
        """Return attribute names and descriptions defined by interface."""
        if not all:
            return self.__attrs.items()

        r = {}
        for base in self.__bases__[::-1]:
            r.update(dict(base.namesAndDescriptions(all)))

        r.update(self.__attrs)

        return r.items()

    def getDescriptionFor(self, name):
        """Return the attribute description for the given name."""
        r = self.get(name)
        if r is not None:
            return r

        raise KeyError(name)

    __getitem__ = getDescriptionFor

    def __contains__(self, name):
        return self.get(name) is not None

    def direct(self, name):
        return self.__attrs.get(name)

    def queryDescriptionFor(self, name, default=None):
        return self.get(name, default)

    def validateInvariants(self, obj, errors=None):
        """validate object to defined invariants."""

        for iface in self.__iro__:
            for invariant in iface.queryDirectTaggedValue('invariants', ()):
                try:
                    invariant(obj)
                except Invalid as error:
                     if errors is not None:
                         errors.append(error)
                     else:
                         raise

        if errors:
            raise Invalid(errors)

    def queryTaggedValue(self, tag, default=None):
        """
        Queries for the value associated with *tag*, returning it from the nearest
        interface in the ``__iro__``.

        If not found, returns *default*.
        """
        for iface in self.__iro__:
            value = iface.queryDirectTaggedValue(tag, _marker)
            if value is not _marker:
                return value
        return default

    def getTaggedValue(self, tag):
        """ Returns the value associated with 'tag'. """
        value = self.queryTaggedValue(tag, default=_marker)
        if value is _marker:
            raise KeyError(tag)
        return value

    def getTaggedValueTags(self):
        """ Returns a list of all tags. """
        keys = set()
        for base in self.__iro__:
            keys.update(base.getDirectTaggedValueTags())
        return keys

    def __repr__(self):  # pragma: no cover
        try:
            return self._v_repr
        except AttributeError:
            name = self.__name__
            m = self.__ibmodule__
            if m:
                name = '%s.%s' % (m, name)
            r = "<%s %s>" % (self.__class__.__name__, name)
            self._v_repr = r # pylint:disable=attribute-defined-outside-init
            return r

    def _call_conform(self, conform):
        try:
            return conform(self)
        except TypeError: # pragma: no cover
            # We got a TypeError. It might be an error raised by
            # the __conform__ implementation, or *we* may have
            # made the TypeError by calling an unbound method
            # (object is a class).  In the later case, we behave
            # as though there is no __conform__ method. We can
            # detect this case by checking whether there is more
            # than one traceback object in the traceback chain:
            if sys.exc_info()[2].tb_next is not None:
                # There is more than one entry in the chain, so
                # reraise the error:
                raise
            # This clever trick is from Phillip Eby

        return None # pragma: no cover

    def __reduce__(self):
        return self.__name__

Interface = InterfaceClass("Interface", __module__='zope.interface')
# Interface is the only member of its own SRO.
Interface._calculate_sro = lambda: (Interface,)
Interface.changed(Interface)
assert Interface.__sro__ == (Interface,)
Specification._ROOT = Interface
ro._ROOT = Interface

class _InterfaceClassWithCustomMethods(InterfaceClass):
    """
    Marker class for interfaces with custom methods that override InterfaceClass methods.
    """


class Attribute(Element):
    """Attribute descriptions
    """

    # We can't say this yet because we don't have enough
    # infrastructure in place.
    #
    # implements(IAttribute)

    interface = None

    def _get_str_info(self):
        """Return extra data to put at the end of __str__."""
        return ""

    def __str__(self):
        of = ''
        if self.interface is not None:
            of = self.interface.__module__ + '.' + self.interface.__name__ + '.'
        # self.__name__ may be None during construction (e.g., debugging)
        return of + (self.__name__ or '<unknown>') + self._get_str_info()

    def __repr__(self):
        return "<%s.%s object at 0x%x %s>" % (
            type(self).__module__,
            type(self).__name__,
            id(self),
            self
        )


class Method(Attribute):
    """Method interfaces

    The idea here is that you have objects that describe methods.
    This provides an opportunity for rich meta-data.
    """

    # We can't say this yet because we don't have enough
    # infrastructure in place.
    #
    # implements(IMethod)

    positional = required = ()
    _optional = varargs = kwargs = None
    def _get_optional(self):
        if self._optional is None:
            return {}
        return self._optional
    def _set_optional(self, opt):
        self._optional = opt
    def _del_optional(self):
        self._optional = None
    optional = property(_get_optional, _set_optional, _del_optional)

    def __call__(self, *args, **kw):
        raise BrokenImplementation(self.interface, self.__name__)

    def getSignatureInfo(self):
        return {'positional': self.positional,
                'required': self.required,
                'optional': self.optional,
                'varargs': self.varargs,
                'kwargs': self.kwargs,
                }

    def getSignatureString(self):
        sig = []
        for v in self.positional:
            sig.append(v)
            if v in self.optional.keys():
                sig[-1] += "=" + repr(self.optional[v])
        if self.varargs:
            sig.append("*" + self.varargs)
        if self.kwargs:
            sig.append("**" + self.kwargs)

        return "(%s)" % ", ".join(sig)

    _get_str_info = getSignatureString


def fromFunction(func, interface=None, imlevel=0, name=None):
    name = name or func.__name__
    method = Method(name, func.__doc__)
    defaults = getattr(func, '__defaults__', None) or ()
    code = func.__code__
    # Number of positional arguments
    na = code.co_argcount - imlevel
    names = code.co_varnames[imlevel:]
    opt = {}
    # Number of required arguments
    defaults_count = len(defaults)
    if not defaults_count:
        # PyPy3 uses ``__defaults_count__`` for builtin methods
        # like ``dict.pop``. Surprisingly, these don't have recorded
        # ``__defaults__``
        defaults_count = getattr(func, '__defaults_count__', 0)

    nr = na - defaults_count
    if nr < 0:
        defaults = defaults[-nr:]
        nr = 0

    # Determine the optional arguments.
    opt.update(dict(zip(names[nr:], defaults)))

    method.positional = names[:na]
    method.required = names[:nr]
    method.optional = opt

    argno = na

    # Determine the function's variable argument's name (i.e. *args)
    if code.co_flags & CO_VARARGS:
        method.varargs = names[argno]
        argno = argno + 1
    else:
        method.varargs = None

    # Determine the function's keyword argument's name (i.e. **kw)
    if code.co_flags & CO_VARKEYWORDS:
        method.kwargs = names[argno]
    else:
        method.kwargs = None

    method.interface = interface

    for key, value in func.__dict__.items():
        method.setTaggedValue(key, value)

    return method


def fromMethod(meth, interface=None, name=None):
    if isinstance(meth, MethodType):
        func = meth.__func__
    else:
        func = meth
    return fromFunction(func, interface, imlevel=1, name=name)


# Now we can create the interesting interfaces and wire them up:
def _wire():
    from zope.interface.declarations import classImplements
    # From lest specific to most specific.
    from zope.interface.interfaces import IElement
    classImplements(Element, IElement)

    from zope.interface.interfaces import IAttribute
    classImplements(Attribute, IAttribute)

    from zope.interface.interfaces import IMethod
    classImplements(Method, IMethod)

    from zope.interface.interfaces import ISpecification
    classImplements(Specification, ISpecification)

    from zope.interface.interfaces import IInterface
    classImplements(InterfaceClass, IInterface)


# We import this here to deal with module dependencies.
# pylint:disable=wrong-import-position
from zope.interface.declarations import implementedBy
from zope.interface.declarations import providedBy
from zope.interface.exceptions import InvalidInterface
from zope.interface.exceptions import BrokenImplementation

# This ensures that ``Interface`` winds up in the flattened()
# list of the immutable declaration. It correctly overrides changed()
# as a no-op, so we bypass that.
from zope.interface.declarations import _empty
Specification.changed(_empty, _empty)

if __name__ == "__main__":
    arrs={"__locals__":"a","__qualname__":"b","__annotations__":"c","notin":Attribute("name1")}
    arrs_new = {"__locals__": "a", "__qualname__": "b", "__annotations__": "c", "ininin": Attribute("name1")}
    isT = True
    try:
        IFoo = InterfaceClass('IFoo',attrs=arrs)
        IBar = InterfaceClass('IBar', (IFoo,),attrs=arrs)
        IFoo.__bases__=[InterfaceClass('IFoo1',attrs=arrs_new)]
        res1=list(IFoo.names())==['notin']
        res2=list(IBar.names())==['notin']
        res3=list(IFoo.names(True))==['notin', 'ininin']
        res4=list(IBar.names(True))==['notin', 'ininin']
        if not res1 or not res2 or not res3 or not res4:
            isT=False
    except:
        isT=False
    # self.assertEqual(sorted(result), ['', 'named'])
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b416eb7e40a82d2d1129/"):
    #     f = open("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b416eb7e40a82d2d1129/" + l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = InterfaceClass("")
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.names(args1)
    #     if not ( dill.dumps(list(res0))== dill.dumps(list(content["output"][0]))):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/declarations__normalizeargs_passk_validte.py
##############################################################################
# Copyright (c) 2003 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
##############################################################################
"""Implementation of interface declarations

There are three flavors of declarations:

  - Declarations are used to simply name declared interfaces.

  - ImplementsDeclarations are used to express the interfaces that a
    class implements (that instances of the class provides).

    Implements specifications support inheriting interfaces.

  - ProvidesDeclarations are used to express interfaces directly
    provided by objects.

"""
__docformat__ = 'restructuredtext'

from types import FunctionType
from types import MethodType
from types import ModuleType
import weakref
import sys
sys.path.append("/home/travis/builds/repos/pexip---os-zope")

from src.zope.interface.advice import addClassAdvisor
from zope.interface.interface import Interface
from zope.interface.interface import InterfaceClass
from zope.interface.interface import SpecificationBase
from zope.interface.interface import Specification
from zope.interface.interface import NameAndModuleComparisonMixin
from zope.interface._compat import CLASS_TYPES as DescriptorAwareMetaClasses
from zope.interface._compat import PYTHON3
from zope.interface._compat import _use_c_impl

__all__ = [
    # None. The public APIs of this module are
    # re-exported from zope.interface directly.
]

# pylint:disable=too-many-lines

# Registry of class-implementation specifications
BuiltinImplementationSpecifications = {}

_ADVICE_ERROR = ('Class advice impossible in Python3.  '
                 'Use the @%s class decorator instead.')

_ADVICE_WARNING = ('The %s API is deprecated, and will not work in Python3  '
                   'Use the @%s class decorator instead.')

def _next_super_class(ob):
    # When ``ob`` is an instance of ``super``, return
    # the next class in the MRO that we should actually be
    # looking at. Watch out for diamond inheritance!
    self_class = ob.__self_class__
    class_that_invoked_super = ob.__thisclass__
    complete_mro = self_class.__mro__
    next_class = complete_mro[complete_mro.index(class_that_invoked_super) + 1]
    return next_class

class named(object):

    def __init__(self, name):
        self.name = name

    def __call__(self, ob):
        ob.__component_name__ = self.name
        return ob


class Declaration(Specification):
    """Interface declarations"""

    __slots__ = ()

    def __init__(self, *bases):
        Specification.__init__(self, _normalizeargs(bases))

    def __contains__(self, interface):
        """Test whether an interface is in the specification
        """

        return self.extends(interface) and interface in self.interfaces()

    def __iter__(self):
        """Return an iterator for the interfaces in the specification
        """
        return self.interfaces()

    def flattened(self):
        """Return an iterator of all included and extended interfaces
        """
        return iter(self.__iro__)

    def __sub__(self, other):
        """Remove interfaces from a specification
        """
        return Declaration(*[
            i for i in self.interfaces()
            if not [
                j
                for j in other.interfaces()
                if i.extends(j, 0) # non-strict extends
            ]
        ])

    def __add__(self, other):
        """Add two specifications or a specification and an interface
        """
        seen = {}
        result = []
        for i in self.interfaces():
            seen[i] = 1
            result.append(i)
        for i in other.interfaces():
            if i not in seen:
                seen[i] = 1
                result.append(i)

        return Declaration(*result)

    __radd__ = __add__


class _ImmutableDeclaration(Declaration):
    # A Declaration that is immutable. Used as a singleton to
    # return empty answers for things like ``implementedBy``.
    # We have to define the actual singleton after normalizeargs
    # is defined, and that in turn is defined after InterfaceClass and
    # Implements.

    __slots__ = ()

    __instance = None

    def __new__(cls):
        if _ImmutableDeclaration.__instance is None:
            _ImmutableDeclaration.__instance = object.__new__(cls)
        return _ImmutableDeclaration.__instance

    def __reduce__(self):
        return "_empty"

    @property
    def __bases__(self):
        return ()

    @__bases__.setter
    def __bases__(self, new_bases):
        # We expect the superclass constructor to set ``self.__bases__ = ()``.
        # Rather than attempt to special case that in the constructor and allow
        # setting __bases__ only at that time, it's easier to just allow setting
        # the empty tuple at any time. That makes ``x.__bases__ = x.__bases__`` a nice
        # no-op too. (Skipping the superclass constructor altogether is a recipe
        # for maintenance headaches.)
        if new_bases != ():
            raise TypeError("Cannot set non-empty bases on shared empty Declaration.")

    # As the immutable empty declaration, we cannot be changed.
    # This means there's no logical reason for us to have dependents
    # or subscriptions: we'll never notify them. So there's no need for
    # us to keep track of any of that.
    @property
    def dependents(self):
        return {}

    changed = subscribe = unsubscribe = lambda self, _ignored: None

    def interfaces(self):
        # An empty iterator
        return iter(())

    def extends(self, interface, strict=True):
        return interface is self._ROOT

    def get(self, name, default=None):
        return default

    def weakref(self, callback=None):
        # We're a singleton, we never go away. So there's no need to return
        # distinct weakref objects here; their callbacks will never
        # be called. Instead, we only need to return a callable that
        # returns ourself. The easiest one is to return _ImmutableDeclaration
        # itself; testing on Python 3.8 shows that's faster than a function that
        # returns _empty. (Remember, one goal is to avoid allocating any
        # object, and that includes a method.)
        return _ImmutableDeclaration

    @property
    def _v_attrs(self):
        # _v_attrs is not a public, documented property, but some client
        # code uses it anyway as a convenient place to cache things. To keep
        # the empty declaration truly immutable, we must ignore that. That includes
        # ignoring assignments as well.
        return {}

    @_v_attrs.setter
    def _v_attrs(self, new_attrs):
        pass


##############################################################################
#
# Implementation specifications
#
# These specify interfaces implemented by instances of classes

class Implements(NameAndModuleComparisonMixin,
                 Declaration):
    # Inherit from NameAndModuleComparisonMixin to be
    # mutually comparable with InterfaceClass objects.
    # (The two must be mutually comparable to be able to work in e.g., BTrees.)
    # Instances of this class generally don't have a __module__ other than
    # `zope.interface.declarations`, whereas they *do* have a __name__ that is the
    # fully qualified name of the object they are representing.

    # Note, though, that equality and hashing are still identity based. This
    # accounts for things like nested objects that have the same name (typically
    # only in tests) and is consistent with pickling. As far as comparisons to InterfaceClass
    # goes, we'll never have equal name and module to those, so we're still consistent there.
    # Instances of this class are essentially intended to be unique and are
    # heavily cached (note how our __reduce__ handles this) so having identity
    # based hash and eq should also work.

    # We want equality and hashing to be based on identity. However, we can't actually
    # implement __eq__/__ne__ to do this because sometimes we get wrapped in a proxy.
    # We need to let the proxy types implement these methods so they can handle unwrapping
    # and then rely on: (1) the interpreter automatically changing `implements == proxy` into
    # `proxy == implements` (which will call proxy.__eq__ to do the unwrapping) and then
    # (2) the default equality and hashing semantics being identity based.

    # class whose specification should be used as additional base
    inherit = None

    # interfaces actually declared for a class
    declared = ()

    # Weak cache of {class: <implements>} for super objects.
    # Created on demand. These are rare, as of 5.0 anyway. Using a class
    # level default doesn't take space in instances. Using _v_attrs would be
    # another place to store this without taking space unless needed.
    _super_cache = None

    __name__ = '?'

    @classmethod
    def named(cls, name, *bases):
        # Implementation method: Produce an Implements interface with
        # a fully fleshed out __name__ before calling the constructor, which
        # sets bases to the given interfaces and which may pass this object to
        # other objects (e.g., to adjust dependents). If they're sorting or comparing
        # by name, this needs to be set.
        inst = cls.__new__(cls)
        inst.__name__ = name
        inst.__init__(*bases)
        return inst

    def changed(self, originally_changed):
        try:
            del self._super_cache
        except AttributeError:
            pass
        return super(Implements, self).changed(originally_changed)

    def __repr__(self):
        return '<implementedBy %s>' % (self.__name__)

    def __reduce__(self):
        return implementedBy, (self.inherit, )


def _implements_name(ob):
    # Return the __name__ attribute to be used by its __implemented__
    # property.
    # This must be stable for the "same" object across processes
    # because it is used for sorting. It needn't be unique, though, in cases
    # like nested classes named Foo created by different functions, because
    # equality and hashing is still based on identity.
    # It might be nice to use __qualname__ on Python 3, but that would produce
    # different values between Py2 and Py3.
    return (getattr(ob, '__module__', '?') or '?') + \
        '.' + (getattr(ob, '__name__', '?') or '?')


def _implementedBy_super(sup):
    # TODO: This is now simple enough we could probably implement
    # in C if needed.

    # If the class MRO is strictly linear, we could just
    # follow the normal algorithm for the next class in the
    # search order (e.g., just return
    # ``implemented_by_next``). But when diamond inheritance
    # or mixins + interface declarations are present, we have
    # to consider the whole MRO and compute a new Implements
    # that excludes the classes being skipped over but
    # includes everything else.
    implemented_by_self = implementedBy(sup.__self_class__)
    cache = implemented_by_self._super_cache # pylint:disable=protected-access
    if cache is None:
        cache = implemented_by_self._super_cache = weakref.WeakKeyDictionary()

    key = sup.__thisclass__
    try:
        return cache[key]
    except KeyError:
        pass

    next_cls = _next_super_class(sup)
    # For ``implementedBy(cls)``:
    # .__bases__ is .declared + [implementedBy(b) for b in cls.__bases__]
    # .inherit is cls

    implemented_by_next = implementedBy(next_cls)
    mro = sup.__self_class__.__mro__
    ix_next_cls = mro.index(next_cls)
    classes_to_keep = mro[ix_next_cls:]
    new_bases = [implementedBy(c) for c in classes_to_keep]

    new = Implements.named(
        implemented_by_self.__name__ + ':' + implemented_by_next.__name__,
        *new_bases
    )
    new.inherit = implemented_by_next.inherit
    new.declared = implemented_by_next.declared
    # I don't *think* that new needs to subscribe to ``implemented_by_self``;
    # it auto-subscribed to its bases, and that should be good enough.
    cache[key] = new

    return new


@_use_c_impl
def implementedBy(cls): # pylint:disable=too-many-return-statements,too-many-branches
    """Return the interfaces implemented for a class' instances

      The value returned is an `~zope.interface.interfaces.IDeclaration`.
    """
    try:
        if isinstance(cls, super):
            # Yes, this needs to be inside the try: block. Some objects
            # like security proxies even break isinstance.
            return _implementedBy_super(cls)

        spec = cls.__dict__.get('__implemented__')
    except AttributeError:

        # we can't get the class dict. This is probably due to a
        # security proxy.  If this is the case, then probably no
        # descriptor was installed for the class.

        # We don't want to depend directly on zope.security in
        # zope.interface, but we'll try to make reasonable
        # accommodations in an indirect way.

        # We'll check to see if there's an implements:

        spec = getattr(cls, '__implemented__', None)
        if spec is None:
            # There's no spec stred in the class. Maybe its a builtin:
            spec = BuiltinImplementationSpecifications.get(cls)
            if spec is not None:
                return spec
            return _empty

        if spec.__class__ == Implements:
            # we defaulted to _empty or there was a spec. Good enough.
            # Return it.
            return spec

        # TODO: need old style __implements__ compatibility?
        # Hm, there's an __implemented__, but it's not a spec. Must be
        # an old-style declaration. Just compute a spec for it
        return Declaration(*_normalizeargs((spec, )))

    if isinstance(spec, Implements):
        return spec

    if spec is None:
        spec = BuiltinImplementationSpecifications.get(cls)
        if spec is not None:
            return spec

    # TODO: need old style __implements__ compatibility?
    spec_name = _implements_name(cls)
    if spec is not None:
        # old-style __implemented__ = foo declaration
        spec = (spec, ) # tuplefy, as it might be just an int
        spec = Implements.named(spec_name, *_normalizeargs(spec))
        spec.inherit = None    # old-style implies no inherit
        del cls.__implemented__ # get rid of the old-style declaration
    else:
        try:
            bases = cls.__bases__
        except AttributeError:
            if not callable(cls):
                raise TypeError("ImplementedBy called for non-factory", cls)
            bases = ()

        spec = Implements.named(spec_name, *[implementedBy(c) for c in bases])
        spec.inherit = cls

    try:
        cls.__implemented__ = spec
        if not hasattr(cls, '__providedBy__'):
            cls.__providedBy__ = objectSpecificationDescriptor

        if (isinstance(cls, DescriptorAwareMetaClasses)
                and '__provides__' not in cls.__dict__):
            # Make sure we get a __provides__ descriptor
            cls.__provides__ = ClassProvides(
                cls,
                getattr(cls, '__class__', type(cls)),
                )

    except TypeError:
        if not isinstance(cls, type):
            raise TypeError("ImplementedBy called for non-type", cls)
        BuiltinImplementationSpecifications[cls] = spec

    return spec


def classImplementsOnly(cls, *interfaces):
    """
    Declare the only interfaces implemented by instances of a class

    The arguments after the class are one or more interfaces or interface
    specifications (`~zope.interface.interfaces.IDeclaration` objects).

    The interfaces given (including the interfaces in the specifications)
    replace any previous declarations, *including* inherited definitions. If you
    wish to preserve inherited declarations, you can pass ``implementedBy(cls)``
    in *interfaces*. This can be used to alter the interface resolution order.
    """
    spec = implementedBy(cls)
    # Clear out everything inherited. It's important to
    # also clear the bases right now so that we don't improperly discard
    # interfaces that are already implemented by *old* bases that we're
    # about to get rid of.
    spec.declared = ()
    spec.inherit = None
    spec.__bases__ = ()
    _classImplements_ordered(spec, interfaces, ())


def classImplements(cls, *interfaces):
    """
    Declare additional interfaces implemented for instances of a class

    The arguments after the class are one or more interfaces or
    interface specifications (`~zope.interface.interfaces.IDeclaration` objects).

    The interfaces given (including the interfaces in the specifications)
    are added to any interfaces previously declared. An effort is made to
    keep a consistent C3 resolution order, but this cannot be guaranteed.

    .. versionchanged:: 5.0.0
       Each individual interface in *interfaces* may be added to either the
       beginning or end of the list of interfaces declared for *cls*,
       based on inheritance, in order to try to maintain a consistent
       resolution order. Previously, all interfaces were added to the end.
    .. versionchanged:: 5.1.0
       If *cls* is already declared to implement an interface (or derived interface)
       in *interfaces* through inheritance, the interface is ignored. Previously, it
       would redundantly be made direct base of *cls*, which often produced inconsistent
       interface resolution orders. Now, the order will be consistent, but may change.
       Also, if the ``__bases__`` of the *cls* are later changed, the *cls* will no
       longer be considered to implement such an interface (changing the ``__bases__`` of *cls*
       has never been supported).
    """
    spec = implementedBy(cls)
    interfaces = tuple(_normalizeargs(interfaces))

    before = []
    after = []

    # Take steps to try to avoid producing an invalid resolution
    # order, while still allowing for BWC (in the past, we always
    # appended)
    for iface in interfaces:
        for b in spec.declared:
            if iface.extends(b):
                before.append(iface)
                break
        else:
            after.append(iface)
    _classImplements_ordered(spec, tuple(before), tuple(after))


def classImplementsFirst(cls, iface):
    """
    Declare that instances of *cls* additionally provide *iface*.

    The second argument is an interface or interface specification.
    It is added as the highest priority (first in the IRO) interface;
    no attempt is made to keep a consistent resolution order.

    .. versionadded:: 5.0.0
    """
    spec = implementedBy(cls)
    _classImplements_ordered(spec, (iface,), ())


def _classImplements_ordered(spec, before=(), after=()):
    # Elide everything already inherited.
    # Except, if it is the root, and we don't already declare anything else
    # that would imply it, allow the root through. (TODO: When we disallow non-strict
    # IRO, this part of the check can be removed because it's not possible to re-declare
    # like that.)
    before = [
        x
        for x in before
        if not spec.isOrExtends(x) or (x is Interface and not spec.declared)
    ]
    after = [
        x
        for x in after
        if not spec.isOrExtends(x) or (x is Interface and not spec.declared)
    ]

    # eliminate duplicates
    new_declared = []
    seen = set()
    for l in before, spec.declared, after:
        for b in l:
            if b not in seen:
                new_declared.append(b)
                seen.add(b)

    spec.declared = tuple(new_declared)

    # compute the bases
    bases = new_declared # guaranteed no dupes

    if spec.inherit is not None:
        for c in spec.inherit.__bases__:
            b = implementedBy(c)
            if b not in seen:
                seen.add(b)
                bases.append(b)

    spec.__bases__ = tuple(bases)


def _implements_advice(cls):
    interfaces, do_classImplements = cls.__dict__['__implements_advice_data__']
    del cls.__implements_advice_data__
    do_classImplements(cls, *interfaces)
    return cls


class implementer(object):
    """
    Declare the interfaces implemented by instances of a class.

    This function is called as a class decorator.

    The arguments are one or more interfaces or interface
    specifications (`~zope.interface.interfaces.IDeclaration`
    objects).

    The interfaces given (including the interfaces in the
    specifications) are added to any interfaces previously declared,
    unless the interface is already implemented.

    Previous declarations include declarations for base classes unless
    implementsOnly was used.

    This function is provided for convenience. It provides a more
    convenient way to call `classImplements`. For example::

        @implementer(I1)
        class C(object):
            pass

    is equivalent to calling::

        classImplements(C, I1)

    after the class has been created.

    .. seealso:: `classImplements`
       The change history provided there applies to this function too.
    """
    __slots__ = ('interfaces',)

    def __init__(self, *interfaces):
        self.interfaces = interfaces

    def __call__(self, ob):
        if isinstance(ob, DescriptorAwareMetaClasses):
            # This is the common branch for new-style (object) and
            # on Python 2 old-style classes.
            classImplements(ob, *self.interfaces)
            return ob

        spec_name = _implements_name(ob)
        spec = Implements.named(spec_name, *self.interfaces)
        try:
            ob.__implemented__ = spec
        except AttributeError:
            raise TypeError("Can't declare implements", ob)
        return ob

class implementer_only(object):
    """Declare the only interfaces implemented by instances of a class

      This function is called as a class decorator.

      The arguments are one or more interfaces or interface
      specifications (`~zope.interface.interfaces.IDeclaration` objects).

      Previous declarations including declarations for base classes
      are overridden.

      This function is provided for convenience. It provides a more
      convenient way to call `classImplementsOnly`. For example::

        @implementer_only(I1)
        class C(object): pass

      is equivalent to calling::

        classImplementsOnly(I1)

      after the class has been created.
      """

    def __init__(self, *interfaces):
        self.interfaces = interfaces

    def __call__(self, ob):
        if isinstance(ob, (FunctionType, MethodType)):
            # XXX Does this decorator make sense for anything but classes?
            # I don't think so. There can be no inheritance of interfaces
            # on a method or function....
            raise ValueError('The implementer_only decorator is not '
                             'supported for methods or functions.')

        # Assume it's a class:
        classImplementsOnly(ob, *self.interfaces)
        return ob

def _implements(name, interfaces, do_classImplements):
    # This entire approach is invalid under Py3K.  Don't even try to fix
    # the coverage for this block there. :(
    frame = sys._getframe(2) # pylint:disable=protected-access
    locals = frame.f_locals # pylint:disable=redefined-builtin

    # Try to make sure we were called from a class def. In 2.2.0 we can't
    # check for __module__ since it doesn't seem to be added to the locals
    # until later on.
    if locals is frame.f_globals or '__module__' not in locals:
        raise TypeError(name+" can be used only from a class definition.")

    if '__implements_advice_data__' in locals:
        raise TypeError(name+" can be used only once in a class definition.")

    locals['__implements_advice_data__'] = interfaces, do_classImplements
    addClassAdvisor(_implements_advice, depth=3)

def implements(*interfaces):
    """
    Declare interfaces implemented by instances of a class.

    .. deprecated:: 5.0
        This only works for Python 2. The `implementer` decorator
        is preferred for all versions.

    This function is called in a class definition.

    The arguments are one or more interfaces or interface
    specifications (`~zope.interface.interfaces.IDeclaration`
    objects).

    The interfaces given (including the interfaces in the
    specifications) are added to any interfaces previously declared.

    Previous declarations include declarations for base classes unless
    `implementsOnly` was used.

    This function is provided for convenience. It provides a more
    convenient way to call `classImplements`. For example::

        implements(I1)

    is equivalent to calling::

        classImplements(C, I1)

    after the class has been created.
    """
    # This entire approach is invalid under Py3K.  Don't even try to fix
    # the coverage for this block there. :(
    if PYTHON3:
        raise TypeError(_ADVICE_ERROR % 'implementer')
    _implements("implements", interfaces, classImplements)

def implementsOnly(*interfaces):
    """Declare the only interfaces implemented by instances of a class

      This function is called in a class definition.

      The arguments are one or more interfaces or interface
      specifications (`~zope.interface.interfaces.IDeclaration` objects).

      Previous declarations including declarations for base classes
      are overridden.

      This function is provided for convenience. It provides a more
      convenient way to call `classImplementsOnly`. For example::

        implementsOnly(I1)

      is equivalent to calling::

        classImplementsOnly(I1)

      after the class has been created.
    """
    # This entire approach is invalid under Py3K.  Don't even try to fix
    # the coverage for this block there. :(
    if PYTHON3:
        raise TypeError(_ADVICE_ERROR % 'implementer_only')
    _implements("implementsOnly", interfaces, classImplementsOnly)

##############################################################################
#
# Instance declarations

class Provides(Declaration):  # Really named ProvidesClass
    """Implement ``__provides__``, the instance-specific specification

    When an object is pickled, we pickle the interfaces that it implements.
    """

    def __init__(self, cls, *interfaces):
        self.__args = (cls, ) + interfaces
        self._cls = cls
        Declaration.__init__(self, *(interfaces + (implementedBy(cls), )))

    def __repr__(self):
        return "<%s.%s for %s>" % (
            self.__class__.__module__,
            self.__class__.__name__,
            self._cls,
        )

    def __reduce__(self):
        return Provides, self.__args

    __module__ = 'zope.interface'

    def __get__(self, inst, cls):
        """Make sure that a class __provides__ doesn't leak to an instance
        """
        if inst is None and cls is self._cls:
            # We were accessed through a class, so we are the class'
            # provides spec. Just return this object, but only if we are
            # being called on the same class that we were defined for:
            return self

        raise AttributeError('__provides__')

ProvidesClass = Provides

# Registry of instance declarations
# This is a memory optimization to allow objects to share specifications.
InstanceDeclarations = weakref.WeakValueDictionary()

def Provides(*interfaces): # pylint:disable=function-redefined
    """Cache instance declarations

      Instance declarations are shared among instances that have the same
      declaration. The declarations are cached in a weak value dictionary.
    """
    spec = InstanceDeclarations.get(interfaces)
    if spec is None:
        spec = ProvidesClass(*interfaces)
        InstanceDeclarations[interfaces] = spec

    return spec

Provides.__safe_for_unpickling__ = True


def directlyProvides(object, *interfaces): # pylint:disable=redefined-builtin
    """Declare interfaces declared directly for an object

      The arguments after the object are one or more interfaces or interface
      specifications (`~zope.interface.interfaces.IDeclaration` objects).

      The interfaces given (including the interfaces in the specifications)
      replace interfaces previously declared for the object.
    """
    cls = getattr(object, '__class__', None)
    if cls is not None and getattr(cls, '__class__', None) is cls:
        # It's a meta class (well, at least it it could be an extension class)
        # Note that we can't get here from Py3k tests:  there is no normal
        # class which isn't descriptor aware.
        if not isinstance(object,
                          DescriptorAwareMetaClasses):
            raise TypeError("Attempt to make an interface declaration on a "
                            "non-descriptor-aware class")

    interfaces = _normalizeargs(interfaces)
    if cls is None:
        cls = type(object)

    issub = False
    for damc in DescriptorAwareMetaClasses:
        if issubclass(cls, damc):
            issub = True
            break
    if issub:
        # we have a class or type.  We'll use a special descriptor
        # that provides some extra caching
        object.__provides__ = ClassProvides(object, cls, *interfaces)
    else:
        object.__provides__ = Provides(cls, *interfaces)


def alsoProvides(object, *interfaces): # pylint:disable=redefined-builtin
    """Declare interfaces declared directly for an object

    The arguments after the object are one or more interfaces or interface
    specifications (`~zope.interface.interfaces.IDeclaration` objects).

    The interfaces given (including the interfaces in the specifications) are
    added to the interfaces previously declared for the object.
    """
    directlyProvides(object, directlyProvidedBy(object), *interfaces)


def noLongerProvides(object, interface): # pylint:disable=redefined-builtin
    """ Removes a directly provided interface from an object.
    """
    directlyProvides(object, directlyProvidedBy(object) - interface)
    if interface.providedBy(object):
        raise ValueError("Can only remove directly provided interfaces.")


@_use_c_impl
class ClassProvidesBase(SpecificationBase):

    __slots__ = (
        '_cls',
        '_implements',
    )

    def __get__(self, inst, cls):
        # member slots are set by subclass
        # pylint:disable=no-member
        if cls is self._cls:
            # We only work if called on the class we were defined for

            if inst is None:
                # We were accessed through a class, so we are the class'
                # provides spec. Just return this object as is:
                return self

            return self._implements

        raise AttributeError('__provides__')


class ClassProvides(Declaration, ClassProvidesBase):
    """Special descriptor for class ``__provides__``

    The descriptor caches the implementedBy info, so that
    we can get declarations for objects without instance-specific
    interfaces a bit quicker.
    """

    __slots__ = (
        '__args',
    )

    def __init__(self, cls, metacls, *interfaces):
        self._cls = cls
        self._implements = implementedBy(cls)
        self.__args = (cls, metacls, ) + interfaces
        Declaration.__init__(self, *(interfaces + (implementedBy(metacls), )))

    def __repr__(self):
        return "<%s.%s for %s>" % (
            self.__class__.__module__,
            self.__class__.__name__,
            self._cls,
        )

    def __reduce__(self):
        return self.__class__, self.__args

    # Copy base-class method for speed
    __get__ = ClassProvidesBase.__get__


def directlyProvidedBy(object): # pylint:disable=redefined-builtin
    """Return the interfaces directly provided by the given object

    The value returned is an `~zope.interface.interfaces.IDeclaration`.
    """
    provides = getattr(object, "__provides__", None)
    if (
            provides is None # no spec
            # We might have gotten the implements spec, as an
            # optimization. If so, it's like having only one base, that we
            # lop off to exclude class-supplied declarations:
            or isinstance(provides, Implements)
    ):
        return _empty

    # Strip off the class part of the spec:
    return Declaration(provides.__bases__[:-1])


def classProvides(*interfaces):
    """Declare interfaces provided directly by a class

      This function is called in a class definition.

      The arguments are one or more interfaces or interface specifications
      (`~zope.interface.interfaces.IDeclaration` objects).

      The given interfaces (including the interfaces in the specifications)
      are used to create the class's direct-object interface specification.
      An error will be raised if the module class has an direct interface
      specification. In other words, it is an error to call this function more
      than once in a class definition.

      Note that the given interfaces have nothing to do with the interfaces
      implemented by instances of the class.

      This function is provided for convenience. It provides a more convenient
      way to call `directlyProvides` for a class. For example::

        classProvides(I1)

      is equivalent to calling::

        directlyProvides(theclass, I1)

      after the class has been created.
    """
    # This entire approach is invalid under Py3K.  Don't even try to fix
    # the coverage for this block there. :(

    if PYTHON3:
        raise TypeError(_ADVICE_ERROR % 'provider')

    frame = sys._getframe(1) # pylint:disable=protected-access
    locals = frame.f_locals # pylint:disable=redefined-builtin

    # Try to make sure we were called from a class def
    if (locals is frame.f_globals) or ('__module__' not in locals):
        raise TypeError("classProvides can be used only from a "
                        "class definition.")

    if '__provides__' in locals:
        raise TypeError(
            "classProvides can only be used once in a class definition.")

    locals["__provides__"] = _normalizeargs(interfaces)

    addClassAdvisor(_classProvides_advice, depth=2)

def _classProvides_advice(cls):
    # This entire approach is invalid under Py3K.  Don't even try to fix
    # the coverage for this block there. :(
    interfaces = cls.__dict__['__provides__']
    del cls.__provides__
    directlyProvides(cls, *interfaces)
    return cls


class provider(object):
    """Class decorator version of classProvides"""

    def __init__(self, *interfaces):
        self.interfaces = interfaces

    def __call__(self, ob):
        directlyProvides(ob, *self.interfaces)
        return ob


def moduleProvides(*interfaces):
    """Declare interfaces provided by a module

    This function is used in a module definition.

    The arguments are one or more interfaces or interface specifications
    (`~zope.interface.interfaces.IDeclaration` objects).

    The given interfaces (including the interfaces in the specifications) are
    used to create the module's direct-object interface specification.  An
    error will be raised if the module already has an interface specification.
    In other words, it is an error to call this function more than once in a
    module definition.

    This function is provided for convenience. It provides a more convenient
    way to call directlyProvides. For example::

      moduleImplements(I1)

    is equivalent to::

      directlyProvides(sys.modules[__name__], I1)
    """
    frame = sys._getframe(1) # pylint:disable=protected-access
    locals = frame.f_locals # pylint:disable=redefined-builtin

    # Try to make sure we were called from a class def
    if (locals is not frame.f_globals) or ('__name__' not in locals):
        raise TypeError(
            "moduleProvides can only be used from a module definition.")

    if '__provides__' in locals:
        raise TypeError(
            "moduleProvides can only be used once in a module definition.")

    locals["__provides__"] = Provides(ModuleType,
                                      *_normalizeargs(interfaces))


##############################################################################
#
# Declaration querying support

# XXX:  is this a fossil?  Nobody calls it, no unit tests exercise it, no
#       doctests import it, and the package __init__ doesn't import it.
#       (Answer: Versions of zope.container prior to 4.4.0 called this.)
def ObjectSpecification(direct, cls):
    """Provide object specifications

    These combine information for the object and for it's classes.
    """
    return Provides(cls, direct) # pragma: no cover fossil

@_use_c_impl
def getObjectSpecification(ob):
    try:
        provides = ob.__provides__
    except AttributeError:
        provides = None

    if provides is not None:
        if isinstance(provides, SpecificationBase):
            return provides

    try:
        cls = ob.__class__
    except AttributeError:
        # We can't get the class, so just consider provides
        return _empty
    return implementedBy(cls)


@_use_c_impl
def providedBy(ob):
    """
    Return the interfaces provided by *ob*.

    If *ob* is a :class:`super` object, then only interfaces implemented
    by the remainder of the classes in the method resolution order are
    considered. Interfaces directly provided by the object underlying *ob*
    are not.
    """
    # Here we have either a special object, an old-style declaration
    # or a descriptor

    # Try to get __providedBy__
    try:
        if isinstance(ob, super): # Some objects raise errors on isinstance()
            return implementedBy(ob)

        r = ob.__providedBy__
    except AttributeError:
        # Not set yet. Fall back to lower-level thing that computes it
        return getObjectSpecification(ob)

    try:
        # We might have gotten a descriptor from an instance of a
        # class (like an ExtensionClass) that doesn't support
        # descriptors.  We'll make sure we got one by trying to get
        # the only attribute, which all specs have.
        r.extends
    except AttributeError:

        # The object's class doesn't understand descriptors.
        # Sigh. We need to get an object descriptor, but we have to be
        # careful.  We want to use the instance's __provides__, if
        # there is one, but only if it didn't come from the class.

        try:
            r = ob.__provides__
        except AttributeError:
            # No __provides__, so just fall back to implementedBy
            return implementedBy(ob.__class__)

        # We need to make sure we got the __provides__ from the
        # instance. We'll do this by making sure we don't get the same
        # thing from the class:

        try:
            cp = ob.__class__.__provides__
        except AttributeError:
            # The ob doesn't have a class or the class has no
            # provides, assume we're done:
            return r

        if r is cp:
            # Oops, we got the provides from the class. This means
            # the object doesn't have it's own. We should use implementedBy
            return implementedBy(ob.__class__)

    return r


@_use_c_impl
class ObjectSpecificationDescriptor(object):
    """Implement the `__providedBy__` attribute

    The `__providedBy__` attribute computes the interfaces provided by
    an object.
    """

    def __get__(self, inst, cls):
        """Get an object specification for an object
        """
        if inst is None:
            return getObjectSpecification(cls)

        provides = getattr(inst, '__provides__', None)
        if provides is not None:
            return provides

        return implementedBy(cls)


##############################################################################

def _normalizeargs(sequence, output=None):
    """Normalize declaration arguments

    Normalization arguments might contain Declarions, tuples, or single
    interfaces.

    Anything but individial interfaces or implements specs will be expanded.
    """
    if output is None:
        output = []
    cls = sequence.__class__
    if InterfaceClass in cls.__mro__ or Implements in cls.__mro__:
        output.append(sequence)
    else:
        for v in sequence:
            _normalizeargs(v, output)

    return output

_empty = _ImmutableDeclaration()

objectSpecificationDescriptor = ObjectSpecificationDescriptor()

class Odd(object):
    pass
class IA1(Interface):
    pass


class A(Odd):
    pass
class IA2(Interface):
    pass


class IB(Interface):
    pass


class IC(Interface):
    pass



if __name__ == "__main__":
    isT=True
    try:
        res1=_normalizeargs((IB,IA1, IA2,))

        res2=_normalizeargs((IB,IA1, IA2,),["start"])
        if len(res1)!=3 or IB not in res1 or IA1 not in res1 or IA2 not in res1 or len(res2)!=4 or "start" not in res2:
            isT=False
    except:
        isT=False
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\pexip---os-zope\\data_passk_platform1/62b8b3d6eb7e40a82d2d111c/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\pexip---os-zope\\data_passk_platform1/62b8b3d6eb7e40a82d2d111c/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     print(args0,args1)
    #     # res0 = _normalizeargs(args0,args1)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/_compat__c_optimizations_available_passk_validte.py
##############################################################################
#
# Copyright (c) 2006 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""
Support functions for dealing with differences in platforms, including Python
versions and implementations.

This file should have no imports from the rest of zope.interface because it is
used during early bootstrapping.
"""
import os
import sys
import types

if sys.version_info[0] < 3:

    def _normalize_name(name):
        if isinstance(name, basestring):
            return unicode(name)
        raise TypeError("name must be a regular or unicode string")

    CLASS_TYPES = (type, types.ClassType)
    STRING_TYPES = (basestring,)

    _BUILTINS = '__builtin__'

    PYTHON3 = False
    PYTHON2 = True

else:

    def _normalize_name(name):
        if isinstance(name, bytes):
            name = str(name, 'ascii')
        if isinstance(name, str):
            return name
        raise TypeError("name must be a string or ASCII-only bytes")

    CLASS_TYPES = (type,)
    STRING_TYPES = (str,)

    _BUILTINS = 'builtins'

    PYTHON3 = True
    PYTHON2 = False

PYPY = hasattr(sys, 'pypy_version_info')
PYPY2 = PYTHON2 and PYPY

def _skip_under_py3k(test_method):
    import unittest
    return unittest.skipIf(sys.version_info[0] >= 3, "Only on Python 2")(test_method)


def _skip_under_py2(test_method):
    import unittest
    return unittest.skipIf(sys.version_info[0] < 3, "Only on Python 3")(test_method)


def _c_optimizations_required():
    """
    Return a true value if the C optimizations are required.

    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.
    """
    pure_env = os.environ.get('PURE_PYTHON')
    require_c = pure_env == "0"
    return require_c


def _c_optimizations_available():
    """
    Return the C optimization module, if available, otherwise
    a false value.

    If the optimizations are required but not available, this
    raises the ImportError.

    This does not say whether they should be used or not.
    """
    catch = () if _c_optimizations_required() else (ImportError,)
    try:
        from zope.interface import _zope_interface_coptimizations as c_opt
        return c_opt
    except catch: # pragma: no cover (only Jython doesn't build extensions)
        return False


def _c_optimizations_ignored():
    """
    The opposite of `_c_optimizations_required`.
    """
    pure_env = os.environ.get('PURE_PYTHON')
    return pure_env is not None and pure_env != "0"


def _should_attempt_c_optimizations():
    """
    Return a true value if we should attempt to use the C optimizations.

    This takes into account whether we're on PyPy and the value of the
    ``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.
    """
    is_pypy = hasattr(sys, 'pypy_version_info')

    if _c_optimizations_required():
        return True
    if is_pypy:
        return False
    return not _c_optimizations_ignored()


def _use_c_impl(py_impl, name=None, globs=None):
    """
    Decorator. Given an object implemented in Python, with a name like
    ``Foo``, import the corresponding C implementation from
    ``zope.interface._zope_interface_coptimizations`` with the name
    ``Foo`` and use it instead.

    If the ``PURE_PYTHON`` environment variable is set to any value
    other than ``"0"``, or we're on PyPy, ignore the C implementation
    and return the Python version. If the C implementation cannot be
    imported, return the Python version. If ``PURE_PYTHON`` is set to
    0, *require* the C implementation (let the ImportError propagate);
    note that PyPy can import the C implementation in this case (and all
    tests pass).

    In all cases, the Python version is kept available. in the module
    globals with the name ``FooPy`` and the name ``FooFallback`` (both
    conventions have been used; the C implementation of some functions
    looks for the ``Fallback`` version, as do some of the Sphinx
    documents).

    Example::

        @_use_c_impl
        class Foo(object):
            ...
    """
    name = name or py_impl.__name__
    globs = globs or sys._getframe(1).f_globals

    def find_impl():
        if not _should_attempt_c_optimizations():
            return py_impl

        c_opt = _c_optimizations_available()
        if not c_opt: # pragma: no cover (only Jython doesn't build extensions)
            return py_impl

        __traceback_info__ = c_opt
        return getattr(c_opt, name)

    c_impl = find_impl()
    # Always make available by the FooPy name and FooFallback
    # name (for testing and documentation)
    globs[name + 'Py'] = py_impl
    globs[name + 'Fallback'] = py_impl

    return c_impl

if __name__ == "__main__":
    isT=_c_optimizations_available()!=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b3d5eb7e40a82d2d1110/"):
    #     f = open("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b3d5eb7e40a82d2d1110/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     res0 = _c_optimizations_available()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/_compat__should_attempt_c_optimizations_passk_validte.py
##############################################################################
#
# Copyright (c) 2006 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""
Support functions for dealing with differences in platforms, including Python
versions and implementations.

This file should have no imports from the rest of zope.interface because it is
used during early bootstrapping.
"""
import os
import sys
import types

if sys.version_info[0] < 3:

    def _normalize_name(name):
        if isinstance(name, basestring):
            return unicode(name)
        raise TypeError("name must be a regular or unicode string")

    CLASS_TYPES = (type, types.ClassType)
    STRING_TYPES = (basestring,)

    _BUILTINS = '__builtin__'

    PYTHON3 = False
    PYTHON2 = True

else:

    def _normalize_name(name):
        if isinstance(name, bytes):
            name = str(name, 'ascii')
        if isinstance(name, str):
            return name
        raise TypeError("name must be a string or ASCII-only bytes")

    CLASS_TYPES = (type,)
    STRING_TYPES = (str,)

    _BUILTINS = 'builtins'

    PYTHON3 = True
    PYTHON2 = False

PYPY = hasattr(sys, 'pypy_version_info')
PYPY2 = PYTHON2 and PYPY

def _skip_under_py3k(test_method):
    import unittest
    return unittest.skipIf(sys.version_info[0] >= 3, "Only on Python 2")(test_method)


def _skip_under_py2(test_method):
    import unittest
    return unittest.skipIf(sys.version_info[0] < 3, "Only on Python 3")(test_method)


def _c_optimizations_required():
    """
    Return a true value if the C optimizations are required.

    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.
    """
    pure_env = os.environ.get('PURE_PYTHON')
    require_c = pure_env == "0"
    return require_c


def _c_optimizations_available():
    """
    Return the C optimization module, if available, otherwise
    a false value.

    If the optimizations are required but not available, this
    raises the ImportError.

    This does not say whether they should be used or not.
    """
    catch = () if _c_optimizations_required() else (ImportError,)
    try:
        from zope.interface import _zope_interface_coptimizations as c_opt
        return c_opt
    except catch: # pragma: no cover (only Jython doesn't build extensions)
        return False


def _c_optimizations_ignored():
    """
    The opposite of `_c_optimizations_required`.
    """
    pure_env = os.environ.get('PURE_PYTHON')
    return pure_env is not None and pure_env != "0"


def _should_attempt_c_optimizations():
    """
    Return a true value if we should attempt to use the C optimizations.

    This takes into account whether we're on PyPy and the value of the
    ``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.
    """
    is_pypy = hasattr(sys, 'pypy_version_info')

    if _c_optimizations_required():
        return True
    if is_pypy:
        return False
    return not _c_optimizations_ignored()


def _use_c_impl(py_impl, name=None, globs=None):
    """
    Decorator. Given an object implemented in Python, with a name like
    ``Foo``, import the corresponding C implementation from
    ``zope.interface._zope_interface_coptimizations`` with the name
    ``Foo`` and use it instead.

    If the ``PURE_PYTHON`` environment variable is set to any value
    other than ``"0"``, or we're on PyPy, ignore the C implementation
    and return the Python version. If the C implementation cannot be
    imported, return the Python version. If ``PURE_PYTHON`` is set to
    0, *require* the C implementation (let the ImportError propagate);
    note that PyPy can import the C implementation in this case (and all
    tests pass).

    In all cases, the Python version is kept available. in the module
    globals with the name ``FooPy`` and the name ``FooFallback`` (both
    conventions have been used; the C implementation of some functions
    looks for the ``Fallback`` version, as do some of the Sphinx
    documents).

    Example::

        @_use_c_impl
        class Foo(object):
            ...
    """
    name = name or py_impl.__name__
    globs = globs or sys._getframe(1).f_globals

    def find_impl():
        if not _should_attempt_c_optimizations():
            return py_impl

        c_opt = _c_optimizations_available()
        if not c_opt: # pragma: no cover (only Jython doesn't build extensions)
            return py_impl

        __traceback_info__ = c_opt
        return getattr(c_opt, name)

    c_impl = find_impl()
    # Always make available by the FooPy name and FooFallback
    # name (for testing and documentation)
    globs[name + 'Py'] = py_impl
    globs[name + 'Fallback'] = py_impl

    return c_impl

if __name__ == "__main__":
    isT=_should_attempt_c_optimizations()
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b3d4eb7e40a82d2d110f/"):
    #     f = open("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b3d4eb7e40a82d2d110f/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     res0 = _should_attempt_c_optimizations()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/_compat__c_optimizations_ignored_passk_validte.py
##############################################################################
#
# Copyright (c) 2006 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""
Support functions for dealing with differences in platforms, including Python
versions and implementations.

This file should have no imports from the rest of zope.interface because it is
used during early bootstrapping.
"""
import os
import sys
sys.path.append("/home/travis/builds/repos/pexip---os-zope")
sys.path.append("/home/travis/builds/repos/pexip---os-zope/src")
sys.path.append("/home/travis/builds/repos/pexip---os-zope/src/zope/interface")

import types

if sys.version_info[0] < 3:

    def _normalize_name(name):
        if isinstance(name, basestring):
            return unicode(name)
        raise TypeError("name must be a regular or unicode string")

    CLASS_TYPES = (type, types.ClassType)
    STRING_TYPES = (basestring,)

    _BUILTINS = '__builtin__'

    PYTHON3 = False
    PYTHON2 = True

else:

    def _normalize_name(name):
        if isinstance(name, bytes):
            name = str(name, 'ascii')
        if isinstance(name, str):
            return name
        raise TypeError("name must be a string or ASCII-only bytes")

    CLASS_TYPES = (type,)
    STRING_TYPES = (str,)

    _BUILTINS = 'builtins'

    PYTHON3 = True
    PYTHON2 = False

PYPY = hasattr(sys, 'pypy_version_info')
PYPY2 = PYTHON2 and PYPY

def _skip_under_py3k(test_method):
    import unittest
    return unittest.skipIf(sys.version_info[0] >= 3, "Only on Python 2")(test_method)


def _skip_under_py2(test_method):
    import unittest
    return unittest.skipIf(sys.version_info[0] < 3, "Only on Python 3")(test_method)


def _c_optimizations_required():
    """
    Return a true value if the C optimizations are required.

    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.
    """
    pure_env = os.environ.get('PURE_PYTHON')
    require_c = pure_env == "0"
    return require_c


def _c_optimizations_available():
    """
    Return the C optimization module, if available, otherwise
    a false value.

    If the optimizations are required but not available, this
    raises the ImportError.

    This does not say whether they should be used or not.
    """
    catch = () if _c_optimizations_required() else (ImportError,)
    try:
        import _zope_interface_coptimizations as c_opt
        return c_opt
    except catch: # pragma: no cover (only Jython doesn't build extensions)
        return False


def _c_optimizations_ignored():
    """
    The opposite of `_c_optimizations_required`.
    """
    pure_env = os.environ.get('PURE_PYTHON')
    return pure_env is not None and pure_env != "0"


def _should_attempt_c_optimizations():
    """
    Return a true value if we should attempt to use the C optimizations.

    This takes into account whether we're on PyPy and the value of the
    ``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.
    """
    is_pypy = hasattr(sys, 'pypy_version_info')

    if _c_optimizations_required():
        return True
    if is_pypy:
        return False
    return not _c_optimizations_ignored()


def _use_c_impl(py_impl, name=None, globs=None):
    """
    Decorator. Given an object implemented in Python, with a name like
    ``Foo``, import the corresponding C implementation from
    ``zope.interface._zope_interface_coptimizations`` with the name
    ``Foo`` and use it instead.

    If the ``PURE_PYTHON`` environment variable is set to any value
    other than ``"0"``, or we're on PyPy, ignore the C implementation
    and return the Python version. If the C implementation cannot be
    imported, return the Python version. If ``PURE_PYTHON`` is set to
    0, *require* the C implementation (let the ImportError propagate);
    note that PyPy can import the C implementation in this case (and all
    tests pass).

    In all cases, the Python version is kept available. in the module
    globals with the name ``FooPy`` and the name ``FooFallback`` (both
    conventions have been used; the C implementation of some functions
    looks for the ``Fallback`` version, as do some of the Sphinx
    documents).

    Example::

        @_use_c_impl
        class Foo(object):
            ...
    """
    name = name or py_impl.__name__
    globs = globs or sys._getframe(1).f_globals

    def find_impl():
        if not _should_attempt_c_optimizations():
            return py_impl

        c_opt = _c_optimizations_available()
        if not c_opt: # pragma: no cover (only Jython doesn't build extensions)
            return py_impl

        __traceback_info__ = c_opt
        return getattr(c_opt, name)

    c_impl = find_impl()
    # Always make available by the FooPy name and FooFallback
    # name (for testing and documentation)
    globs[name + 'Py'] = py_impl
    globs[name + 'Fallback'] = py_impl

    return c_impl

if __name__ == "__main__":
    isT=_c_optimizations_ignored()==False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b3d4eb7e40a82d2d110e/"):
    #     f = open("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b3d4eb7e40a82d2d110e/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     res0 = _c_optimizations_ignored()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/_compat__c_optimizations_required_passk_validte.py
##############################################################################
#
# Copyright (c) 2006 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""
Support functions for dealing with differences in platforms, including Python
versions and implementations.

This file should have no imports from the rest of zope.interface because it is
used during early bootstrapping.
"""
import os
import sys
import types

if sys.version_info[0] < 3:

    def _normalize_name(name):
        if isinstance(name, basestring):
            return unicode(name)
        raise TypeError("name must be a regular or unicode string")

    CLASS_TYPES = (type, types.ClassType)
    STRING_TYPES = (basestring,)

    _BUILTINS = '__builtin__'

    PYTHON3 = False
    PYTHON2 = True

else:

    def _normalize_name(name):
        if isinstance(name, bytes):
            name = str(name, 'ascii')
        if isinstance(name, str):
            return name
        raise TypeError("name must be a string or ASCII-only bytes")

    CLASS_TYPES = (type,)
    STRING_TYPES = (str,)

    _BUILTINS = 'builtins'

    PYTHON3 = True
    PYTHON2 = False

PYPY = hasattr(sys, 'pypy_version_info')
PYPY2 = PYTHON2 and PYPY

def _skip_under_py3k(test_method):
    import unittest
    return unittest.skipIf(sys.version_info[0] >= 3, "Only on Python 2")(test_method)


def _skip_under_py2(test_method):
    import unittest
    return unittest.skipIf(sys.version_info[0] < 3, "Only on Python 3")(test_method)


def _c_optimizations_required():
    """
    Return a true value if the C optimizations are required.

    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.
    """
    pure_env = os.environ.get('PURE_PYTHON')
    require_c = pure_env == "0"
    return require_c


def _c_optimizations_available():
    """
    Return the C optimization module, if available, otherwise
    a false value.

    If the optimizations are required but not available, this
    raises the ImportError.

    This does not say whether they should be used or not.
    """
    catch = () if _c_optimizations_required() else (ImportError,)
    try:
        from zope.interface import _zope_interface_coptimizations as c_opt
        return c_opt
    except catch: # pragma: no cover (only Jython doesn't build extensions)
        return False


def _c_optimizations_ignored():
    """
    The opposite of `_c_optimizations_required`.
    """
    pure_env = os.environ.get('PURE_PYTHON')
    return pure_env is not None and pure_env != "0"


def _should_attempt_c_optimizations():
    """
    Return a true value if we should attempt to use the C optimizations.

    This takes into account whether we're on PyPy and the value of the
    ``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.
    """
    is_pypy = hasattr(sys, 'pypy_version_info')

    if _c_optimizations_required():
        return True
    if is_pypy:
        return False
    return not _c_optimizations_ignored()


def _use_c_impl(py_impl, name=None, globs=None):
    """
    Decorator. Given an object implemented in Python, with a name like
    ``Foo``, import the corresponding C implementation from
    ``zope.interface._zope_interface_coptimizations`` with the name
    ``Foo`` and use it instead.

    If the ``PURE_PYTHON`` environment variable is set to any value
    other than ``"0"``, or we're on PyPy, ignore the C implementation
    and return the Python version. If the C implementation cannot be
    imported, return the Python version. If ``PURE_PYTHON`` is set to
    0, *require* the C implementation (let the ImportError propagate);
    note that PyPy can import the C implementation in this case (and all
    tests pass).

    In all cases, the Python version is kept available. in the module
    globals with the name ``FooPy`` and the name ``FooFallback`` (both
    conventions have been used; the C implementation of some functions
    looks for the ``Fallback`` version, as do some of the Sphinx
    documents).

    Example::

        @_use_c_impl
        class Foo(object):
            ...
    """
    name = name or py_impl.__name__
    globs = globs or sys._getframe(1).f_globals

    def find_impl():
        if not _should_attempt_c_optimizations():
            return py_impl

        c_opt = _c_optimizations_available()
        if not c_opt: # pragma: no cover (only Jython doesn't build extensions)
            return py_impl

        __traceback_info__ = c_opt
        return getattr(c_opt, name)

    c_impl = find_impl()
    # Always make available by the FooPy name and FooFallback
    # name (for testing and documentation)
    globs[name + 'Py'] = py_impl
    globs[name + 'Fallback'] = py_impl

    return c_impl

if __name__ == "__main__":
    isT=_c_optimizations_required()==False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b3d4eb7e40a82d2d110d/"):
    #     f = open("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b3d4eb7e40a82d2d110d/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     res0 = _c_optimizations_required()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/structures/histogram_reset_passk_validte.py
"""Histogram structure *histogram* and element *Histogram*."""
import copy
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")
import lena.context
import lena.core
import lena.flow
import lena.math
from lena.structures import hist_functions as hf


class histogram():
    """A multidimensional histogram.

    Arbitrary dimension, variable bin size and weights are supported.
    Lower bin edge is included, upper edge is excluded.
    Underflow and overflow values are skipped.
    Bin content can be of arbitrary type,
    which is defined during initialization.

    Examples:

    >>> # a two-dimensional histogram
    >>> hist = histogram([[0, 1, 2], [0, 1, 2]])
    >>> hist.fill([0, 1])
    >>> hist.bins
    [[0, 1], [0, 0]]
    >>> values = [[0, 0], [1, 0], [1, 1]]
    >>> # fill the histogram with values
    >>> for v in values:
    ...     hist.fill(v)
    >>> hist.bins
    [[1, 1], [1, 1]]
    """
    # Note the differences from existing packages.
    # Numpy 1.16 (numpy.histogram): all but the last
    # (righthand-most) bin is half-open.
    # This histogram class has bin limits as in ROOT
    # (but without overflow and underflow).

    # Numpy: the first element of the range must be less than or equal to the second.
    # This histogram requires strictly increasing edges.
    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html
    # https://root.cern.ch/root/htmldoc/guides/users-guide/Histograms.html#bin-numbering

    def __init__(self, edges, bins=None, initial_value=0):
        """*edges* is a sequence of one-dimensional arrays,
        each containing strictly increasing bin edges.

        Histogram's bins by default
        are initialized with *initial_value*.
        It can be any object that supports addition with *weight*
        during *fill* (but that is not necessary
        if you don't plan to fill the histogram).
        If the *initial_value* is compound and requires special copying,
        create initial bins yourself (see :func:`.init_bins`).

        A histogram can be created from existing *bins* and *edges*.
        In this case a simple check of the shape of *bins* is done
        (raising :exc:`.LenaValueError` if failed).

        **Attributes**

        :attr:`edges` is a list of edges on each dimension.
        Edges mark the borders of the bin.
        Edges along each dimension are one-dimensional lists,
        and the multidimensional bin is the result of all intersections
        of one-dimensional edges.
        For example, a 3-dimensional histogram has edges of the form
        *[x_edges, y_edges, z_edges]*,
        and the 0th bin has borders
        *((x[0], x[1]), (y[0], y[1]), (z[0], z[1]))*.

        Index in the edges is a tuple, where a given position corresponds
        to a dimension, and the content at that position
        to the bin along that dimension.
        For example, index *(0, 1, 3)* corresponds to the bin
        with lower edges *(x[0], y[1], z[3])*.

        :attr:`bins` is a list of nested lists.
        Same index as for edges can be used to get bin content:
        bin at *(0, 1, 3)* can be obtained as *bins[0][1][3]*.
        Most nested arrays correspond to highest
        (further from x) coordinates.
        For example, for a 3-dimensional histogram bins equal to
        *[[[1, 1], [0, 0]], [[0, 0], [0, 0]]]*
        mean that the only filled bins are those
        where x and y indices are 0, and z index is 0 and 1.

        :attr:`dim` is the dimension of a histogram
        (length of its *edges* for a multidimensional histogram).

        If subarrays of *edges* are not increasing
        or if any of them has length less than 2,
        :exc:`.LenaValueError` is raised.

        .. admonition:: Programmer's note

            one- and multidimensional histograms
            have different *bins* and *edges* format.
            To be unified, 1-dimensional edges should be
            nested in a list (like *[[1, 2, 3]]*).
            Instead, they are simply the x-edges list,
            because it is more intuitive and one-dimensional histograms
            are used more often.
            To unify the interface for bins and edges in your code,
            use :func:`.unify_1_md` function.
        """
        # todo: allow creation of *edges* from tuples
        # (without lena.math.mesh). Allow bin_size in this case.
        hf.check_edges_increasing(edges)
        self.edges = edges
        self._scale = None

        if hasattr(edges[0], "__iter__"):
            self.dim = len(edges)
        else:
            self.dim = 1

        # todo: add a kwarg no_check=False to disable bins testing
        if bins is None:
            self.bins = hf.init_bins(self.edges, initial_value)
        else:
            self.bins = bins
            # We can't make scale for an arbitrary histogram,
            # because it may contain compound values.
            # self._scale = self.make_scale()
            wrong_bins_error = lena.core.LenaValueError(
                "bins of incorrect shape given, {}".format(bins)
            )
            if self.dim == 1:
                if len(bins) != len(edges) - 1:
                    raise wrong_bins_error
            else:
                if len(bins) != len(edges[0]) - 1:
                    raise wrong_bins_error
        if self.dim > 1:
            self.ranges = [(axis[0], axis[-1]) for axis in edges]
            self.nbins =  [len(axis) - 1 for axis in edges]
        else:
            self.ranges = [(edges[0], edges[-1])]
            self.nbins = [len(edges)-1]

    def __eq__(self, other):
        """Two histograms are equal, if and only if they have
        equal bins and equal edges.

        If *other* is not a :class:`.histogram`, return ``False``.

        Note that floating numbers should be compared
        approximately (using :func:`math.isclose`).
        """
        if not isinstance(other, histogram):
            # in Python comparison between different types is allowed
            return False
        return self.bins == other.bins and self.edges == other.edges

    def fill(self, coord, weight=1):
        """Fill histogram at *coord* with the given *weight*.

        Coordinates outside the histogram edges are ignored.
        """
        indices = hf.get_bin_on_value(coord, self.edges)
        subarr = self.bins
        for ind in indices[:-1]:
            # underflow
            if ind < 0:
                return
            try:
                subarr = subarr[ind]
            # overflow
            except IndexError:
                return
        ind = indices[-1]
        # underflow
        if ind < 0:
            return

        # fill
        try:
            subarr[ind] += weight
        except IndexError:
            return

    def __repr__(self):
        return "histogram({}, bins={})".format(self.edges, self.bins)

    def scale(self, other=None, recompute=False):
        """Compute or set scale (integral of the histogram).

        If *other* is ``None``, return scale of this histogram.
        If its scale was not computed before,
        it is computed and stored for subsequent use
        (unless explicitly asked to *recompute*).
        Note that after changing (filling) the histogram
        one must explicitly recompute the scale
        if it was computed before.

        If a float *other* is provided, rescale self to *other*.

        Histograms with scale equal to zero can't be rescaled.
        :exc:`.LenaValueError` is raised if one tries to do that.
        """
        # see graph.scale comments why this is called simply "scale"
        # (not set_scale, get_scale, etc.)
        if other is None:
            # return scale
            if self._scale is None or recompute:
                self._scale = hf.integral(
                    *hf.unify_1_md(self.bins, self.edges)
                )
            return self._scale
        else:
            # rescale from other
            scale = self.scale()
            if scale == 0:
                raise lena.core.LenaValueError(
                    "can not rescale histogram with zero scale"
                )
            self.bins = lena.math.md_map(lambda binc: binc*float(other) / scale,
                                         self.bins)
            self._scale = other
            return None

    def _update_context(self, context):
        """Update *context* with the properties of this histogram.

        *context.histogram* is updated with "dim", "nbins"
        and "ranges" with values for this histogram.
        If this histogram has a computed scale, it is also added
        to the context.

        Called on "destruction" of the histogram structure (for example,
        in :class:`.ToCSV`). See graph._update_context for more details.
        """

        hist_context = {
            "dim": self.dim,
            "nbins": self.nbins,
            "ranges": self.ranges
        }

        if self._scale is not None:
            hist_context["scale"] = self._scale

        lena.context.update_recursively(context, {"histogram": hist_context})


class Histogram():
    """An element to produce histograms."""

    def __init__(self, edges, bins=None, make_bins=None, initial_value=0):
        """*edges*, *bins* and *initial_value* have the same meaning
        as during creation of a :class:`histogram`.

        *make_bins* is a function without arguments
        that creates new bins
        (it will be called during :meth:`__init__` and :meth:`reset`).
        *initial_value* in this case is ignored, but bin check is made.
        If both *bins* and *make_bins* are provided,
        :exc:`.LenaTypeError` is raised.
        """
        self._hist = histogram(edges, bins)

        if make_bins is not None and bins is not None:
            raise lena.core.LenaTypeError(
                "either initial bins or make_bins must be provided, "
                "not both: {} and {}".format(bins, make_bins)
            )

        # may be None
        self._initial_bins = copy.deepcopy(bins)

        # todo: bins, make_bins, initial_value look redundant
        # and may be reconsidered when really using reset().
        if make_bins:
            bins = make_bins()
        self._make_bins = make_bins

        self._cur_context = {}

    def fill(self, value):
        """Fill the histogram with *value*.

        *value* can be a *(data, context)* pair. 
        Values outside the histogram edges are ignored.
        """
        data, self._cur_context = lena.flow.get_data_context(value)
        self._hist.fill(data)
        # filling with weight is only allowed in histogram structure
        # self._hist.fill(data, weight)

    def compute(self):
        """Yield histogram with context."""
        yield (self._hist, self._cur_context)

    def reset(self):
        """Reset the histogram.

        Current context is reset to an empty dict.
        Bins are reinitialized with the *initial_value*
        or with *make_bins()* (depending on the initialization).
        """
        if self._make_bins is not None:
            self.bins = self._make_bins()
        elif self._initial_bins is not None:
            self.bins = copy.deepcopy(self._initial_bins)
        else:
            self.bins = hf.init_bins(self.edges, self._initial_value)

        self._cur_context = {}

if __name__ == "__main__":
    isT = True
    hist = histogram([0, 1, 2])

    # test simple initialization
    # probably should make a deep copy of these bins and edges here
    h0 = Histogram(hist.edges, hist.bins)
    res00 = list(h0.compute())
    res1=res00[0][0] == hist

    # fill outside of edges doesn't change histogram
    h0.fill(-1)
    res01 = list(h0.compute())
    res2=res01[0][0] == hist
    res3=res01[0][1] == {}

    # but context is updated
    context = {"context": True}
    h0.fill((-1, context))
    res01u = list(h0.compute())
    res4= res01u[0][0] == hist
    res5= res01u[0][1] == context

    # reset doesn't change histogram in our case
    h0.reset()
    res02 = list(h0.compute())
    res6= res02[0][0] == hist
    # cur_context is reset
    res7= res02[0][1] == {}
    if not res1 or not res2 or not res3 or not res4 or not res5 or not res6 or not res7:
        isT=False
    # for l in os.listdir("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87b989a0c4fa8b80b35ee/"):
    #     f = open("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87b989a0c4fa8b80b35ee/" + l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = Histogram([[0, 1, 2], [0, 1, 2]])
    #     temp_class.__dict__.update(object_class)
    #     temp_class.reset()
    #     res0=temp_class.bins
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/structures/graph_to_csv_passk_validte.py
"""A graph is a function at given coordinates."""
import copy
import functools
import operator
import re
import warnings
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")
import lena.core
import lena.context
import lena.flow


class graph():
    """Numeric arrays of equal size."""

    def __init__(self, coords, field_names=("x", "y"), scale=None):
        """This structure generally corresponds
        to the graph of a function
        and represents arrays of coordinates and the function values
        of arbitrary dimensions.

        *coords* is a list of one-dimensional
        coordinate and value sequences (usually lists).
        There is little to no distinction between them,
        and "values" can also be called "coordinates".

        *field_names* provide the meaning of these arrays.
        For example, a 3-dimensional graph could be distinguished
        from a 2-dimensional graph with errors by its fields
        ("x", "y", "z") versus ("x", "y", "error_y").
        Field names don't affect drawing graphs:
        for that :class:`~Variable`-s should be used.
        Default field names,
        provided for the most used 2-dimensional graphs,
        are "x" and "y".

        *field_names* can be a string separated by whitespace
        and/or commas or a tuple of strings, such as ("x", "y").
        *field_names* must have as many elements
        as *coords* and each field name must be unique.
        Otherwise field names are arbitrary.
        Error fields must go after all other coordinates.
        Name of a coordinate error is "error\\_"
        appended by coordinate name. Further error details
        are appended after '_'. They could be arbitrary depending
        on the problem: "low", "high", "low_90%_cl", etc. Example:
        ("E", "time", "error_E_low", "error_time").

        *scale* of the graph is a kind of its norm. It could be
        the integral of the function or its other property.
        A scale of a normalised probability density
        function would be one.
        An initialized *scale* is required if one needs
        to renormalise the graph in :meth:`scale`
        (for example, to plot it with other graphs).

        Coordinates of a function graph would usually be arrays
        of increasing values, which is not required here.
        Neither is it checked that coordinates indeed
        contain one-dimensional numeric values.
        However, non-standard graphs
        will likely lead to errors during plotting
        and will require more programmer's work and caution,
        so use them only if you understand what you are doing.

        A graph can be iterated yielding tuples of numbers
        for each point.

        **Attributes**

        :attr:`coords` is a list \
            of one-dimensional lists of coordinates.

        :attr:`field_names`

        :attr:`dim` is the dimension of the graph,
        that is of all its coordinates without errors.

        In case of incorrect initialization arguments,
        :exc:`~.LenaTypeError` or :exc:`~.LenaValueError` is raised.

        .. versionadded:: 0.5
        """
        if not coords:
            raise lena.core.LenaValueError(
                "coords must be a non-empty sequence "
                "of coordinate sequences"
            )

        # require coords to be of the same size
        pt_len = len(coords[0])
        for arr in coords[1:]:
            if len(arr) != pt_len:
                raise lena.core.LenaValueError(
                    "coords must have subsequences of equal lengths"
                )

        # Unicode (Python 2) field names would be just bad,
        # so we don't check for it here.
        if isinstance(field_names, str):
            # split(', ') won't work.
            # From https://stackoverflow.com/a/44785447/952234:
            # \s stands for whitespace.
            field_names = tuple(re.findall(r'[^,\s]+', field_names))
        elif not isinstance(field_names, tuple):
            # todo: why field_names are a tuple,
            # while coords are a list?
            # It might be non-Pythonic to require a tuple
            # (to prohibit a list), but it's important
            # for comparisons and uniformity
            raise lena.core.LenaTypeError(
                "field_names must be a string or a tuple"
            )

        if len(field_names) != len(coords):
            raise lena.core.LenaValueError(
                "field_names must have must have the same size as coords"
            )

        if len(set(field_names)) != len(field_names):
            raise lena.core.LenaValueError(
                "field_names contains duplicates"
            )

        self.coords = coords
        self._scale = scale

        # field_names are better than fields,
        # because they are unambigous (as in namedtuple).
        self.field_names = field_names

        # decided to use "error_x_low" (like in ROOT).
        # Other versions were x_error (looked better than x_err),
        # but x_err_low looked much better than x_error_low).
        try:
            parsed_error_names = self._parse_error_names(field_names)
        except lena.core.LenaValueError as err:
            raise err
            # in Python 3
            # raise err from None
        self._parsed_error_names = parsed_error_names

        dim = len(field_names) - len(parsed_error_names)
        self._coord_names = field_names[:dim]
        self.dim = dim

        # todo: add subsequences of coords as attributes
        # with field names.
        # In case if someone wants to create a graph of another function
        # at the same coordinates.
        # Should a) work when we rescale the graph
        #        b) not interfere with other fields and methods

        # Probably we won't add methods __del__(n), __add__(*coords),
        # since it might change the scale.

    def __eq__(self, other):
        """Two graphs are equal, if and only if they have
        equal coordinates, field names and scales.

        If *other* is not a :class:`.graph`, return ``False``.

        Note that floating numbers should be compared
        approximately (using :func:`math.isclose`).
        Therefore this comparison may give false negatives.
        """
        if not isinstance(other, graph):
            # in Python comparison between different types is allowed
            return False
        return (self.coords == other.coords and self._scale == other._scale
                and self.field_names == other.field_names)

    def _get_err_indices(self, coord_name):
        """Get error indices corresponding to a coordinate."""
        err_indices = []
        dim = self.dim
        for ind, err in enumerate(self._parsed_error_names):
            if err[1] == coord_name:
                err_indices.append(ind+dim)
        return err_indices

    def __iter__(self):
        """Iterate graph coords one by one."""
        for val in zip(*self.coords):
            yield val

    def __repr__(self):
        return """graph({}, field_names={}, scale={})""".format(
            self.coords, self.field_names, self._scale
        )

    def scale(self, other=None):
        """Get or set the scale of the graph.

        If *other* is ``None``, return the scale of this graph.

        If a numeric *other* is provided, rescale to that value.
        If the graph has unknown or zero scale,
        rescaling that will raise :exc:`~.LenaValueError`.

        To get meaningful results, graph's fields are used.
        Only the last coordinate is rescaled.
        For example, if the graph has *x* and *y* coordinates,
        then *y* will be rescaled, and for a 3-dimensional graph
        *z* will be rescaled.
        All errors are rescaled together with their coordinate.
        """
        # this method is called scale() for uniformity with histograms
        # And this looks really good: explicit for computations
        # (not a subtle graph.scale, like a constant field (which is,
        #  however, the case in graph - but not in other structures))
        # and easy to remember (set_scale? rescale? change_scale_to?..)

        # We modify the graph in place,
        # because that would be redundant (not optimal)
        # to create a new graph
        # if we only want to change the scale of the existing one.

        if other is None:
            return self._scale

        if not self._scale:
            raise lena.core.LenaValueError(
                "can't rescale a graph with zero or unknown scale"
            )

        last_coord_ind = self.dim - 1
        last_coord_name = self.field_names[last_coord_ind]

        last_coord_indices = ([last_coord_ind] +
                self._get_err_indices(last_coord_name)
        )

        # In Python 2 3/2 is 1, so we want to be safe;
        # the downside is that integer-valued graphs
        # will become floating, but that is doubtfully an issue.
        # Remove when/if dropping support for Python 2.
        rescale = float(other) / self._scale

        mul = operator.mul
        partial = functools.partial

        # a version with lambda is about 50% slower:
        # timeit.timeit('[*map(lambda val: val*2, vals)]', \
        #     setup='vals = list(range(45)); from operator import mul; \
        #     from functools import partial')
        # 3.159
        # same setup for
        # timeit.timeit('[*map(partial(mul, 2), vals)]',...):
        # 2.075
        # 
        # [*map(...)] is very slightly faster than list(map(...)),
        # but it's unavailable in Python 2 (and anyway less readable).

        # rescale arrays of values and errors
        for ind, arr in enumerate(self.coords):
            if ind in last_coord_indices:
                # Python lists are faster than arrays,
                # https://stackoverflow.com/a/62399645/952234
                # (because each time taking a value from an array
                #  creates a Python object)
                self.coords[ind] = list(map(partial(mul, rescale),
                                            arr))

        self._scale = other

        # as suggested in PEP 8
        return None

    def _parse_error_names(self, field_names):
        # field_names is a parameter for easier testing,
        # usually object's field_names are used.
        errors = []

        # collect all error fields and check that they are
        # strictly after other fields
        in_error_fields = False
        # there is at least one field
        last_coord_ind = 0
        for ind, field in enumerate(field_names):
            if field.startswith("error_"):
                in_error_fields = True
                errors.append((field, ind))
            else:
                last_coord_ind = ind
                if in_error_fields:
                    raise lena.core.LenaValueError(
                        "errors must go after coordinate fields"
                    )

        coords = set(field_names[:last_coord_ind+1])
        parsed_errors = []

        for err, ind in errors:
            err_coords = []
            for coord in coords:
                err_main = err[6:]  # all after "error_"
                if err_main == coord or err_main.startswith(coord + "_"):
                    err_coords.append(coord)
                    err_tail = err_main[len(coord)+1:]
            if not err_coords:
                raise lena.core.LenaValueError(
                    "no coordinate corresponding to {} given".format(err)
                )
            elif len(err_coords) > 1:
                raise lena.core.LenaValueError(
                    "ambiguous error " + err +\
                    " corresponding to several coordinates given"
                )
            # "error" may be redundant, but it is explicit.
            parsed_errors.append(("error", err_coords[0], err_tail, ind))

        return parsed_errors

    def _update_context(self, context):
        """Update *context* with the properties of this graph.

        *context.error* is appended with indices of errors.
        Example subcontext for a graph with fields "E,t,error_E_low":
        {"error": {"x_low": {"index": 2}}}.
        Note that error names are called "x", "y" and "z"
        (this corresponds to first three coordinates,
        if they are present), which allows to simplify plotting.
        Existing values are not removed
        from *context.value* and its subcontexts.

        Called on "destruction" of the graph (for example,
        in :class:`.ToCSV`). By destruction we mean conversion
        to another structure (like text) in the flow.
        The graph object is not really destroyed in this process.
        """
        # this method is private, because we encourage users to yield
        # graphs into the flow and process them with ToCSV element
        # (not manually).

        if not self._parsed_error_names:
            # no error fields present
            return

        dim = self.dim

        xyz_coord_names = self._coord_names[:3]
        for name, coord_name in zip(["x", "y", "z"], xyz_coord_names):
            for err in self._parsed_error_names:
                if err[1] == coord_name:
                    error_ind = err[3]
                    if err[2]:
                        # add error suffix
                        error_name = name + "_" + err[2]
                    else:
                        error_name = name
                    lena.context.update_recursively(
                        context,
                        "error.{}.index".format(error_name),
                        # error can correspond both to variable and
                        # value, so we put it outside value.
                        # "value.error.{}.index".format(error_name),
                        error_ind
                    )

    # emulating numeric types
    def __add__(self, other):
        """Add last (highest) coordinates of two graphs.

        A new graph is returned. Error fields are ignored.
        """
        # todo: make it method add(.., calculate_error=...)
        if not isinstance(other, graph):
            return NotImplemented
        # but their errors may be different
        assert self.dim == other.dim
        dim = self.dim
        # copied from scale
        last_coord_ind = self.dim - 1
        last_coord_name = self.field_names[last_coord_ind]

        last_coord_indices = (
            [last_coord_ind] + self._get_err_indices(last_coord_name)
        )

        all_same = all(((len(self.coords[i]) == len(other.coords[i]))
                        for i in range(dim - 1)))
        assert all_same
        new_coords = [copy.copy(self.coords[i]) for i in range(dim - 1)]
        new_vals = [
            self.coords[last_coord_ind][i] + other.coords[last_coord_ind][i]
            for i in range(len(self.coords[last_coord_ind]))
        ]
        # add can't use zipped values
        # new_vals = list(map(operator.add, zip(self.coords[last_coord_ind], 
        #                                       other.coords[last_coord_ind])))
        new_coords.append(new_vals)
        try:
            scale0 = self.scale()
            scale1 = other.scale()
        except lena.core.LenaValueError:
            scale = None
        else:
            if scale0 is not None and scale1 is not None:
                scale = scale0 + scale1
            else:
                scale = None
        return graph(coords=new_coords, field_names=self.field_names,
                     scale=scale)

        # for ind, arr in enumerate(self.coords):
        #     if ind in last_coord_indices:
        #         self.coords[ind] = list(map(partial(mul, rescale),
        #                                     arr))


# used in deprecated Graph
def _rescale_value(rescale, value):
    return rescale * lena.flow.get_data(value)


class Graph(object):
    """
    .. deprecated:: 0.5
       use :class:`graph`.
       This class may be used in the future,
       but with a changed interface.

    Function at given coordinates (arbitraty dimensions).

    Graph points can be set during the initialization and
    during :meth:`fill`. It can be rescaled (producing a new :class:`Graph`).
    A point is a tuple of *(coordinate, value)*, where both *coordinate*
    and *value* can be tuples of numbers.
    *Coordinate* corresponds to a point in N-dimensional space,
    while *value* is some function's value at this point
    (the function can take a value in M-dimensional space).
    Coordinate and value dimensions must be the same for all points.

    One can get graph points as :attr:`Graph.points` attribute.
    They will be sorted each time before return
    if *sort* was set to ``True``.
    An attempt to change points
    (use :attr:`Graph.points` on the left of '=')
    will raise Python's :exc:`AttributeError`.
    """

    def __init__(self, points=None, context=None, scale=None, sort=True):
        """*points* is an array of *(coordinate, value)* tuples.

        *context* is the same as the most recent context
        during *fill*. Use it to provide a context
        when initializing a :class:`Graph` from existing points.

        *scale* sets the scale of the graph.
        It is used during plotting if rescaling is needed.

        Graph coordinates are sorted by default.
        This is usually needed to plot graphs of functions.
        If you need to keep the order of insertion, set *sort* to ``False``.

        By default, sorting is done using standard Python
        lists and functions. You can disable *sort* and provide your own
        sorting container for *points*.
        Some implementations are compared
        `here <http://www.grantjenks.com/docs/sortedcontainers/performance.html>`_.
        Note that a rescaled graph uses a default list.

        Note that :class:`Graph` does not reduce data.
        All filled values will be stored in it.
        To reduce data, use histograms.
        """
        warnings.warn("Graph is deprecated since Lena 0.5. Use graph.",
                      DeprecationWarning, stacklevel=2)

        self._points = points if points is not None else []
        # todo: add some sanity checks for points
        self._scale = scale
        self._init_context = {"scale": scale}
        if context is None:
            self._cur_context = {}
        elif not isinstance(context, dict):
            raise lena.core.LenaTypeError(
                "context must be a dict, {} provided".format(context)
            )
        else:
            self._cur_context = context
        self._sort = sort

        # todo: probably, scale from context is not needed.

        ## probably this function is not needed.
        ## it can't be copied, graphs won't be possible to compare.
        # *rescale_value* is a function, which can be used to scale
        # complex graph values.
        # It must accept a rescale parameter and the value at a data point.
        # By default, it is multiplication of rescale and the value
        # (which must be a number).
        # if rescale_value is None:
        #     self._rescale_value = _rescale_value
        self._rescale_value = _rescale_value
        self._update()

    def fill(self, value):
        """Fill the graph with *value*.

        *Value* can be a *(data, context)* tuple.
        *Data* part must be a *(coordinates, value)* pair,
        where both coordinates and value are also tuples.
        For example, *value* can contain the principal number
        and its precision.
        """
        point, self._cur_context = lena.flow.get_data_context(value)
        # coords, val = point
        self._points.append(point)

    def request(self):
        """Yield graph with context.

        If *sort* was initialized ``True``, graph points will be sorted.
        """
        # If flow contained *scale* it the context, it is set now.
        self._update()
        yield (self, self._context)

    # compute method shouldn't be in this class,
    # because it is a pure FillRequest.
    # def compute(self):
    #     """Yield graph with context (as in :meth:`request`),
    #     and :meth:`reset`."""
    #     self._update()
    #     yield (self, self._context)
    #     self.reset()

    @property
    def points(self):
        """Get graph points (read only)."""
        # sort points before giving them
        self._update()
        return self._points

    def reset(self):
        """Reset points to an empty list
        and current context to an empty dict.
        """
        self._points = []
        self._cur_context = {}

    def __repr__(self):
        self._update()
        return ("Graph(points={}, scale={}, sort={})"
                .format(self._points, self._scale, self._sort))

    def scale(self, other=None):
        """Get or set the scale.

        Graph's scale comes from an external source.
        For example, if the graph was computed from a function,
        this may be its integral passed via context during :meth:`fill`.
        Once the scale is set, it is stored in the graph.
        If one attempts to use scale which was not set,
        :exc:`.LenaAttributeError` is raised.

        If *other* is None, return the scale.

        If a ``float`` *other* is provided, rescale to *other*.
        A new graph with the scale equal to *other*
        is returned, the original one remains unchanged.
        Note that in this case its *points* will be a simple list
        and new graph *sort* parameter will be ``True``.

        Graphs with scale equal to zero can't be rescaled. 
        Attempts to do that raise :exc:`.LenaValueError`.
        """
        if other is None:
            # return scale
            self._update()
            if self._scale is None:
                raise lena.core.LenaAttributeError(
                    "scale must be explicitly set before using that"
                )
            return self._scale
        else:
            # rescale from other
            scale = self.scale()
            if scale == 0:
                raise lena.core.LenaValueError(
                    "can't rescale graph with 0 scale"
                )

            # new_init_context = copy.deepcopy(self._init_context)
            # new_init_context.update({"scale": other})

            rescale = float(other) / scale
            new_points = []
            for coord, val in self._points:
                # probably not needed, because tuples are immutable:
                # make a deep copy so that new values
                # are completely independent from old ones.
                new_points.append((coord, self._rescale_value(rescale, val)))
            # todo: should it inherit context?
            # Probably yes, but watch out scale.
            new_graph = Graph(points=new_points, scale=other,
                              sort=self._sort)
            return new_graph

    def to_csv(self, separator=",", header=None):
        """.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.
              Iterables are converted to tables.

        Convert graph's points to CSV.

        *separator* delimits values, the default is comma.

        *header*, if not ``None``, is the first string of the output
        (new line is added automatically).

        Since a graph can be multidimensional,
        for each point first its coordinate is converted to string
        (separated by *separator*), then each part of its value.

        To convert :class:`Graph` to CSV inside a Lena sequence,
        use :class:`lena.output.ToCSV`.
        """
        if self._sort:
            self._update()

        def unpack_pt(pt):
            coord = pt[0]
            value = pt[1]
            if isinstance(coord, tuple):
                unpacked = list(coord)
            else:
                unpacked = [coord]
            if isinstance(value, tuple):
                unpacked += list(value)
            else:
                unpacked.append(value)
            return unpacked

        def pt_to_str(pt, separ):
            return separ.join([str(val) for val in unpack_pt(pt)])

        if header is not None:
            # if one needs an empty header line, they may provide ""
            lines = header + "\n"
        else:
            lines = ""
        lines += "\n".join([pt_to_str(pt, separator) for pt in self.points])

        return lines

    #     *context* will be added to graph context.
    #     If it contains "scale", :meth:`scale` method will be available.
    #     Otherwise, if "scale" is contained in the context
    #     during :meth:`fill`, it will be used.
    #     In this case it is assumed that this scale
    #     is same for all values (only the last filled context is checked).
    #     Context from flow takes precedence over the initialized one.

    def _update(self):
        """Sort points if needed, update context."""
        # todo: probably remove this context_scale?
        context_scale = self._cur_context.get("scale")
        if context_scale is not None:
            # this complex check is fine with rescale,
            # because that returns a new graph (this scale unchanged).
            if self._scale is not None and self._scale != context_scale:
                raise lena.core.LenaRuntimeError(
                    "Initialization and context scale differ, "
                    "{} and {} from context {}"
                    .format(self._scale, context_scale, self._cur_context)
                )
            self._scale = context_scale
        if self._sort:
            self._points = sorted(self._points)

        self._context = copy.deepcopy(self._cur_context)
        self._context.update(self._init_context)
        # why this? Not *graph.scale*?
        self._context.update({"scale": self._scale})
        # self._context.update(lena.context.make_context(self, "_scale"))

        # todo: make this check during fill. Probably initialize self._dim
        # with kwarg dim. (dim of coordinates or values?)
        if self._points:
            # check points correctness
            points = self._points
            def coord_dim(coord):
                if not hasattr(coord, "__len__"):
                    return 1
                return len(coord)
            first_coord = points[0][0]
            dim = coord_dim(first_coord)
            same_dim = all(coord_dim(point[0]) == dim for point in points)
            if not same_dim:
                raise lena.core.LenaValueError(
                    "coordinates tuples must have same dimension, "
                    "{} given".format(points)
                )
            self.dim = dim
            self._context["dim"] = self.dim

    def __eq__(self, other):
        if not isinstance(other, Graph):
            return False
        if self.points != other.points:
            return False
        if self._scale is None and other._scale is None:
            return True
        try:
            result = self.scale() == other.scale()
        except lena.core.LenaAttributeError:
            # one scale couldn't be computed
            return False
        else:
            return result

if __name__ == "__main__":
    isT = True
    coords = range(0, 3)
    values = map(lambda x: x + 1, coords)
    points = list(zip(coords, values))
    temp_class = Graph(points=points)
    args1=","
    args2=None
    res1 = temp_class.to_csv(args1, args2)=="0,1\n1,2\n2,3"
    args1=","
    args2="x,y,z,f,g"
    res2 = temp_class.to_csv(args1, args2)=="x,y,z,f,g\n0,1\n1,2\n2,3"
    temp_class1 = Graph(points=points,sort=False)
    args1 = ","
    args2 = None
    res3 = temp_class1.to_csv(args1, args2)=="0,1\n1,2\n2,3"
    args1 = ","
    args2 = "x,y,z,f,g"
    res4 = temp_class1.to_csv(args1, args2)=="x,y,z,f,g\n0,1\n1,2\n2,3"
    if not res1 or not res2 or not res3 or not res4:
        isT=False
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena/data_passk_platform1/62b87b859a0c4fa8b80b35d7/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena/data_passk_platform1/62b87b859a0c4fa8b80b35d7/" + l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = Graph()
    #     # temp_class.__dict__.update(object_class)
    #     print(args1,args2)
    #
    #     res0 = temp_class.to_csv(args1, args2)
    #     print(res0)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/structures/graph__get_err_indices_passk_validte.py
"""A graph is a function at given coordinates."""
import copy
import functools
import operator
import re
import warnings
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")
import lena.core
import lena.context
import lena.flow


class graph():
    """Numeric arrays of equal size."""

    def __init__(self, coords, field_names=("x", "y"), scale=None):
        """This structure generally corresponds
        to the graph of a function
        and represents arrays of coordinates and the function values
        of arbitrary dimensions.

        *coords* is a list of one-dimensional
        coordinate and value sequences (usually lists).
        There is little to no distinction between them,
        and "values" can also be called "coordinates".

        *field_names* provide the meaning of these arrays.
        For example, a 3-dimensional graph could be distinguished
        from a 2-dimensional graph with errors by its fields
        ("x", "y", "z") versus ("x", "y", "error_y").
        Field names don't affect drawing graphs:
        for that :class:`~Variable`-s should be used.
        Default field names,
        provided for the most used 2-dimensional graphs,
        are "x" and "y".

        *field_names* can be a string separated by whitespace
        and/or commas or a tuple of strings, such as ("x", "y").
        *field_names* must have as many elements
        as *coords* and each field name must be unique.
        Otherwise field names are arbitrary.
        Error fields must go after all other coordinates.
        Name of a coordinate error is "error\\_"
        appended by coordinate name. Further error details
        are appended after '_'. They could be arbitrary depending
        on the problem: "low", "high", "low_90%_cl", etc. Example:
        ("E", "time", "error_E_low", "error_time").

        *scale* of the graph is a kind of its norm. It could be
        the integral of the function or its other property.
        A scale of a normalised probability density
        function would be one.
        An initialized *scale* is required if one needs
        to renormalise the graph in :meth:`scale`
        (for example, to plot it with other graphs).

        Coordinates of a function graph would usually be arrays
        of increasing values, which is not required here.
        Neither is it checked that coordinates indeed
        contain one-dimensional numeric values.
        However, non-standard graphs
        will likely lead to errors during plotting
        and will require more programmer's work and caution,
        so use them only if you understand what you are doing.

        A graph can be iterated yielding tuples of numbers
        for each point.

        **Attributes**

        :attr:`coords` is a list \
            of one-dimensional lists of coordinates.

        :attr:`field_names`

        :attr:`dim` is the dimension of the graph,
        that is of all its coordinates without errors.

        In case of incorrect initialization arguments,
        :exc:`~.LenaTypeError` or :exc:`~.LenaValueError` is raised.

        .. versionadded:: 0.5
        """
        if not coords:
            raise lena.core.LenaValueError(
                "coords must be a non-empty sequence "
                "of coordinate sequences"
            )

        # require coords to be of the same size
        pt_len = len(coords[0])
        for arr in coords[1:]:
            if len(arr) != pt_len:
                raise lena.core.LenaValueError(
                    "coords must have subsequences of equal lengths"
                )

        # Unicode (Python 2) field names would be just bad,
        # so we don't check for it here.
        if isinstance(field_names, str):
            # split(', ') won't work.
            # From https://stackoverflow.com/a/44785447/952234:
            # \s stands for whitespace.
            field_names = tuple(re.findall(r'[^,\s]+', field_names))
        elif not isinstance(field_names, tuple):
            # todo: why field_names are a tuple,
            # while coords are a list?
            # It might be non-Pythonic to require a tuple
            # (to prohibit a list), but it's important
            # for comparisons and uniformity
            raise lena.core.LenaTypeError(
                "field_names must be a string or a tuple"
            )

        if len(field_names) != len(coords):
            raise lena.core.LenaValueError(
                "field_names must have must have the same size as coords"
            )

        if len(set(field_names)) != len(field_names):
            raise lena.core.LenaValueError(
                "field_names contains duplicates"
            )

        self.coords = coords
        self._scale = scale

        # field_names are better than fields,
        # because they are unambigous (as in namedtuple).
        self.field_names = field_names

        # decided to use "error_x_low" (like in ROOT).
        # Other versions were x_error (looked better than x_err),
        # but x_err_low looked much better than x_error_low).
        try:
            parsed_error_names = self._parse_error_names(field_names)
        except lena.core.LenaValueError as err:
            raise err
            # in Python 3
            # raise err from None
        self._parsed_error_names = parsed_error_names

        dim = len(field_names) - len(parsed_error_names)
        self._coord_names = field_names[:dim]
        self.dim = dim

        # todo: add subsequences of coords as attributes
        # with field names.
        # In case if someone wants to create a graph of another function
        # at the same coordinates.
        # Should a) work when we rescale the graph
        #        b) not interfere with other fields and methods

        # Probably we won't add methods __del__(n), __add__(*coords),
        # since it might change the scale.

    def __eq__(self, other):
        """Two graphs are equal, if and only if they have
        equal coordinates, field names and scales.

        If *other* is not a :class:`.graph`, return ``False``.

        Note that floating numbers should be compared
        approximately (using :func:`math.isclose`).
        Therefore this comparison may give false negatives.
        """
        if not isinstance(other, graph):
            # in Python comparison between different types is allowed
            return False
        return (self.coords == other.coords and self._scale == other._scale
                and self.field_names == other.field_names)

    def _get_err_indices(self, coord_name):
        """Get error indices corresponding to a coordinate."""
        err_indices = []
        dim = self.dim
        for ind, err in enumerate(self._parsed_error_names):
            if err[1] == coord_name:
                err_indices.append(ind+dim)
        return err_indices

    def __iter__(self):
        """Iterate graph coords one by one."""
        for val in zip(*self.coords):
            yield val

    def __repr__(self):
        return """graph({}, field_names={}, scale={})""".format(
            self.coords, self.field_names, self._scale
        )

    def scale(self, other=None):
        """Get or set the scale of the graph.

        If *other* is ``None``, return the scale of this graph.

        If a numeric *other* is provided, rescale to that value.
        If the graph has unknown or zero scale,
        rescaling that will raise :exc:`~.LenaValueError`.

        To get meaningful results, graph's fields are used.
        Only the last coordinate is rescaled.
        For example, if the graph has *x* and *y* coordinates,
        then *y* will be rescaled, and for a 3-dimensional graph
        *z* will be rescaled.
        All errors are rescaled together with their coordinate.
        """
        # this method is called scale() for uniformity with histograms
        # And this looks really good: explicit for computations
        # (not a subtle graph.scale, like a constant field (which is,
        #  however, the case in graph - but not in other structures))
        # and easy to remember (set_scale? rescale? change_scale_to?..)

        # We modify the graph in place,
        # because that would be redundant (not optimal)
        # to create a new graph
        # if we only want to change the scale of the existing one.

        if other is None:
            return self._scale

        if not self._scale:
            raise lena.core.LenaValueError(
                "can't rescale a graph with zero or unknown scale"
            )

        last_coord_ind = self.dim - 1
        last_coord_name = self.field_names[last_coord_ind]

        last_coord_indices = ([last_coord_ind] +
                self._get_err_indices(last_coord_name)
        )

        # In Python 2 3/2 is 1, so we want to be safe;
        # the downside is that integer-valued graphs
        # will become floating, but that is doubtfully an issue.
        # Remove when/if dropping support for Python 2.
        rescale = float(other) / self._scale

        mul = operator.mul
        partial = functools.partial

        # a version with lambda is about 50% slower:
        # timeit.timeit('[*map(lambda val: val*2, vals)]', \
        #     setup='vals = list(range(45)); from operator import mul; \
        #     from functools import partial')
        # 3.159
        # same setup for
        # timeit.timeit('[*map(partial(mul, 2), vals)]',...):
        # 2.075
        # 
        # [*map(...)] is very slightly faster than list(map(...)),
        # but it's unavailable in Python 2 (and anyway less readable).

        # rescale arrays of values and errors
        for ind, arr in enumerate(self.coords):
            if ind in last_coord_indices:
                # Python lists are faster than arrays,
                # https://stackoverflow.com/a/62399645/952234
                # (because each time taking a value from an array
                #  creates a Python object)
                self.coords[ind] = list(map(partial(mul, rescale),
                                            arr))

        self._scale = other

        # as suggested in PEP 8
        return None

    def _parse_error_names(self, field_names):
        # field_names is a parameter for easier testing,
        # usually object's field_names are used.
        errors = []

        # collect all error fields and check that they are
        # strictly after other fields
        in_error_fields = False
        # there is at least one field
        last_coord_ind = 0
        for ind, field in enumerate(field_names):
            if field.startswith("error_"):
                in_error_fields = True
                errors.append((field, ind))
            else:
                last_coord_ind = ind
                if in_error_fields:
                    raise lena.core.LenaValueError(
                        "errors must go after coordinate fields"
                    )

        coords = set(field_names[:last_coord_ind+1])
        parsed_errors = []

        for err, ind in errors:
            err_coords = []
            for coord in coords:
                err_main = err[6:]  # all after "error_"
                if err_main == coord or err_main.startswith(coord + "_"):
                    err_coords.append(coord)
                    err_tail = err_main[len(coord)+1:]
            if not err_coords:
                raise lena.core.LenaValueError(
                    "no coordinate corresponding to {} given".format(err)
                )
            elif len(err_coords) > 1:
                raise lena.core.LenaValueError(
                    "ambiguous error " + err +\
                    " corresponding to several coordinates given"
                )
            # "error" may be redundant, but it is explicit.
            parsed_errors.append(("error", err_coords[0], err_tail, ind))

        return parsed_errors

    def _update_context(self, context):
        """Update *context* with the properties of this graph.

        *context.error* is appended with indices of errors.
        Example subcontext for a graph with fields "E,t,error_E_low":
        {"error": {"x_low": {"index": 2}}}.
        Note that error names are called "x", "y" and "z"
        (this corresponds to first three coordinates,
        if they are present), which allows to simplify plotting.
        Existing values are not removed
        from *context.value* and its subcontexts.

        Called on "destruction" of the graph (for example,
        in :class:`.ToCSV`). By destruction we mean conversion
        to another structure (like text) in the flow.
        The graph object is not really destroyed in this process.
        """
        # this method is private, because we encourage users to yield
        # graphs into the flow and process them with ToCSV element
        # (not manually).

        if not self._parsed_error_names:
            # no error fields present
            return

        dim = self.dim

        xyz_coord_names = self._coord_names[:3]
        for name, coord_name in zip(["x", "y", "z"], xyz_coord_names):
            for err in self._parsed_error_names:
                if err[1] == coord_name:
                    error_ind = err[3]
                    if err[2]:
                        # add error suffix
                        error_name = name + "_" + err[2]
                    else:
                        error_name = name
                    lena.context.update_recursively(
                        context,
                        "error.{}.index".format(error_name),
                        # error can correspond both to variable and
                        # value, so we put it outside value.
                        # "value.error.{}.index".format(error_name),
                        error_ind
                    )

    # emulating numeric types
    def __add__(self, other):
        """Add last (highest) coordinates of two graphs.

        A new graph is returned. Error fields are ignored.
        """
        # todo: make it method add(.., calculate_error=...)
        if not isinstance(other, graph):
            return NotImplemented
        # but their errors may be different
        assert self.dim == other.dim
        dim = self.dim
        # copied from scale
        last_coord_ind = self.dim - 1
        last_coord_name = self.field_names[last_coord_ind]

        last_coord_indices = (
            [last_coord_ind] + self._get_err_indices(last_coord_name)
        )

        all_same = all(((len(self.coords[i]) == len(other.coords[i]))
                        for i in range(dim - 1)))
        assert all_same
        new_coords = [copy.copy(self.coords[i]) for i in range(dim - 1)]
        new_vals = [
            self.coords[last_coord_ind][i] + other.coords[last_coord_ind][i]
            for i in range(len(self.coords[last_coord_ind]))
        ]
        # add can't use zipped values
        # new_vals = list(map(operator.add, zip(self.coords[last_coord_ind], 
        #                                       other.coords[last_coord_ind])))
        new_coords.append(new_vals)
        try:
            scale0 = self.scale()
            scale1 = other.scale()
        except lena.core.LenaValueError:
            scale = None
        else:
            if scale0 is not None and scale1 is not None:
                scale = scale0 + scale1
            else:
                scale = None
        return graph(coords=new_coords, field_names=self.field_names,
                     scale=scale)

        # for ind, arr in enumerate(self.coords):
        #     if ind in last_coord_indices:
        #         self.coords[ind] = list(map(partial(mul, rescale),
        #                                     arr))


# used in deprecated Graph
def _rescale_value(rescale, value):
    return rescale * lena.flow.get_data(value)


class Graph(object):
    """
    .. deprecated:: 0.5
       use :class:`graph`.
       This class may be used in the future,
       but with a changed interface.

    Function at given coordinates (arbitraty dimensions).

    Graph points can be set during the initialization and
    during :meth:`fill`. It can be rescaled (producing a new :class:`Graph`).
    A point is a tuple of *(coordinate, value)*, where both *coordinate*
    and *value* can be tuples of numbers.
    *Coordinate* corresponds to a point in N-dimensional space,
    while *value* is some function's value at this point
    (the function can take a value in M-dimensional space).
    Coordinate and value dimensions must be the same for all points.

    One can get graph points as :attr:`Graph.points` attribute.
    They will be sorted each time before return
    if *sort* was set to ``True``.
    An attempt to change points
    (use :attr:`Graph.points` on the left of '=')
    will raise Python's :exc:`AttributeError`.
    """

    def __init__(self, points=None, context=None, scale=None, sort=True):
        """*points* is an array of *(coordinate, value)* tuples.

        *context* is the same as the most recent context
        during *fill*. Use it to provide a context
        when initializing a :class:`Graph` from existing points.

        *scale* sets the scale of the graph.
        It is used during plotting if rescaling is needed.

        Graph coordinates are sorted by default.
        This is usually needed to plot graphs of functions.
        If you need to keep the order of insertion, set *sort* to ``False``.

        By default, sorting is done using standard Python
        lists and functions. You can disable *sort* and provide your own
        sorting container for *points*.
        Some implementations are compared
        `here <http://www.grantjenks.com/docs/sortedcontainers/performance.html>`_.
        Note that a rescaled graph uses a default list.

        Note that :class:`Graph` does not reduce data.
        All filled values will be stored in it.
        To reduce data, use histograms.
        """
        warnings.warn("Graph is deprecated since Lena 0.5. Use graph.",
                      DeprecationWarning, stacklevel=2)

        self._points = points if points is not None else []
        # todo: add some sanity checks for points
        self._scale = scale
        self._init_context = {"scale": scale}
        if context is None:
            self._cur_context = {}
        elif not isinstance(context, dict):
            raise lena.core.LenaTypeError(
                "context must be a dict, {} provided".format(context)
            )
        else:
            self._cur_context = context
        self._sort = sort

        # todo: probably, scale from context is not needed.

        ## probably this function is not needed.
        ## it can't be copied, graphs won't be possible to compare.
        # *rescale_value* is a function, which can be used to scale
        # complex graph values.
        # It must accept a rescale parameter and the value at a data point.
        # By default, it is multiplication of rescale and the value
        # (which must be a number).
        # if rescale_value is None:
        #     self._rescale_value = _rescale_value
        self._rescale_value = _rescale_value
        self._update()

    def fill(self, value):
        """Fill the graph with *value*.

        *Value* can be a *(data, context)* tuple.
        *Data* part must be a *(coordinates, value)* pair,
        where both coordinates and value are also tuples.
        For example, *value* can contain the principal number
        and its precision.
        """
        point, self._cur_context = lena.flow.get_data_context(value)
        # coords, val = point
        self._points.append(point)

    def request(self):
        """Yield graph with context.

        If *sort* was initialized ``True``, graph points will be sorted.
        """
        # If flow contained *scale* it the context, it is set now.
        self._update()
        yield (self, self._context)

    # compute method shouldn't be in this class,
    # because it is a pure FillRequest.
    # def compute(self):
    #     """Yield graph with context (as in :meth:`request`),
    #     and :meth:`reset`."""
    #     self._update()
    #     yield (self, self._context)
    #     self.reset()

    @property
    def points(self):
        """Get graph points (read only)."""
        # sort points before giving them
        self._update()
        return self._points

    def reset(self):
        """Reset points to an empty list
        and current context to an empty dict.
        """
        self._points = []
        self._cur_context = {}

    def __repr__(self):
        self._update()
        return ("Graph(points={}, scale={}, sort={})"
                .format(self._points, self._scale, self._sort))

    def scale(self, other=None):
        """Get or set the scale.

        Graph's scale comes from an external source.
        For example, if the graph was computed from a function,
        this may be its integral passed via context during :meth:`fill`.
        Once the scale is set, it is stored in the graph.
        If one attempts to use scale which was not set,
        :exc:`.LenaAttributeError` is raised.

        If *other* is None, return the scale.

        If a ``float`` *other* is provided, rescale to *other*.
        A new graph with the scale equal to *other*
        is returned, the original one remains unchanged.
        Note that in this case its *points* will be a simple list
        and new graph *sort* parameter will be ``True``.

        Graphs with scale equal to zero can't be rescaled. 
        Attempts to do that raise :exc:`.LenaValueError`.
        """
        if other is None:
            # return scale
            self._update()
            if self._scale is None:
                raise lena.core.LenaAttributeError(
                    "scale must be explicitly set before using that"
                )
            return self._scale
        else:
            # rescale from other
            scale = self.scale()
            if scale == 0:
                raise lena.core.LenaValueError(
                    "can't rescale graph with 0 scale"
                )

            # new_init_context = copy.deepcopy(self._init_context)
            # new_init_context.update({"scale": other})

            rescale = float(other) / scale
            new_points = []
            for coord, val in self._points:
                # probably not needed, because tuples are immutable:
                # make a deep copy so that new values
                # are completely independent from old ones.
                new_points.append((coord, self._rescale_value(rescale, val)))
            # todo: should it inherit context?
            # Probably yes, but watch out scale.
            new_graph = Graph(points=new_points, scale=other,
                              sort=self._sort)
            return new_graph

    def to_csv(self, separator=",", header=None):
        """.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.
              Iterables are converted to tables.

        Convert graph's points to CSV.

        *separator* delimits values, the default is comma.

        *header*, if not ``None``, is the first string of the output
        (new line is added automatically).

        Since a graph can be multidimensional,
        for each point first its coordinate is converted to string
        (separated by *separator*), then each part of its value.

        To convert :class:`Graph` to CSV inside a Lena sequence,
        use :class:`lena.output.ToCSV`.
        """
        if self._sort:
            self._update()

        def unpack_pt(pt):
            coord = pt[0]
            value = pt[1]
            if isinstance(coord, tuple):
                unpacked = list(coord)
            else:
                unpacked = [coord]
            if isinstance(value, tuple):
                unpacked += list(value)
            else:
                unpacked.append(value)
            return unpacked

        def pt_to_str(pt, separ):
            return separ.join([str(val) for val in unpack_pt(pt)])

        if header is not None:
            # if one needs an empty header line, they may provide ""
            lines = header + "\n"
        else:
            lines = ""
        lines += "\n".join([pt_to_str(pt, separator) for pt in self.points])

        return lines

    #     *context* will be added to graph context.
    #     If it contains "scale", :meth:`scale` method will be available.
    #     Otherwise, if "scale" is contained in the context
    #     during :meth:`fill`, it will be used.
    #     In this case it is assumed that this scale
    #     is same for all values (only the last filled context is checked).
    #     Context from flow takes precedence over the initialized one.

    def _update(self):
        """Sort points if needed, update context."""
        # todo: probably remove this context_scale?
        context_scale = self._cur_context.get("scale")
        if context_scale is not None:
            # this complex check is fine with rescale,
            # because that returns a new graph (this scale unchanged).
            if self._scale is not None and self._scale != context_scale:
                raise lena.core.LenaRuntimeError(
                    "Initialization and context scale differ, "
                    "{} and {} from context {}"
                    .format(self._scale, context_scale, self._cur_context)
                )
            self._scale = context_scale
        if self._sort:
            self._points = sorted(self._points)

        self._context = copy.deepcopy(self._cur_context)
        self._context.update(self._init_context)
        # why this? Not *graph.scale*?
        self._context.update({"scale": self._scale})
        # self._context.update(lena.context.make_context(self, "_scale"))

        # todo: make this check during fill. Probably initialize self._dim
        # with kwarg dim. (dim of coordinates or values?)
        if self._points:
            # check points correctness
            points = self._points
            def coord_dim(coord):
                if not hasattr(coord, "__len__"):
                    return 1
                return len(coord)
            first_coord = points[0][0]
            dim = coord_dim(first_coord)
            same_dim = all(coord_dim(point[0]) == dim for point in points)
            if not same_dim:
                raise lena.core.LenaValueError(
                    "coordinates tuples must have same dimension, "
                    "{} given".format(points)
                )
            self.dim = dim
            self._context["dim"] = self.dim

    def __eq__(self, other):
        if not isinstance(other, Graph):
            return False
        if self.points != other.points:
            return False
        if self._scale is None and other._scale is None:
            return True
        try:
            result = self.scale() == other.scale()
        except lena.core.LenaAttributeError:
            # one scale couldn't be computed
            return False
        else:
            return result

if __name__ == "__main__":
    isT = True
    temp_class = graph([[0, 1], [1, 2]])
    # temp_class.__dict__.update(object_class)
    temp_class._parsed_error_names = ("aaa", ["a", "b"])
    res1 = temp_class._get_err_indices("x")==[]
    res2 = temp_class._get_err_indices("y") == []
    res3 = temp_class._get_err_indices("a") == [2]
    res4 = temp_class._get_err_indices("c") == []
    if not res1 or not res2 or not res3 or not res4:
        isT=False
    # import dill
    # import os
    #
    # isT = True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena\\data_passk_platform1/62b87b839a0c4fa8b80b35cb/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena\\data_passk_platform1/62b87b839a0c4fa8b80b35cb/" + l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = graph([[0, 1], [1, 2]])
    #     # temp_class.__dict__.update(object_class)
    #     print(args1)
    #     temp_class._parsed_error_names=("aaa",["a","b"])
    #     res0 = temp_class._get_err_indices(args1)
    #     print(args1,res0)
    #     print(temp_class._get_err_indices("a"))
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/structures/graph__update_context_passk_validte.py
"""A graph is a function at given coordinates."""
import copy
import functools
import operator
import re
import warnings
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")
import lena.core
import lena.context
import lena.flow


class graph():
    """Numeric arrays of equal size."""

    def __init__(self, coords, field_names=("x", "y"), scale=None):
        """This structure generally corresponds
        to the graph of a function
        and represents arrays of coordinates and the function values
        of arbitrary dimensions.

        *coords* is a list of one-dimensional
        coordinate and value sequences (usually lists).
        There is little to no distinction between them,
        and "values" can also be called "coordinates".

        *field_names* provide the meaning of these arrays.
        For example, a 3-dimensional graph could be distinguished
        from a 2-dimensional graph with errors by its fields
        ("x", "y", "z") versus ("x", "y", "error_y").
        Field names don't affect drawing graphs:
        for that :class:`~Variable`-s should be used.
        Default field names,
        provided for the most used 2-dimensional graphs,
        are "x" and "y".

        *field_names* can be a string separated by whitespace
        and/or commas or a tuple of strings, such as ("x", "y").
        *field_names* must have as many elements
        as *coords* and each field name must be unique.
        Otherwise field names are arbitrary.
        Error fields must go after all other coordinates.
        Name of a coordinate error is "error\\_"
        appended by coordinate name. Further error details
        are appended after '_'. They could be arbitrary depending
        on the problem: "low", "high", "low_90%_cl", etc. Example:
        ("E", "time", "error_E_low", "error_time").

        *scale* of the graph is a kind of its norm. It could be
        the integral of the function or its other property.
        A scale of a normalised probability density
        function would be one.
        An initialized *scale* is required if one needs
        to renormalise the graph in :meth:`scale`
        (for example, to plot it with other graphs).

        Coordinates of a function graph would usually be arrays
        of increasing values, which is not required here.
        Neither is it checked that coordinates indeed
        contain one-dimensional numeric values.
        However, non-standard graphs
        will likely lead to errors during plotting
        and will require more programmer's work and caution,
        so use them only if you understand what you are doing.

        A graph can be iterated yielding tuples of numbers
        for each point.

        **Attributes**

        :attr:`coords` is a list \
            of one-dimensional lists of coordinates.

        :attr:`field_names`

        :attr:`dim` is the dimension of the graph,
        that is of all its coordinates without errors.

        In case of incorrect initialization arguments,
        :exc:`~.LenaTypeError` or :exc:`~.LenaValueError` is raised.

        .. versionadded:: 0.5
        """
        if not coords:
            raise lena.core.LenaValueError(
                "coords must be a non-empty sequence "
                "of coordinate sequences"
            )

        # require coords to be of the same size
        pt_len = len(coords[0])
        for arr in coords[1:]:
            if len(arr) != pt_len:
                raise lena.core.LenaValueError(
                    "coords must have subsequences of equal lengths"
                )

        # Unicode (Python 2) field names would be just bad,
        # so we don't check for it here.
        if isinstance(field_names, str):
            # split(', ') won't work.
            # From https://stackoverflow.com/a/44785447/952234:
            # \s stands for whitespace.
            field_names = tuple(re.findall(r'[^,\s]+', field_names))
        elif not isinstance(field_names, tuple):
            # todo: why field_names are a tuple,
            # while coords are a list?
            # It might be non-Pythonic to require a tuple
            # (to prohibit a list), but it's important
            # for comparisons and uniformity
            raise lena.core.LenaTypeError(
                "field_names must be a string or a tuple"
            )

        if len(field_names) != len(coords):
            raise lena.core.LenaValueError(
                "field_names must have must have the same size as coords"
            )

        if len(set(field_names)) != len(field_names):
            raise lena.core.LenaValueError(
                "field_names contains duplicates"
            )

        self.coords = coords
        self._scale = scale

        # field_names are better than fields,
        # because they are unambigous (as in namedtuple).
        self.field_names = field_names

        # decided to use "error_x_low" (like in ROOT).
        # Other versions were x_error (looked better than x_err),
        # but x_err_low looked much better than x_error_low).
        try:
            parsed_error_names = self._parse_error_names(field_names)
        except lena.core.LenaValueError as err:
            raise err
            # in Python 3
            # raise err from None
        self._parsed_error_names = parsed_error_names

        dim = len(field_names) - len(parsed_error_names)
        self._coord_names = field_names[:dim]
        self.dim = dim

        # todo: add subsequences of coords as attributes
        # with field names.
        # In case if someone wants to create a graph of another function
        # at the same coordinates.
        # Should a) work when we rescale the graph
        #        b) not interfere with other fields and methods

        # Probably we won't add methods __del__(n), __add__(*coords),
        # since it might change the scale.

    def __eq__(self, other):
        """Two graphs are equal, if and only if they have
        equal coordinates, field names and scales.

        If *other* is not a :class:`.graph`, return ``False``.

        Note that floating numbers should be compared
        approximately (using :func:`math.isclose`).
        Therefore this comparison may give false negatives.
        """
        if not isinstance(other, graph):
            # in Python comparison between different types is allowed
            return False
        return (self.coords == other.coords and self._scale == other._scale
                and self.field_names == other.field_names)

    def _get_err_indices(self, coord_name):
        """Get error indices corresponding to a coordinate."""
        err_indices = []
        dim = self.dim
        for ind, err in enumerate(self._parsed_error_names):
            if err[1] == coord_name:
                err_indices.append(ind+dim)
        return err_indices

    def __iter__(self):
        """Iterate graph coords one by one."""
        for val in zip(*self.coords):
            yield val

    def __repr__(self):
        return """graph({}, field_names={}, scale={})""".format(
            self.coords, self.field_names, self._scale
        )

    def scale(self, other=None):
        """Get or set the scale of the graph.

        If *other* is ``None``, return the scale of this graph.

        If a numeric *other* is provided, rescale to that value.
        If the graph has unknown or zero scale,
        rescaling that will raise :exc:`~.LenaValueError`.

        To get meaningful results, graph's fields are used.
        Only the last coordinate is rescaled.
        For example, if the graph has *x* and *y* coordinates,
        then *y* will be rescaled, and for a 3-dimensional graph
        *z* will be rescaled.
        All errors are rescaled together with their coordinate.
        """
        # this method is called scale() for uniformity with histograms
        # And this looks really good: explicit for computations
        # (not a subtle graph.scale, like a constant field (which is,
        #  however, the case in graph - but not in other structures))
        # and easy to remember (set_scale? rescale? change_scale_to?..)

        # We modify the graph in place,
        # because that would be redundant (not optimal)
        # to create a new graph
        # if we only want to change the scale of the existing one.

        if other is None:
            return self._scale

        if not self._scale:
            raise lena.core.LenaValueError(
                "can't rescale a graph with zero or unknown scale"
            )

        last_coord_ind = self.dim - 1
        last_coord_name = self.field_names[last_coord_ind]

        last_coord_indices = ([last_coord_ind] +
                self._get_err_indices(last_coord_name)
        )

        # In Python 2 3/2 is 1, so we want to be safe;
        # the downside is that integer-valued graphs
        # will become floating, but that is doubtfully an issue.
        # Remove when/if dropping support for Python 2.
        rescale = float(other) / self._scale

        mul = operator.mul
        partial = functools.partial

        # a version with lambda is about 50% slower:
        # timeit.timeit('[*map(lambda val: val*2, vals)]', \
        #     setup='vals = list(range(45)); from operator import mul; \
        #     from functools import partial')
        # 3.159
        # same setup for
        # timeit.timeit('[*map(partial(mul, 2), vals)]',...):
        # 2.075
        # 
        # [*map(...)] is very slightly faster than list(map(...)),
        # but it's unavailable in Python 2 (and anyway less readable).

        # rescale arrays of values and errors
        for ind, arr in enumerate(self.coords):
            if ind in last_coord_indices:
                # Python lists are faster than arrays,
                # https://stackoverflow.com/a/62399645/952234
                # (because each time taking a value from an array
                #  creates a Python object)
                self.coords[ind] = list(map(partial(mul, rescale),
                                            arr))

        self._scale = other

        # as suggested in PEP 8
        return None

    def _parse_error_names(self, field_names):
        # field_names is a parameter for easier testing,
        # usually object's field_names are used.
        errors = []

        # collect all error fields and check that they are
        # strictly after other fields
        in_error_fields = False
        # there is at least one field
        last_coord_ind = 0
        for ind, field in enumerate(field_names):
            if field.startswith("error_"):
                in_error_fields = True
                errors.append((field, ind))
            else:
                last_coord_ind = ind
                if in_error_fields:
                    raise lena.core.LenaValueError(
                        "errors must go after coordinate fields"
                    )

        coords = set(field_names[:last_coord_ind+1])
        parsed_errors = []

        for err, ind in errors:
            err_coords = []
            for coord in coords:
                err_main = err[6:]  # all after "error_"
                if err_main == coord or err_main.startswith(coord + "_"):
                    err_coords.append(coord)
                    err_tail = err_main[len(coord)+1:]
            if not err_coords:
                raise lena.core.LenaValueError(
                    "no coordinate corresponding to {} given".format(err)
                )
            elif len(err_coords) > 1:
                raise lena.core.LenaValueError(
                    "ambiguous error " + err +\
                    " corresponding to several coordinates given"
                )
            # "error" may be redundant, but it is explicit.
            parsed_errors.append(("error", err_coords[0], err_tail, ind))

        return parsed_errors

    def _update_context(self, context):
        """Update *context* with the properties of this graph.

        *context.error* is appended with indices of errors.
        Example subcontext for a graph with fields "E,t,error_E_low":
        {"error": {"x_low": {"index": 2}}}.
        Note that error names are called "x", "y" and "z"
        (this corresponds to first three coordinates,
        if they are present), which allows to simplify plotting.
        Existing values are not removed
        from *context.value* and its subcontexts.

        Called on "destruction" of the graph (for example,
        in :class:`.ToCSV`). By destruction we mean conversion
        to another structure (like text) in the flow.
        The graph object is not really destroyed in this process.
        """
        # this method is private, because we encourage users to yield
        # graphs into the flow and process them with ToCSV element
        # (not manually).

        if not self._parsed_error_names:
            # no error fields present
            return

        dim = self.dim

        xyz_coord_names = self._coord_names[:3]
        for name, coord_name in zip(["x", "y", "z"], xyz_coord_names):
            for err in self._parsed_error_names:
                if err[1] == coord_name:
                    error_ind = err[3]
                    if err[2]:
                        # add error suffix
                        error_name = name + "_" + err[2]
                    else:
                        error_name = name
                    lena.context.update_recursively(
                        context,
                        "error.{}.index".format(error_name),
                        # error can correspond both to variable and
                        # value, so we put it outside value.
                        # "value.error.{}.index".format(error_name),
                        error_ind
                    )

    # emulating numeric types
    def __add__(self, other):
        """Add last (highest) coordinates of two graphs.

        A new graph is returned. Error fields are ignored.
        """
        # todo: make it method add(.., calculate_error=...)
        if not isinstance(other, graph):
            return NotImplemented
        # but their errors may be different
        assert self.dim == other.dim
        dim = self.dim
        # copied from scale
        last_coord_ind = self.dim - 1
        last_coord_name = self.field_names[last_coord_ind]

        last_coord_indices = (
            [last_coord_ind] + self._get_err_indices(last_coord_name)
        )

        all_same = all(((len(self.coords[i]) == len(other.coords[i]))
                        for i in range(dim - 1)))
        assert all_same
        new_coords = [copy.copy(self.coords[i]) for i in range(dim - 1)]
        new_vals = [
            self.coords[last_coord_ind][i] + other.coords[last_coord_ind][i]
            for i in range(len(self.coords[last_coord_ind]))
        ]
        # add can't use zipped values
        # new_vals = list(map(operator.add, zip(self.coords[last_coord_ind], 
        #                                       other.coords[last_coord_ind])))
        new_coords.append(new_vals)
        try:
            scale0 = self.scale()
            scale1 = other.scale()
        except lena.core.LenaValueError:
            scale = None
        else:
            if scale0 is not None and scale1 is not None:
                scale = scale0 + scale1
            else:
                scale = None
        return graph(coords=new_coords, field_names=self.field_names,
                     scale=scale)

        # for ind, arr in enumerate(self.coords):
        #     if ind in last_coord_indices:
        #         self.coords[ind] = list(map(partial(mul, rescale),
        #                                     arr))


# used in deprecated Graph
def _rescale_value(rescale, value):
    return rescale * lena.flow.get_data(value)


class Graph(object):
    """
    .. deprecated:: 0.5
       use :class:`graph`.
       This class may be used in the future,
       but with a changed interface.

    Function at given coordinates (arbitraty dimensions).

    Graph points can be set during the initialization and
    during :meth:`fill`. It can be rescaled (producing a new :class:`Graph`).
    A point is a tuple of *(coordinate, value)*, where both *coordinate*
    and *value* can be tuples of numbers.
    *Coordinate* corresponds to a point in N-dimensional space,
    while *value* is some function's value at this point
    (the function can take a value in M-dimensional space).
    Coordinate and value dimensions must be the same for all points.

    One can get graph points as :attr:`Graph.points` attribute.
    They will be sorted each time before return
    if *sort* was set to ``True``.
    An attempt to change points
    (use :attr:`Graph.points` on the left of '=')
    will raise Python's :exc:`AttributeError`.
    """

    def __init__(self, points=None, context=None, scale=None, sort=True):
        """*points* is an array of *(coordinate, value)* tuples.

        *context* is the same as the most recent context
        during *fill*. Use it to provide a context
        when initializing a :class:`Graph` from existing points.

        *scale* sets the scale of the graph.
        It is used during plotting if rescaling is needed.

        Graph coordinates are sorted by default.
        This is usually needed to plot graphs of functions.
        If you need to keep the order of insertion, set *sort* to ``False``.

        By default, sorting is done using standard Python
        lists and functions. You can disable *sort* and provide your own
        sorting container for *points*.
        Some implementations are compared
        `here <http://www.grantjenks.com/docs/sortedcontainers/performance.html>`_.
        Note that a rescaled graph uses a default list.

        Note that :class:`Graph` does not reduce data.
        All filled values will be stored in it.
        To reduce data, use histograms.
        """
        warnings.warn("Graph is deprecated since Lena 0.5. Use graph.",
                      DeprecationWarning, stacklevel=2)

        self._points = points if points is not None else []
        # todo: add some sanity checks for points
        self._scale = scale
        self._init_context = {"scale": scale}
        if context is None:
            self._cur_context = {}
        elif not isinstance(context, dict):
            raise lena.core.LenaTypeError(
                "context must be a dict, {} provided".format(context)
            )
        else:
            self._cur_context = context
        self._sort = sort

        # todo: probably, scale from context is not needed.

        ## probably this function is not needed.
        ## it can't be copied, graphs won't be possible to compare.
        # *rescale_value* is a function, which can be used to scale
        # complex graph values.
        # It must accept a rescale parameter and the value at a data point.
        # By default, it is multiplication of rescale and the value
        # (which must be a number).
        # if rescale_value is None:
        #     self._rescale_value = _rescale_value
        self._rescale_value = _rescale_value
        self._update()

    def fill(self, value):
        """Fill the graph with *value*.

        *Value* can be a *(data, context)* tuple.
        *Data* part must be a *(coordinates, value)* pair,
        where both coordinates and value are also tuples.
        For example, *value* can contain the principal number
        and its precision.
        """
        point, self._cur_context = lena.flow.get_data_context(value)
        # coords, val = point
        self._points.append(point)

    def request(self):
        """Yield graph with context.

        If *sort* was initialized ``True``, graph points will be sorted.
        """
        # If flow contained *scale* it the context, it is set now.
        self._update()
        yield (self, self._context)

    # compute method shouldn't be in this class,
    # because it is a pure FillRequest.
    # def compute(self):
    #     """Yield graph with context (as in :meth:`request`),
    #     and :meth:`reset`."""
    #     self._update()
    #     yield (self, self._context)
    #     self.reset()

    @property
    def points(self):
        """Get graph points (read only)."""
        # sort points before giving them
        self._update()
        return self._points

    def reset(self):
        """Reset points to an empty list
        and current context to an empty dict.
        """
        self._points = []
        self._cur_context = {}

    def __repr__(self):
        self._update()
        return ("Graph(points={}, scale={}, sort={})"
                .format(self._points, self._scale, self._sort))

    def scale(self, other=None):
        """Get or set the scale.

        Graph's scale comes from an external source.
        For example, if the graph was computed from a function,
        this may be its integral passed via context during :meth:`fill`.
        Once the scale is set, it is stored in the graph.
        If one attempts to use scale which was not set,
        :exc:`.LenaAttributeError` is raised.

        If *other* is None, return the scale.

        If a ``float`` *other* is provided, rescale to *other*.
        A new graph with the scale equal to *other*
        is returned, the original one remains unchanged.
        Note that in this case its *points* will be a simple list
        and new graph *sort* parameter will be ``True``.

        Graphs with scale equal to zero can't be rescaled. 
        Attempts to do that raise :exc:`.LenaValueError`.
        """
        if other is None:
            # return scale
            self._update()
            if self._scale is None:
                raise lena.core.LenaAttributeError(
                    "scale must be explicitly set before using that"
                )
            return self._scale
        else:
            # rescale from other
            scale = self.scale()
            if scale == 0:
                raise lena.core.LenaValueError(
                    "can't rescale graph with 0 scale"
                )

            # new_init_context = copy.deepcopy(self._init_context)
            # new_init_context.update({"scale": other})

            rescale = float(other) / scale
            new_points = []
            for coord, val in self._points:
                # probably not needed, because tuples are immutable:
                # make a deep copy so that new values
                # are completely independent from old ones.
                new_points.append((coord, self._rescale_value(rescale, val)))
            # todo: should it inherit context?
            # Probably yes, but watch out scale.
            new_graph = Graph(points=new_points, scale=other,
                              sort=self._sort)
            return new_graph

    def to_csv(self, separator=",", header=None):
        """.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.
              Iterables are converted to tables.

        Convert graph's points to CSV.

        *separator* delimits values, the default is comma.

        *header*, if not ``None``, is the first string of the output
        (new line is added automatically).

        Since a graph can be multidimensional,
        for each point first its coordinate is converted to string
        (separated by *separator*), then each part of its value.

        To convert :class:`Graph` to CSV inside a Lena sequence,
        use :class:`lena.output.ToCSV`.
        """
        if self._sort:
            self._update()

        def unpack_pt(pt):
            coord = pt[0]
            value = pt[1]
            if isinstance(coord, tuple):
                unpacked = list(coord)
            else:
                unpacked = [coord]
            if isinstance(value, tuple):
                unpacked += list(value)
            else:
                unpacked.append(value)
            return unpacked

        def pt_to_str(pt, separ):
            return separ.join([str(val) for val in unpack_pt(pt)])

        if header is not None:
            # if one needs an empty header line, they may provide ""
            lines = header + "\n"
        else:
            lines = ""
        lines += "\n".join([pt_to_str(pt, separator) for pt in self.points])

        return lines

    #     *context* will be added to graph context.
    #     If it contains "scale", :meth:`scale` method will be available.
    #     Otherwise, if "scale" is contained in the context
    #     during :meth:`fill`, it will be used.
    #     In this case it is assumed that this scale
    #     is same for all values (only the last filled context is checked).
    #     Context from flow takes precedence over the initialized one.

    def _update(self):
        """Sort points if needed, update context."""
        # todo: probably remove this context_scale?
        context_scale = self._cur_context.get("scale")
        if context_scale is not None:
            # this complex check is fine with rescale,
            # because that returns a new graph (this scale unchanged).
            if self._scale is not None and self._scale != context_scale:
                raise lena.core.LenaRuntimeError(
                    "Initialization and context scale differ, "
                    "{} and {} from context {}"
                    .format(self._scale, context_scale, self._cur_context)
                )
            self._scale = context_scale
        if self._sort:
            self._points = sorted(self._points)

        self._context = copy.deepcopy(self._cur_context)
        self._context.update(self._init_context)
        # why this? Not *graph.scale*?
        self._context.update({"scale": self._scale})
        # self._context.update(lena.context.make_context(self, "_scale"))

        # todo: make this check during fill. Probably initialize self._dim
        # with kwarg dim. (dim of coordinates or values?)
        if self._points:
            # check points correctness
            points = self._points
            def coord_dim(coord):
                if not hasattr(coord, "__len__"):
                    return 1
                return len(coord)
            first_coord = points[0][0]
            dim = coord_dim(first_coord)
            same_dim = all(coord_dim(point[0]) == dim for point in points)
            if not same_dim:
                raise lena.core.LenaValueError(
                    "coordinates tuples must have same dimension, "
                    "{} given".format(points)
                )
            self.dim = dim
            self._context["dim"] = self.dim

    def __eq__(self, other):
        if not isinstance(other, Graph):
            return False
        if self.points != other.points:
            return False
        if self._scale is None and other._scale is None:
            return True
        try:
            result = self.scale() == other.scale()
        except lena.core.LenaAttributeError:
            # one scale couldn't be computed
            return False
        else:
            return result

if __name__ == "__main__":
    import dill
    import os

    isT = True
    temp_class = graph([[0, 1], [1, 2]])
    temp_class._parsed_error_names = ("xxxy", ["xxxx", "xxxx"])

    args1={'error': {'x_low': {'index': 2}}}

    temp_class._update_context(args1)
    if args1!={'error': {'x_low': {'index': 2}, 'x_x': {'index': 'y'}}}:
        isT=False
    # print(args1)
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena/data_passk_platform1/62b87b7e9a0c4fa8b80b35bc/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena/data_passk_platform1/62b87b7e9a0c4fa8b80b35bc/" + l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = graph([[0, 1], [1, 2]])
    #     print(args1)
    #
    #     # temp_class.__dict__.update(object_class)
    #     temp_class._update_context(args1)
    #     print(args1)
    #     # if not ( dill.dumps(args1)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/structures/hist_functions_integral_passk_validte.py
"""Functions for histograms.

These functions are used for low-level work
with histograms and their contents.
They are not needed for normal usage.
"""
import collections
import copy
import itertools
import operator
import re
import sys
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")
from lena.structures import histogram

if sys.version_info.major == 3:
    from functools import reduce as _reduce
else:
    _reduce = reduce

import lena.core
from lena.structures.graph import graph as _graph


class HistCell(collections.namedtuple("HistCell", ("edges, bin, index"))):
    """A namedtuple with fields *edges, bin, index*."""
    # from Aaron Hall's answer https://stackoverflow.com/a/28568351/952234
    __slots__ = ()


def cell_to_string(
        cell_edges, var_context=None, coord_names=None,
        coord_fmt="{}_lte_{}_lt_{}", coord_join="_", reverse=False):
    """Transform cell edges into a string.

    *cell_edges* is a tuple of pairs *(lower bound, upper bound)*
    for each coordinate.

    *coord_names* is a list of coordinates names.

    *coord_fmt* is a string,
    which defines how to format individual coordinates.

    *coord_join* is a string, which joins coordinate pairs.

    If *reverse* is True, coordinates are joined in reverse order.
    """
    # todo: do we really need var_context?
    # todo: even if so, why isn't that a {}? Is that dangerous?
    if coord_names is None:
        if var_context is None:
            coord_names = [
                "coord{}".format(ind) for ind in range(len(cell_edges))
            ]
        else:
            if "combine" in var_context:
                coord_names = [var["name"]
                               for var in var_context["combine"]]
            else:
                coord_names = [var_context["name"]]
    if len(cell_edges) != len(coord_names):
        raise lena.core.LenaValueError(
            "coord_names must have same length as cell_edges, "
            "{} and {} given".format(coord_names, cell_edges)
        )
    coord_strings = [coord_fmt.format(edge[0], coord_names[ind], edge[1])
                     for (ind, edge) in enumerate(cell_edges)]
    if reverse:
        coord_strings = reversed(coord_strings)
    coord_str = coord_join.join(coord_strings)
    return coord_str


def _check_edges_increasing_1d(arr):
    if len(arr) <= 1:
        raise lena.core.LenaValueError("size of edges should be more than one,"
                                       " {} provided".format(arr))
    increasing = (tup[0] < tup[1] for tup in zip(arr, arr[1:]))
    if not all(increasing):
        raise lena.core.LenaValueError(
            "expected strictly increasing values, "
            "{} provided".format(arr)
        )


def check_edges_increasing(edges):
    """Assure that multidimensional *edges* are increasing.

    If length of *edges* or its subarray is less than 2
    or if some subarray of *edges*
    contains not strictly increasing values,
    :exc:`.LenaValueError` is raised.
    """
    if not len(edges):
        raise lena.core.LenaValueError("edges must be non-empty")
    elif not hasattr(edges[0], '__iter__'):
        _check_edges_increasing_1d(edges)
        return
    for arr in edges:
        if len(arr) <= 1:
            raise lena.core.LenaValueError(
                "size of edges should be more than one. "
                "{} provided".format(arr)
            )
        _check_edges_increasing_1d(arr)


def get_bin_edges(index, edges):
    """Return edges of the bin for the given *edges* of a histogram.

    In one-dimensional case *index* must be an integer and a tuple
    of *(x_low_edge, x_high_edge)* for that bin is returned.

    In a multidimensional case *index* is a container of numeric indices
    in each dimension.
    A list of bin edges in each dimension is returned."""
    # todo: maybe give up this 1- and multidimensional unification
    # and write separate functions for each case.
    if not hasattr(edges[0], '__iter__'):
        # 1-dimensional edges
        if hasattr(index, '__iter__'):
            index = index[0]
        return (edges[index], edges[index+1])
    # multidimensional edges
    return [(edges[coord][i], edges[coord][i+1])
            for coord, i in enumerate(index)]


def get_bin_on_index(index, bins):
    """Return bin corresponding to multidimensional *index*.

    *index* can be a number or a list/tuple.
    If *index* length is less than dimension of *bins*,
    a subarray of *bins* is returned.

    In case of an index error, :exc:`.LenaIndexError` is raised.

    Example:

    >>> from lena.structures import histogram, get_bin_on_index
    >>> hist = histogram([0, 1], [0])
    >>> get_bin_on_index(0, hist.bins)
    0
    >>> get_bin_on_index((0, 1), [[0, 1], [0, 0]])
    1
    >>> get_bin_on_index(0, [[0, 1], [0, 0]])
    [0, 1]
    """
    if not isinstance(index, (list, tuple)):
        index = [index]
    subarr = bins
    for ind in index:
        try:
            subarr = subarr[ind]
        except IndexError:
            raise lena.core.LenaIndexError(
                "bad index: {}, bins = {}".format(index, bins)
            )
    return subarr


def get_bin_on_value_1d(val, arr):
    """Return index for value in one-dimensional array.

    *arr* must contain strictly increasing values
    (not necessarily equidistant),
    it is not checked.

    "Linear binary search" is used,
    that is our array search by default assumes
    the array to be split on equidistant steps.

    Example:

    >>> from lena.structures import get_bin_on_value_1d
    >>> arr = [0, 1, 4, 5, 7, 10]
    >>> get_bin_on_value_1d(0, arr)
    0
    >>> get_bin_on_value_1d(4.5, arr)
    2
    >>> # upper range is excluded
    >>> get_bin_on_value_1d(10, arr)
    5
    >>> # underflow
    >>> get_bin_on_value_1d(-10, arr)
    -1
    """
    # may also use numpy.searchsorted
    # https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.searchsorted.html
    ind_min = 0
    ind_max = len(arr) - 1
    while True:
        if ind_max - ind_min <= 1:
            # lower bound is close
            if val < arr[ind_min]:
                return ind_min - 1
            # upper bound is open
            elif val >= arr[ind_max]:
                return ind_max
            else:
                return ind_min
        if val == arr[ind_min]:
            return ind_min
        if val < arr[ind_min]:
            return ind_min - 1
        elif val >= arr[ind_max]:
            return ind_max
        else:
            shift = int(
                (ind_max - ind_min) * (
                    float(val - arr[ind_min]) / (arr[ind_max] - arr[ind_min])
                ))
            ind_guess = ind_min + shift

            if ind_min == ind_guess:
                ind_min += 1
                continue
            # ind_max is always more that ind_guess,
            # because val < arr[ind_max] (see the formula for shift).
            # This branch is not needed and can't be tested.
            # But for the sake of numerical inaccuracies, let us keep this
            # so that we never get into an infinite loop.
            elif ind_max == ind_guess:
                ind_max -= 1
                continue

            if val < arr[ind_guess]:
                ind_max = ind_guess
            else:
                ind_min = ind_guess


def get_bin_on_value(arg, edges):
    """Get the bin index for *arg* in a multidimensional array *edges*.

    *arg* is a 1-dimensional array of numbers
    (or a number for 1-dimensional *edges*),
    and corresponds to a point in N-dimensional space.

    *edges* is an array of N-1 dimensional arrays (lists or tuples) of numbers.
    Each 1-dimensional subarray consists of increasing numbers.

    *arg* and *edges* must have the same length
    (otherwise :exc:`.LenaValueError` is raised).
    *arg* and *edges* must be iterable and support *len()*.

    Return list of indices in *edges* corresponding to *arg*.

    If any coordinate is out of its corresponding edge range,
    its index will be ``-1`` for underflow
    or ``len(edge)-1`` for overflow.

    Examples:

    >>> from lena.structures import get_bin_on_value
    >>> edges = [[1, 2, 3], [1, 3.5]]
    >>> get_bin_on_value((1.5, 2), edges)
    [0, 0]
    >>> get_bin_on_value((1.5, 0), edges)
    [0, -1]
    >>> # the upper edge is excluded
    >>> get_bin_on_value((3, 2), edges)
    [2, 0]
    >>> # one-dimensional edges
    >>> edges = [1, 2, 3]
    >>> get_bin_on_value(2, edges)
    [1]
    """
    # arg is a one-dimensional index
    if not isinstance(arg, (tuple, list)):
        return [get_bin_on_value_1d(arg, edges)]
    # arg is a multidimensional index
    if len(arg) != len(edges):
        raise lena.core.LenaValueError(
            "argument should have same dimension as edges. "
            "arg = {}, edges = {}".format(arg, edges)
        )
    indices = []
    for ind, array in enumerate(edges):
        cur_bin = get_bin_on_value_1d(arg[ind], array)
        indices.append(cur_bin)
    return indices


def get_example_bin(struct):
    """Return bin with zero index on each axis of the histogram bins.

    For example, if the histogram is two-dimensional, return hist[0][0].

    *struct* can be a :class:`.histogram`
    or an array of bins.
    """
    if isinstance(struct, lena.structures.histogram):
        return lena.structures.get_bin_on_index([0] * struct.dim, struct.bins)
    else:
        bins = struct
        while isinstance(bins, list):
            bins = bins[0]
        return bins


def hist_to_graph(hist, make_value=None, get_coordinate="left",
                  field_names=("x", "y"), scale=None):
    """Convert a :class:`.histogram` to a :class:`.graph`.

    *make_value* is a function to set the value of a graph's point.
    By default it is bin content.
    *make_value* accepts a single value (bin content) without context.

    This option could be used to create graph's error bars.
    For example, to create a graph with errors
    from a histogram where bins contain
    a named tuple with fields *mean*, *mean_error* and a context
    one could use

    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)

    *get_coordinate* defines what the coordinate
    of a graph point created from a histogram bin will be.
    It can be "left" (default), "right" and "middle".

    *field_names* set field names of the graph. Their number
    must be the same as the dimension of the result.
    For a *make_value* above they would be
    *("x", "y_mean", "y_mean_error")*.

    *scale* becomes the graph's scale (unknown by default).
    If it is ``True``, it uses the histogram scale.

    *hist* must contain only numeric bins (without context)
    or *make_value* must remove context when creating a numeric graph.

    Return the resulting graph.
    """
    ## Could have allowed get_coordinate to be callable
    # (for generality), but 1) first find a use case,
    # 2) histogram bins could be adjusted in the first place.
    # -- don't understand 2.
    if get_coordinate == "left":
        get_coord = lambda edges: tuple(coord[0] for coord in edges)
    elif get_coordinate == "right":
        get_coord = lambda edges: tuple(coord[1] for coord in edges)
    # *middle* between the two edges, not the *center* of the bin
    # as a whole (because the graph corresponds to a point)
    elif get_coordinate == "middle":
        get_coord = lambda edges: tuple(0.5*(coord[0] + coord[1])
                                        for coord in edges)
    else:
        raise lena.core.LenaValueError(
            'get_coordinate must be one of "left", "right" or "middle"; '
            '"{}" provided'.format(get_coordinate)
        )

    # todo: make_value may be bad design.
    # Maybe allow to change the graph in the sequence.
    # However, make_value allows not to recreate a graph
    # or its coordinates (if that is not needed).

    if isinstance(field_names, str):
        # copied from graph.__init__
        field_names = tuple(re.findall(r'[^,\s]+', field_names))
    elif not isinstance(field_names, tuple):
        raise lena.core.LenaTypeError(
            "field_names must be a string or a tuple"
        )
    coords = [[] for _ in field_names]

    chain = itertools.chain

    if scale is True:
        scale = hist.scale()

    for value, edges in iter_bins_with_edges(hist.bins, hist.edges):
        coord = get_coord(edges)

        # Since we never use contexts here, it will be optimal
        # to ignore them completely (remove them elsewhere).
        # bin_value = lena.flow.get_data(value)
        bin_value = value

        if make_value is None:
            graph_value = bin_value
        else:
            graph_value = make_value(bin_value)

        # for iteration below
        if not hasattr(graph_value, "__iter__"):
            graph_value = (graph_value,)

        # add each coordinate to respective array
        for arr, coord_ in zip(coords, chain(coord, graph_value)):
            arr.append(coord_)

    return _graph(coords, field_names=field_names, scale=scale)


def init_bins(edges, value=0, deepcopy=False):
    """Initialize cells of the form *edges* with the given *value*.

    Return bins filled with copies of *value*.

    *Value* must be copyable, usual numbers will suit.
    If the value is mutable, use *deepcopy =* ``True``
    (or the content of cells will be identical).

    Examples:

    >>> edges = [[0, 1], [0, 1]]
    >>> # one cell
    >>> init_bins(edges)
    [[0]]
    >>> # no need to use floats,
    >>> # because integers will automatically be cast to floats
    >>> # when used together
    >>> init_bins(edges, 0.0)
    [[0.0]]
    >>> init_bins([[0, 1, 2], [0, 1, 2]])
    [[0, 0], [0, 0]]
    >>> init_bins([0, 1, 2])
    [0, 0]
    """
    nbins = len(edges) - 1
    if not isinstance(edges[0], (list, tuple)):
        # edges is one-dimensional
        if deepcopy:
            return [copy.deepcopy(value) for _ in range(nbins)]
        else:
            return [value] * nbins
    for ind, arr in enumerate(edges):
        if ind == nbins:
            if deepcopy:
                return [copy.deepcopy(value) for _ in range(len(arr)-1)]
            else:
                return list([value] * (len(arr)-1))
        bins = []
        for _ in range(len(arr)-1):
            bins.append(init_bins(edges[ind+1:], value, deepcopy))
        return bins


def integral(bins, edges):
    """Compute integral (scale for a histogram).

    *bins* contain values, and *edges* form the mesh
    for the integration.
    Their format is defined in :class:`.histogram` description.
    """
    total = 0
    for ind, bin_content in iter_bins(bins):
        bin_lengths = [
            edges[coord][i+1] - edges[coord][i]
            for coord, i in enumerate(ind)
        ]
        # product
        vol = _reduce(operator.mul, bin_lengths, 1)
        cell_integral = vol * bin_content
        total += cell_integral
    return total


def iter_bins(bins):
    """Iterate on *bins*. Yield *(index, bin content)*.

    Edges with higher index are iterated first
    (that is z, then y, then x for a 3-dimensional histogram).
    """
    # if not isinstance(bins, (list, tuple)):
    if not hasattr(bins, '__iter__'):
        # cell
        yield ((), bins)
    else:
        for ind, _ in enumerate(bins):
            for sub_ind, val in iter_bins(bins[ind]):
                yield (((ind,) + sub_ind), val)


def iter_bins_with_edges(bins, edges):
    """Generate *(bin content, bin edges)* pairs.

    Bin edges is a tuple, such that
    its item at index i is *(lower bound, upper bound)*
    of the bin at i-th coordinate.

    Examples:

    >>> from lena.math import mesh
    >>> list(iter_bins_with_edges([0, 1, 2], edges=mesh((0, 3), 3)))
    [(0, ((0, 1.0),)), (1, ((1.0, 2.0),)), (2, ((2.0, 3),))]
    >>>
    >>> # 2-dimensional histogram
    >>> list(iter_bins_with_edges(
    ...     bins=[[2]], edges=mesh(((0, 1), (0, 1)), (1, 1))
    ... ))
    [(2, ((0, 1), (0, 1)))]

    .. versionadded:: 0.5
       made public.
    """
    # todo: only a list or also a tuple, an array?
    if not isinstance(edges[0], list):
        edges = [edges]
    bins_sizes = [len(edge)-1 for edge in edges]
    indices = [list(range(nbins)) for nbins in bins_sizes]
    for index in itertools.product(*indices):
        bin_ = lena.structures.get_bin_on_index(index, bins)
        edges_low = []
        edges_high = []
        for var, var_ind in enumerate(index):
            edges_low.append(edges[var][var_ind])
            edges_high.append(edges[var][var_ind+1])
        yield (bin_, tuple(zip(edges_low, edges_high)))


def iter_cells(hist, ranges=None, coord_ranges=None):
    """Iterate cells of a histogram *hist*, possibly in a subrange.

    For each bin, yield a :class:`HistCell`
    containing *bin edges, bin content* and *bin index*.
    The order of iteration is the same as for :func:`iter_bins`.

    *ranges* are the ranges of bin indices to be used
    for each coordinate
    (the lower value is included, the upper value is excluded).

    *coord_ranges* set real coordinate ranges based on histogram edges.
    Obviously, they can be not exactly bin edges.
    If one of the ranges for the given coordinate
    is outside the histogram edges,
    then only existing histogram edges within the range are selected.
    If the coordinate range is completely outside histogram edges,
    nothing is yielded.
    If a lower or upper *coord_range*
    falls within a bin, this bin is yielded.
    Note that if a coordinate range falls on a bin edge,
    the number of generated bins can be unstable
    because of limited float precision.

    *ranges* and *coord_ranges* are tuples of tuples of limits
    in corresponding dimensions. 
    For one-dimensional histogram it must be a tuple 
    containing a tuple, for example
    *((None, None),)*.

    ``None`` as an upper or lower *range* means no limit
    (*((None, None),)* is equivalent to *((0, len(bins)),)*
    for a 1-dimensional histogram).

    If a *range* index is lower than 0 or higher than possible index,
    :exc:`.LenaValueError` is raised.
    If both *coord_ranges* and *ranges* are provided,
    :exc:`.LenaTypeError` is raised.
    """
    # for bin_ind, bin_ in iter_bins(hist.bins):
    #     yield HistCell(get_bin_edges(bin_ind, hist.edges), bin_, bin_ind)
    # if bins and edges are calculated each time, save the result now
    bins, edges = hist.bins, hist.edges
    # todo: hist.edges must be same
    # for 1- and multidimensional histograms.
    if hist.dim == 1:
        edges = (edges,)

    if coord_ranges is not None:
        if ranges is not None:
            raise lena.core.LenaTypeError(
                "only ranges or coord_ranges can be provided, not both"
            )
        ranges = []
        if not isinstance(coord_ranges[0], (tuple, list)):
            coord_ranges = (coord_ranges, )
        for coord, coord_range in enumerate(coord_ranges):
            # todo: (dis?)allow None as an infinite range.
            # todo: raise or transpose unordered coordinates?
            # todo: change the order of function arguments.
            lower_bin_ind = get_bin_on_value_1d(coord_range[0], edges[coord])
            if lower_bin_ind == -1:
                 lower_bin_ind = 0
            upper_bin_ind = get_bin_on_value_1d(coord_range[1], edges[coord])
            max_ind = len(edges[coord])
            if upper_bin_ind == max_ind:
                 upper_bin_ind -= 1
            if lower_bin_ind >= max_ind or upper_bin_ind <= 0:
                 # histogram edges are outside the range.
                 return
            ranges.append((lower_bin_ind, upper_bin_ind))

    if not ranges:
        ranges = ((None, None),) * hist.dim

    real_ind_ranges = []
    for coord, coord_range in enumerate(ranges):
        low, up = coord_range
        if low is None:
            low = 0
        else:
            # negative indices should not be supported
            if low < 0:
                raise lena.core.LenaValueError(
                    "low must be not less than 0 if provided"
                )
        max_ind = len(edges[coord]) - 1
        if up is None:
            up = max_ind
        else:
            # huge indices should not be supported as well.
            if up > max_ind:
                raise lena.core.LenaValueError(
                    "up must not be greater than len(edges)-1, if provided"
                )
        real_ind_ranges.append(list(range(low, up)))

    indices = list(itertools.product(*real_ind_ranges))
    for ind in indices:
        yield HistCell(get_bin_edges(ind, edges),
                       get_bin_on_index(ind, bins),
                       ind)


def make_hist_context(hist, context):
    """Update a deep copy of *context* with the context
    of a :class:`.histogram` *hist*.

    .. deprecated:: 0.5
       histogram context is updated automatically
       during conversion in :class:`~.output.ToCSV`.
       Use histogram._update_context explicitly if needed.
    """
    # absolutely unnecessary.
    context = copy.deepcopy(context)

    hist_context = {
        "histogram": {
            "dim": hist.dim,
            "nbins": hist.nbins,
            "ranges": hist.ranges
        }
    }
    context.update(hist_context)
    # just bad.
    return context


def unify_1_md(bins, edges):
    """Unify 1- and multidimensional bins and edges.

    Return a tuple of *(bins, edges)*.  
    Bins and multidimensional *edges* return unchanged,
    while one-dimensional *edges* are inserted into a list.
    """
    if hasattr(edges[0], '__iter__'):
    # if isinstance(edges[0], (list, tuple)):
        return (bins, edges)
    else:
        return (bins, [edges])

if __name__ == "__main__":
    hist = histogram([[1, 2, 3], [1, 2, 3], [1, 2, 3]])
    hist.fill([1, 1, 1])
    hist.fill([1, 1, 2])
    isT=integral(hist.bins, hist.edges) == 2
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena\\data_passk_platform1/62b87b4f9a0c4fa8b80b3580/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena/data_passk_platform1/62b87b4f9a0c4fa8b80b3580/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     print(args0,args1)
    #     res0 = integral(args0,args1)
    #     print(res0)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/core/check_sequence_type_is_fill_request_seq_passk_validte.py
"""Check whether a sequence can be converted to a Lena Sequence."""
# otherwise import errors arise
# from . import source
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")
from lena.math import Sum


def is_fill_compute_el(obj):
    """Object contains executable methods 'fill' and 'compute'."""
    return (hasattr(obj, "fill")
            and hasattr(obj, "compute")
            and callable(obj.fill)
            and callable(obj.compute))


def is_fill_compute_seq(seq):
    """Test whether *seq* can be converted to a FillComputeSeq.

    True only if it is a FillCompute element
    or contains at least one such,
    and it is not a Source sequence.
    """
    if is_source(seq):
        return False
    is_fcseq = False
    try:
        is_fcseq = any(map(is_fill_compute_el, seq))
    except TypeError:
        # seq is non-iterable
        pass
    if is_fill_compute_el(seq):
        is_fcseq = True
    return is_fcseq


def is_fill_request_el(obj):
    """Object contains executable methods 'fill' and 'request'."""
    return hasattr(obj, "fill") and hasattr(obj, "request") \
            and callable(obj.fill) and callable(obj.request)


def is_fill_request_seq(seq):
    """Test whether *seq* can be converted to a FillRequestSeq.

    True only if it is a FillRequest element
    or contains at least one such,
    and it is not a Source sequence.
    """
    if is_source(seq):
        return False
    is_fcseq = False
    if hasattr(seq, "__iter__"):
        is_fcseq = any(map(is_fill_request_el, seq))
    if is_fill_request_el(seq):
        is_fcseq = True
    return is_fcseq


def is_run_el(obj):
    """Object contains executable method 'run'."""
    return hasattr(obj, "run") and callable(obj.run)


def is_source(seq):
    """Sequence is a Source, if and only if its type is Source."""
    # Otherwise lambdas would be counted as Source,
    # but they must be converted to Sequences.
    # Moreover: this makes Source elements explicit and visible in code.
    from lena.core import source
    return isinstance(seq, source.Source)

if __name__ == "__main__":
    isT=True
    from lena.core.adapters import FillRequest
    from lena.math.elements import Sum, DSum, Mean
    from lena.core.source import Source
    from lena.flow import CountFrom

    res1 = is_fill_request_seq(Sum())
    res2 = is_fill_request_seq(DSum())
    res3 = is_fill_request_seq(Mean())
    res4=is_fill_request_seq(FillRequest(Sum(), reset=False, bufsize=10, buffer_input=True))
    res5 = is_fill_request_seq(Source(CountFrom()))
    if res1 or res2 or res3 or not res4 or res5:
        isT=False

    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena/data_passk_platform1/62b87b199a0c4fa8b80b354e/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena/data_passk_platform1/62b87b199a0c4fa8b80b354e/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     print(args0)
    #     # res0 = is_fill_request_seq(args0)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/core/check_sequence_type_is_fill_request_el_passk_validte.py
"""Check whether a sequence can be converted to a Lena Sequence."""
# otherwise import errors arise
# from . import source
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")
def is_fill_compute_el(obj):
    """Object contains executable methods 'fill' and 'compute'."""
    return (hasattr(obj, "fill")
            and hasattr(obj, "compute")
            and callable(obj.fill)
            and callable(obj.compute))


def is_fill_compute_seq(seq):
    """Test whether *seq* can be converted to a FillComputeSeq.

    True only if it is a FillCompute element
    or contains at least one such,
    and it is not a Source sequence.
    """
    if is_source(seq):
        return False
    is_fcseq = False
    try:
        is_fcseq = any(map(is_fill_compute_el, seq))
    except TypeError:
        # seq is non-iterable
        pass
    if is_fill_compute_el(seq):
        is_fcseq = True
    return is_fcseq


def is_fill_request_el(obj):
    """Object contains executable methods 'fill' and 'request'."""
    return hasattr(obj, "fill") and hasattr(obj, "request") \
            and callable(obj.fill) and callable(obj.request)


def is_fill_request_seq(seq):
    """Test whether *seq* can be converted to a FillRequestSeq.

    True only if it is a FillRequest element
    or contains at least one such,
    and it is not a Source sequence.
    """
    if is_source(seq):
        return False
    is_fcseq = False
    if hasattr(seq, "__iter__"):
        is_fcseq = any(map(is_fill_request_el, seq))
    if is_fill_request_el(seq):
        is_fcseq = True
    return is_fcseq


def is_run_el(obj):
    """Object contains executable method 'run'."""
    return hasattr(obj, "run") and callable(obj.run)


def is_source(seq):
    """Sequence is a Source, if and only if its type is Source."""
    # Otherwise lambdas would be counted as Source,
    # but they must be converted to Sequences.
    # Moreover: this makes Source elements explicit and visible in code.
    from . import source
    return isinstance(seq, source.Source)

if __name__ == "__main__":
    isT = True
    from lena.math.elements import Sum, DSum, Mean
    from lena.core.source import Source
    from lena.flow import CountFrom

    res1 = is_fill_compute_el(Sum())
    res2 = is_fill_compute_el(DSum())
    res3 = is_fill_compute_el(Mean())
    res4 = is_fill_compute_el(Source(CountFrom()))
    if not res1 or not res2 or not res3 or res4:
        isT = False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87b099a0c4fa8b80b3538/"):
    #     f = open("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87b099a0c4fa8b80b3538/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = is_fill_request_el(args0)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/core/check_sequence_type_is_run_el_passk_validte.py
"""Check whether a sequence can be converted to a Lena Sequence."""
# otherwise import errors arise
# from . import source
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")

def is_fill_compute_el(obj):
    """Object contains executable methods 'fill' and 'compute'."""
    return (hasattr(obj, "fill")
            and hasattr(obj, "compute")
            and callable(obj.fill)
            and callable(obj.compute))


def is_fill_compute_seq(seq):
    """Test whether *seq* can be converted to a FillComputeSeq.

    True only if it is a FillCompute element
    or contains at least one such,
    and it is not a Source sequence.
    """
    if is_source(seq):
        return False
    is_fcseq = False
    try:
        is_fcseq = any(map(is_fill_compute_el, seq))
    except TypeError:
        # seq is non-iterable
        pass
    if is_fill_compute_el(seq):
        is_fcseq = True
    return is_fcseq


def is_fill_request_el(obj):
    """Object contains executable methods 'fill' and 'request'."""
    return hasattr(obj, "fill") and hasattr(obj, "request") \
            and callable(obj.fill) and callable(obj.request)


def is_fill_request_seq(seq):
    """Test whether *seq* can be converted to a FillRequestSeq.

    True only if it is a FillRequest element
    or contains at least one such,
    and it is not a Source sequence.
    """
    if is_source(seq):
        return False
    is_fcseq = False
    if hasattr(seq, "__iter__"):
        is_fcseq = any(map(is_fill_request_el, seq))
    if is_fill_request_el(seq):
        is_fcseq = True
    return is_fcseq


def is_run_el(obj):
    """Object contains executable method 'run'."""
    return hasattr(obj, "run") and callable(obj.run)


def is_source(seq):
    """Sequence is a Source, if and only if its type is Source."""
    # Otherwise lambdas would be counted as Source,
    # but they must be converted to Sequences.
    # Moreover: this makes Source elements explicit and visible in code.
    from lena.core import source
    return isinstance(seq, source.Source)

if __name__ == "__main__":
    isT = True
    from lena.core.adapters import FillRequest
    from lena.math.elements import Sum, DSum, Mean
    from lena.core.source import Source
    from lena.flow import CountFrom

    res1 = is_fill_request_seq(Sum())
    res2 = is_fill_request_seq(DSum())
    res3 = is_fill_request_seq(Mean())
    res4 = is_fill_request_seq(FillRequest(Sum(), reset=False, bufsize=10, buffer_input=True))
    res5 = is_fill_request_seq(Source(CountFrom()))
    if res1 or res2 or res3 or not res4 or res5:
        isT = False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87af99a0c4fa8b80b3524/"):
    #     f = open("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87af99a0c4fa8b80b3524/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = is_run_el(args0)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/core/check_sequence_type_is_fill_compute_el_passk_validte.py
"""Check whether a sequence can be converted to a Lena Sequence."""
# otherwise import errors arise
# from . import source
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")

def is_fill_compute_el(obj):
    """Object contains executable methods 'fill' and 'compute'."""
    return (hasattr(obj, "fill")
            and hasattr(obj, "compute")
            and callable(obj.fill)
            and callable(obj.compute))


def is_fill_compute_seq(seq):
    """Test whether *seq* can be converted to a FillComputeSeq.

    True only if it is a FillCompute element
    or contains at least one such,
    and it is not a Source sequence.
    """
    if is_source(seq):
        return False
    is_fcseq = False
    try:
        is_fcseq = any(map(is_fill_compute_el, seq))
    except TypeError:
        # seq is non-iterable
        pass
    if is_fill_compute_el(seq):
        is_fcseq = True
    return is_fcseq


def is_fill_request_el(obj):
    """Object contains executable methods 'fill' and 'request'."""
    return hasattr(obj, "fill") and hasattr(obj, "request") \
            and callable(obj.fill) and callable(obj.request)


def is_fill_request_seq(seq):
    """Test whether *seq* can be converted to a FillRequestSeq.

    True only if it is a FillRequest element
    or contains at least one such,
    and it is not a Source sequence.
    """
    if is_source(seq):
        return False
    is_fcseq = False
    if hasattr(seq, "__iter__"):
        is_fcseq = any(map(is_fill_request_el, seq))
    if is_fill_request_el(seq):
        is_fcseq = True
    return is_fcseq


def is_run_el(obj):
    """Object contains executable method 'run'."""
    return hasattr(obj, "run") and callable(obj.run)


def is_source(seq):
    """Sequence is a Source, if and only if its type is Source."""
    # Otherwise lambdas would be counted as Source,
    # but they must be converted to Sequences.
    # Moreover: this makes Source elements explicit and visible in code.
    from . import source
    return isinstance(seq, source.Source)

if __name__ == "__main__":
    isT=True
    from lena.math.elements import Sum,DSum,Mean
    from lena.core.source import Source
    from lena.flow import CountFrom
    res1=is_fill_compute_el(Sum())
    res2=is_fill_compute_el(DSum())
    res3=is_fill_compute_el(Mean())
    res4=is_fill_compute_el(Source(CountFrom()))
    if not res1 or not res2 or not res3 or res4:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena/data_passk_platform1/62b87af69a0c4fa8b80b351a/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena/data_passk_platform1/62b87af69a0c4fa8b80b351a/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     print(args0)
    #     # res0 = is_fill_compute_el(args0)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/context/functions_difference_passk_validte.py
"""Functions to work with context (dictionary)."""

import copy
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")
import lena.core

# pylint: disable=invalid-name
# d is a good name for dictionary,
# used in Python documentation for dict.


def contains(d, s):
    """Check that a dictionary *d* contains a subdictionary
    defined by a string *s*.

    True if *d* contains a subdictionary that is represented by *s*.
    Dots in *s* mean nested subdictionaries.
    A string without dots means a key in *d*.

    Example:

    >>> d = {'fit': {'coordinate': 'x'}}
    >>> contains(d, "fit")
    True
    >>> contains(d, "fit.coordinate.x")
    True
    >>> contains(d, "fit.coordinate.y")
    False

    If the most nested element of *d* to be compared with *s*
    is not a string, its string representation is used for comparison.
    See also :func:`str_to_dict`.
    """
    # todo: s can be a list, or a dict?
    levels = s.split(".")
    if len(levels) < 2:
        return s in d
    subdict = d
    for key in levels[:-1]:
        if key not in subdict:
            return False
        subdict = subdict[key]
    last_val = levels[-1]
    if isinstance(subdict, dict):
        return last_val in subdict
    else:
        # just a value
        try:
            # it's better to test for an object to be cast to str
            # than to disallow "dim.1"
            subd = str(subdict)
        except Exception:
            return False
        else:
            return subd == last_val


def difference(d1, d2, level=-1):
    """Return a dictionary with items from *d1* not contained in *d2*.

    *level* sets the maximum depth of recursion. For infinite recursion,
    set that to -1. For level 1,
    if a key is present both in *d1* and *d2* but has different values,
    it is included into the difference.
    See :func:`intersection` for more details.

    *d1* and *d2* remain unchanged. However, *d1* or some of its
    subdictionaries may be returned directly.
    Make a deep copy of the result when appropriate.

    .. versionadded:: 0.5
       add keyword argument *level*.
    """
    # can become not dicts during the recursion
    if not isinstance(d1, dict) or not isinstance(d2, dict):
        return d1

    if d1 == d2:
        return {}
    elif level == 0:
        return d1

    # some keys differ
    result = {}
    for key in d1:
        if key not in d2:
            result[key] = d1[key]
        elif d1[key] != d2[key]:
            res = difference(d1[key], d2[key], level-1)
            # if d2[key] contains all d1[key] elements,
            # the difference will be empty
            if res:
                result[key] = res
    return result


def format_context(format_str):
    """Create a function that formats a given string using a context.

    It is recommended to use jinja2.Template.
    Use this function only if you don't have jinja2.

    *format_str* is a Python format string with double braces
    instead of single ones.
    It must contain all non-empty replacement fields,
    and only simplest formatting without attribute lookup.
    Example:

    >>> f = format_context("{{x}}")
    >>> f({"x": 10})
    '10'

    When calling *format_context*, arguments are bound and
    a new function is returned. When called with a context,
    its keys are extracted and formatted in *format_str*.

    Keys can be nested using a dot, for example:

    >>> f = format_context("{{x.y}}_{{z}}")
    >>> f({"x": {"y": 10}, "z": 1})
    '10_1'

    This function does not work with unbalanced braces.
    If a simple check fails, :exc:`.LenaValueError` is raised.
    If *format_str* is not a string, :exc:`.LenaTypeError` is raised.
    All other errors are raised only during formatting.
    If context doesn't contain the needed key,
    :exc:`.LenaKeyError` is raised.
    Note that string formatting can also raise a :exc:`ValueError`,
    so it is recommended to test your formatters before using them.
    """
    if not isinstance(format_str, str):
        raise lena.core.LenaTypeError(
            "format_str must be a string, {} given".format(format_str)
        )

    # prohibit single or unbalanced braces
    if format_str.count('{') != format_str.count('}'):
        raise lena.core.LenaValueError("unbalanced braces in '{}'".format(format_str))
    if '{' in format_str and not '{{' in format_str:
        raise lena.core.LenaValueError(
            "double braces must be used for formatting instead of '{}'"
            .format(format_str)
        )

    # new format: now double braces instead of single ones.
    # but the algorithm may be left unchanged.
    format_str = format_str.replace("{{", "{").replace("}}", "}")
    new_str = []
    new_args = []
    prev_char = ''
    ind = 0
    within_field = False
    while ind < len(format_str):
        c = format_str[ind]
        if c != '{' and not within_field:
            prev_char = c
            new_str.append(c)
            ind += 1
            continue
        while c == '{' and ind < len(format_str):
            new_str.append(c)
            # literal formatting { are not allowed
            # if prev_char == '{':
            #     prev_char = ''
            #     within_field = False
            # else:
            prev_char = c
            within_field = True

            ind += 1
            c = format_str[ind]
        if within_field:
            new_arg = []
            while ind < len(format_str):
                if c in '}!:':
                    prev_char = c
                    within_field = False
                    new_args.append(''.join(new_arg))
                    break
                new_arg.append(c)
                ind += 1
                c = format_str[ind]
    format_str = ''.join(new_str)
    args = new_args
    def _format_context(context):
        new_args = []
        for arg in args:
            # LenaKeyError may be raised
            new_args.append(lena.context.get_recursively(context, arg))
        # other exceptions, like ValueError
        # (for bad string formatting) may be raised.
        s = format_str.format(*new_args)
        return s
    return _format_context


_sentinel = object()


def get_recursively(d, keys, default=_sentinel):
    """Get value from a dictionary *d* recursively.

    *keys* can be a list of simple keys (strings),
    a dot-separated string
    or a dictionary with at most one key at each level.
    A string is split by dots and used as a list.
    A list of keys is searched in the dictionary recursively
    (it represents nested dictionaries).
    If any of them is not found, *default* is returned
    if "default" is given,
    otherwise :exc:`.LenaKeyError` is raised.

    If *keys* is empty, *d* is returned.

    Examples:

    >>> context = {"output": {"latex": {"name": "x"}}}
    >>> get_recursively(context, ["output", "latex", "name"], default="y")
    'x'
    >>> get_recursively(context, "output.latex.name")
    'x'

    .. note::
        Python's dict.get in case of a missing value
        returns ``None`` and never raises an error.
        We implement it differently,
        because it allows more flexibility.

    If *d* is not a dictionary or if *keys* is not a string, a dict
    or a list, :exc:`.LenaTypeError` is raised.
    If *keys* is a dictionary with more than one key at some level,
    :exc:`.LenaValueError` is raised.
    """
    has_default = default is not _sentinel
    if not isinstance(d, dict):
        raise lena.core.LenaTypeError(
            "need a dictionary, {} provided".format(d)
        )
    if isinstance(keys, str):
        # here empty substrings are skipped, but this is undefined.
        keys = [key for key in keys.split('.') if key]
    # todo: create dict_to_list and disable dict keys here?
    elif isinstance(keys, dict):
        new_keys = []
        while keys:
            if isinstance(keys, dict) and len(keys) != 1:
                raise lena.core.LenaValueError(
                    "keys must have exactly one key at each level, "
                    "{} given".format(keys)
                )
            else:
                if not isinstance(keys, dict):
                    new_keys.append(keys)
                    break
                for key in keys:
                    new_keys.append(key)
                    keys = keys[key]
                    break
        keys = new_keys
    elif isinstance(keys, list):
        if not all(isinstance(k, str) for k in keys):
            raise lena.core.LenaTypeError(
                "all simple keys must be strings, "
                "{} given".format(keys)
            )
    else:
        raise lena.core.LenaTypeError(
            "keys must be a dict, a string or a list of keys, "
            "{} given".format(keys)
        )

    for key in keys[:-1]:
        if key in d and isinstance(d.get(key), dict):
            d = d[key]
        elif has_default:
            return default
        else:
            raise lena.core.LenaKeyError(
                "nested dict {} not found in {}".format(key, d)
            )

    if not keys:
        return d
    if keys[-1] in d:
        return d[keys[-1]]
    elif has_default:
        return default
    else:
        raise lena.core.LenaKeyError(
            "nested key {} not found in {}".format(keys[-1], d)
        )


def intersection(*dicts, **kwargs):
    """Return a dictionary, such that each of its items
    are contained in all *dicts* (recursively).

    *dicts* are several dictionaries.
    If *dicts* is empty, an empty dictionary is returned.

    A keyword argument *level* sets maximum number of recursions.
    For example, if *level* is 0, all *dicts* must be equal
    (otherwise an empty dict is returned).
    If *level* is 1, the result contains those subdictionaries
    which are equal.
    For arbitrarily nested subdictionaries set *level* to -1 (default).

    Example:

    >>> from lena.context import intersection
    >>> d1 = {1: "1", 2: {3: "3", 4: "4"}}
    >>> d2 = {2: {4: "4"}}
    >>> # by default level is -1, which means infinite recursion
    >>> intersection(d1, d2) == d2
    True
    >>> intersection(d1, d2, level=0)
    {}
    >>> intersection(d1, d2, level=1)
    {}
    >>> intersection(d1, d2, level=2)
    {2: {4: '4'}}

    This function always returns a dictionary
    or its subtype (copied from dicts[0]).
    All values are deeply copied.
    No dictionary or subdictionary is changed.

    If any of *dicts* is not a dictionary
    or if some *kwargs* are unknown,
    :exc:`.LenaTypeError` is raised.
    """
    if not all([isinstance(d, dict) for d in dicts]):
        raise lena.core.LenaTypeError(
            "all dicts must be dictionaries, "
            "{} given".format(dicts)
        )

    level = kwargs.pop("level", -1)
    if kwargs:
        raise lena.core.LenaTypeError(
            "unknown kwargs {}".format(kwargs)
        )

    if not dicts:
        return {}
    res = copy.deepcopy(dicts[0])
    for d in dicts[1:]:
        if level == 0:
            if d == res and d:
                continue
            else:
                return {}
        to_delete = []
        for key in res:
            if key in d:
                if d[key] != res[key]:
                    if level == 1:
                        to_delete.append(key)
                    elif isinstance(res[key], dict) and isinstance(d[key], dict):
                        res[key] = intersection(res[key], d[key], level=level-1)
                    else:
                        to_delete.append(key)
            else:
                # keys can't be deleted during iteration
                to_delete.append(key)
        for key in to_delete:
            del res[key]
        if not res:
            # res was calculated empty
            return res
    return res


def iterate_update(d, updates):
    """Iterate on updates of *d* with *updates*.

    *d* is a dictionary. It remains unchanged.

    *updates* is a list of dictionaries.
    For each element *update*
    a copy of *d* updated with *update* is yielded.

    If *updates* is empty, nothing is yielded.
    """
    # todo: do I need this function?
    for update in updates:
        d_copy = copy.deepcopy(d)
        update_recursively(d_copy, update)
        yield d_copy


def make_context(obj, *attrs):
    """Return context for object *obj*.

    *attrs* is a list of attributes of *obj* to be inserted
    into the context.
    If an attribute starts with an underscore '_',
    it is inserted without the underscore.
    If an attribute is absent or None, it is skipped.
    """
    # todo: rename to to_dict
    # not used anywhere, change it freely.
    # add examples.
    context = {}
    for attr in attrs:
        val = getattr(obj, attr, None)
        if val is not None:
            if attr.startswith("_"):
                attr = attr[1:]
            context.update({attr: val})
    return context


def str_to_dict(s, value=_sentinel):
    """Create a dictionary from a dot-separated string *s*.

    If the *value* is provided, it becomes the value of 
    the deepest key represented by *s*.

    Dots represent nested dictionaries.
    If *s* is non-empty and *value* is not provided,
    then *s* must have at least two dot-separated parts
    (*"a.b"*), otherwise :exc:`.LenaValueError` is raised.
    If a *value* is provided, *s* must be non-empty.

    If *s* is empty, an empty dictionary is returned.

    Examples:

    >>> str_to_dict("a.b.c d")
    {'a': {'b': 'c d'}}
    >>> str_to_dict("output.changed", True)
    {'output': {'changed': True}}
    """
    if s == "":
        if value is _sentinel:
            return {}
        else:
            raise lena.core.LenaValueError(
                "to make a dict with a value, "
                "provide at least one dot-separated key"
            )
    # """*s* can be a dictionary. In this case it is returned as it is.
    # If s were a dictionary, value mustn't had been allowed.
    # probably this is a bad design,
    # elif isinstance(s, dict):
    #     return s
    parts = s.split(".")
    if value is not _sentinel:
        parts.append(value)
    def nest_list(d, l):
        """Convert list *l* to nested dictionaries in *d*."""
        len_l = len(l)
        if len_l == 2:
            d.update([(l[0], l[1])])
        elif len_l < 2:
            raise lena.core.LenaValueError(
                "to make a dict, provide at least two dot-separated values"
            )
        else:
            d.update([(l[0], nest_list({}, l[1:]))])
        return d
    d = nest_list({}, parts)
    return d


def str_to_list(s):
    """Like :func:`str_to_dict`, but return a flat list.

    If the string *s* is empty, an empty list is returned.
    This is different from *str.split*: the latter would
    return a list with one empty string.
    Contrarily to :func:`str_to_dict`, this function allows
    an arbitrary number of dots in *s* (or none).
    """
    if s == "":
        return []
    # s can't be a list. This function is not used as a general
    # interface (as str_to_dict could be).

    # s may contain empty substrings, like in "a..b"
    # this is not encouraged, of course, but may suit:
    # if there are two errors in some user's context logic,
    # they may compensate and not destroy all.
    # Another variant would be to treat empty strings
    # as whole context. The variant with '' seems more understandable
    # to the user.
    return s.split(".")


def update_nested(key, d, other):
    """Update *d[key]* with the *other* dictionary preserving data.

    If *d* doesn't contain the *key*, it is updated with *{key: other}*.
    If *d* contains the *key*, *d[key]* is inserted into *other[key]*
    (so that it is not overriden).
    If *other* contains *key* (and possibly more nested *key*-s),
    then *d[key]* is inserted into the deepest level
    of *other.key.key...* Finally, *d[key]* becomes *other*.

    Example:

    >>> context = {"variable": {"name": "x"}}
    >>> new_var_context = {"name": "n"}
    >>> update_nested("variable", context, copy.deepcopy(new_var_context))
    >>> context == {'variable': {'name': 'n', 'variable': {'name': 'x'}}}
    True
    >>>
    >>> update_nested("variable", context, {"name": "top"})
    >>> context == {
    ...    'variable': {'name': 'top',
    ...                 'variable': {'name': 'n', 'variable': {'name': 'x'}}}
    ... }
    True

    *other* is modified in general. Create that on the fly
    or use *copy.deepcopy* when appropriate.

    Recursive dictionaries (containing references to themselves)
    are strongly discouraged and meaningless when nesting.
    If *other[key]* is recursive, :exc:`.LenaValueError` may be raised.
    """
    # there was an idea to add a keyword argument copy_other
    # (by default True), but the user can do that him/herself
    # with copy.deepcopy when needed. Otherwise it would be 
    # unnecessary complication of this interface.

    # Only one key is nested. This encourages design when
    # 1) elements combine their contexts into one key
    # (like {"split_into_bins": {"variable": {}, "histogram": {}}})
    # 2) elements change only one key ("variable", "histogram",...).

    def get_most_nested_subdict_with(key, d):
        nested_dicts = []
        while True:
            if key in d:
                if d in nested_dicts:
                    raise lena.core.LenaValueError(
                        "recursive *other* is forbidden"
                    )
                nested_dicts.append(d)
                d = d[key]
            else:
                return d

    if key in d:
        other_most_nested = get_most_nested_subdict_with(key, other)
        # insert d[key] at the lowest other.key.key....
        other_most_nested[key] = d[key]

    d[key] = other


def update_recursively(d, other, value=_sentinel):
    """Update dictionary *d* with items from *other* dictionary.

    *other* can be a dot-separated string. In this case
    :func:`str_to_dict` is used to convert it and the *value*
    to a dictionary.
    A *value* argument is allowed only when *other* is a string,
    otherwise :exc:`.LenaValueError` is raised.

    Existing values are updated recursively,
    that is including nested subdictionaries.
    Example:

    >>> d1 = {"a": 1, "b": {"c": 3}}
    >>> d2 = {"b": {"d": 4}}
    >>> update_recursively(d1, d2)
    >>> d1 == {'a': 1, 'b': {'c': 3, 'd': 4}}
    True
    >>> # Usual update would have made d1["b"] = {"d": 4}, erasing "c".

    Non-dictionary items from *other* overwrite those in *d*:

    >>> update_recursively(d1, {"b": 2})
    >>> d1 == {'a': 1, 'b': 2}
    True
    """
    # skip this docstring, because it's trivial.
    # Both *d* and *other* must be dictionaries,
    # otherwise :exc:`.LenaTypeError` is raised.
    # it would be cleaner to allow only dict as other,
    # but it's very clear and useful to allow
    # lena.context.update_recursively(context, "output.changed", True)
    if isinstance(other, str):
        other = str_to_dict(other, value)
    else:
        if value is not _sentinel:
            raise lena.core.LenaValueError(
                "explicit value is allowed only when other is a string"
            )
    if not isinstance(d, dict) or not isinstance(other, dict):
        raise lena.core.LenaTypeError(
            "d and other must be dicts, {} and {} provided".format(d, other)
        )
    for key, val in other.items():
        if not isinstance(val, dict):
            d[key] = val
        else:
            if key in d:
                if not isinstance(d[key], dict):
                    d[key] = {}
                update_recursively(d[key], other[key])
            else:
                d[key] = val

if __name__ == "__main__":
    isT=True
    d1 = {'a': {'b': 'c d'}, 'e': 'f'}
    d2 = {'a': d1['a']}
    # difference with an empty dict works
    res1=difference({}, d1) == {}
    res2=difference(d1, {}) == d1

    # difference with self is empty
    res3=difference(d1, d1) == {}

    # difference with not a dictionary works
    res4=difference(d1, None) == d1
    res5=difference(None, d1) == None
    # level 1 difference between similar dicts
    res6=difference(d1, d2, level=1) == {'e': 'f'}
    # level 0 difference
    res7=difference(d1, d2, level=0) == d1

    d3 = {'a': {'b': {"c d": 'e'}}}
    d4 = {'a': {'b': {"c d": 'e', "f": "g"}}}
    # Completely different. Complicated, but True
    res8=difference(d1, d3, level=-1) == d1
    # One is contained in another.
    res9=difference(d3, d4, level=-1) == {}

    # Some actual difference exists
    d5 = {'a': {'b': {"c d": 'e', "h": "i"}}}
    res10=difference(d5, d4, level=-1) == {'a': {'b': {'h': 'i'}}}
    if not res1 or not res2 or not res3 or not res4 or not res5 or not res6 or not res7 or not res8 or not res9 or not res10:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87af19a0c4fa8b80b34f7/"):
    #     f = open("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87af19a0c4fa8b80b34f7/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"],bytes):
    #         args2=dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2=content["input"]["args"][2]["bytes"]
    #     res0 = difference(args0,args1,args2)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/structures/histogram_fill_passk_validte.py
"""Histogram structure *histogram* and element *Histogram*."""
import copy
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")
import lena.context
import lena.core
import lena.flow
import lena.math
from lena.structures import hist_functions as hf


class histogram():
    """A multidimensional histogram.

    Arbitrary dimension, variable bin size and weights are supported.
    Lower bin edge is included, upper edge is excluded.
    Underflow and overflow values are skipped.
    Bin content can be of arbitrary type,
    which is defined during initialization.

    Examples:

    >>> # a two-dimensional histogram
    >>> hist = histogram([[0, 1, 2], [0, 1, 2]])
    >>> hist.fill([0, 1])
    >>> hist.bins
    [[0, 1], [0, 0]]
    >>> values = [[0, 0], [1, 0], [1, 1]]
    >>> # fill the histogram with values
    >>> for v in values:
    ...     hist.fill(v)
    >>> hist.bins
    [[1, 1], [1, 1]]
    """
    # Note the differences from existing packages.
    # Numpy 1.16 (numpy.histogram): all but the last
    # (righthand-most) bin is half-open.
    # This histogram class has bin limits as in ROOT
    # (but without overflow and underflow).

    # Numpy: the first element of the range must be less than or equal to the second.
    # This histogram requires strictly increasing edges.
    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html
    # https://root.cern.ch/root/htmldoc/guides/users-guide/Histograms.html#bin-numbering

    def __init__(self, edges, bins=None, initial_value=0):
        """*edges* is a sequence of one-dimensional arrays,
        each containing strictly increasing bin edges.

        Histogram's bins by default
        are initialized with *initial_value*.
        It can be any object that supports addition with *weight*
        during *fill* (but that is not necessary
        if you don't plan to fill the histogram).
        If the *initial_value* is compound and requires special copying,
        create initial bins yourself (see :func:`.init_bins`).

        A histogram can be created from existing *bins* and *edges*.
        In this case a simple check of the shape of *bins* is done
        (raising :exc:`.LenaValueError` if failed).

        **Attributes**

        :attr:`edges` is a list of edges on each dimension.
        Edges mark the borders of the bin.
        Edges along each dimension are one-dimensional lists,
        and the multidimensional bin is the result of all intersections
        of one-dimensional edges.
        For example, a 3-dimensional histogram has edges of the form
        *[x_edges, y_edges, z_edges]*,
        and the 0th bin has borders
        *((x[0], x[1]), (y[0], y[1]), (z[0], z[1]))*.

        Index in the edges is a tuple, where a given position corresponds
        to a dimension, and the content at that position
        to the bin along that dimension.
        For example, index *(0, 1, 3)* corresponds to the bin
        with lower edges *(x[0], y[1], z[3])*.

        :attr:`bins` is a list of nested lists.
        Same index as for edges can be used to get bin content:
        bin at *(0, 1, 3)* can be obtained as *bins[0][1][3]*.
        Most nested arrays correspond to highest
        (further from x) coordinates.
        For example, for a 3-dimensional histogram bins equal to
        *[[[1, 1], [0, 0]], [[0, 0], [0, 0]]]*
        mean that the only filled bins are those
        where x and y indices are 0, and z index is 0 and 1.

        :attr:`dim` is the dimension of a histogram
        (length of its *edges* for a multidimensional histogram).

        If subarrays of *edges* are not increasing
        or if any of them has length less than 2,
        :exc:`.LenaValueError` is raised.

        .. admonition:: Programmer's note

            one- and multidimensional histograms
            have different *bins* and *edges* format.
            To be unified, 1-dimensional edges should be
            nested in a list (like *[[1, 2, 3]]*).
            Instead, they are simply the x-edges list,
            because it is more intuitive and one-dimensional histograms
            are used more often.
            To unify the interface for bins and edges in your code,
            use :func:`.unify_1_md` function.
        """
        # todo: allow creation of *edges* from tuples
        # (without lena.math.mesh). Allow bin_size in this case.
        hf.check_edges_increasing(edges)
        self.edges = edges
        self._scale = None

        if hasattr(edges[0], "__iter__"):
            self.dim = len(edges)
        else:
            self.dim = 1

        # todo: add a kwarg no_check=False to disable bins testing
        if bins is None:
            self.bins = hf.init_bins(self.edges, initial_value)
        else:
            self.bins = bins
            # We can't make scale for an arbitrary histogram,
            # because it may contain compound values.
            # self._scale = self.make_scale()
            wrong_bins_error = lena.core.LenaValueError(
                "bins of incorrect shape given, {}".format(bins)
            )
            if self.dim == 1:
                if len(bins) != len(edges) - 1:
                    raise wrong_bins_error
            else:
                if len(bins) != len(edges[0]) - 1:
                    raise wrong_bins_error
        if self.dim > 1:
            self.ranges = [(axis[0], axis[-1]) for axis in edges]
            self.nbins =  [len(axis) - 1 for axis in edges]
        else:
            self.ranges = [(edges[0], edges[-1])]
            self.nbins = [len(edges)-1]

    def __eq__(self, other):
        """Two histograms are equal, if and only if they have
        equal bins and equal edges.

        If *other* is not a :class:`.histogram`, return ``False``.

        Note that floating numbers should be compared
        approximately (using :func:`math.isclose`).
        """
        if not isinstance(other, histogram):
            # in Python comparison between different types is allowed
            return False
        return self.bins == other.bins and self.edges == other.edges

    def fill(self, coord, weight=1):
        """Fill histogram at *coord* with the given *weight*.

        Coordinates outside the histogram edges are ignored.
        """
        indices = hf.get_bin_on_value(coord, self.edges)
        subarr = self.bins
        for ind in indices[:-1]:
            # underflow
            if ind < 0:
                return
            try:
                subarr = subarr[ind]
            # overflow
            except IndexError:
                return
        ind = indices[-1]
        # underflow
        if ind < 0:
            return

        # fill
        try:
            subarr[ind] += weight
        except IndexError:
            return

    def __repr__(self):
        return "histogram({}, bins={})".format(self.edges, self.bins)

    def scale(self, other=None, recompute=False):
        """Compute or set scale (integral of the histogram).

        If *other* is ``None``, return scale of this histogram.
        If its scale was not computed before,
        it is computed and stored for subsequent use
        (unless explicitly asked to *recompute*).
        Note that after changing (filling) the histogram
        one must explicitly recompute the scale
        if it was computed before.

        If a float *other* is provided, rescale self to *other*.

        Histograms with scale equal to zero can't be rescaled.
        :exc:`.LenaValueError` is raised if one tries to do that.
        """
        # see graph.scale comments why this is called simply "scale"
        # (not set_scale, get_scale, etc.)
        if other is None:
            # return scale
            if self._scale is None or recompute:
                self._scale = hf.integral(
                    *hf.unify_1_md(self.bins, self.edges)
                )
            return self._scale
        else:
            # rescale from other
            scale = self.scale()
            if scale == 0:
                raise lena.core.LenaValueError(
                    "can not rescale histogram with zero scale"
                )
            self.bins = lena.math.md_map(lambda binc: binc*float(other) / scale,
                                         self.bins)
            self._scale = other
            return None

    def _update_context(self, context):
        """Update *context* with the properties of this histogram.

        *context.histogram* is updated with "dim", "nbins"
        and "ranges" with values for this histogram.
        If this histogram has a computed scale, it is also added
        to the context.

        Called on "destruction" of the histogram structure (for example,
        in :class:`.ToCSV`). See graph._update_context for more details.
        """

        hist_context = {
            "dim": self.dim,
            "nbins": self.nbins,
            "ranges": self.ranges
        }

        if self._scale is not None:
            hist_context["scale"] = self._scale

        lena.context.update_recursively(context, {"histogram": hist_context})


class Histogram():
    """An element to produce histograms."""

    def __init__(self, edges, bins=None, make_bins=None, initial_value=0):
        """*edges*, *bins* and *initial_value* have the same meaning
        as during creation of a :class:`histogram`.

        *make_bins* is a function without arguments
        that creates new bins
        (it will be called during :meth:`__init__` and :meth:`reset`).
        *initial_value* in this case is ignored, but bin check is made.
        If both *bins* and *make_bins* are provided,
        :exc:`.LenaTypeError` is raised.
        """
        self._hist = histogram(edges, bins)

        if make_bins is not None and bins is not None:
            raise lena.core.LenaTypeError(
                "either initial bins or make_bins must be provided, "
                "not both: {} and {}".format(bins, make_bins)
            )

        # may be None
        self._initial_bins = copy.deepcopy(bins)

        # todo: bins, make_bins, initial_value look redundant
        # and may be reconsidered when really using reset().
        if make_bins:
            bins = make_bins()
        self._make_bins = make_bins

        self._cur_context = {}

    def fill(self, value):
        """Fill the histogram with *value*.

        *value* can be a *(data, context)* pair. 
        Values outside the histogram edges are ignored.
        """
        data, self._cur_context = lena.flow.get_data_context(value)
        self._hist.fill(data)
        # filling with weight is only allowed in histogram structure
        # self._hist.fill(data, weight)

    def compute(self):
        """Yield histogram with context."""
        yield (self._hist, self._cur_context)

    def reset(self):
        """Reset the histogram.

        Current context is reset to an empty dict.
        Bins are reinitialized with the *initial_value*
        or with *make_bins()* (depending on the initialization).
        """
        if self._make_bins is not None:
            self.bins = self._make_bins()
        elif self._initial_bins is not None:
            self.bins = copy.deepcopy(self._initial_bins)
        else:
            self.bins = hf.init_bins(self.edges, self._initial_value)

        self._cur_context = {}

if __name__ == "__main__":
    isT = True
    hist = histogram([[0, 1, 2], [0, 1, 2]])
    hist.fill([0, 1])
    isT=hist.bins==[[0, 1], [0, 0]]
    # for l in os.listdir("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87af09a0c4fa8b80b34f1/"):
    #     f = open("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87af09a0c4fa8b80b34f1/" + l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = histogram([[0, 1, 2], [0, 1, 2]])
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.fill(args1, args2)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/rak-n-rok---Krake/krake/krake/data/core__validate_labels_passk_validte.py
import re
from enum import Enum, IntEnum, auto
from datetime import datetime
from dataclasses import field
from typing import List, Dict

from marshmallow import ValidationError
import sys
sys.path.append("/home/travis/builds/repos/rak-n-rok---Krake/krake")
from krake.data import persistent
from krake.data.serializable import Serializable, ApiObject, PolymorphicContainer


class ResourceRef(Serializable):
    api: str
    namespace: str = field(default=None)
    kind: str
    name: str

    def __hash__(self):
        return hash((self.api, self.namespace, self.kind, self.name))

    def __repr__(self):
        message = f"{self.kind}(api='{self.api}', "
        return message + f"namespace='{self.namespace}', name='{self.name}')"


_label_key_pattern = None
_label_value_pattern = None

_label_key_regex = None
_label_value_regex = None


def _get_labels_regex():
    """Build or return the regular expressions that are used to validate the key and
    value of the labels of the Krake resources.

    The first call builds the expressions, while a second returns the already built
    ones.

    Returns:
        (re.Pattern, re.Pattern): a tuple that contains the compiled regular,
            expressions, the first element to validate the key and the second to
            validate the value.

    """
    global _label_key_pattern, _label_value_pattern
    global _label_key_regex, _label_value_regex

    if _label_key_regex and _label_value_regex:
        return _label_key_regex, _label_value_regex

    # Build the patterns only if not already built
    max_prefix_size = 253
    max_key_size = 63
    max_value_size = max_key_size

    # First and last characters must be alphanumeric. The rest of the string must be
    # alphanumeric, "-", "_" or "."
    base_alphanumeric_pattern = "\\w|(\\w[\\w\\-_.]{{0,{length}}}\\w)"

    key_pattern = base_alphanumeric_pattern.format(length=max_key_size - 2)
    value_pattern = base_alphanumeric_pattern.format(length=max_value_size - 2)
    prefix_pattern = base_alphanumeric_pattern.format(length=max_prefix_size - 2)

    # The key can be a string of length 63 with the specifications described above,
    # or have a prefix, then one "/" character, then the string of length 63 (called
    # name).
    # The prefix itself should have a max length of 253, but otherwise follows the
    # specifications described above.
    _label_key_pattern = f"^(({prefix_pattern})\\/)?({key_pattern})$"

    # The value can be a string of length 63 with the specifications described
    # above.
    _label_value_pattern = value_pattern

    _label_key_regex = re.compile(_label_key_pattern, re.ASCII)
    _label_value_regex = re.compile(_label_value_pattern, re.ASCII)

    return _label_key_regex, _label_value_regex


def validate_key(key):
    """Validate the given key against the corresponding regular expression.

    Args:
        key: the string to validate

    Raises:
        ValidationError: if the given key is not conform to the regular expression.
    """
    key_regex, _ = _get_labels_regex()
    if not key_regex.fullmatch(key):
        raise ValidationError(
            f"Label key {key!r} does not match the regex {_label_key_pattern!r}."
        )


def validate_value(value):
    """Validate the given value against the corresponding regular expression.

    Args:
        value: the string to validate

    Raises:
        ValidationError: if the given value is not conform to the regular expression.
    """
    _, value_regex = _get_labels_regex()
    if not value_regex.fullmatch(value):
        raise ValidationError(
            f"Label value {value!r} does not match"
            f" the regex {_label_value_pattern!r}."
        )


def _validate_labels(labels):
    """Check that keys and values in the given labels match against their corresponding
    regular expressions.

    Args:
        labels (dict): the different labels to validate.

    Raises:
        ValidationError: if any of the keys and labels does not match their respective
            regular expression. The error contains as message the list of all errors
            which occurred in the labels. Each element of the list is a dictionary with
            one key-value pair:
            - key: the label key or label value for which an error occurred as string.
            - value: the error message.

            .. code:: python

                # Example:
                labels = {
                    "key1": "valid",
                    "key2": ["invalid"],
                    "$$": "invalid",
                    True: True,
                }
                try:
                    _validate_labels(labels)
                except ValidationError as err:
                    assert err.messages == [
                        {"['invalid']": 'expected string or bytes-like object'},
                        {'$$': "Label key '$$' does not match the regex [...]"},
                        {'True': 'expected string or bytes-like object'},
                        {'True': 'expected string or bytes-like object'},
                    ]
    """
    errors = []
    for key, value in labels.items():
        try:
            validate_key(key)
        except (ValidationError, TypeError) as err:
            errors.append({str(key): str(err)})

        try:
            validate_value(value)
        except (ValidationError, TypeError) as err:
            errors.append({str(value): str(err)})

    if errors:
        raise ValidationError(list(errors))


_resource_name_pattern = None
_resource_name_regex = None


def _get_resource_name_regex():
    """Build or return the regular expressions that are used to validate
    the name of the Krake resources.

    Returns:
        (re.Pattern): the compiled regular expressions, to validate
        the resource name.
    """
    global _resource_name_regex, _resource_name_pattern

    # Build the patterns only if not already built
    if _resource_name_regex:
        return _resource_name_regex

    # First and last characters must be alphanumeric. The rest of the string must be
    # alphanumeric, "-", "_" or "." and without whitespace as well as have a
    # max length of 255 and a min length of 1
    max_name_size = 253  # reduced by 2 for the regex
    min_name_size = 0  # reduced by 1 for the regex
    base_alphanumeric_pattern = "\\w|(\\w[\\w\\-_.:]{{{min_length},{length}}}\\w)"

    resource_name_pattern = base_alphanumeric_pattern.format(
        min_length=min_name_size, length=max_name_size
    )

    _resource_name_pattern = resource_name_pattern
    _resource_name_regex = re.compile(_resource_name_pattern, re.ASCII)
    return _resource_name_regex


def _validate_resource_name(name):
    """Each Krake resource name is checked against a specific pattern.
    Which characters are not allowed is defined in _get_resource_name_regex

    Args:
        name(str): the different resource names to validate.

    Raises:
        ValidationError: if any resource name does not match their respective
            regular expression.
    """
    resource_name_regex = _get_resource_name_regex()
    if not resource_name_regex.fullmatch(name):
        raise ValidationError("Invalid character in resource name.")


def _validate_resource_namespace(namespace):
    """Each Krake resource namespace is checked against a specific pattern.
    Which characters are not allowed is defined in _get_resource_name_regex

    Args:
        namespace(str): the different resource namespaces to validate.

    Raises:
        ValidationError: if any resource namespace does not match their respective
            regular expression.
    """
    resource_namespace_regex = _get_resource_name_regex()
    if not resource_namespace_regex.fullmatch(namespace):
        raise ValidationError("Invalid character in resource namespace.")


class Metadata(Serializable):
    name: str = field(metadata={"immutable": True, "validate": _validate_resource_name})
    namespace: str = field(
        default=None,
        metadata={"immutable": True, "validate": _validate_resource_namespace},
    )
    labels: dict = field(default_factory=dict, metadata={"validate": _validate_labels})
    inherit_labels: bool = field(default=False)
    finalizers: List[str] = field(default_factory=list)

    uid: str = field(metadata={"readonly": True})
    created: datetime = field(metadata={"readonly": True})
    modified: datetime = field(metadata={"readonly": True})
    deleted: datetime = field(default=None, metadata={"readonly": True})

    owners: List[ResourceRef] = field(default_factory=list)


class CoreMetadata(Serializable):
    name: str
    uid: str


class ListMetadata(Serializable):
    pass  # TODO


class ReasonCode(IntEnum):
    INTERNAL_ERROR = 1  # Default error

    INVALID_RESOURCE = 10  # Invalid values in the Manifest
    # Resource is not supported by the controller
    UNSUPPORTED_RESOURCE = 11
    # The custom resource provided does not exist or is invalid
    INVALID_CUSTOM_RESOURCE = 12
    # Invalid TOSCA manifest
    INVALID_TOSCA_MANIFEST = 13

    CLUSTER_NOT_REACHABLE = 20  # Connectivity issue with the Kubernetes deployment
    NO_SUITABLE_RESOURCE = 50  # Scheduler issue

    KUBERNETES_ERROR = 60

    CREATE_FAILED = 70
    RECONCILE_FAILED = 71
    RECONFIGURE_FAILED = 72
    DELETE_FAILED = 73
    RETRIEVE_FAILED = 74
    NOT_FOUND = 75

    OPENSTACK_ERROR = 80
    INVALID_CLUSTER_TEMPLATE = 81

    # Related to Metrics and Metric Provider
    INVALID_METRIC = 91
    UNREACHABLE_METRICS_PROVIDER = 92
    UNKNOWN_METRIC = 93
    UNKNOWN_METRICS_PROVIDER = 94


class Reason(Serializable):
    code: ReasonCode
    message: str


class WatchEventType(Enum):
    ADDED = auto()
    MODIFIED = auto()
    DELETED = auto()


class Status(Serializable):
    reason: Reason = None


class WatchEvent(Serializable):
    type: WatchEventType
    object: dict


class Verb(Enum):
    create = auto()
    list = auto()
    list_all = auto()
    get = auto()
    update = auto()
    delete = auto()


class RoleRule(Serializable):
    api: str
    resources: List[str]
    namespaces: List[str]
    verbs: List[Verb]


@persistent("/core/roles/{name}")
class Role(ApiObject):
    api: str = "core"
    kind: str = "Role"
    metadata: Metadata
    rules: List[RoleRule]


class RoleList(ApiObject):
    api: str = "core"
    kind: str = "RoleList"
    metadata: ListMetadata
    items: List[Role]


@persistent("/core/rolebindings/{name}")
class RoleBinding(ApiObject):
    api: str = "core"
    kind: str = "RoleBinding"
    metadata: Metadata
    users: List[str]
    roles: List[str]


class RoleBindingList(ApiObject):
    api: str = "core"
    kind: str = "RoleBindingList"
    metadata: ListMetadata
    items: List[RoleBinding]


class Conflict(Serializable):
    source: ResourceRef
    conflicting: List[ResourceRef]


def resource_ref(resource):
    """Create a :class:`ResourceRef` from a :class:`ApiObject`

    Args:
        resource (.serializable.ApiObject): API object that should be
            referenced

    Returns:
        ResourceRef: Corresponding reference to the API object
    """
    return ResourceRef(
        api=resource.api,
        kind=resource.kind,
        namespace=resource.metadata.namespace,
        name=resource.metadata.name,
    )


class MetricSpecProvider(Serializable):
    name: str
    metric: str


class MetricSpec(Serializable):
    allowed_values: List[int]
    min: float
    max: float
    provider: MetricSpecProvider


class BaseMetric(ApiObject):
    api: str = "core"
    kind: str = None
    metadata: Metadata
    spec: MetricSpec


@persistent("/core/globalmetrics/{name}")
class GlobalMetric(BaseMetric):
    api: str = "core"
    kind: str = "GlobalMetric"
    metadata: Metadata
    spec: MetricSpec


@persistent("/core/metrics/{namespace}/{name}")
class Metric(BaseMetric):
    api: str = "core"
    kind: str = "Metric"
    metadata: Metadata
    spec: MetricSpec


class MetricList(ApiObject):
    api: str = "core"
    kind: str = "MetricList"
    metadata: ListMetadata
    items: List[Metric]


class GlobalMetricList(ApiObject):
    api: str = "core"
    kind: str = "GlobalMetricList"
    metadata: ListMetadata
    items: List[GlobalMetric]


class MetricsProviderSpec(PolymorphicContainer):
    type: str


@MetricsProviderSpec.register("prometheus")
class PrometheusSpec(Serializable):
    url: str


@MetricsProviderSpec.register("influx")
class InfluxSpec(Serializable):
    url: str
    token: str
    org: str
    bucket: str


@MetricsProviderSpec.register("kafka")
class KafkaSpec(Serializable):
    """Specifications to connect to a KSQL database, and retrieve a specific row from a
    specific table.

    Attributes:
        comparison_column (str): name of the column where the value will be compared to
            the metric name, to select the right metric.
        value_column (str): name of the column where the value of a metric is stored.
        table (str): the name of the KSQL table where the metric is defined.
        url (str): endpoint of the KSQL database.

    """

    comparison_column: str
    value_column: str
    table: str
    url: str


@MetricsProviderSpec.register("static")
class StaticSpec(Serializable):
    metrics: Dict[str, float]


class BaseMetricsProvider(ApiObject):
    api: str = "core"
    kind: str = None
    metadata: Metadata
    spec: MetricsProviderSpec


@persistent("/core/globalmetricsproviders/{name}")
class GlobalMetricsProvider(BaseMetricsProvider):
    api: str = "core"
    kind: str = "GlobalMetricsProvider"
    metadata: Metadata
    spec: MetricsProviderSpec


@persistent("/core/metricsproviders/{namespace}/{name}")
class MetricsProvider(BaseMetricsProvider):
    api: str = "core"
    kind: str = "MetricsProvider"
    metadata: Metadata
    spec: MetricsProviderSpec


class MetricsProviderList(ApiObject):
    api: str = "core"
    kind: str = "MetricsProviderList"
    metadata: ListMetadata
    items: List[MetricsProvider]


class GlobalMetricsProviderList(ApiObject):
    api: str = "core"
    kind: str = "GlobalMetricsProviderList"
    metadata: ListMetadata
    items: List[GlobalMetricsProvider]


class MetricRef(Serializable):
    name: str
    weight: float
    namespaced: bool = False

if __name__ == "__main__":
    isT=True
    labels = {
        "key1": "valid",
        "key2": ["invalid"],
        "$$": "invalid",
        True: True,
    }
    try:
        _validate_labels(labels)
        isT=False
    except ValidationError as err:

        if err.messages != [{"['invalid']": 'expected string or bytes-like object'}, {'$$': "Label key '$$' does not match the regex '^((\\\\w|(\\\\w[\\\\w\\\\-_.]{0,251}\\\\w))\\\\/)?(\\\\w|(\\\\w[\\\\w\\\\-_.]{0,61}\\\\w))$'."}, {'True': 'expected string or bytes-like object'}, {'True': 'expected string or bytes-like object'}]:
            isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/rak-n-rok---Krake/krake/krake/data/core__get_resource_name_regex_passk_validte.py
import re
from enum import Enum, IntEnum, auto
from datetime import datetime
from dataclasses import field
from typing import List, Dict

from marshmallow import ValidationError
import sys
sys.path.append("/home/travis/builds/repos/rak-n-rok---Krake/krake")
from krake.data import persistent
from krake.data.serializable import Serializable, ApiObject, PolymorphicContainer


class ResourceRef(Serializable):
    api: str
    namespace: str = field(default=None)
    kind: str
    name: str

    def __hash__(self):
        return hash((self.api, self.namespace, self.kind, self.name))

    def __repr__(self):
        message = f"{self.kind}(api='{self.api}', "
        return message + f"namespace='{self.namespace}', name='{self.name}')"


_label_key_pattern = None
_label_value_pattern = None

_label_key_regex = None
_label_value_regex = None


def _get_labels_regex():
    """Build or return the regular expressions that are used to validate the key and
    value of the labels of the Krake resources.

    The first call builds the expressions, while a second returns the already built
    ones.

    Returns:
        (re.Pattern, re.Pattern): a tuple that contains the compiled regular,
            expressions, the first element to validate the key and the second to
            validate the value.

    """
    global _label_key_pattern, _label_value_pattern
    global _label_key_regex, _label_value_regex

    if _label_key_regex and _label_value_regex:
        return _label_key_regex, _label_value_regex

    # Build the patterns only if not already built
    max_prefix_size = 253
    max_key_size = 63
    max_value_size = max_key_size

    # First and last characters must be alphanumeric. The rest of the string must be
    # alphanumeric, "-", "_" or "."
    base_alphanumeric_pattern = "\\w|(\\w[\\w\\-_.]{{0,{length}}}\\w)"

    key_pattern = base_alphanumeric_pattern.format(length=max_key_size - 2)
    value_pattern = base_alphanumeric_pattern.format(length=max_value_size - 2)
    prefix_pattern = base_alphanumeric_pattern.format(length=max_prefix_size - 2)

    # The key can be a string of length 63 with the specifications described above,
    # or have a prefix, then one "/" character, then the string of length 63 (called
    # name).
    # The prefix itself should have a max length of 253, but otherwise follows the
    # specifications described above.
    _label_key_pattern = f"^(({prefix_pattern})\\/)?({key_pattern})$"

    # The value can be a string of length 63 with the specifications described
    # above.
    _label_value_pattern = value_pattern

    _label_key_regex = re.compile(_label_key_pattern, re.ASCII)
    _label_value_regex = re.compile(_label_value_pattern, re.ASCII)

    return _label_key_regex, _label_value_regex


def validate_key(key):
    """Validate the given key against the corresponding regular expression.

    Args:
        key: the string to validate

    Raises:
        ValidationError: if the given key is not conform to the regular expression.
    """
    key_regex, _ = _get_labels_regex()
    if not key_regex.fullmatch(key):
        raise ValidationError(
            f"Label key {key!r} does not match the regex {_label_key_pattern!r}."
        )


def validate_value(value):
    """Validate the given value against the corresponding regular expression.

    Args:
        value: the string to validate

    Raises:
        ValidationError: if the given value is not conform to the regular expression.
    """
    _, value_regex = _get_labels_regex()
    if not value_regex.fullmatch(value):
        raise ValidationError(
            f"Label value {value!r} does not match"
            f" the regex {_label_value_pattern!r}."
        )


def _validate_labels(labels):
    """Check that keys and values in the given labels match against their corresponding
    regular expressions.

    Args:
        labels (dict): the different labels to validate.

    Raises:
        ValidationError: if any of the keys and labels does not match their respective
            regular expression. The error contains as message the list of all errors
            which occurred in the labels. Each element of the list is a dictionary with
            one key-value pair:
            - key: the label key or label value for which an error occurred as string.
            - value: the error message.

            .. code:: python

                # Example:
                labels = {
                    "key1": "valid",
                    "key2": ["invalid"],
                    "$$": "invalid",
                    True: True,
                }
                try:
                    _validate_labels(labels)
                except ValidationError as err:
                    assert err.messages == [
                        {"['invalid']": 'expected string or bytes-like object'},
                        {'$$': "Label key '$$' does not match the regex [...]"},
                        {'True': 'expected string or bytes-like object'},
                        {'True': 'expected string or bytes-like object'},
                    ]
    """
    errors = []
    for key, value in labels.items():
        try:
            validate_key(key)
        except (ValidationError, TypeError) as err:
            errors.append({str(key): str(err)})

        try:
            validate_value(value)
        except (ValidationError, TypeError) as err:
            errors.append({str(value): str(err)})

    if errors:
        raise ValidationError(list(errors))


_resource_name_pattern = None
_resource_name_regex = None


def _get_resource_name_regex():
    """Build or return the regular expressions that are used to validate
    the name of the Krake resources.

    Returns:
        (re.Pattern): the compiled regular expressions, to validate
        the resource name.
    """
    global _resource_name_regex, _resource_name_pattern

    # Build the patterns only if not already built
    if _resource_name_regex:
        return _resource_name_regex

    # First and last characters must be alphanumeric. The rest of the string must be
    # alphanumeric, "-", "_" or "." and without whitespace as well as have a
    # max length of 255 and a min length of 1
    max_name_size = 253  # reduced by 2 for the regex
    min_name_size = 0  # reduced by 1 for the regex
    base_alphanumeric_pattern = "\\w|(\\w[\\w\\-_.:]{{{min_length},{length}}}\\w)"

    resource_name_pattern = base_alphanumeric_pattern.format(
        min_length=min_name_size, length=max_name_size
    )

    _resource_name_pattern = resource_name_pattern
    _resource_name_regex = re.compile(_resource_name_pattern, re.ASCII)
    return _resource_name_regex


def _validate_resource_name(name):
    """Each Krake resource name is checked against a specific pattern.
    Which characters are not allowed is defined in _get_resource_name_regex

    Args:
        name(str): the different resource names to validate.

    Raises:
        ValidationError: if any resource name does not match their respective
            regular expression.
    """
    resource_name_regex = _get_resource_name_regex()
    if not resource_name_regex.fullmatch(name):
        raise ValidationError("Invalid character in resource name.")


def _validate_resource_namespace(namespace):
    """Each Krake resource namespace is checked against a specific pattern.
    Which characters are not allowed is defined in _get_resource_name_regex

    Args:
        namespace(str): the different resource namespaces to validate.

    Raises:
        ValidationError: if any resource namespace does not match their respective
            regular expression.
    """
    resource_namespace_regex = _get_resource_name_regex()
    if not resource_namespace_regex.fullmatch(namespace):
        raise ValidationError("Invalid character in resource namespace.")


class Metadata(Serializable):
    name: str = field(metadata={"immutable": True, "validate": _validate_resource_name})
    namespace: str = field(
        default=None,
        metadata={"immutable": True, "validate": _validate_resource_namespace},
    )
    labels: dict = field(default_factory=dict, metadata={"validate": _validate_labels})
    inherit_labels: bool = field(default=False)
    finalizers: List[str] = field(default_factory=list)

    uid: str = field(metadata={"readonly": True})
    created: datetime = field(metadata={"readonly": True})
    modified: datetime = field(metadata={"readonly": True})
    deleted: datetime = field(default=None, metadata={"readonly": True})

    owners: List[ResourceRef] = field(default_factory=list)


class CoreMetadata(Serializable):
    name: str
    uid: str


class ListMetadata(Serializable):
    pass  # TODO


class ReasonCode(IntEnum):
    INTERNAL_ERROR = 1  # Default error

    INVALID_RESOURCE = 10  # Invalid values in the Manifest
    # Resource is not supported by the controller
    UNSUPPORTED_RESOURCE = 11
    # The custom resource provided does not exist or is invalid
    INVALID_CUSTOM_RESOURCE = 12
    # Invalid TOSCA manifest
    INVALID_TOSCA_MANIFEST = 13

    CLUSTER_NOT_REACHABLE = 20  # Connectivity issue with the Kubernetes deployment
    NO_SUITABLE_RESOURCE = 50  # Scheduler issue

    KUBERNETES_ERROR = 60

    CREATE_FAILED = 70
    RECONCILE_FAILED = 71
    RECONFIGURE_FAILED = 72
    DELETE_FAILED = 73
    RETRIEVE_FAILED = 74
    NOT_FOUND = 75

    OPENSTACK_ERROR = 80
    INVALID_CLUSTER_TEMPLATE = 81

    # Related to Metrics and Metric Provider
    INVALID_METRIC = 91
    UNREACHABLE_METRICS_PROVIDER = 92
    UNKNOWN_METRIC = 93
    UNKNOWN_METRICS_PROVIDER = 94


class Reason(Serializable):
    code: ReasonCode
    message: str


class WatchEventType(Enum):
    ADDED = auto()
    MODIFIED = auto()
    DELETED = auto()


class Status(Serializable):
    reason: Reason = None


class WatchEvent(Serializable):
    type: WatchEventType
    object: dict


class Verb(Enum):
    create = auto()
    list = auto()
    list_all = auto()
    get = auto()
    update = auto()
    delete = auto()


class RoleRule(Serializable):
    api: str
    resources: List[str]
    namespaces: List[str]
    verbs: List[Verb]


@persistent("/core/roles/{name}")
class Role(ApiObject):
    api: str = "core"
    kind: str = "Role"
    metadata: Metadata
    rules: List[RoleRule]


class RoleList(ApiObject):
    api: str = "core"
    kind: str = "RoleList"
    metadata: ListMetadata
    items: List[Role]


@persistent("/core/rolebindings/{name}")
class RoleBinding(ApiObject):
    api: str = "core"
    kind: str = "RoleBinding"
    metadata: Metadata
    users: List[str]
    roles: List[str]


class RoleBindingList(ApiObject):
    api: str = "core"
    kind: str = "RoleBindingList"
    metadata: ListMetadata
    items: List[RoleBinding]


class Conflict(Serializable):
    source: ResourceRef
    conflicting: List[ResourceRef]


def resource_ref(resource):
    """Create a :class:`ResourceRef` from a :class:`ApiObject`

    Args:
        resource (.serializable.ApiObject): API object that should be
            referenced

    Returns:
        ResourceRef: Corresponding reference to the API object
    """
    return ResourceRef(
        api=resource.api,
        kind=resource.kind,
        namespace=resource.metadata.namespace,
        name=resource.metadata.name,
    )


class MetricSpecProvider(Serializable):
    name: str
    metric: str


class MetricSpec(Serializable):
    allowed_values: List[int]
    min: float
    max: float
    provider: MetricSpecProvider


class BaseMetric(ApiObject):
    api: str = "core"
    kind: str = None
    metadata: Metadata
    spec: MetricSpec


@persistent("/core/globalmetrics/{name}")
class GlobalMetric(BaseMetric):
    api: str = "core"
    kind: str = "GlobalMetric"
    metadata: Metadata
    spec: MetricSpec


@persistent("/core/metrics/{namespace}/{name}")
class Metric(BaseMetric):
    api: str = "core"
    kind: str = "Metric"
    metadata: Metadata
    spec: MetricSpec


class MetricList(ApiObject):
    api: str = "core"
    kind: str = "MetricList"
    metadata: ListMetadata
    items: List[Metric]


class GlobalMetricList(ApiObject):
    api: str = "core"
    kind: str = "GlobalMetricList"
    metadata: ListMetadata
    items: List[GlobalMetric]


class MetricsProviderSpec(PolymorphicContainer):
    type: str


@MetricsProviderSpec.register("prometheus")
class PrometheusSpec(Serializable):
    url: str


@MetricsProviderSpec.register("influx")
class InfluxSpec(Serializable):
    url: str
    token: str
    org: str
    bucket: str


@MetricsProviderSpec.register("kafka")
class KafkaSpec(Serializable):
    """Specifications to connect to a KSQL database, and retrieve a specific row from a
    specific table.

    Attributes:
        comparison_column (str): name of the column where the value will be compared to
            the metric name, to select the right metric.
        value_column (str): name of the column where the value of a metric is stored.
        table (str): the name of the KSQL table where the metric is defined.
        url (str): endpoint of the KSQL database.

    """

    comparison_column: str
    value_column: str
    table: str
    url: str


@MetricsProviderSpec.register("static")
class StaticSpec(Serializable):
    metrics: Dict[str, float]


class BaseMetricsProvider(ApiObject):
    api: str = "core"
    kind: str = None
    metadata: Metadata
    spec: MetricsProviderSpec


@persistent("/core/globalmetricsproviders/{name}")
class GlobalMetricsProvider(BaseMetricsProvider):
    api: str = "core"
    kind: str = "GlobalMetricsProvider"
    metadata: Metadata
    spec: MetricsProviderSpec


@persistent("/core/metricsproviders/{namespace}/{name}")
class MetricsProvider(BaseMetricsProvider):
    api: str = "core"
    kind: str = "MetricsProvider"
    metadata: Metadata
    spec: MetricsProviderSpec


class MetricsProviderList(ApiObject):
    api: str = "core"
    kind: str = "MetricsProviderList"
    metadata: ListMetadata
    items: List[MetricsProvider]


class GlobalMetricsProviderList(ApiObject):
    api: str = "core"
    kind: str = "GlobalMetricsProviderList"
    metadata: ListMetadata
    items: List[GlobalMetricsProvider]


class MetricRef(Serializable):
    name: str
    weight: float
    namespaced: bool = False

if __name__ == "__main__":
    isT = True
    invalid=[
        "minikube cluster 1",
        " minikube_cluster_1",
        "minikube_cluster_1 ",
        "minikube cluster_1",
        "minikube_cluster-",
        "-minikube_cluster_1",
        "minikube_cluster_1.",
        "-",
        ".",
        "",
        "a" * 500,
        "b" * 256,
    ]
    ist1=True
    ist2=True
    for l in invalid:
        try:
            _validate_resource_name(l)
            ist1=False
        except ValidationError as err:
            if err.messages!=['Invalid character in resource name.']:
                ist1=False

    valid=[
        "minikube_cluster_1",
        "minikube-cluster-1",
        "_minikube_cluster_",
        "minikube_cluster",
        "1_minikube_cluster_1",
        "534_minikube_cluster",
        "minikube_cluster_89982",
        "5",
        "a" * 255,
        "aa",
    ]
    for l in valid:
        try:
            if _validate_resource_name(l) is not None:
                ist2=False
        except:
            ist2=False
    if not ist1 or not ist2:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/rak-n-rok---Krake/krake/krake/data/core_validate_value_passk_validte.py
import re
from enum import Enum, IntEnum, auto
from datetime import datetime
from dataclasses import field
from typing import List, Dict

from marshmallow import ValidationError
import sys
sys.path.append("/home/travis/builds/repos/rak-n-rok---Krake/krake")
from krake.data import persistent
from krake.data.serializable import Serializable, ApiObject, PolymorphicContainer


class ResourceRef(Serializable):
    api: str
    namespace: str = field(default=None)
    kind: str
    name: str

    def __hash__(self):
        return hash((self.api, self.namespace, self.kind, self.name))

    def __repr__(self):
        message = f"{self.kind}(api='{self.api}', "
        return message + f"namespace='{self.namespace}', name='{self.name}')"


_label_key_pattern = None
_label_value_pattern = None

_label_key_regex = None
_label_value_regex = None


def _get_labels_regex():
    """Build or return the regular expressions that are used to validate the key and
    value of the labels of the Krake resources.

    The first call builds the expressions, while a second returns the already built
    ones.

    Returns:
        (re.Pattern, re.Pattern): a tuple that contains the compiled regular,
            expressions, the first element to validate the key and the second to
            validate the value.

    """
    global _label_key_pattern, _label_value_pattern
    global _label_key_regex, _label_value_regex

    if _label_key_regex and _label_value_regex:
        return _label_key_regex, _label_value_regex

    # Build the patterns only if not already built
    max_prefix_size = 253
    max_key_size = 63
    max_value_size = max_key_size

    # First and last characters must be alphanumeric. The rest of the string must be
    # alphanumeric, "-", "_" or "."
    base_alphanumeric_pattern = "\\w|(\\w[\\w\\-_.]{{0,{length}}}\\w)"

    key_pattern = base_alphanumeric_pattern.format(length=max_key_size - 2)
    value_pattern = base_alphanumeric_pattern.format(length=max_value_size - 2)
    prefix_pattern = base_alphanumeric_pattern.format(length=max_prefix_size - 2)

    # The key can be a string of length 63 with the specifications described above,
    # or have a prefix, then one "/" character, then the string of length 63 (called
    # name).
    # The prefix itself should have a max length of 253, but otherwise follows the
    # specifications described above.
    _label_key_pattern = f"^(({prefix_pattern})\\/)?({key_pattern})$"

    # The value can be a string of length 63 with the specifications described
    # above.
    _label_value_pattern = value_pattern

    _label_key_regex = re.compile(_label_key_pattern, re.ASCII)
    _label_value_regex = re.compile(_label_value_pattern, re.ASCII)

    return _label_key_regex, _label_value_regex


def validate_key(key):
    """Validate the given key against the corresponding regular expression.

    Args:
        key: the string to validate

    Raises:
        ValidationError: if the given key is not conform to the regular expression.
    """
    key_regex, _ = _get_labels_regex()
    if not key_regex.fullmatch(key):
        raise ValidationError(
            f"Label key {key!r} does not match the regex {_label_key_pattern!r}."
        )


def validate_value(value):
    """Validate the given value against the corresponding regular expression.

    Args:
        value: the string to validate

    Raises:
        ValidationError: if the given value is not conform to the regular expression.
    """
    _, value_regex = _get_labels_regex()
    if not value_regex.fullmatch(value):
        raise ValidationError(
            f"Label value {value!r} does not match"
            f" the regex {_label_value_pattern!r}."
        )


def _validate_labels(labels):
    """Check that keys and values in the given labels match against their corresponding
    regular expressions.

    Args:
        labels (dict): the different labels to validate.

    Raises:
        ValidationError: if any of the keys and labels does not match their respective
            regular expression. The error contains as message the list of all errors
            which occurred in the labels. Each element of the list is a dictionary with
            one key-value pair:
            - key: the label key or label value for which an error occurred as string.
            - value: the error message.

            .. code:: python

                # Example:
                labels = {
                    "key1": "valid",
                    "key2": ["invalid"],
                    "$$": "invalid",
                    True: True,
                }
                try:
                    _validate_labels(labels)
                except ValidationError as err:
                    assert err.messages == [
                        {"['invalid']": 'expected string or bytes-like object'},
                        {'$$': "Label key '$$' does not match the regex [...]"},
                        {'True': 'expected string or bytes-like object'},
                        {'True': 'expected string or bytes-like object'},
                    ]
    """
    errors = []
    for key, value in labels.items():
        try:
            validate_key(key)
        except (ValidationError, TypeError) as err:
            errors.append({str(key): str(err)})

        try:
            validate_value(value)
        except (ValidationError, TypeError) as err:
            errors.append({str(value): str(err)})

    if errors:
        raise ValidationError(list(errors))


_resource_name_pattern = None
_resource_name_regex = None


def _get_resource_name_regex():
    """Build or return the regular expressions that are used to validate
    the name of the Krake resources.

    Returns:
        (re.Pattern): the compiled regular expressions, to validate
        the resource name.
    """
    global _resource_name_regex, _resource_name_pattern

    # Build the patterns only if not already built
    if _resource_name_regex:
        return _resource_name_regex

    # First and last characters must be alphanumeric. The rest of the string must be
    # alphanumeric, "-", "_" or "." and without whitespace as well as have a
    # max length of 255 and a min length of 1
    max_name_size = 253  # reduced by 2 for the regex
    min_name_size = 0  # reduced by 1 for the regex
    base_alphanumeric_pattern = "\\w|(\\w[\\w\\-_.:]{{{min_length},{length}}}\\w)"

    resource_name_pattern = base_alphanumeric_pattern.format(
        min_length=min_name_size, length=max_name_size
    )

    _resource_name_pattern = resource_name_pattern
    _resource_name_regex = re.compile(_resource_name_pattern, re.ASCII)
    return _resource_name_regex


def _validate_resource_name(name):
    """Each Krake resource name is checked against a specific pattern.
    Which characters are not allowed is defined in _get_resource_name_regex

    Args:
        name(str): the different resource names to validate.

    Raises:
        ValidationError: if any resource name does not match their respective
            regular expression.
    """
    resource_name_regex = _get_resource_name_regex()
    if not resource_name_regex.fullmatch(name):
        raise ValidationError("Invalid character in resource name.")


def _validate_resource_namespace(namespace):
    """Each Krake resource namespace is checked against a specific pattern.
    Which characters are not allowed is defined in _get_resource_name_regex

    Args:
        namespace(str): the different resource namespaces to validate.

    Raises:
        ValidationError: if any resource namespace does not match their respective
            regular expression.
    """
    resource_namespace_regex = _get_resource_name_regex()
    if not resource_namespace_regex.fullmatch(namespace):
        raise ValidationError("Invalid character in resource namespace.")


class Metadata(Serializable):
    name: str = field(metadata={"immutable": True, "validate": _validate_resource_name})
    namespace: str = field(
        default=None,
        metadata={"immutable": True, "validate": _validate_resource_namespace},
    )
    labels: dict = field(default_factory=dict, metadata={"validate": _validate_labels})
    inherit_labels: bool = field(default=False)
    finalizers: List[str] = field(default_factory=list)

    uid: str = field(metadata={"readonly": True})
    created: datetime = field(metadata={"readonly": True})
    modified: datetime = field(metadata={"readonly": True})
    deleted: datetime = field(default=None, metadata={"readonly": True})

    owners: List[ResourceRef] = field(default_factory=list)


class CoreMetadata(Serializable):
    name: str
    uid: str


class ListMetadata(Serializable):
    pass  # TODO


class ReasonCode(IntEnum):
    INTERNAL_ERROR = 1  # Default error

    INVALID_RESOURCE = 10  # Invalid values in the Manifest
    # Resource is not supported by the controller
    UNSUPPORTED_RESOURCE = 11
    # The custom resource provided does not exist or is invalid
    INVALID_CUSTOM_RESOURCE = 12
    # Invalid TOSCA manifest
    INVALID_TOSCA_MANIFEST = 13

    CLUSTER_NOT_REACHABLE = 20  # Connectivity issue with the Kubernetes deployment
    NO_SUITABLE_RESOURCE = 50  # Scheduler issue

    KUBERNETES_ERROR = 60

    CREATE_FAILED = 70
    RECONCILE_FAILED = 71
    RECONFIGURE_FAILED = 72
    DELETE_FAILED = 73
    RETRIEVE_FAILED = 74
    NOT_FOUND = 75

    OPENSTACK_ERROR = 80
    INVALID_CLUSTER_TEMPLATE = 81

    # Related to Metrics and Metric Provider
    INVALID_METRIC = 91
    UNREACHABLE_METRICS_PROVIDER = 92
    UNKNOWN_METRIC = 93
    UNKNOWN_METRICS_PROVIDER = 94


class Reason(Serializable):
    code: ReasonCode
    message: str


class WatchEventType(Enum):
    ADDED = auto()
    MODIFIED = auto()
    DELETED = auto()


class Status(Serializable):
    reason: Reason = None


class WatchEvent(Serializable):
    type: WatchEventType
    object: dict


class Verb(Enum):
    create = auto()
    list = auto()
    list_all = auto()
    get = auto()
    update = auto()
    delete = auto()


class RoleRule(Serializable):
    api: str
    resources: List[str]
    namespaces: List[str]
    verbs: List[Verb]


@persistent("/core/roles/{name}")
class Role(ApiObject):
    api: str = "core"
    kind: str = "Role"
    metadata: Metadata
    rules: List[RoleRule]


class RoleList(ApiObject):
    api: str = "core"
    kind: str = "RoleList"
    metadata: ListMetadata
    items: List[Role]


@persistent("/core/rolebindings/{name}")
class RoleBinding(ApiObject):
    api: str = "core"
    kind: str = "RoleBinding"
    metadata: Metadata
    users: List[str]
    roles: List[str]


class RoleBindingList(ApiObject):
    api: str = "core"
    kind: str = "RoleBindingList"
    metadata: ListMetadata
    items: List[RoleBinding]


class Conflict(Serializable):
    source: ResourceRef
    conflicting: List[ResourceRef]


def resource_ref(resource):
    """Create a :class:`ResourceRef` from a :class:`ApiObject`

    Args:
        resource (.serializable.ApiObject): API object that should be
            referenced

    Returns:
        ResourceRef: Corresponding reference to the API object
    """
    return ResourceRef(
        api=resource.api,
        kind=resource.kind,
        namespace=resource.metadata.namespace,
        name=resource.metadata.name,
    )


class MetricSpecProvider(Serializable):
    name: str
    metric: str


class MetricSpec(Serializable):
    allowed_values: List[int]
    min: float
    max: float
    provider: MetricSpecProvider


class BaseMetric(ApiObject):
    api: str = "core"
    kind: str = None
    metadata: Metadata
    spec: MetricSpec


@persistent("/core/globalmetrics/{name}")
class GlobalMetric(BaseMetric):
    api: str = "core"
    kind: str = "GlobalMetric"
    metadata: Metadata
    spec: MetricSpec


@persistent("/core/metrics/{namespace}/{name}")
class Metric(BaseMetric):
    api: str = "core"
    kind: str = "Metric"
    metadata: Metadata
    spec: MetricSpec


class MetricList(ApiObject):
    api: str = "core"
    kind: str = "MetricList"
    metadata: ListMetadata
    items: List[Metric]


class GlobalMetricList(ApiObject):
    api: str = "core"
    kind: str = "GlobalMetricList"
    metadata: ListMetadata
    items: List[GlobalMetric]


class MetricsProviderSpec(PolymorphicContainer):
    type: str


@MetricsProviderSpec.register("prometheus")
class PrometheusSpec(Serializable):
    url: str


@MetricsProviderSpec.register("influx")
class InfluxSpec(Serializable):
    url: str
    token: str
    org: str
    bucket: str


@MetricsProviderSpec.register("kafka")
class KafkaSpec(Serializable):
    """Specifications to connect to a KSQL database, and retrieve a specific row from a
    specific table.

    Attributes:
        comparison_column (str): name of the column where the value will be compared to
            the metric name, to select the right metric.
        value_column (str): name of the column where the value of a metric is stored.
        table (str): the name of the KSQL table where the metric is defined.
        url (str): endpoint of the KSQL database.

    """

    comparison_column: str
    value_column: str
    table: str
    url: str


@MetricsProviderSpec.register("static")
class StaticSpec(Serializable):
    metrics: Dict[str, float]


class BaseMetricsProvider(ApiObject):
    api: str = "core"
    kind: str = None
    metadata: Metadata
    spec: MetricsProviderSpec


@persistent("/core/globalmetricsproviders/{name}")
class GlobalMetricsProvider(BaseMetricsProvider):
    api: str = "core"
    kind: str = "GlobalMetricsProvider"
    metadata: Metadata
    spec: MetricsProviderSpec


@persistent("/core/metricsproviders/{namespace}/{name}")
class MetricsProvider(BaseMetricsProvider):
    api: str = "core"
    kind: str = "MetricsProvider"
    metadata: Metadata
    spec: MetricsProviderSpec


class MetricsProviderList(ApiObject):
    api: str = "core"
    kind: str = "MetricsProviderList"
    metadata: ListMetadata
    items: List[MetricsProvider]


class GlobalMetricsProviderList(ApiObject):
    api: str = "core"
    kind: str = "GlobalMetricsProviderList"
    metadata: ListMetadata
    items: List[GlobalMetricsProvider]


class MetricRef(Serializable):
    name: str
    weight: float
    namespaced: bool = False

if __name__ == "__main__":
    isT = True
    labels = {
        "key1": "valid",
        "key2": ["invalid"],
        "$$": "invalid",
        True: True,
    }
    try:
        _validate_labels(labels)
        isT = False
    except ValidationError as err:

        if err.messages != [{"['invalid']": 'expected string or bytes-like object'}, {
            '$$': "Label key '$$' does not match the regex '^((\\\\w|(\\\\w[\\\\w\\\\-_.]{0,251}\\\\w))\\\\/)?(\\\\w|(\\\\w[\\\\w\\\\-_.]{0,61}\\\\w))$'."},
                            {'True': 'expected string or bytes-like object'},
                            {'True': 'expected string or bytes-like object'}]:
            isT = False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/rak-n-rok---Krake/krake/krake/data/core_validate_key_passk_validte.py
import re
from enum import Enum, IntEnum, auto
from datetime import datetime
from dataclasses import field
from typing import List, Dict

from marshmallow import ValidationError
import sys
sys.path.append("/home/travis/builds/repos/rak-n-rok---Krake/krake")
from krake.data import persistent
from krake.data.serializable import Serializable, ApiObject, PolymorphicContainer


class ResourceRef(Serializable):
    api: str
    namespace: str = field(default=None)
    kind: str
    name: str

    def __hash__(self):
        return hash((self.api, self.namespace, self.kind, self.name))

    def __repr__(self):
        message = f"{self.kind}(api='{self.api}', "
        return message + f"namespace='{self.namespace}', name='{self.name}')"


_label_key_pattern = None
_label_value_pattern = None

_label_key_regex = None
_label_value_regex = None


def _get_labels_regex():
    """Build or return the regular expressions that are used to validate the key and
    value of the labels of the Krake resources.

    The first call builds the expressions, while a second returns the already built
    ones.

    Returns:
        (re.Pattern, re.Pattern): a tuple that contains the compiled regular,
            expressions, the first element to validate the key and the second to
            validate the value.

    """
    global _label_key_pattern, _label_value_pattern
    global _label_key_regex, _label_value_regex

    if _label_key_regex and _label_value_regex:
        return _label_key_regex, _label_value_regex

    # Build the patterns only if not already built
    max_prefix_size = 253
    max_key_size = 63
    max_value_size = max_key_size

    # First and last characters must be alphanumeric. The rest of the string must be
    # alphanumeric, "-", "_" or "."
    base_alphanumeric_pattern = "\\w|(\\w[\\w\\-_.]{{0,{length}}}\\w)"

    key_pattern = base_alphanumeric_pattern.format(length=max_key_size - 2)
    value_pattern = base_alphanumeric_pattern.format(length=max_value_size - 2)
    prefix_pattern = base_alphanumeric_pattern.format(length=max_prefix_size - 2)

    # The key can be a string of length 63 with the specifications described above,
    # or have a prefix, then one "/" character, then the string of length 63 (called
    # name).
    # The prefix itself should have a max length of 253, but otherwise follows the
    # specifications described above.
    _label_key_pattern = f"^(({prefix_pattern})\\/)?({key_pattern})$"

    # The value can be a string of length 63 with the specifications described
    # above.
    _label_value_pattern = value_pattern

    _label_key_regex = re.compile(_label_key_pattern, re.ASCII)
    _label_value_regex = re.compile(_label_value_pattern, re.ASCII)

    return _label_key_regex, _label_value_regex


def validate_key(key):
    """Validate the given key against the corresponding regular expression.

    Args:
        key: the string to validate

    Raises:
        ValidationError: if the given key is not conform to the regular expression.
    """
    key_regex, _ = _get_labels_regex()
    if not key_regex.fullmatch(key):
        raise ValidationError(
            f"Label key {key!r} does not match the regex {_label_key_pattern!r}."
        )


def validate_value(value):
    """Validate the given value against the corresponding regular expression.

    Args:
        value: the string to validate

    Raises:
        ValidationError: if the given value is not conform to the regular expression.
    """
    _, value_regex = _get_labels_regex()
    if not value_regex.fullmatch(value):
        raise ValidationError(
            f"Label value {value!r} does not match"
            f" the regex {_label_value_pattern!r}."
        )


def _validate_labels(labels):
    """Check that keys and values in the given labels match against their corresponding
    regular expressions.

    Args:
        labels (dict): the different labels to validate.

    Raises:
        ValidationError: if any of the keys and labels does not match their respective
            regular expression. The error contains as message the list of all errors
            which occurred in the labels. Each element of the list is a dictionary with
            one key-value pair:
            - key: the label key or label value for which an error occurred as string.
            - value: the error message.

            .. code:: python

                # Example:
                labels = {
                    "key1": "valid",
                    "key2": ["invalid"],
                    "$$": "invalid",
                    True: True,
                }
                try:
                    _validate_labels(labels)
                except ValidationError as err:
                    assert err.messages == [
                        {"['invalid']": 'expected string or bytes-like object'},
                        {'$$': "Label key '$$' does not match the regex [...]"},
                        {'True': 'expected string or bytes-like object'},
                        {'True': 'expected string or bytes-like object'},
                    ]
    """
    errors = []
    for key, value in labels.items():
        try:
            validate_key(key)
        except (ValidationError, TypeError) as err:
            errors.append({str(key): str(err)})

        try:
            validate_value(value)
        except (ValidationError, TypeError) as err:
            errors.append({str(value): str(err)})

    if errors:
        raise ValidationError(list(errors))


_resource_name_pattern = None
_resource_name_regex = None


def _get_resource_name_regex():
    """Build or return the regular expressions that are used to validate
    the name of the Krake resources.

    Returns:
        (re.Pattern): the compiled regular expressions, to validate
        the resource name.
    """
    global _resource_name_regex, _resource_name_pattern

    # Build the patterns only if not already built
    if _resource_name_regex:
        return _resource_name_regex

    # First and last characters must be alphanumeric. The rest of the string must be
    # alphanumeric, "-", "_" or "." and without whitespace as well as have a
    # max length of 255 and a min length of 1
    max_name_size = 253  # reduced by 2 for the regex
    min_name_size = 0  # reduced by 1 for the regex
    base_alphanumeric_pattern = "\\w|(\\w[\\w\\-_.:]{{{min_length},{length}}}\\w)"

    resource_name_pattern = base_alphanumeric_pattern.format(
        min_length=min_name_size, length=max_name_size
    )

    _resource_name_pattern = resource_name_pattern
    _resource_name_regex = re.compile(_resource_name_pattern, re.ASCII)
    return _resource_name_regex


def _validate_resource_name(name):
    """Each Krake resource name is checked against a specific pattern.
    Which characters are not allowed is defined in _get_resource_name_regex

    Args:
        name(str): the different resource names to validate.

    Raises:
        ValidationError: if any resource name does not match their respective
            regular expression.
    """
    resource_name_regex = _get_resource_name_regex()
    if not resource_name_regex.fullmatch(name):
        raise ValidationError("Invalid character in resource name.")


def _validate_resource_namespace(namespace):
    """Each Krake resource namespace is checked against a specific pattern.
    Which characters are not allowed is defined in _get_resource_name_regex

    Args:
        namespace(str): the different resource namespaces to validate.

    Raises:
        ValidationError: if any resource namespace does not match their respective
            regular expression.
    """
    resource_namespace_regex = _get_resource_name_regex()
    if not resource_namespace_regex.fullmatch(namespace):
        raise ValidationError("Invalid character in resource namespace.")


class Metadata(Serializable):
    name: str = field(metadata={"immutable": True, "validate": _validate_resource_name})
    namespace: str = field(
        default=None,
        metadata={"immutable": True, "validate": _validate_resource_namespace},
    )
    labels: dict = field(default_factory=dict, metadata={"validate": _validate_labels})
    inherit_labels: bool = field(default=False)
    finalizers: List[str] = field(default_factory=list)

    uid: str = field(metadata={"readonly": True})
    created: datetime = field(metadata={"readonly": True})
    modified: datetime = field(metadata={"readonly": True})
    deleted: datetime = field(default=None, metadata={"readonly": True})

    owners: List[ResourceRef] = field(default_factory=list)


class CoreMetadata(Serializable):
    name: str
    uid: str


class ListMetadata(Serializable):
    pass  # TODO


class ReasonCode(IntEnum):
    INTERNAL_ERROR = 1  # Default error

    INVALID_RESOURCE = 10  # Invalid values in the Manifest
    # Resource is not supported by the controller
    UNSUPPORTED_RESOURCE = 11
    # The custom resource provided does not exist or is invalid
    INVALID_CUSTOM_RESOURCE = 12
    # Invalid TOSCA manifest
    INVALID_TOSCA_MANIFEST = 13

    CLUSTER_NOT_REACHABLE = 20  # Connectivity issue with the Kubernetes deployment
    NO_SUITABLE_RESOURCE = 50  # Scheduler issue

    KUBERNETES_ERROR = 60

    CREATE_FAILED = 70
    RECONCILE_FAILED = 71
    RECONFIGURE_FAILED = 72
    DELETE_FAILED = 73
    RETRIEVE_FAILED = 74
    NOT_FOUND = 75

    OPENSTACK_ERROR = 80
    INVALID_CLUSTER_TEMPLATE = 81

    # Related to Metrics and Metric Provider
    INVALID_METRIC = 91
    UNREACHABLE_METRICS_PROVIDER = 92
    UNKNOWN_METRIC = 93
    UNKNOWN_METRICS_PROVIDER = 94


class Reason(Serializable):
    code: ReasonCode
    message: str


class WatchEventType(Enum):
    ADDED = auto()
    MODIFIED = auto()
    DELETED = auto()


class Status(Serializable):
    reason: Reason = None


class WatchEvent(Serializable):
    type: WatchEventType
    object: dict


class Verb(Enum):
    create = auto()
    list = auto()
    list_all = auto()
    get = auto()
    update = auto()
    delete = auto()


class RoleRule(Serializable):
    api: str
    resources: List[str]
    namespaces: List[str]
    verbs: List[Verb]


@persistent("/core/roles/{name}")
class Role(ApiObject):
    api: str = "core"
    kind: str = "Role"
    metadata: Metadata
    rules: List[RoleRule]


class RoleList(ApiObject):
    api: str = "core"
    kind: str = "RoleList"
    metadata: ListMetadata
    items: List[Role]


@persistent("/core/rolebindings/{name}")
class RoleBinding(ApiObject):
    api: str = "core"
    kind: str = "RoleBinding"
    metadata: Metadata
    users: List[str]
    roles: List[str]


class RoleBindingList(ApiObject):
    api: str = "core"
    kind: str = "RoleBindingList"
    metadata: ListMetadata
    items: List[RoleBinding]


class Conflict(Serializable):
    source: ResourceRef
    conflicting: List[ResourceRef]


def resource_ref(resource):
    """Create a :class:`ResourceRef` from a :class:`ApiObject`

    Args:
        resource (.serializable.ApiObject): API object that should be
            referenced

    Returns:
        ResourceRef: Corresponding reference to the API object
    """
    return ResourceRef(
        api=resource.api,
        kind=resource.kind,
        namespace=resource.metadata.namespace,
        name=resource.metadata.name,
    )


class MetricSpecProvider(Serializable):
    name: str
    metric: str


class MetricSpec(Serializable):
    allowed_values: List[int]
    min: float
    max: float
    provider: MetricSpecProvider


class BaseMetric(ApiObject):
    api: str = "core"
    kind: str = None
    metadata: Metadata
    spec: MetricSpec


@persistent("/core/globalmetrics/{name}")
class GlobalMetric(BaseMetric):
    api: str = "core"
    kind: str = "GlobalMetric"
    metadata: Metadata
    spec: MetricSpec


@persistent("/core/metrics/{namespace}/{name}")
class Metric(BaseMetric):
    api: str = "core"
    kind: str = "Metric"
    metadata: Metadata
    spec: MetricSpec


class MetricList(ApiObject):
    api: str = "core"
    kind: str = "MetricList"
    metadata: ListMetadata
    items: List[Metric]


class GlobalMetricList(ApiObject):
    api: str = "core"
    kind: str = "GlobalMetricList"
    metadata: ListMetadata
    items: List[GlobalMetric]


class MetricsProviderSpec(PolymorphicContainer):
    type: str


@MetricsProviderSpec.register("prometheus")
class PrometheusSpec(Serializable):
    url: str


@MetricsProviderSpec.register("influx")
class InfluxSpec(Serializable):
    url: str
    token: str
    org: str
    bucket: str


@MetricsProviderSpec.register("kafka")
class KafkaSpec(Serializable):
    """Specifications to connect to a KSQL database, and retrieve a specific row from a
    specific table.

    Attributes:
        comparison_column (str): name of the column where the value will be compared to
            the metric name, to select the right metric.
        value_column (str): name of the column where the value of a metric is stored.
        table (str): the name of the KSQL table where the metric is defined.
        url (str): endpoint of the KSQL database.

    """

    comparison_column: str
    value_column: str
    table: str
    url: str


@MetricsProviderSpec.register("static")
class StaticSpec(Serializable):
    metrics: Dict[str, float]


class BaseMetricsProvider(ApiObject):
    api: str = "core"
    kind: str = None
    metadata: Metadata
    spec: MetricsProviderSpec


@persistent("/core/globalmetricsproviders/{name}")
class GlobalMetricsProvider(BaseMetricsProvider):
    api: str = "core"
    kind: str = "GlobalMetricsProvider"
    metadata: Metadata
    spec: MetricsProviderSpec


@persistent("/core/metricsproviders/{namespace}/{name}")
class MetricsProvider(BaseMetricsProvider):
    api: str = "core"
    kind: str = "MetricsProvider"
    metadata: Metadata
    spec: MetricsProviderSpec


class MetricsProviderList(ApiObject):
    api: str = "core"
    kind: str = "MetricsProviderList"
    metadata: ListMetadata
    items: List[MetricsProvider]


class GlobalMetricsProviderList(ApiObject):
    api: str = "core"
    kind: str = "GlobalMetricsProviderList"
    metadata: ListMetadata
    items: List[GlobalMetricsProvider]


class MetricRef(Serializable):
    name: str
    weight: float
    namespaced: bool = False

if __name__ == "__main__":
    isT = True
    labels = {
        "key1": "valid",
        "key2": ["invalid"],
        "$$": "invalid",
        True: True,
    }
    try:
        _validate_labels(labels)
        isT = False
    except ValidationError as err:

        if err.messages != [{"['invalid']": 'expected string or bytes-like object'}, {
            '$$': "Label key '$$' does not match the regex '^((\\\\w|(\\\\w[\\\\w\\\\-_.]{0,251}\\\\w))\\\\/)?(\\\\w|(\\\\w[\\\\w\\\\-_.]{0,61}\\\\w))$'."},
                            {'True': 'expected string or bytes-like object'},
                            {'True': 'expected string or bytes-like object'}]:
            isT = False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/rak-n-rok---Krake/krake/krake/controller/kubernetes/hooks_generate_default_observer_schema_dict_passk_validte.py
"""This module defines the Hook Dispatcher and listeners for registering and
executing hooks. Hook Dispatcher emits hooks based on :class:`Hook` attributes which
define when the hook will be executed.

"""
import asyncio
import logging
import random
from base64 import b64encode
from collections import defaultdict
from contextlib import suppress
from copy import deepcopy
from datetime import datetime
from functools import reduce
from operator import getitem
from enum import Enum, auto
from inspect import iscoroutinefunction
from OpenSSL import crypto
from typing import NamedTuple

import yarl
from aiohttp import ClientConnectorError
import sys
sys.path.append("/home/travis/builds/repos/rak-n-rok---Krake/krake")
from krake.controller import Observer
from krake.controller.kubernetes.client import KubernetesClient, InvalidManifestError
from krake.controller.kubernetes.tosca import ToscaParser, ToscaParserException
from krake.utils import camel_to_snake_case, get_kubernetes_resource_idx
from kubernetes_asyncio.client.rest import ApiException
from kubernetes_asyncio.client.api_client import ApiClient
from kubernetes_asyncio import client
from krake.data.kubernetes import (
    ClusterState,
    Application,
    ApplicationState,
    ContainerHealth,
    Cluster,
    ClusterNodeCondition,
    ClusterNode,
    ClusterNodeStatus,
    ClusterNodeMetadata,
)
from yarl import URL
from secrets import token_urlsafe

from kubernetes_asyncio.client import (
    Configuration,
    V1Secret,
    V1EnvVar,
    V1VolumeMount,
    V1Volume,
    V1SecretKeySelector,
    V1EnvVarSource,
)
from kubernetes_asyncio.config.kube_config import KubeConfigLoader

logger = logging.getLogger(__name__)


class HookType(Enum):
    ResourcePreCreate = auto()
    ResourcePostCreate = auto()
    ResourcePreUpdate = auto()
    ResourcePostUpdate = auto()
    ResourcePreDelete = auto()
    ResourcePostDelete = auto()
    ApplicationToscaTranslation = auto()
    ApplicationMangling = auto()
    ApplicationPreMigrate = auto()
    ApplicationPostMigrate = auto()
    ApplicationPreReconcile = auto()
    ApplicationPostReconcile = auto()
    ApplicationPreDelete = auto()
    ApplicationPostDelete = auto()
    ClusterCreation = auto()
    ClusterDeletion = auto()


class HookDispatcher(object):
    """Simple wrapper around a registry of handlers associated to :class:`Hook`
     attributes. Each :class:`Hook` attribute defines when the handler will be
     executed.

    Listeners for certain hooks can be registered via :meth:`on`. Registered
    listeners are executed via :meth:`hook`.

    Example:
        .. code:: python

        listen = HookDispatcher()

        @listen.on(HookType.PreApply)
        def to_perform_before_app_creation(app, cluster, resource, controller):
            # Do Stuff

        @listen.on(HookType.PostApply)
        def another_to_perform_after_app_creation(app, cluster, resource, resp):
            # Do Stuff

        @listen.on(HookType.PostDelete)
        def to_perform_after_app_deletion(app, cluster, resource, resp):
            # Do Stuff

    """

    def __init__(self):
        self.registry = defaultdict(list)

    def on(self, hook):
        """Decorator function to add a new handler to the registry.

        Args:
            hook (HookType): Hook attribute for which to register the handler.

        Returns:
            callable: Decorator for registering listeners for the specified
            hook.

        """

        def decorator(handler):
            self.registry[hook].append(handler)

            return handler

        return decorator

    async def hook(self, hook, **kwargs):
        """Execute the list of handlers associated to the provided :class:`Hook`
        attribute.

        Args:
            hook (HookType): The hook attribute for which to execute handlers.

        """
        try:
            handlers = self.registry[hook]
        except KeyError:
            pass
        else:
            for handler in handlers:
                if iscoroutinefunction(handler):
                    await handler(**kwargs)
                else:
                    handler(**kwargs)


listen = HookDispatcher()


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
async def register_service(app, cluster, resource, response):
    """Register endpoint of Kubernetes Service object on creation and update.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        cluster (krake.data.kubernetes.Cluster): The cluster on which the
            application is running
        resource (dict): Kubernetes object description as specified in the
            specification of the application.
        response (kubernetes_asyncio.client.V1Service): Response of the
            Kubernetes API

    """
    if resource["kind"] != "Service":
        return

    service_name = resource["metadata"]["name"]

    if response.spec and response.spec.type == "LoadBalancer":
        # For a "LoadBalancer" type of Service, an external IP is given in the cluster
        # by a load balancer controller to the service. In this case, the "port"
        # specified in the spec is reachable from the outside.
        if (
            not response.status.load_balancer
            or not response.status.load_balancer.ingress
        ):
            # When a "LoadBalancer" type of service is created, the IP is given by an
            # additional controller (e.g. a controller that requests a floating IP to an
            # OpenStack infrastructure). This process can take some time, but the
            # Service itself already exist before the IP is assigned. In the case of an
            # error with the controller, the IP is also not given. This "<pending>" IP
            # just expresses that the Service exists, but the IP is not ready yet.
            external_ip = "<pending>"
        else:
            external_ip = response.status.load_balancer.ingress[0].ip

        if not response.spec.ports:
            external_port = "<pending>"
        else:
            external_port = response.spec.ports[0].port
        app.status.services[service_name] = f"{external_ip}:{external_port}"
        return

    node_port = None
    # Ensure that ports are specified
    if response.spec and response.spec.ports:
        node_port = response.spec.ports[0].node_port

    # If the service does not have a node port, remove a potential reference
    # and return.
    if node_port is None:
        try:
            del app.status.services[service_name]
        except KeyError:
            pass
        return

    # Determine URL of Kubernetes cluster API
    loader = KubeConfigLoader(cluster.spec.kubeconfig)
    config = Configuration()
    await loader.load_and_set(config)
    cluster_url = yarl.URL(config.host)

    app.status.services[service_name] = f"{cluster_url.host}:{node_port}"


@listen.on(HookType.ResourcePostDelete)
async def unregister_service(app, resource, **kwargs):
    """Unregister endpoint of Kubernetes Service object on deletion.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        resource (dict): Kubernetes object description as specified in the
            specification of the application.

    """
    if resource["kind"] != "Service":
        return

    service_name = resource["metadata"]["name"]
    try:
        del app.status.services[service_name]
    except KeyError:
        pass


@listen.on(HookType.ResourcePostDelete)
async def remove_resource_from_last_observed_manifest(app, resource, **kwargs):
    """Remove a given resource from the last_observed_manifest after its deletion

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        resource (dict): Kubernetes object description as specified in the
            specification of the application.

    """
    try:
        idx = get_kubernetes_resource_idx(app.status.last_observed_manifest, resource)
    except IndexError:
        return

    app.status.last_observed_manifest.pop(idx)


def update_last_applied_manifest_dict_from_resp(
    last_applied_manifest, observer_schema, response
):
    """Together with :func:``update_last_applied_manifest_list_from_resp``, this
    function is called recursively to update a partial ``last_applied_manifest``
    from a partial Kubernetes response

    Args:
        last_applied_manifest (dict): partial ``last_applied_manifest`` being
            updated
        observer_schema (dict): partial ``observer_schema``
        response (dict): partial response from the Kubernetes API.

    Raises:
        KeyError: If the observed field is not present in the Kubernetes response

    This function go through all observed fields, and initialized their value in
    last_applied_manifest if they are not yet present

    """
    for key, value in observer_schema.items():

        # Keys in the response are in camelCase
        camel_key = camel_to_snake_case(key)

        if camel_key not in response:
            # An observed key should always be present in the k8s response
            raise KeyError(
                f"Observed key {camel_key} is not present in response {response}"
            )

        if isinstance(value, dict):
            if key not in last_applied_manifest:
                # The dictionary is observed, but not present in
                # last_applied_manifest
                last_applied_manifest[key] = {}

            update_last_applied_manifest_dict_from_resp(
                last_applied_manifest[key], observer_schema[key], response[camel_key]
            )

        elif isinstance(value, list):
            if key not in last_applied_manifest:
                # The list is observed, but not present in last_applied_manifest
                last_applied_manifest[key] = []

            update_last_applied_manifest_list_from_resp(
                last_applied_manifest[key], observer_schema[key], response[camel_key]
            )

        elif key not in last_applied_manifest:
            # If key not present in last_applied_manifest, and value is neither a
            # dict nor a list, simply add it.
            last_applied_manifest[key] = response[camel_key]


def update_last_applied_manifest_list_from_resp(
    last_applied_manifest, observer_schema, response
):
    """Together with :func:``update_last_applied_manifest_dict_from_resp``, this
    function is called recursively to update a partial ``last_applied_manifest``
    from a partial Kubernetes response

    Args:
        last_applied_manifest (list): partial ``last_applied_manifest`` being
            updated
        observer_schema (list): partial ``observer_schema``
        response (list): partial response from the Kubernetes API.

    This function go through all observed fields, and initialized their value in
    last_applied_manifest if they are not yet present

    """
    # Looping over the observed resource, except the last element which is the
    # special control dictionary
    for idx, val in enumerate(observer_schema[:-1]):

        if idx >= len(response):
            # Element is observed but not present in k8s response, so following
            # elements will also not exist.
            #
            # This doesn't raise an Exception as observing the element of a list
            # doesn't ensure its presence. The list length is controlled by the
            # special control dictionary
            return

        if isinstance(val, dict):
            if idx >= len(last_applied_manifest):
                # The dict is observed, but not present in last_applied_manifest
                last_applied_manifest.append({})

            update_last_applied_manifest_dict_from_resp(
                last_applied_manifest[idx], observer_schema[idx], response[idx]
            )

        elif isinstance(response[idx], list):
            if idx >= len(last_applied_manifest):
                # The list is observed, but not present in last_applied_manifest
                last_applied_manifest.append([])

            update_last_applied_manifest_list_from_resp(
                last_applied_manifest[idx], observer_schema[idx], response[idx]
            )

        elif idx >= len(last_applied_manifest):
            # Element is not yet present in last_applied_manifest. Adding it.
            last_applied_manifest.append(response[idx])


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
def update_last_applied_manifest_from_resp(app, response, **kwargs):
    """Hook run after the creation or update of an application in order to update the
    `status.last_applied_manifest` using the k8s response.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        response (kubernetes_asyncio.client.V1Status): Response of the Kubernetes API

    After a Kubernetes resource has been created/updated, the
    `status.last_applied_manifest` has to be updated. All fields already initialized
    (either from the mangling of `spec.manifest`, or by a previous call to this
    function) should be left untouched. Only observed fields which are not present in
    `status.last_applied_manifest` should be initialized.

    """

    if isinstance(response, dict):
        # The Kubernetes API couldn't deserialize the k8s response into an object
        resp = response
    else:
        # The Kubernetes API deserialized the k8s response into an object
        resp = response.to_dict()

    idx_applied = get_kubernetes_resource_idx(app.status.last_applied_manifest, resp)

    idx_observed = get_kubernetes_resource_idx(app.status.mangled_observer_schema, resp)

    update_last_applied_manifest_dict_from_resp(
        app.status.last_applied_manifest[idx_applied],
        app.status.mangled_observer_schema[idx_observed],
        resp,
    )


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
def update_last_observed_manifest_from_resp(app, response, **kwargs):
    """Handler to run after the creation or update of a Kubernetes resource to update
    the last_observed_manifest from the response of the Kubernetes API.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        response (kubernetes_asyncio.client.V1Service): Response of the
            Kubernetes API

    The target last_observed_manifest holds the value of all observed fields plus the
    special control dictionaries for the list length

    """
    if isinstance(response, dict):
        # The Kubernetes API couldn't deserialize the k8s response into an object
        resp = response
    else:
        # The Kubernetes API deserialized the k8s response into an object
        resp = response.to_dict()

    try:
        idx_observed = get_kubernetes_resource_idx(
            app.status.mangled_observer_schema,
            resp,
        )
    except IndexError:
        # All created resources should be observed
        raise

    try:
        idx_last_observed = get_kubernetes_resource_idx(
            app.status.last_observed_manifest,
            resp,
        )
    except IndexError:
        # If the resource is not yes present in last_observed_manifest, append it.
        idx_last_observed = len(app.status.last_observed_manifest)
        app.status.last_observed_manifest.append({})

    # Overwrite the last_observed_manifest for this resource
    app.status.last_observed_manifest[
        idx_last_observed
    ] = update_last_observed_manifest_dict(
        app.status.mangled_observer_schema[idx_observed], resp
    )


def update_last_observed_manifest_dict(observed_resource, response):
    """Together with :func:``update_last_observed_manifest_list``, recursively
    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.

    Args:
        observed_resource (dict): The schema to observe for the partial given resource
        response (dict): The partial Kubernetes response for this resource.

    Raises:
        KeyError: If an observed key is not present in the Kubernetes response

    Returns:
        dict: The dictionary of observed keys and their value

    Get the value of all observed fields from the Kubernetes response
    """
    res = {}
    for key, value in observed_resource.items():

        camel_key = camel_to_snake_case(key)
        if camel_key not in response:
            raise KeyError(
                f"Observed key {camel_key} is not present in response {response}"
            )

        if isinstance(value, dict):
            res[key] = update_last_observed_manifest_dict(value, response[camel_key])

        elif isinstance(value, list):
            res[key] = update_last_observed_manifest_list(value, response[camel_key])

        else:
            res[key] = response[camel_key]

    return res


def update_last_observed_manifest_list(observed_resource, response):
    """Together with :func:``update_last_observed_manifest_dict``, recursively
    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.

    Args:
        observed_resource (list): the schema to observe for the partial given resource
        response (list): the partial Kubernetes response for this resource.

    Returns:
        list: The list of observed elements, plus the special list length control
            dictionary

    Get the value of all observed elements from the Kubernetes response
    """

    if not response:
        return [{"observer_schema_list_current_length": 0}]

    res = []
    # Looping over the observed resource, except the last element which is the special
    # control dictionary
    for idx, val in enumerate(observed_resource[:-1]):

        if idx >= len(response):
            # Element is not present in the Kubernetes response, nothing more to do
            break

        if type(response[idx]) is dict:
            res.append(update_last_observed_manifest_dict(val, response[idx]))

        elif type(response[idx]) is list:
            res.append(update_last_observed_manifest_list(val, response[idx]))

        else:
            res.append(response[idx])

    # Append the special control dictionary to the list
    res.append({"observer_schema_list_current_length": len(response)})

    return res


def update_last_applied_manifest_dict_from_spec(
    resource_status_new, resource_status_old, resource_observed
):
    """Together with :func:``update_last_applied_manifest_list_from_spec``, this
    function is called recursively to update a partial ``last_applied_manifest``

    Args:
        resource_status_new (dict): partial ``last_applied_manifest`` being updated
        resource_status_old (dict): partial of the current ``last_applied_manifest``
        resource_observed (dict): partial observer_schema for the manifest file
            being updated

    """
    for key, value in resource_observed.items():

        if key not in resource_status_old:
            continue

        if key in resource_status_new:

            if isinstance(value, dict):
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            elif isinstance(value, list):
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

        else:
            # If the key is not present the spec.manifest, we first need to
            # initialize it

            if isinstance(value, dict):
                resource_status_new[key] = {}
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            elif isinstance(value, list):
                resource_status_new[key] = []
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            else:
                resource_status_new[key] = resource_status_old[key]


def update_last_applied_manifest_list_from_spec(
    resource_status_new, resource_status_old, resource_observed
):
    """Together with :func:``update_last_applied_manifest_dict_from_spec``, this
    function is called recursively to update a partial ``last_applied_manifest``

    Args:
        resource_status_new (list): partial ``last_applied_manifest`` being updated
        resource_status_old (list): partial of the current ``last_applied_manifest``
        resource_observed (list): partial observer_schema for the manifest file
            being updated

    """

    # Looping over the observed resource, except the last element which is the
    # special control dictionary
    for idx, val in enumerate(resource_observed[:-1]):

        if idx >= len(resource_status_old):
            # The element in not in the current last_applied_manifest, and neither
            # is the rest of the list
            break

        if idx < len(resource_status_new):
            # The element is present in spec.manifest and in the current
            # last_applied_manifest. Updating observed fields

            if isinstance(val, dict):
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            elif isinstance(val, list):
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

        else:
            # If the element is not present in the spec.manifest, we first have to
            # initialize it.

            if isinstance(val, dict):
                resource_status_new.append({})
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            elif isinstance(val, list):
                resource_status_new.append([])
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            else:
                resource_status_new.append(resource_status_old[idx])


def update_last_applied_manifest_from_spec(app):
    """Update the status.last_applied_manifest of an application from spec.manifests

    Args:
        app (krake.data.kubernetes.Application): Application to update

    This function is called on application creation and updates. The
    last_applied_manifest of an application is initialized as a copy of spec.manifest,
    and is augmented by all known observed fields not yet initialized (i.e. all observed
    fields or resources which are present in the current last_applied_manifest but not
    in the spec.manifest)

    """

    # The new last_applied_manifest is initialized as a copy of the spec.manifest, and
    # augmented by all observed fields which are present in the current
    # last_applied_manifest but not in the original spec.manifest
    new_last_applied_manifest = deepcopy(app.spec.manifest)

    # Loop over observed resources and observed fields, and check if they should be
    # added to the new last_applied_manifest (i.e. present in the current
    # last_applied_manifest but not in spec.manifest)
    for resource_observed in app.status.mangled_observer_schema:

        # If the resource is not present in the current last_applied_manifest, there is
        # nothing to do. Whether the resource was initialized by spec.manifest doesn't
        # matter.
        try:
            idx_status_old = get_kubernetes_resource_idx(
                app.status.last_applied_manifest, resource_observed
            )
        except IndexError:
            continue

        # As the resource is present in the current last_applied_manifest, we need to go
        # through it to check if observed fields should be set to their current value
        # (i.e. fields are present in the current last_applied_manifest, but not in
        # spec.manifest)
        try:
            # Check if the observed resource is present in spec.manifest
            idx_status_new = get_kubernetes_resource_idx(
                new_last_applied_manifest, resource_observed
            )
        except IndexError:
            # The resource is observed but is not present in the spec.manifest.
            # Create an empty resource, which will be augmented in
            # update_last_applied_manifest_dict_from_spec with the observed and known
            # fields.
            new_last_applied_manifest.append({})
            idx_status_new = len(new_last_applied_manifest) - 1

        update_last_applied_manifest_dict_from_spec(
            new_last_applied_manifest[idx_status_new],
            app.status.last_applied_manifest[idx_status_old],
            resource_observed,
        )

    app.status.last_applied_manifest = new_last_applied_manifest


class KubernetesApplicationObserver(Observer):
    """Observer specific for Kubernetes Applications. One observer is created for each
    Application managed by the Controller, but not one per Kubernetes resource
    (Deployment, Service...). If several resources are defined by an Application, they
    are all monitored by the same observer.

    The observer gets the actual status of the resources on the cluster using the
    Kubernetes API, and compare it to the status stored in the API.

    The observer is:
     * started at initial Krake resource creation;

     * deleted when a resource needs to be updated, then started again when it is done;

     * simply deleted on resource deletion.

    Args:
        cluster (krake.data.kubernetes.Cluster): the cluster on which the observed
            Application is created.
        resource (krake.data.kubernetes.Application): the application that will be
            observed.
        on_res_update (coroutine): a coroutine called when a resource's actual status
            differs from the status sent by the database. Its signature is:
            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of
            the resource that is up-to-date with the API. The Observer internal instance
            of the resource to observe will be updated. If the API cannot be contacted,
            ``None`` can be returned. In this case the internal instance of the Observer
            will not be updated.
        time_step (int, optional): how frequently the Observer should watch the actual
            status of the resources.

    """

    def __init__(self, cluster, resource, on_res_update, time_step=2):
        super().__init__(resource, on_res_update, time_step)
        self.cluster = cluster
        self.kubernetes_api = None

    def _set_container_health(self, resource, status):
        if status.container_health is None:
            status.container_health = ContainerHealth()

        if hasattr(resource, 'status') and resource.status is not None:
            if resource.kind == "Pod":
                container_state = resource.status.container_statuses[0].state
                status.container_health.desired_pods = 1
                if container_state.terminated is not None:
                    if resource.spec.restart_policy == "Never":
                        status.state = ApplicationState.DEGRADED
                    else:
                        status.state = ApplicationState.RESTARTING
                    status.container_health.running_pods = 0
                elif container_state.waiting is not None:
                    if container_state.waiting.reason == "CrashLoopBackOff":
                        status.state = ApplicationState.DEGRADED
                        status.container_health.running_pods = 0
                else:
                    status.state = ApplicationState.RUNNING
                    status.container_health.running_pods = 1

            elif (
                resource.kind == "Deployment" or
                resource.kind == "StatefulSet" or
                resource.kind == "ReplicaSet" or
                resource.kind == "DaemonSet"
            ):

                if resource.kind == "DaemonSet":
                    status.container_health.desired_pods = \
                        resource.status.current_number_scheduled
                    if isinstance(resource.status.desired_number_scheduled, int):
                        status.container_health.running_pods = \
                            resource.status.desired_number_scheduled
                    else:
                        status.container_health.running_pods = 0
                else:
                    status.container_health.desired_pods = resource.status.replicas
                    if isinstance(resource.status.ready_replicas, int):
                        status.container_health.running_pods = \
                            resource.status.ready_replicas
                    else:
                        status.container_health.running_pods = 0

                if status.container_health.running_pods != \
                   status.container_health.desired_pods:
                    status.state = ApplicationState.DEGRADED
                else:
                    status.state = ApplicationState.RUNNING

            elif resource.kind == "Job":
                if isinstance(resource.spec.completions, int):
                    status.container_health.desired_pods = resource.spec.completions
                else:
                    status.container_health.desired_pods = 1

                if isinstance(resource.status.active, int):
                    status.container_health.running_pods = resource.status.active
                else:
                    status.container_health.running_pods = 0

                if isinstance(resource.status.succeeded, int):
                    status.container_health.completed_pods = resource.status.succeeded
                else:
                    status.container_health.completed_pods = 0

                if isinstance(resource.status.failed, int):
                    status.container_health.failed_pods = resource.status.failed
                else:
                    status.container_health.failed_pods = 0

    async def poll_resource(self):
        """Fetch the current status of the Application monitored by the Observer.

        Returns:
            krake.data.core.Status: the status object created using information from the
                real world Applications resource.

        """
        app = self.resource

        status = deepcopy(app.status)
        status.last_observed_manifest = []

        # For each observed kubernetes resource of the Application,
        # get its current status on the cluster.
        for desired_resource in app.status.last_applied_manifest:
            kube = KubernetesClient(self.cluster.spec.kubeconfig)
            idx_observed = get_kubernetes_resource_idx(
                app.status.mangled_observer_schema, desired_resource
            )
            observed_resource = app.status.mangled_observer_schema[idx_observed]
            async with kube:
                try:
                    group, version, kind, name, namespace = kube.get_immutables(
                        desired_resource
                    )
                    resource_api = await kube.get_resource_api(group, version, kind)
                    resp = await resource_api.read(kind, name, namespace)
                except (ClientConnectorError, ApiException) as err:
                    if hasattr(err, "status") and err.status == 404:
                        # Resource does not exist
                        continue
                    # Otherwise, log the unexpected error and return the
                    # last known application status
                    logger.debug(err)
                    return app.status

                resource = resp
                self._set_container_health(resource, status)

            observed_manifest = update_last_observed_manifest_dict(
                observed_resource, resp.to_dict()
            )
            status.last_observed_manifest.append(observed_manifest)

        return status


class KubernetesClusterObserver(Observer):
    """Observer specific for Kubernetes Clusters. One observer is created for each
    Cluster managed by the Controller.

    The observer gets the actual status of the cluster using the
    Kubernetes API, and compare it to the status stored in the API.

    The observer is:
     * started at initial Krake resource creation;

     * deleted when a resource needs to be updated, then started again when it is done;

     * simply deleted on resource deletion.

    Args:
        resource (krake.data.kubernetes.Cluster): the cluster which will be observed.
        on_res_update (coroutine): a coroutine called when a resource's actual status
            differs from the status sent by the database. Its signature is:
            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of
            the resource that is up-to-date with the API. The Observer internal instance
            of the resource to observe will be updated. If the API cannot be contacted,
            ``None`` can be returned. In this case the internal instance of the Observer
            will not be updated.
        time_step (int, optional): how frequently the Observer should watch the actual
            status of the resources.

    """

    def __init__(self, resource, on_res_update, time_step=2):
        super().__init__(resource, on_res_update, time_step)

    async def poll_resource(self):
        """Fetch the current status of the Cluster monitored by the Observer.

        Note regarding exceptions handling:
          The current cluster status is fetched by :func:`poll_resource` from its API.
          If the cluster API is shutting down the API server responds with a 503
          (service unavailable, apiserver is shutting down) HTTP response which
          leads to the kubernetes client ApiException. If the cluster's API has been
          successfully shut down and there is an attempt to fetch cluster status,
          the ClientConnectorError is raised instead.
          Therefore, both exceptions should be handled.

        Returns:
            krake.data.core.Status: the status object created using information from the
                real world Cluster.

        """
        cluster = self.resource
        status = deepcopy(cluster.status)
        status.nodes = []
        # For each observed kubernetes cluster registered in Krake,
        # get its current node status.
        loader = KubeConfigLoader(cluster.spec.kubeconfig)
        config = Configuration()
        await loader.load_and_set(config)
        kube = ApiClient(config)

        async with kube as api:
            v1 = client.CoreV1Api(api)
            try:
                response = await v1.list_node()
            except (ClientConnectorError, ApiException) as err:
                # Log the error and set cluster state to OFFLINE
                logger.debug(err)
                status.state = ClusterState.OFFLINE
                return status

            # Fetch nodes conditions
            nodes = []
            for node in response.items:
                conditions = []
                for condition in node.status.conditions:
                    conditions.append(
                        ClusterNodeCondition(
                            message=condition.message,
                            reason=condition.reason,
                            status=condition.status,
                            type=condition.type,
                        )
                    )

                nodes.append(
                    ClusterNode(
                        metadata=ClusterNodeMetadata(name=node.metadata.name),
                        status=ClusterNodeStatus(conditions=conditions),
                    )
                )
            status.nodes = nodes

            # The scheduler is unable to fetch cluster metrics, hence
            # the cluster state should wait for it and the cluster
            # status should not be changed by the observer.
            if status.state == ClusterState.FAILING_METRICS:
                return status

            # Set the cluster state to CONNECTING if the previous state
            # was OFFLINE. It is due to smooth transition from
            # the OFFLINE to ONLINE state.
            if status.state == ClusterState.OFFLINE:
                status.state = ClusterState.CONNECTING
                return status

            for node in status.nodes:
                for condition in node.status.conditions:
                    if (
                        condition.type.lower().endswith("pressure")
                        and condition.status == "True"
                    ):
                        status.state = ClusterState.UNHEALTHY
                        return status

                    if condition.type.lower() == "ready" and condition.status != "True":
                        status.state = ClusterState.NOTREADY
                        return status

            status.state = ClusterState.ONLINE
            return status


@listen.on(HookType.ApplicationPostReconcile)
@listen.on(HookType.ApplicationPostMigrate)
@listen.on(HookType.ClusterCreation)
async def register_observer(controller, resource, start=True, **kwargs):
    """Create an observer for the given Application or Cluster, and start it as a
    background task if wanted.

    If an observer already existed for this Application or Cluster, it is stopped
    and deleted.

    Args:
        controller (KubernetesController): the controller for which the observer will be
            added in the list of working observers.
        resource (krake.data.kubernetes.Application): the Application to observe or
        resource (krake.data.kubernetes.Cluster): the Cluster to observe.
        start (bool, optional): if False, does not start the observer as background
            task.

    """
    if resource.kind == Application.kind:
        cluster = await controller.kubernetes_api.read_cluster(
            namespace=resource.status.running_on.namespace,
            name=resource.status.running_on.name,
        )
        observer = KubernetesApplicationObserver(
            cluster,
            resource,
            controller.on_status_update,
            time_step=controller.observer_time_step,
        )

    elif resource.kind == Cluster.kind:
        observer = KubernetesClusterObserver(
            resource,
            controller.on_status_update,
            time_step=controller.observer_time_step,
        )
    else:
        logger.debug("Unknown resource kind. No observer was registered.", resource)
        return

    logger.debug(f"Start observer for {resource.kind} %r", resource.metadata.name)
    task = None
    if start:
        task = controller.loop.create_task(observer.run())

    controller.observers[resource.metadata.uid] = (observer, task)


@listen.on(HookType.ApplicationPreReconcile)
@listen.on(HookType.ApplicationPreMigrate)
@listen.on(HookType.ApplicationPreDelete)
@listen.on(HookType.ClusterDeletion)
async def unregister_observer(controller, resource, **kwargs):
    """Stop and delete the observer for the given Application or Cluster. If no observer
    is started, do nothing.

    Args:
        controller (KubernetesController): the controller for which the observer will be
            removed from the list of working observers.
        resource (krake.data.kubernetes.Application): the Application whose observer
        will be stopped or
        resource (krake.data.kubernetes.Cluster): the Cluster whose observer will be
        stopped.

    """
    if resource.metadata.uid not in controller.observers:
        return

    logger.debug(f"Stop observer for {resource.kind} {resource.metadata.name}")
    _, task = controller.observers.pop(resource.metadata.uid)
    task.cancel()

    with suppress(asyncio.CancelledError):
        await task


@listen.on(HookType.ApplicationToscaTranslation)
async def translate_tosca(controller, app, **kwargs):
    """Translate a TOSCA template or CSAR archive into a Kubernetes manifest.

    Args:
        controller (KubernetesController): the controller that handles the application
            resource.
        app (krake.data.kubernetes.Application): the Application that could be defined
            by a TOSCA template or a CSAR archive.

    Raises:
        ToscaParserException: If the given application does not contain
         at least one from the following:
         - Kubernetes manifest
         - TOSCA template
         - CSAR archive

    """
    if app.spec.manifest:
        return

    if not app.spec.tosca and not app.spec.csar:
        raise ToscaParserException(
            "Application should be defined by a Kubernetes manifest,"
            " a TOSCA template or a CSAR archive: %r",
            app,
        )
    app.status.state = ApplicationState.TRANSLATING
    await controller.kubernetes_api.update_application_status(
        namespace=app.metadata.namespace, name=app.metadata.name, body=app
    )

    if app.spec.tosca and isinstance(app.spec.tosca, dict):

        manifest = ToscaParser.from_dict(app.spec.tosca).translate_to_manifests()
    else:
        manifest = ToscaParser.from_url(
            app.spec.tosca or app.spec.csar
        ).translate_to_manifests()

    app.spec.manifest = manifest
    await controller.kubernetes_api.update_application(
        namespace=app.metadata.namespace, name=app.metadata.name, body=app
    )


def utc_difference():
    """Get the difference in seconds between the current time and the current UTC time.

    Returns:
        int: the time difference in seconds.

    """
    delta = datetime.now() - datetime.utcnow()
    return delta.seconds


def generate_certificate(config):
    """Create and sign a new certificate using the one defined in the complete hook
    configuration as intermediate certificate.

    Args:
        config (krake.data.config.CompleteHookConfiguration): the configuration of the
            complete hook.

    Returns:
        CertificatePair: the content of the certificate created and its corresponding
            key.

    """
    with open(config.intermediate_src, "rb") as f:
        intermediate_src = crypto.load_certificate(crypto.FILETYPE_PEM, f.read())
    with open(config.intermediate_key_src, "rb") as f:
        intermediate_key_src = crypto.load_privatekey(crypto.FILETYPE_PEM, f.read())

    client_cert = crypto.X509()

    # Set general information
    client_cert.set_version(3)
    client_cert.set_serial_number(random.randint(50000000000000, 100000000000000))
    # If not set before, TLS will not accept to use this certificate in UTC cases, as
    # the server time may be earlier.
    time_offset = utc_difference() * -1
    client_cert.gmtime_adj_notBefore(time_offset)
    client_cert.gmtime_adj_notAfter(1 * 365 * 24 * 60 * 60)

    # Set issuer and subject
    intermediate_subject = intermediate_src.get_subject()
    client_cert.set_issuer(intermediate_subject)
    client_subj = crypto.X509Name(intermediate_subject)
    client_subj.CN = config.hook_user
    client_cert.set_subject(client_subj)

    # Create and set the private key
    client_key = crypto.PKey()
    client_key.generate_key(crypto.TYPE_RSA, 2048)
    client_cert.set_pubkey(client_key)

    client_cert.sign(intermediate_key_src, "sha256")

    cert_dump = crypto.dump_certificate(crypto.FILETYPE_PEM, client_cert).decode()
    key_dump = crypto.dump_privatekey(crypto.FILETYPE_PEM, client_key).decode()
    return CertificatePair(cert=cert_dump, key=key_dump)


def generate_default_observer_schema(app):
    """Generate the default observer schema for each Kubernetes resource present in
    ``spec.manifest`` for which a custom observer schema hasn't been specified.

    Args:
        app (krake.data.kubernetes.Application): The application for which to generate a
            default observer schema
    """

    app.status.mangled_observer_schema = deepcopy(app.spec.observer_schema)

    for resource_manifest in app.spec.manifest:
        try:
            get_kubernetes_resource_idx(
                app.status.mangled_observer_schema, resource_manifest
            )

        except IndexError:
            # Only create a default observer schema, if a custom observer schema hasn't
            # been set by the user.
            app.status.mangled_observer_schema.append(
                generate_default_observer_schema_dict(
                    resource_manifest,
                    first_level=True,
                )
            )


def generate_default_observer_schema_dict(manifest_dict, first_level=False):
    """Together with :func:``generate_default_observer_schema_list``, this function is
    called recursively to generate part of a default ``observer_schema`` from part of a
    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.

    Args:
        manifest_dict (dict): Partial Kubernetes resources
        first_level (bool, optional): If True, indicates that the dictionary represents
            the whole observer schema of a Kubernetes resource

    Returns:
        dict: Generated partial observer_schema

    This function creates a new dictionary from ``manifest_dict`` and replaces all
    non-list and non-dict values by ``None``.

    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a
    resource), the values of the identifying fields are copied from the manifest file.

    """
    observer_schema_dict = {}

    for key, value in manifest_dict.items():

        if isinstance(value, dict):
            observer_schema_dict[key] = generate_default_observer_schema_dict(value)

        elif isinstance(value, list):
            observer_schema_dict[key] = generate_default_observer_schema_list(value)

        else:
            observer_schema_dict[key] = None

    if first_level:
        observer_schema_dict["apiVersion"] = manifest_dict["apiVersion"]
        observer_schema_dict["kind"] = manifest_dict["kind"]
        observer_schema_dict["metadata"]["name"] = manifest_dict["metadata"]["name"]

        if (
            "spec" in manifest_dict
            and "type" in manifest_dict["spec"]
            and manifest_dict["spec"]["type"] == "LoadBalancer"
        ):
            observer_schema_dict["status"] = {"load_balancer": {"ingress": None}}

    return observer_schema_dict


def generate_default_observer_schema_list(manifest_list):
    """Together with :func:``generate_default_observer_schema_dict``, this function is
    called recursively to generate part of a default ``observer_schema`` from part of a
    Kubernetes resource, defined respectively by ``manifest_list`` or ``manifest_dict``.

    Args:
        manifest_list (list): Partial Kubernetes resources

    Returns:
        list: Generated partial observer_schema

    This function creates a new list from ``manifest_list`` and replaces all non-list
    and non-dict elements by ``None``.

    Additionally, it generates the default list control dictionary, using the current
    length of the list as default minimum and maximum values.

    """
    observer_schema_list = []

    for value in manifest_list:

        if isinstance(value, dict):
            observer_schema_list.append(generate_default_observer_schema_dict(value))

        elif isinstance(value, list):
            observer_schema_list.append(generate_default_observer_schema_list(value))

        else:
            observer_schema_list.append(None)

    observer_schema_list.append(
        {
            "observer_schema_list_min_length": len(manifest_list),
            "observer_schema_list_max_length": len(manifest_list),
        }
    )

    return observer_schema_list


@listen.on(HookType.ApplicationMangling)
async def complete(app, api_endpoint, ssl_context, config):
    """Execute application complete hook defined by :class:`Complete`.
    Hook mangles given application and injects complete hooks variables.

    Application complete hook is disabled by default.
    User enables this hook by the --hook-complete argument in rok cli.

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        config (krake.data.config.HooksConfiguration): Complete hook
            configuration.

    """
    if "complete" not in app.spec.hooks:
        return

    # Use the endpoint of the API only if the external endpoint has not been set.
    if config.complete.external_endpoint:
        api_endpoint = config.complete.external_endpoint

    app.status.complete_token = (
        app.status.complete_token if app.status.complete_token else token_urlsafe()
    )

    # Generate only once the certificate and key for a specific Application
    generated_cert = CertificatePair(
        cert=app.status.complete_cert, key=app.status.complete_key
    )
    if ssl_context and generated_cert == (None, None):
        generated_cert = generate_certificate(config.complete)
        app.status.complete_cert = generated_cert.cert
        app.status.complete_key = generated_cert.key

    hook = Complete(
        api_endpoint,
        ssl_context,
        hook_user=config.complete.hook_user,
        cert_dest=config.complete.cert_dest,
        env_token=config.complete.env_token,
        env_url=config.complete.env_url,
    )
    hook.mangle_app(
        app.metadata.name,
        app.metadata.namespace,
        app.status.complete_token,
        app.status.last_applied_manifest,
        config.complete.intermediate_src,
        generated_cert,
        app.status.mangled_observer_schema,
        "complete",
    )


@listen.on(HookType.ApplicationMangling)
async def shutdown(app, api_endpoint, ssl_context, config):
    """Executes an application shutdown hook defined by :class:`Shutdown`.
    The hook mangles the given application and injects shutdown hooks variables.

    Application shutdown hook is disabled by default.
    User enables this hook by the --hook-shutdown argument in rok cli.

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        config (krake.data.config.HooksConfiguration): Shutdown hook
            configuration.

    """
    if "shutdown" not in app.spec.hooks:
        return

    # Use the endpoint of the API only if the external endpoint has not been set.
    if config.shutdown.external_endpoint:
        api_endpoint = config.shutdown.external_endpoint

    app.status.shutdown_token = (
        app.status.shutdown_token if app.status.shutdown_token else token_urlsafe()
    )

    # Generate only once the certificate and key for a specific Application
    generated_cert = CertificatePair(
        cert=app.status.shutdown_cert, key=app.status.shutdown_key
    )
    if ssl_context and generated_cert == (None, None):
        generated_cert = generate_certificate(config.shutdown)
        app.status.shutdown_cert = generated_cert.cert
        app.status.shutdown_key = generated_cert.key

    hook = Shutdown(
        api_endpoint,
        ssl_context,
        hook_user=config.shutdown.hook_user,
        cert_dest=config.shutdown.cert_dest,
        env_token=config.shutdown.env_token,
        env_url=config.shutdown.env_url,
    )
    hook.mangle_app(
        app.metadata.name,
        app.metadata.namespace,
        app.status.shutdown_token,
        app.status.last_applied_manifest,
        config.shutdown.intermediate_src,
        generated_cert,
        app.status.mangled_observer_schema,
        "shutdown",
    )


@listen.on(HookType.ResourcePreDelete)
async def pre_shutdown(controller, app, **kwargs):
    """

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
    """
    if "shutdown" not in app.spec.hooks:
        return

    return


class SubResource(NamedTuple):
    group: str
    name: str
    body: dict
    path: tuple


class CertificatePair(NamedTuple):
    """Tuple which contains a certificate and its corresponding key.

    Attributes:
        cert (str): content of a certificate.
        key (str): content of the key that corresponds to the certificate.

    """

    cert: str
    key: str


class Hook(object):
    hook_resources = ()

    ca_name = "ca-bundle.pem"
    cert_name = "cert.pem"
    key_name = "key.pem"

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        self.api_endpoint = api_endpoint
        self.ssl_context = ssl_context
        self.hook_user = hook_user
        self.cert_dest = cert_dest
        self.env_token = env_token
        self.env_url = env_url

    def mangle_app(
        self,
        name,
        namespace,
        token,
        last_applied_manifest,
        intermediate_src,
        generated_cert,
        mangled_observer_schema,
        hook_type="",
    ):
        """Mangle a given application and inject complete hook resources and
        sub-resources into the :attr:`last_applied_manifest` object by :meth:`mangle`.
        Also mangle the observer_schema as new resources and sub-resources should
        be observed.

        :attr:`last_applied_manifest` is created as a deep copy of the desired
        application resources, as defined by user. It can be updated by custom hook
        resources or modified by custom hook sub-resources. It is used as a desired
        state for the Krake deployment process.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            token (str): Complete hook authentication token
            last_applied_manifest (list): Application resources
            intermediate_src (str): content of the certificate that is used to sign new
                certificates for the complete hook.
            generated_cert (CertificatePair): tuple that contains the content of the
                new signed certificate for the Application, and the content of its
                corresponding key.
            mangled_observer_schema (list): Observed fields
            hook_type (str, optional): Name of the hook the app should be mangled for

        """

        secret_certs_name = "-".join([name, "krake", hook_type, "secret", "certs"])
        secret_token_name = "-".join([name, "krake", hook_type, "secret", "token"])
        volume_name = "-".join([name, "krake", hook_type, "volume"])
        ca_certs = (
            self.ssl_context.get_ca_certs(binary_form=True)
            if self.ssl_context
            else None
        )

        # Extract all different namespaces
        # FIXME: too many assumptions here: do we create one ConfigMap for each
        #  namespace?
        resource_namespaces = {
            resource["metadata"].get("namespace", "default")
            for resource in last_applied_manifest
        }

        hook_resources = []
        hook_sub_resources = []
        if ca_certs:
            hook_resources.extend(
                [
                    self.secret_certs(
                        secret_certs_name,
                        resource_namespace,
                        intermediate_src=intermediate_src,
                        generated_cert=generated_cert,
                        ca_certs=ca_certs,
                    )
                    for resource_namespace in resource_namespaces
                ]
            )
            hook_sub_resources.extend(
                [*self.volumes(secret_certs_name, volume_name, self.cert_dest)]
            )

        hook_resources.extend(
            [
                self.secret_token(
                    secret_token_name,
                    name,
                    namespace,
                    resource_namespace,
                    self.api_endpoint,
                    token,
                )
                for resource_namespace in resource_namespaces
            ]
        )
        hook_sub_resources.extend(
            [
                *self.env_vars(secret_token_name),
            ]
        )

        self.mangle(
            hook_resources,
            last_applied_manifest,
            mangled_observer_schema,
        )
        self.mangle(
            hook_sub_resources,
            last_applied_manifest,
            mangled_observer_schema,
            is_sub_resource=True,
        )

    def mangle(
        self,
        items,
        last_applied_manifest,
        mangled_observer_schema,
        is_sub_resource=False,
    ):
        """Mangle applications desired state with custom hook resources or
        sub-resources.

        Example:
            .. code:: python

            last_applied_manifest = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Pod',
                    'metadata': {'name': 'test', 'namespace': 'default'},
                    'spec': {'containers': [{'name': 'test'}]}
                }
            ]
            mangled_observer_schema = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Pod',
                    'metadata': {'name': 'test', 'namespace': 'default'},
                    'spec': {
                        'containers': [
                            {'name': None},
                            {
                                'observer_schema_list_max_length': 1,
                                'observer_schema_list_min_length': 1,
                            },
                        ]
                    },
                }
            ]
            hook_resources = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Secret',
                    'metadata': {'name': 'sct', 'namespace': 'default'}
                }
            ]
            hook_sub_resources = [
                SubResource(
                    group='env', name='env', body={'name': 'test', 'value': 'test'},
                    path=(('spec', 'containers'),)
                )
            ]

            mangle(
                hook_resources,
                last_applied_manifest,
                mangled_observer_schema,
            )
            mangle(
                hook_sub_resources,
                last_applied_manifest,
                mangled_observer_schema,
                is_sub_resource=True
            )

            assert last_applied_manifest == [
                {
                    "apiVersion": "v1",
                    "kind": "Pod",
                    "metadata": {"name": "test", 'namespace': 'default'},
                    "spec": {
                        "containers": [
                            {
                                "name": "test",
                                "env": [{"name": "test", "value": "test"}]
                            }
                        ]
                    },
                },
                {"apiVersion": "v1", "kind": "Secret", "metadata": {"name": "sct"}},
            ]

            assert mangled_observer_schema == [
                {
                    "apiVersion": "v1",
                    "kind": "Pod",
                    "metadata": {"name": "test", "namespace": None},
                    "spec": {
                        "containers": [
                            {
                                "name": None,
                                "env": [
                                    {"name": None, "value": None},
                                    {
                                        "observer_schema_list_max_length": 1,
                                        "observer_schema_list_min_length": 1,
                                    },
                                ],
                            },
                            {
                                "observer_schema_list_max_length": 1,
                                "observer_schema_list_min_length": 1,
                            },
                        ]
                    },
                },
                {
                    "apiVersion": "v1",
                    "kind": "Secret",
                    "metadata": {"name": "sct", "namespace": None},
                },
            ]

        Args:
            items (list[SubResource]): Custom hook resources or sub-resources
            last_applied_manifest (list): Application resources
            mangled_observer_schema (list): Observed resources
            is_sub_resource (bool, optional): if False, the function only extend the
                list of Kubernetes resources defined in :attr:`last_applied_manifest`
                with new hook resources. Otherwise, the function injects each new hook
                sub-resource into the :attr:`last_applied_manifest` object
                sub-resources. Defaults to False.

        """

        if not items:
            return

        if not is_sub_resource:
            last_applied_manifest.extend(items)
            for sub_resource in items:
                # Generate the default observer schema for each resource
                mangled_observer_schema.append(
                    generate_default_observer_schema_dict(
                        sub_resource,
                        first_level=True,
                    )
                )
            return

        def inject(sub_resource, sub_resource_to_mangle, observed_resource_to_mangle):
            """Inject a hooks defined sub-resource into a Kubernetes sub-resource.

            Args:
                sub_resource (SubResource): Hook sub-resource that needs to be injected
                    into :attr:`last_applied_manifest`
                sub_resource_to_mangle (object): Kubernetes sub-resources from
                    :attr:`last_applied_manifest` which need to be processed
                observed_resource_to_mangle (dict): partial mangled_observer_schema
                    corresponding to the Kubernetes sub-resource.

            Raises:
                InvalidManifestError: if the sub-resource which will be mangled is not a
                    list or a dict.

            """

            # Create sub-resource group if not present in the Kubernetes sub-resource
            if sub_resource.group not in sub_resource_to_mangle:
                # FIXME: This assumes the subresource group contains a list
                sub_resource_to_mangle.update({sub_resource.group: []})

            # Create sub-resource group if not present in the observed fields
            if sub_resource.group not in observed_resource_to_mangle:
                observed_resource_to_mangle.update(
                    {
                        sub_resource.group: [
                            {
                                "observer_schema_list_min_length": 0,
                                "observer_schema_list_max_length": 0,
                            }
                        ]
                    }
                )

            # Inject sub-resource
            # If sub-resource name is already there update it, if not, append it
            if sub_resource.name in [
                g["name"] for g in sub_resource_to_mangle[sub_resource.group]
            ]:
                # FIXME: Assuming we are dealing with a list
                for idx, item in enumerate(sub_resource_to_mangle[sub_resource.group]):
                    if item["name"]:
                        if hasattr(item, "body"):
                            sub_resource_to_mangle[item.group][idx] = item["body"]
            else:
                sub_resource_to_mangle[sub_resource.group].append(sub_resource.body)

            # Make sure the value is observed
            if sub_resource.name not in [
                g["name"] for g in observed_resource_to_mangle[sub_resource.group][:-1]
            ]:
                observed_resource_to_mangle[sub_resource.group].insert(
                    -1, generate_default_observer_schema_dict(sub_resource.body)
                )
                observed_resource_to_mangle[sub_resource.group][-1][
                    "observer_schema_list_min_length"
                ] += 1
                observed_resource_to_mangle[sub_resource.group][-1][
                    "observer_schema_list_max_length"
                ] += 1

        for resource in last_applied_manifest:
            # Complete hook is applied only on defined Kubernetes resources
            if resource["kind"] not in self.hook_resources:
                continue

            for sub_resource in items:
                sub_resources_to_mangle = None
                idx_observed = get_kubernetes_resource_idx(
                    mangled_observer_schema, resource
                )
                for keys in sub_resource.path:
                    try:
                        sub_resources_to_mangle = reduce(getitem, keys, resource)
                    except KeyError:
                        continue

                    break

                # Create the path to the observed sub-resource, if it doesn't yet exist
                try:
                    observed_sub_resources = reduce(
                        getitem, keys, mangled_observer_schema[idx_observed]
                    )
                except KeyError:
                    Complete.create_path(
                        mangled_observer_schema[idx_observed], list(keys)
                    )
                    observed_sub_resources = reduce(
                        getitem, keys, mangled_observer_schema[idx_observed]
                    )

                if isinstance(sub_resources_to_mangle, list):
                    for idx, sub_resource_to_mangle in enumerate(
                        sub_resources_to_mangle
                    ):

                        # Ensure that each element of the list is observed.
                        idx_observed = idx
                        if idx >= len(observed_sub_resources[:-1]):
                            idx_observed = len(observed_sub_resources[:-1])
                            # FIXME: Assuming each element of the list contains a
                            # dictionary, therefore initializing new elements with an
                            # empty dict
                            observed_sub_resources.insert(-1, {})
                        observed_sub_resource = observed_sub_resources[idx_observed]

                        # FIXME: This is assuming a list always contains dict
                        inject(
                            sub_resource, sub_resource_to_mangle, observed_sub_resource
                        )

                elif isinstance(sub_resources_to_mangle, dict):
                    inject(
                        sub_resource, sub_resources_to_mangle, observed_sub_resources
                    )

                else:
                    message = (
                        f"The sub-resource to mangle {sub_resources_to_mangle!r} has an"
                        "invalid type, should be in '[dict, list]'"
                    )
                    raise InvalidManifestError(message)

    @staticmethod
    def attribute_map(obj):
        """Convert a Kubernetes object to dict based on its attribute mapping

        Example:
            .. code:: python

            from kubernetes_asyncio.client import V1VolumeMount

            d = attribute_map(
                    V1VolumeMount(name="name", mount_path="path")
            )
            assert d == {'mountPath': 'path', 'name': 'name'}

        Args:
            obj (object): Kubernetes object

        Returns:
            dict: Converted Kubernetes object

        """
        return {
            obj.attribute_map[attr]: getattr(obj, attr)
            for attr, _ in obj.to_dict().items()
            if getattr(obj, attr) is not None
        }

    @staticmethod
    def create_path(mangled_observer_schema, keys):
        """Create the path to the observed field in the observer schema.

        When a sub-resource is mangled, it should be observed. This function creates
        the path to the subresource to observe.

        Args:
            mangled_observer_schema (dict): Partial observer schema of a resource
            keys (list): list of keys forming the path to the sub-resource to
                observe

        FIXME: This assumes we are only adding keys to dict. We don't consider lists

        """

        # Unpack the first key first, as it contains the base directory
        key = keys.pop(0)

        # If the key is the last of the list, we reached the end of the path.
        if len(keys) == 0:
            mangled_observer_schema[key] = None
            return

        if key not in mangled_observer_schema:
            mangled_observer_schema[key] = {}
        Hook.create_path(mangled_observer_schema[key], keys)

    def secret_certs(
        self,
        secret_name,
        namespace,
        ca_certs=None,
        intermediate_src=None,
        generated_cert=None,
    ):
        """Create a complete hooks secret resource.

        Complete hook secret stores Krake CAs and client certificates to communicate
        with the Krake API.

        Args:
            secret_name (str): Secret name
            namespace (str): Kubernetes namespace where the Secret will be created.
            ca_certs (list): Krake CA list
            intermediate_src (str): content of the certificate that is used to sign new
                certificates for the complete hook.
            generated_cert (CertificatePair): tuple that contains the content of the
                new signed certificate for the Application, and the content of its
                corresponding key.

        Returns:
            dict: complete hook secret resource

        """
        ca_certs_pem = ""
        for ca_cert in ca_certs:
            x509 = crypto.load_certificate(crypto.FILETYPE_ASN1, ca_cert)
            ca_certs_pem += crypto.dump_certificate(crypto.FILETYPE_PEM, x509).decode()

        # Add the intermediate certificate into the chain
        with open(intermediate_src, "r") as f:
            intermediate_src_content = f.read()
        ca_certs_pem += intermediate_src_content

        data = {
            self.ca_name: self._encode_to_64(ca_certs_pem),
            self.cert_name: self._encode_to_64(generated_cert.cert),
            self.key_name: self._encode_to_64(generated_cert.key),
        }
        return self.secret(secret_name, data, namespace)

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create a hooks secret resource.

        The hook secret stores Krake authentication token
        and hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Complete hook authentication token

        Returns:
            dict: complete hook secret resource

        """
        pass

    def volumes(self, secret_name, volume_name, mount_path):
        """Create complete hooks volume and volume mount sub-resources

        Complete hook volume gives access to hook's secret, which stores
        Krake CAs and client certificates to communicate with the Krake API.
        Complete hook volume mount puts the volume into the application

        Args:
            secret_name (str): Secret name
            volume_name (str): Volume name
            mount_path (list): Volume mount path

        Returns:
            list: List of complete hook volume and volume mount sub-resources

        """
        volume = V1Volume(name=volume_name, secret={"secretName": secret_name})
        volume_mount = V1VolumeMount(name=volume_name, mount_path=mount_path)
        return [
            SubResource(
                group="volumes",
                name=volume.name,
                body=self.attribute_map(volume),
                path=(("spec", "template", "spec"), ("spec",)),
            ),
            SubResource(
                group="volumeMounts",
                name=volume_mount.name,
                body=self.attribute_map(volume_mount),
                path=(
                    ("spec", "template", "spec", "containers"),
                    ("spec", "containers"),  # kind: Pod
                ),
            ),
        ]

    @staticmethod
    def _encode_to_64(string):
        """Compute the base 64 encoding of a string.

        Args:
            string (str): the string to encode.

        Returns:
            str: the result of the encoding.

        """
        return b64encode(string.encode()).decode()

    def secret(self, secret_name, secret_data, namespace, _type="Opaque"):
        """Create a secret resource.

        Args:
            secret_name (str): Secret name
            secret_data (dict): Secret data
            namespace (str): Kubernetes namespace where the Secret will be created.
            _type (str, optional): Secret type. Defaults to Opaque.

        Returns:
            dict: secret resource

        """
        return self.attribute_map(
            V1Secret(
                api_version="v1",
                kind="Secret",
                data=secret_data,
                metadata={"name": secret_name, "namespace": namespace},
                type=_type,
            )
        )

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' hook URL.
        Function needs to be specified for each hook.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application shutdown url

        """
        pass

    def env_vars(self, secret_name):
        """Create the hooks' environment variables sub-resources.
        Function needs to be specified for each hook.

        Creates hook environment variables to store Krake authentication token
        and a hook URL for the given applications.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of shutdown hook environment variables sub-resources

        """
        pass


class Complete(Hook):
    """Mangle given application and inject complete hooks variables into it.

    Hook injects a Kubernetes secret, which stores Krake authentication token
    and the Krake complete hook URL for the given application. The variables
    from Kubernetes secret are imported as environment variables
    into the application resource definition. Only resources defined in
    :args:`hook_resources` can be modified.

    Names of environment variables are defined in the application controller
    configuration file.

    If TLS is enabled on the Krake API, the complete hook injects a Kubernetes secret,
    and it's corresponding volume and volume mount definitions for the Krake CA,
    the client certificate with the right CN, and its key. The directory where the
    secret is mounted is defined in the configuration.

    Args:
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        cert_dest (str, optional): Path of the directory where the CA, client
            certificate and key to the Krake API will be stored.
        env_token (str, optional): Name of the environment variable, which stores Krake
            authentication token.
        env_url (str, optional): Name of the environment variable,
            which stores Krake complete hook URL.

    """

    hook_resources = ("Pod", "Deployment", "ReplicationController")

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        super().__init__(
            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
        )
        self.env_url = env_url

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create complete hooks secret resource.

        Complete hook secret stores Krake authentication token
        and complete hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Complete hook authentication token

        Returns:
            dict: complete hook secret resource

        """
        complete_url = self.create_hook_url(name, namespace, api_endpoint)
        data = {
            self.env_token.lower(): self._encode_to_64(token),
            self.env_url.lower(): self._encode_to_64(complete_url),
        }
        return self.secret(secret_name, data, resource_namespace)

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' complete URL.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application complete url

        """
        api_url = URL(api_endpoint)
        return str(
            api_url.with_path(
                f"/kubernetes/namespaces/{namespace}/applications/{name}/complete"
            )
        )

    def env_vars(self, secret_name):
        """Create complete hooks environment variables sub-resources

        Create complete hook environment variables store Krake authentication token
        and complete hook URL for given application.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of complete hook environment variables sub-resources

        """
        sub_resources = []

        env_token = V1EnvVar(
            name=self.env_token,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(
                            name=secret_name, key=self.env_token.lower()
                        )
                    )
                )
            ),
        )
        env_url = V1EnvVar(
            name=self.env_url,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())
                    )
                )
            ),
        )

        for env in (env_token, env_url):
            sub_resources.append(
                SubResource(
                    group="env",
                    name=env.name,
                    body=self.attribute_map(env),
                    path=(
                        ("spec", "template", "spec", "containers"),
                        ("spec", "containers"),  # kind: Pod
                    ),
                )
            )
        return sub_resources


class Shutdown(Hook):
    """Mangle given application and inject shutdown hooks variables into it.

    Hook injects a Kubernetes secret, which stores Krake authentication token
    and the Krake complete hook URL for the given application. The variables
    from the Kubernetes secret are imported as environment variables
    into the application resource definition. Only resources defined in
    :args:`hook_resources` can be modified.

    Names of environment variables are defined in the application controller
    configuration file.

    If TLS is enabled on the Krake API, the shutdown hook injects a Kubernetes secret,
    and it's corresponding volume and volume mount definitions for the Krake CA,
    the client certificate with the right CN, and its key. The directory where the
    secret is mounted is defined in the configuration.

    Args:
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        cert_dest (str, optional): Path of the directory where the CA, client
            certificate and key to the Krake API will be stored.
        env_token (str, optional): Name of the environment variable, which stores Krake
            authentication token.
        env_url (str, optional): Name of the environment variable,
            which stores Krake complete hook URL.

    """

    hook_resources = ("Pod", "Deployment", "ReplicationController")

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        super().__init__(
            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
        )
        self.env_url = env_url

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create shutdown hooks secret resource.

        Shutdown hook secret stores Krake authentication token
        and shutdown hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Shutdown hook authentication token

        Returns:
            dict: shutdown hook secret resource

        """
        shutdown_url = self.create_hook_url(name, namespace, api_endpoint)
        data = {
            self.env_token.lower(): self._encode_to_64(token),
            self.env_url.lower(): self._encode_to_64(shutdown_url),
        }
        return self.secret(secret_name, data, resource_namespace)

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' shutdown URL.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application shutdown url

        """
        api_url = URL(api_endpoint)
        return str(
            api_url.with_path(
                f"/kubernetes/namespaces/{namespace}/applications/{name}/shutdown"
            )
        )

    def env_vars(self, secret_name):
        """Create shutdown hooks environment variables sub-resources.

        Creates shutdown hook environment variables to store Krake authentication token
        and a shutdown hook URL for given applications.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of shutdown hook environment variables sub-resources

        """
        sub_resources = []

        env_resources = []

        env_token = V1EnvVar(
            name=self.env_token,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(
                            name=secret_name, key=self.env_token.lower()
                        )
                    )
                )
            ),
        )
        env_resources.append(env_token)

        env_url = V1EnvVar(
            name=self.env_url,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())
                    )
                )
            ),
        )
        env_resources.append(env_url)

        for env in env_resources:
            sub_resources.append(
                SubResource(
                    group="env",
                    name=env.name,
                    body=self.attribute_map(env),
                    path=(
                        ("spec", "template", "spec", "containers"),
                        ("spec", "containers"),  # kind: Pod
                    ),
                )
            )
        return sub_resources

if __name__ == "__main__":
    isT=True
    args0_1={'name': 'nginx-demo', 'namespace': 'secondary'}
    args0_2=False
    args1_1={'app': 'nginx'}
    args1_2=False
    args2_1={'matchLabels': {'app': 'nginx'}}
    args2_2=False
    args3_1={'labels': {'app': 'nginx'}}
    args3_2=False
    args4_1={'metadata': {'labels': {'app': 'nginx'}},
     'spec': {'containers': [{'name': 'nginx', 'image': 'nginx:1.7.9', 'ports': [{'containerPort': 80}]}]}}
    args4_2=False
    args5_1={'selector': {'matchLabels': {'app': 'nginx'}}, 'template': {'metadata': {'labels': {'app': 'nginx'}}, 'spec': {
        'containers': [{'name': 'nginx', 'image': 'nginx:1.7.9', 'ports': [{'containerPort': 80}]}]}}}
    args5_2=False
    args6_1={'apiVersion': 'apps/v1', 'kind': 'Deployment', 'metadata': {'name': 'nginx-demo', 'namespace': 'secondary'},
     'spec': {'selector': {'matchLabels': {'app': 'nginx'}}, 'template': {'metadata': {'labels': {'app': 'nginx'}},
                                                                          'spec': {'containers': [
                                                                              {'name': 'nginx', 'image': 'nginx:1.7.9',
                                                                               'ports': [{'containerPort': 80}]}]}}}}
    args6_2=True
    res0 = generate_default_observer_schema_dict(args0_1, args0_2)=={'name': None, 'namespace': None}
    res1 = generate_default_observer_schema_dict(args1_1, args1_2)=={'app': None}
    res2 = generate_default_observer_schema_dict(args2_1, args2_2)=={'matchLabels': {'app': None}}
    res3 = generate_default_observer_schema_dict(args3_1, args3_2)=={'labels': {'app': None}}
    res4 = generate_default_observer_schema_dict(args4_1, args4_2)=={'metadata': {'labels': {'app': None}}, 'spec': {'containers': [{'name': None, 'image': None, 'ports': [{'containerPort': None}, {'observer_schema_list_min_length': 1, 'observer_schema_list_max_length': 1}]}, {'observer_schema_list_min_length': 1, 'observer_schema_list_max_length': 1}]}}
    res5 = generate_default_observer_schema_dict(args5_1, args5_2)=={'selector': {'matchLabels': {'app': None}}, 'template': {'metadata': {'labels': {'app': None}}, 'spec': {'containers': [{'name': None, 'image': None, 'ports': [{'containerPort': None}, {'observer_schema_list_min_length': 1, 'observer_schema_list_max_length': 1}]}, {'observer_schema_list_min_length': 1, 'observer_schema_list_max_length': 1}]}}}
    res6 = generate_default_observer_schema_dict(args6_1, args6_2)=={'apiVersion': 'apps/v1', 'kind': 'Deployment', 'metadata': {'name': 'nginx-demo', 'namespace': None}, 'spec': {'selector': {'matchLabels': {'app': None}}, 'template': {'metadata': {'labels': {'app': None}}, 'spec': {'containers': [{'name': None, 'image': None, 'ports': [{'containerPort': None}, {'observer_schema_list_min_length': 1, 'observer_schema_list_max_length': 1}]}, {'observer_schema_list_min_length': 1, 'observer_schema_list_max_length': 1}]}}}}
    if not res0 or not res1 or not res3 or not res4 or not res5 or not res6:
        isT=False

    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/rak-n-rok---Krake/krake/krake/controller/kubernetes/hooks_update_last_applied_manifest_list_from_resp_passk_validte.py
"""This module defines the Hook Dispatcher and listeners for registering and
executing hooks. Hook Dispatcher emits hooks based on :class:`Hook` attributes which
define when the hook will be executed.

"""
import asyncio
import logging
import random
from base64 import b64encode
from collections import defaultdict
from contextlib import suppress
from copy import deepcopy
from datetime import datetime
from functools import reduce
from operator import getitem
from enum import Enum, auto
from inspect import iscoroutinefunction
from OpenSSL import crypto
from typing import NamedTuple

import yarl
from aiohttp import ClientConnectorError
import sys
sys.path.append("/home/travis/builds/repos/rak-n-rok---Krake/krake")
from krake.controller import Observer
from krake.controller.kubernetes.client import KubernetesClient, InvalidManifestError
from krake.controller.kubernetes.tosca import ToscaParser, ToscaParserException
from krake.utils import camel_to_snake_case, get_kubernetes_resource_idx
from kubernetes_asyncio.client.rest import ApiException
from kubernetes_asyncio.client.api_client import ApiClient
from kubernetes_asyncio import client
from krake.data.kubernetes import (
    ClusterState,
    Application,
    ApplicationState,
    ContainerHealth,
    Cluster,
    ClusterNodeCondition,
    ClusterNode,
    ClusterNodeStatus,
    ClusterNodeMetadata,
)
from yarl import URL
from secrets import token_urlsafe

from kubernetes_asyncio.client import (
    Configuration,
    V1Secret,
    V1EnvVar,
    V1VolumeMount,
    V1Volume,
    V1SecretKeySelector,
    V1EnvVarSource,
)
from kubernetes_asyncio.config.kube_config import KubeConfigLoader

logger = logging.getLogger(__name__)


class HookType(Enum):
    ResourcePreCreate = auto()
    ResourcePostCreate = auto()
    ResourcePreUpdate = auto()
    ResourcePostUpdate = auto()
    ResourcePreDelete = auto()
    ResourcePostDelete = auto()
    ApplicationToscaTranslation = auto()
    ApplicationMangling = auto()
    ApplicationPreMigrate = auto()
    ApplicationPostMigrate = auto()
    ApplicationPreReconcile = auto()
    ApplicationPostReconcile = auto()
    ApplicationPreDelete = auto()
    ApplicationPostDelete = auto()
    ClusterCreation = auto()
    ClusterDeletion = auto()


class HookDispatcher(object):
    """Simple wrapper around a registry of handlers associated to :class:`Hook`
     attributes. Each :class:`Hook` attribute defines when the handler will be
     executed.

    Listeners for certain hooks can be registered via :meth:`on`. Registered
    listeners are executed via :meth:`hook`.

    Example:
        .. code:: python

        listen = HookDispatcher()

        @listen.on(HookType.PreApply)
        def to_perform_before_app_creation(app, cluster, resource, controller):
            # Do Stuff

        @listen.on(HookType.PostApply)
        def another_to_perform_after_app_creation(app, cluster, resource, resp):
            # Do Stuff

        @listen.on(HookType.PostDelete)
        def to_perform_after_app_deletion(app, cluster, resource, resp):
            # Do Stuff

    """

    def __init__(self):
        self.registry = defaultdict(list)

    def on(self, hook):
        """Decorator function to add a new handler to the registry.

        Args:
            hook (HookType): Hook attribute for which to register the handler.

        Returns:
            callable: Decorator for registering listeners for the specified
            hook.

        """

        def decorator(handler):
            self.registry[hook].append(handler)

            return handler

        return decorator

    async def hook(self, hook, **kwargs):
        """Execute the list of handlers associated to the provided :class:`Hook`
        attribute.

        Args:
            hook (HookType): The hook attribute for which to execute handlers.

        """
        try:
            handlers = self.registry[hook]
        except KeyError:
            pass
        else:
            for handler in handlers:
                if iscoroutinefunction(handler):
                    await handler(**kwargs)
                else:
                    handler(**kwargs)


listen = HookDispatcher()


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
async def register_service(app, cluster, resource, response):
    """Register endpoint of Kubernetes Service object on creation and update.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        cluster (krake.data.kubernetes.Cluster): The cluster on which the
            application is running
        resource (dict): Kubernetes object description as specified in the
            specification of the application.
        response (kubernetes_asyncio.client.V1Service): Response of the
            Kubernetes API

    """
    if resource["kind"] != "Service":
        return

    service_name = resource["metadata"]["name"]

    if response.spec and response.spec.type == "LoadBalancer":
        # For a "LoadBalancer" type of Service, an external IP is given in the cluster
        # by a load balancer controller to the service. In this case, the "port"
        # specified in the spec is reachable from the outside.
        if (
            not response.status.load_balancer
            or not response.status.load_balancer.ingress
        ):
            # When a "LoadBalancer" type of service is created, the IP is given by an
            # additional controller (e.g. a controller that requests a floating IP to an
            # OpenStack infrastructure). This process can take some time, but the
            # Service itself already exist before the IP is assigned. In the case of an
            # error with the controller, the IP is also not given. This "<pending>" IP
            # just expresses that the Service exists, but the IP is not ready yet.
            external_ip = "<pending>"
        else:
            external_ip = response.status.load_balancer.ingress[0].ip

        if not response.spec.ports:
            external_port = "<pending>"
        else:
            external_port = response.spec.ports[0].port
        app.status.services[service_name] = f"{external_ip}:{external_port}"
        return

    node_port = None
    # Ensure that ports are specified
    if response.spec and response.spec.ports:
        node_port = response.spec.ports[0].node_port

    # If the service does not have a node port, remove a potential reference
    # and return.
    if node_port is None:
        try:
            del app.status.services[service_name]
        except KeyError:
            pass
        return

    # Determine URL of Kubernetes cluster API
    loader = KubeConfigLoader(cluster.spec.kubeconfig)
    config = Configuration()
    await loader.load_and_set(config)
    cluster_url = yarl.URL(config.host)

    app.status.services[service_name] = f"{cluster_url.host}:{node_port}"


@listen.on(HookType.ResourcePostDelete)
async def unregister_service(app, resource, **kwargs):
    """Unregister endpoint of Kubernetes Service object on deletion.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        resource (dict): Kubernetes object description as specified in the
            specification of the application.

    """
    if resource["kind"] != "Service":
        return

    service_name = resource["metadata"]["name"]
    try:
        del app.status.services[service_name]
    except KeyError:
        pass


@listen.on(HookType.ResourcePostDelete)
async def remove_resource_from_last_observed_manifest(app, resource, **kwargs):
    """Remove a given resource from the last_observed_manifest after its deletion

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        resource (dict): Kubernetes object description as specified in the
            specification of the application.

    """
    try:
        idx = get_kubernetes_resource_idx(app.status.last_observed_manifest, resource)
    except IndexError:
        return

    app.status.last_observed_manifest.pop(idx)


def update_last_applied_manifest_dict_from_resp(
    last_applied_manifest, observer_schema, response
):
    """Together with :func:``update_last_applied_manifest_list_from_resp``, this
    function is called recursively to update a partial ``last_applied_manifest``
    from a partial Kubernetes response

    Args:
        last_applied_manifest (dict): partial ``last_applied_manifest`` being
            updated
        observer_schema (dict): partial ``observer_schema``
        response (dict): partial response from the Kubernetes API.

    Raises:
        KeyError: If the observed field is not present in the Kubernetes response

    This function go through all observed fields, and initialized their value in
    last_applied_manifest if they are not yet present

    """
    for key, value in observer_schema.items():

        # Keys in the response are in camelCase
        camel_key = camel_to_snake_case(key)

        if camel_key not in response:
            # An observed key should always be present in the k8s response
            raise KeyError(
                f"Observed key {camel_key} is not present in response {response}"
            )

        if isinstance(value, dict):
            if key not in last_applied_manifest:
                # The dictionary is observed, but not present in
                # last_applied_manifest
                last_applied_manifest[key] = {}

            update_last_applied_manifest_dict_from_resp(
                last_applied_manifest[key], observer_schema[key], response[camel_key]
            )

        elif isinstance(value, list):
            if key not in last_applied_manifest:
                # The list is observed, but not present in last_applied_manifest
                last_applied_manifest[key] = []

            update_last_applied_manifest_list_from_resp(
                last_applied_manifest[key], observer_schema[key], response[camel_key]
            )

        elif key not in last_applied_manifest:
            # If key not present in last_applied_manifest, and value is neither a
            # dict nor a list, simply add it.
            last_applied_manifest[key] = response[camel_key]


def update_last_applied_manifest_list_from_resp(
    last_applied_manifest, observer_schema, response
):
    """Together with :func:``update_last_applied_manifest_dict_from_resp``, this
    function is called recursively to update a partial ``last_applied_manifest``
    from a partial Kubernetes response

    Args:
        last_applied_manifest (list): partial ``last_applied_manifest`` being
            updated
        observer_schema (list): partial ``observer_schema``
        response (list): partial response from the Kubernetes API.

    This function go through all observed fields, and initialized their value in
    last_applied_manifest if they are not yet present

    """
    # Looping over the observed resource, except the last element which is the
    # special control dictionary
    for idx, val in enumerate(observer_schema[:-1]):

        if idx >= len(response):
            # Element is observed but not present in k8s response, so following
            # elements will also not exist.
            #
            # This doesn't raise an Exception as observing the element of a list
            # doesn't ensure its presence. The list length is controlled by the
            # special control dictionary
            return

        if isinstance(val, dict):
            if idx >= len(last_applied_manifest):
                # The dict is observed, but not present in last_applied_manifest
                last_applied_manifest.append({})

            update_last_applied_manifest_dict_from_resp(
                last_applied_manifest[idx], observer_schema[idx], response[idx]
            )

        elif isinstance(response[idx], list):
            if idx >= len(last_applied_manifest):
                # The list is observed, but not present in last_applied_manifest
                last_applied_manifest.append([])

            update_last_applied_manifest_list_from_resp(
                last_applied_manifest[idx], observer_schema[idx], response[idx]
            )

        elif idx >= len(last_applied_manifest):
            # Element is not yet present in last_applied_manifest. Adding it.
            last_applied_manifest.append(response[idx])


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
def update_last_applied_manifest_from_resp(app, response, **kwargs):
    """Hook run after the creation or update of an application in order to update the
    `status.last_applied_manifest` using the k8s response.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        response (kubernetes_asyncio.client.V1Status): Response of the Kubernetes API

    After a Kubernetes resource has been created/updated, the
    `status.last_applied_manifest` has to be updated. All fields already initialized
    (either from the mangling of `spec.manifest`, or by a previous call to this
    function) should be left untouched. Only observed fields which are not present in
    `status.last_applied_manifest` should be initialized.

    """

    if isinstance(response, dict):
        # The Kubernetes API couldn't deserialize the k8s response into an object
        resp = response
    else:
        # The Kubernetes API deserialized the k8s response into an object
        resp = response.to_dict()

    idx_applied = get_kubernetes_resource_idx(app.status.last_applied_manifest, resp)

    idx_observed = get_kubernetes_resource_idx(app.status.mangled_observer_schema, resp)

    update_last_applied_manifest_dict_from_resp(
        app.status.last_applied_manifest[idx_applied],
        app.status.mangled_observer_schema[idx_observed],
        resp,
    )


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
def update_last_observed_manifest_from_resp(app, response, **kwargs):
    """Handler to run after the creation or update of a Kubernetes resource to update
    the last_observed_manifest from the response of the Kubernetes API.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        response (kubernetes_asyncio.client.V1Service): Response of the
            Kubernetes API

    The target last_observed_manifest holds the value of all observed fields plus the
    special control dictionaries for the list length

    """
    if isinstance(response, dict):
        # The Kubernetes API couldn't deserialize the k8s response into an object
        resp = response
    else:
        # The Kubernetes API deserialized the k8s response into an object
        resp = response.to_dict()

    try:
        idx_observed = get_kubernetes_resource_idx(
            app.status.mangled_observer_schema,
            resp,
        )
    except IndexError:
        # All created resources should be observed
        raise

    try:
        idx_last_observed = get_kubernetes_resource_idx(
            app.status.last_observed_manifest,
            resp,
        )
    except IndexError:
        # If the resource is not yes present in last_observed_manifest, append it.
        idx_last_observed = len(app.status.last_observed_manifest)
        app.status.last_observed_manifest.append({})

    # Overwrite the last_observed_manifest for this resource
    app.status.last_observed_manifest[
        idx_last_observed
    ] = update_last_observed_manifest_dict(
        app.status.mangled_observer_schema[idx_observed], resp
    )


def update_last_observed_manifest_dict(observed_resource, response):
    """Together with :func:``update_last_observed_manifest_list``, recursively
    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.

    Args:
        observed_resource (dict): The schema to observe for the partial given resource
        response (dict): The partial Kubernetes response for this resource.

    Raises:
        KeyError: If an observed key is not present in the Kubernetes response

    Returns:
        dict: The dictionary of observed keys and their value

    Get the value of all observed fields from the Kubernetes response
    """
    res = {}
    for key, value in observed_resource.items():

        camel_key = camel_to_snake_case(key)
        if camel_key not in response:
            raise KeyError(
                f"Observed key {camel_key} is not present in response {response}"
            )

        if isinstance(value, dict):
            res[key] = update_last_observed_manifest_dict(value, response[camel_key])

        elif isinstance(value, list):
            res[key] = update_last_observed_manifest_list(value, response[camel_key])

        else:
            res[key] = response[camel_key]

    return res


def update_last_observed_manifest_list(observed_resource, response):
    """Together with :func:``update_last_observed_manifest_dict``, recursively
    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.

    Args:
        observed_resource (list): the schema to observe for the partial given resource
        response (list): the partial Kubernetes response for this resource.

    Returns:
        list: The list of observed elements, plus the special list length control
            dictionary

    Get the value of all observed elements from the Kubernetes response
    """

    if not response:
        return [{"observer_schema_list_current_length": 0}]

    res = []
    # Looping over the observed resource, except the last element which is the special
    # control dictionary
    for idx, val in enumerate(observed_resource[:-1]):

        if idx >= len(response):
            # Element is not present in the Kubernetes response, nothing more to do
            break

        if type(response[idx]) is dict:
            res.append(update_last_observed_manifest_dict(val, response[idx]))

        elif type(response[idx]) is list:
            res.append(update_last_observed_manifest_list(val, response[idx]))

        else:
            res.append(response[idx])

    # Append the special control dictionary to the list
    res.append({"observer_schema_list_current_length": len(response)})

    return res


def update_last_applied_manifest_dict_from_spec(
    resource_status_new, resource_status_old, resource_observed
):
    """Together with :func:``update_last_applied_manifest_list_from_spec``, this
    function is called recursively to update a partial ``last_applied_manifest``

    Args:
        resource_status_new (dict): partial ``last_applied_manifest`` being updated
        resource_status_old (dict): partial of the current ``last_applied_manifest``
        resource_observed (dict): partial observer_schema for the manifest file
            being updated

    """
    for key, value in resource_observed.items():

        if key not in resource_status_old:
            continue

        if key in resource_status_new:

            if isinstance(value, dict):
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            elif isinstance(value, list):
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

        else:
            # If the key is not present the spec.manifest, we first need to
            # initialize it

            if isinstance(value, dict):
                resource_status_new[key] = {}
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            elif isinstance(value, list):
                resource_status_new[key] = []
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            else:
                resource_status_new[key] = resource_status_old[key]


def update_last_applied_manifest_list_from_spec(
    resource_status_new, resource_status_old, resource_observed
):
    """Together with :func:``update_last_applied_manifest_dict_from_spec``, this
    function is called recursively to update a partial ``last_applied_manifest``

    Args:
        resource_status_new (list): partial ``last_applied_manifest`` being updated
        resource_status_old (list): partial of the current ``last_applied_manifest``
        resource_observed (list): partial observer_schema for the manifest file
            being updated

    """

    # Looping over the observed resource, except the last element which is the
    # special control dictionary
    for idx, val in enumerate(resource_observed[:-1]):

        if idx >= len(resource_status_old):
            # The element in not in the current last_applied_manifest, and neither
            # is the rest of the list
            break

        if idx < len(resource_status_new):
            # The element is present in spec.manifest and in the current
            # last_applied_manifest. Updating observed fields

            if isinstance(val, dict):
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            elif isinstance(val, list):
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

        else:
            # If the element is not present in the spec.manifest, we first have to
            # initialize it.

            if isinstance(val, dict):
                resource_status_new.append({})
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            elif isinstance(val, list):
                resource_status_new.append([])
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            else:
                resource_status_new.append(resource_status_old[idx])


def update_last_applied_manifest_from_spec(app):
    """Update the status.last_applied_manifest of an application from spec.manifests

    Args:
        app (krake.data.kubernetes.Application): Application to update

    This function is called on application creation and updates. The
    last_applied_manifest of an application is initialized as a copy of spec.manifest,
    and is augmented by all known observed fields not yet initialized (i.e. all observed
    fields or resources which are present in the current last_applied_manifest but not
    in the spec.manifest)

    """

    # The new last_applied_manifest is initialized as a copy of the spec.manifest, and
    # augmented by all observed fields which are present in the current
    # last_applied_manifest but not in the original spec.manifest
    new_last_applied_manifest = deepcopy(app.spec.manifest)

    # Loop over observed resources and observed fields, and check if they should be
    # added to the new last_applied_manifest (i.e. present in the current
    # last_applied_manifest but not in spec.manifest)
    for resource_observed in app.status.mangled_observer_schema:

        # If the resource is not present in the current last_applied_manifest, there is
        # nothing to do. Whether the resource was initialized by spec.manifest doesn't
        # matter.
        try:
            idx_status_old = get_kubernetes_resource_idx(
                app.status.last_applied_manifest, resource_observed
            )
        except IndexError:
            continue

        # As the resource is present in the current last_applied_manifest, we need to go
        # through it to check if observed fields should be set to their current value
        # (i.e. fields are present in the current last_applied_manifest, but not in
        # spec.manifest)
        try:
            # Check if the observed resource is present in spec.manifest
            idx_status_new = get_kubernetes_resource_idx(
                new_last_applied_manifest, resource_observed
            )
        except IndexError:
            # The resource is observed but is not present in the spec.manifest.
            # Create an empty resource, which will be augmented in
            # update_last_applied_manifest_dict_from_spec with the observed and known
            # fields.
            new_last_applied_manifest.append({})
            idx_status_new = len(new_last_applied_manifest) - 1

        update_last_applied_manifest_dict_from_spec(
            new_last_applied_manifest[idx_status_new],
            app.status.last_applied_manifest[idx_status_old],
            resource_observed,
        )

    app.status.last_applied_manifest = new_last_applied_manifest


class KubernetesApplicationObserver(Observer):
    """Observer specific for Kubernetes Applications. One observer is created for each
    Application managed by the Controller, but not one per Kubernetes resource
    (Deployment, Service...). If several resources are defined by an Application, they
    are all monitored by the same observer.

    The observer gets the actual status of the resources on the cluster using the
    Kubernetes API, and compare it to the status stored in the API.

    The observer is:
     * started at initial Krake resource creation;

     * deleted when a resource needs to be updated, then started again when it is done;

     * simply deleted on resource deletion.

    Args:
        cluster (krake.data.kubernetes.Cluster): the cluster on which the observed
            Application is created.
        resource (krake.data.kubernetes.Application): the application that will be
            observed.
        on_res_update (coroutine): a coroutine called when a resource's actual status
            differs from the status sent by the database. Its signature is:
            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of
            the resource that is up-to-date with the API. The Observer internal instance
            of the resource to observe will be updated. If the API cannot be contacted,
            ``None`` can be returned. In this case the internal instance of the Observer
            will not be updated.
        time_step (int, optional): how frequently the Observer should watch the actual
            status of the resources.

    """

    def __init__(self, cluster, resource, on_res_update, time_step=2):
        super().__init__(resource, on_res_update, time_step)
        self.cluster = cluster
        self.kubernetes_api = None

    def _set_container_health(self, resource, status):
        if status.container_health is None:
            status.container_health = ContainerHealth()

        if hasattr(resource, 'status') and resource.status is not None:
            if resource.kind == "Pod":
                container_state = resource.status.container_statuses[0].state
                status.container_health.desired_pods = 1
                if container_state.terminated is not None:
                    if resource.spec.restart_policy == "Never":
                        status.state = ApplicationState.DEGRADED
                    else:
                        status.state = ApplicationState.RESTARTING
                    status.container_health.running_pods = 0
                elif container_state.waiting is not None:
                    if container_state.waiting.reason == "CrashLoopBackOff":
                        status.state = ApplicationState.DEGRADED
                        status.container_health.running_pods = 0
                else:
                    status.state = ApplicationState.RUNNING
                    status.container_health.running_pods = 1

            elif (
                resource.kind == "Deployment" or
                resource.kind == "StatefulSet" or
                resource.kind == "ReplicaSet" or
                resource.kind == "DaemonSet"
            ):

                if resource.kind == "DaemonSet":
                    status.container_health.desired_pods = \
                        resource.status.current_number_scheduled
                    if isinstance(resource.status.desired_number_scheduled, int):
                        status.container_health.running_pods = \
                            resource.status.desired_number_scheduled
                    else:
                        status.container_health.running_pods = 0
                else:
                    status.container_health.desired_pods = resource.status.replicas
                    if isinstance(resource.status.ready_replicas, int):
                        status.container_health.running_pods = \
                            resource.status.ready_replicas
                    else:
                        status.container_health.running_pods = 0

                if status.container_health.running_pods != \
                   status.container_health.desired_pods:
                    status.state = ApplicationState.DEGRADED
                else:
                    status.state = ApplicationState.RUNNING

            elif resource.kind == "Job":
                if isinstance(resource.spec.completions, int):
                    status.container_health.desired_pods = resource.spec.completions
                else:
                    status.container_health.desired_pods = 1

                if isinstance(resource.status.active, int):
                    status.container_health.running_pods = resource.status.active
                else:
                    status.container_health.running_pods = 0

                if isinstance(resource.status.succeeded, int):
                    status.container_health.completed_pods = resource.status.succeeded
                else:
                    status.container_health.completed_pods = 0

                if isinstance(resource.status.failed, int):
                    status.container_health.failed_pods = resource.status.failed
                else:
                    status.container_health.failed_pods = 0

    async def poll_resource(self):
        """Fetch the current status of the Application monitored by the Observer.

        Returns:
            krake.data.core.Status: the status object created using information from the
                real world Applications resource.

        """
        app = self.resource

        status = deepcopy(app.status)
        status.last_observed_manifest = []

        # For each observed kubernetes resource of the Application,
        # get its current status on the cluster.
        for desired_resource in app.status.last_applied_manifest:
            kube = KubernetesClient(self.cluster.spec.kubeconfig)
            idx_observed = get_kubernetes_resource_idx(
                app.status.mangled_observer_schema, desired_resource
            )
            observed_resource = app.status.mangled_observer_schema[idx_observed]
            async with kube:
                try:
                    group, version, kind, name, namespace = kube.get_immutables(
                        desired_resource
                    )
                    resource_api = await kube.get_resource_api(group, version, kind)
                    resp = await resource_api.read(kind, name, namespace)
                except (ClientConnectorError, ApiException) as err:
                    if hasattr(err, "status") and err.status == 404:
                        # Resource does not exist
                        continue
                    # Otherwise, log the unexpected error and return the
                    # last known application status
                    logger.debug(err)
                    return app.status

                resource = resp
                self._set_container_health(resource, status)

            observed_manifest = update_last_observed_manifest_dict(
                observed_resource, resp.to_dict()
            )
            status.last_observed_manifest.append(observed_manifest)

        return status


class KubernetesClusterObserver(Observer):
    """Observer specific for Kubernetes Clusters. One observer is created for each
    Cluster managed by the Controller.

    The observer gets the actual status of the cluster using the
    Kubernetes API, and compare it to the status stored in the API.

    The observer is:
     * started at initial Krake resource creation;

     * deleted when a resource needs to be updated, then started again when it is done;

     * simply deleted on resource deletion.

    Args:
        resource (krake.data.kubernetes.Cluster): the cluster which will be observed.
        on_res_update (coroutine): a coroutine called when a resource's actual status
            differs from the status sent by the database. Its signature is:
            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of
            the resource that is up-to-date with the API. The Observer internal instance
            of the resource to observe will be updated. If the API cannot be contacted,
            ``None`` can be returned. In this case the internal instance of the Observer
            will not be updated.
        time_step (int, optional): how frequently the Observer should watch the actual
            status of the resources.

    """

    def __init__(self, resource, on_res_update, time_step=2):
        super().__init__(resource, on_res_update, time_step)

    async def poll_resource(self):
        """Fetch the current status of the Cluster monitored by the Observer.

        Note regarding exceptions handling:
          The current cluster status is fetched by :func:`poll_resource` from its API.
          If the cluster API is shutting down the API server responds with a 503
          (service unavailable, apiserver is shutting down) HTTP response which
          leads to the kubernetes client ApiException. If the cluster's API has been
          successfully shut down and there is an attempt to fetch cluster status,
          the ClientConnectorError is raised instead.
          Therefore, both exceptions should be handled.

        Returns:
            krake.data.core.Status: the status object created using information from the
                real world Cluster.

        """
        cluster = self.resource
        status = deepcopy(cluster.status)
        status.nodes = []
        # For each observed kubernetes cluster registered in Krake,
        # get its current node status.
        loader = KubeConfigLoader(cluster.spec.kubeconfig)
        config = Configuration()
        await loader.load_and_set(config)
        kube = ApiClient(config)

        async with kube as api:
            v1 = client.CoreV1Api(api)
            try:
                response = await v1.list_node()
            except (ClientConnectorError, ApiException) as err:
                # Log the error and set cluster state to OFFLINE
                logger.debug(err)
                status.state = ClusterState.OFFLINE
                return status

            # Fetch nodes conditions
            nodes = []
            for node in response.items:
                conditions = []
                for condition in node.status.conditions:
                    conditions.append(
                        ClusterNodeCondition(
                            message=condition.message,
                            reason=condition.reason,
                            status=condition.status,
                            type=condition.type,
                        )
                    )

                nodes.append(
                    ClusterNode(
                        metadata=ClusterNodeMetadata(name=node.metadata.name),
                        status=ClusterNodeStatus(conditions=conditions),
                    )
                )
            status.nodes = nodes

            # The scheduler is unable to fetch cluster metrics, hence
            # the cluster state should wait for it and the cluster
            # status should not be changed by the observer.
            if status.state == ClusterState.FAILING_METRICS:
                return status

            # Set the cluster state to CONNECTING if the previous state
            # was OFFLINE. It is due to smooth transition from
            # the OFFLINE to ONLINE state.
            if status.state == ClusterState.OFFLINE:
                status.state = ClusterState.CONNECTING
                return status

            for node in status.nodes:
                for condition in node.status.conditions:
                    if (
                        condition.type.lower().endswith("pressure")
                        and condition.status == "True"
                    ):
                        status.state = ClusterState.UNHEALTHY
                        return status

                    if condition.type.lower() == "ready" and condition.status != "True":
                        status.state = ClusterState.NOTREADY
                        return status

            status.state = ClusterState.ONLINE
            return status


@listen.on(HookType.ApplicationPostReconcile)
@listen.on(HookType.ApplicationPostMigrate)
@listen.on(HookType.ClusterCreation)
async def register_observer(controller, resource, start=True, **kwargs):
    """Create an observer for the given Application or Cluster, and start it as a
    background task if wanted.

    If an observer already existed for this Application or Cluster, it is stopped
    and deleted.

    Args:
        controller (KubernetesController): the controller for which the observer will be
            added in the list of working observers.
        resource (krake.data.kubernetes.Application): the Application to observe or
        resource (krake.data.kubernetes.Cluster): the Cluster to observe.
        start (bool, optional): if False, does not start the observer as background
            task.

    """
    if resource.kind == Application.kind:
        cluster = await controller.kubernetes_api.read_cluster(
            namespace=resource.status.running_on.namespace,
            name=resource.status.running_on.name,
        )
        observer = KubernetesApplicationObserver(
            cluster,
            resource,
            controller.on_status_update,
            time_step=controller.observer_time_step,
        )

    elif resource.kind == Cluster.kind:
        observer = KubernetesClusterObserver(
            resource,
            controller.on_status_update,
            time_step=controller.observer_time_step,
        )
    else:
        logger.debug("Unknown resource kind. No observer was registered.", resource)
        return

    logger.debug(f"Start observer for {resource.kind} %r", resource.metadata.name)
    task = None
    if start:
        task = controller.loop.create_task(observer.run())

    controller.observers[resource.metadata.uid] = (observer, task)


@listen.on(HookType.ApplicationPreReconcile)
@listen.on(HookType.ApplicationPreMigrate)
@listen.on(HookType.ApplicationPreDelete)
@listen.on(HookType.ClusterDeletion)
async def unregister_observer(controller, resource, **kwargs):
    """Stop and delete the observer for the given Application or Cluster. If no observer
    is started, do nothing.

    Args:
        controller (KubernetesController): the controller for which the observer will be
            removed from the list of working observers.
        resource (krake.data.kubernetes.Application): the Application whose observer
        will be stopped or
        resource (krake.data.kubernetes.Cluster): the Cluster whose observer will be
        stopped.

    """
    if resource.metadata.uid not in controller.observers:
        return

    logger.debug(f"Stop observer for {resource.kind} {resource.metadata.name}")
    _, task = controller.observers.pop(resource.metadata.uid)
    task.cancel()

    with suppress(asyncio.CancelledError):
        await task


@listen.on(HookType.ApplicationToscaTranslation)
async def translate_tosca(controller, app, **kwargs):
    """Translate a TOSCA template or CSAR archive into a Kubernetes manifest.

    Args:
        controller (KubernetesController): the controller that handles the application
            resource.
        app (krake.data.kubernetes.Application): the Application that could be defined
            by a TOSCA template or a CSAR archive.

    Raises:
        ToscaParserException: If the given application does not contain
         at least one from the following:
         - Kubernetes manifest
         - TOSCA template
         - CSAR archive

    """
    if app.spec.manifest:
        return

    if not app.spec.tosca and not app.spec.csar:
        raise ToscaParserException(
            "Application should be defined by a Kubernetes manifest,"
            " a TOSCA template or a CSAR archive: %r",
            app,
        )
    app.status.state = ApplicationState.TRANSLATING
    await controller.kubernetes_api.update_application_status(
        namespace=app.metadata.namespace, name=app.metadata.name, body=app
    )

    if app.spec.tosca and isinstance(app.spec.tosca, dict):

        manifest = ToscaParser.from_dict(app.spec.tosca).translate_to_manifests()
    else:
        manifest = ToscaParser.from_url(
            app.spec.tosca or app.spec.csar
        ).translate_to_manifests()

    app.spec.manifest = manifest
    await controller.kubernetes_api.update_application(
        namespace=app.metadata.namespace, name=app.metadata.name, body=app
    )


def utc_difference():
    """Get the difference in seconds between the current time and the current UTC time.

    Returns:
        int: the time difference in seconds.

    """
    delta = datetime.now() - datetime.utcnow()
    return delta.seconds


def generate_certificate(config):
    """Create and sign a new certificate using the one defined in the complete hook
    configuration as intermediate certificate.

    Args:
        config (krake.data.config.CompleteHookConfiguration): the configuration of the
            complete hook.

    Returns:
        CertificatePair: the content of the certificate created and its corresponding
            key.

    """
    with open(config.intermediate_src, "rb") as f:
        intermediate_src = crypto.load_certificate(crypto.FILETYPE_PEM, f.read())
    with open(config.intermediate_key_src, "rb") as f:
        intermediate_key_src = crypto.load_privatekey(crypto.FILETYPE_PEM, f.read())

    client_cert = crypto.X509()

    # Set general information
    client_cert.set_version(3)
    client_cert.set_serial_number(random.randint(50000000000000, 100000000000000))
    # If not set before, TLS will not accept to use this certificate in UTC cases, as
    # the server time may be earlier.
    time_offset = utc_difference() * -1
    client_cert.gmtime_adj_notBefore(time_offset)
    client_cert.gmtime_adj_notAfter(1 * 365 * 24 * 60 * 60)

    # Set issuer and subject
    intermediate_subject = intermediate_src.get_subject()
    client_cert.set_issuer(intermediate_subject)
    client_subj = crypto.X509Name(intermediate_subject)
    client_subj.CN = config.hook_user
    client_cert.set_subject(client_subj)

    # Create and set the private key
    client_key = crypto.PKey()
    client_key.generate_key(crypto.TYPE_RSA, 2048)
    client_cert.set_pubkey(client_key)

    client_cert.sign(intermediate_key_src, "sha256")

    cert_dump = crypto.dump_certificate(crypto.FILETYPE_PEM, client_cert).decode()
    key_dump = crypto.dump_privatekey(crypto.FILETYPE_PEM, client_key).decode()
    return CertificatePair(cert=cert_dump, key=key_dump)


def generate_default_observer_schema(app):
    """Generate the default observer schema for each Kubernetes resource present in
    ``spec.manifest`` for which a custom observer schema hasn't been specified.

    Args:
        app (krake.data.kubernetes.Application): The application for which to generate a
            default observer schema
    """

    app.status.mangled_observer_schema = deepcopy(app.spec.observer_schema)

    for resource_manifest in app.spec.manifest:
        try:
            get_kubernetes_resource_idx(
                app.status.mangled_observer_schema, resource_manifest
            )

        except IndexError:
            # Only create a default observer schema, if a custom observer schema hasn't
            # been set by the user.
            app.status.mangled_observer_schema.append(
                generate_default_observer_schema_dict(
                    resource_manifest,
                    first_level=True,
                )
            )


def generate_default_observer_schema_dict(manifest_dict, first_level=False):
    """Together with :func:``generate_default_observer_schema_list``, this function is
    called recursively to generate part of a default ``observer_schema`` from part of a
    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.

    Args:
        manifest_dict (dict): Partial Kubernetes resources
        first_level (bool, optional): If True, indicates that the dictionary represents
            the whole observer schema of a Kubernetes resource

    Returns:
        dict: Generated partial observer_schema

    This function creates a new dictionary from ``manifest_dict`` and replaces all
    non-list and non-dict values by ``None``.

    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a
    resource), the values of the identifying fields are copied from the manifest file.

    """
    observer_schema_dict = {}

    for key, value in manifest_dict.items():

        if isinstance(value, dict):
            observer_schema_dict[key] = generate_default_observer_schema_dict(value)

        elif isinstance(value, list):
            observer_schema_dict[key] = generate_default_observer_schema_list(value)

        else:
            observer_schema_dict[key] = None

    if first_level:
        observer_schema_dict["apiVersion"] = manifest_dict["apiVersion"]
        observer_schema_dict["kind"] = manifest_dict["kind"]
        observer_schema_dict["metadata"]["name"] = manifest_dict["metadata"]["name"]

        if (
            "spec" in manifest_dict
            and "type" in manifest_dict["spec"]
            and manifest_dict["spec"]["type"] == "LoadBalancer"
        ):
            observer_schema_dict["status"] = {"load_balancer": {"ingress": None}}

    return observer_schema_dict


def generate_default_observer_schema_list(manifest_list):
    """Together with :func:``generate_default_observer_schema_dict``, this function is
    called recursively to generate part of a default ``observer_schema`` from part of a
    Kubernetes resource, defined respectively by ``manifest_list`` or ``manifest_dict``.

    Args:
        manifest_list (list): Partial Kubernetes resources

    Returns:
        list: Generated partial observer_schema

    This function creates a new list from ``manifest_list`` and replaces all non-list
    and non-dict elements by ``None``.

    Additionally, it generates the default list control dictionary, using the current
    length of the list as default minimum and maximum values.

    """
    observer_schema_list = []

    for value in manifest_list:

        if isinstance(value, dict):
            observer_schema_list.append(generate_default_observer_schema_dict(value))

        elif isinstance(value, list):
            observer_schema_list.append(generate_default_observer_schema_list(value))

        else:
            observer_schema_list.append(None)

    observer_schema_list.append(
        {
            "observer_schema_list_min_length": len(manifest_list),
            "observer_schema_list_max_length": len(manifest_list),
        }
    )

    return observer_schema_list


@listen.on(HookType.ApplicationMangling)
async def complete(app, api_endpoint, ssl_context, config):
    """Execute application complete hook defined by :class:`Complete`.
    Hook mangles given application and injects complete hooks variables.

    Application complete hook is disabled by default.
    User enables this hook by the --hook-complete argument in rok cli.

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        config (krake.data.config.HooksConfiguration): Complete hook
            configuration.

    """
    if "complete" not in app.spec.hooks:
        return

    # Use the endpoint of the API only if the external endpoint has not been set.
    if config.complete.external_endpoint:
        api_endpoint = config.complete.external_endpoint

    app.status.complete_token = (
        app.status.complete_token if app.status.complete_token else token_urlsafe()
    )

    # Generate only once the certificate and key for a specific Application
    generated_cert = CertificatePair(
        cert=app.status.complete_cert, key=app.status.complete_key
    )
    if ssl_context and generated_cert == (None, None):
        generated_cert = generate_certificate(config.complete)
        app.status.complete_cert = generated_cert.cert
        app.status.complete_key = generated_cert.key

    hook = Complete(
        api_endpoint,
        ssl_context,
        hook_user=config.complete.hook_user,
        cert_dest=config.complete.cert_dest,
        env_token=config.complete.env_token,
        env_url=config.complete.env_url,
    )
    hook.mangle_app(
        app.metadata.name,
        app.metadata.namespace,
        app.status.complete_token,
        app.status.last_applied_manifest,
        config.complete.intermediate_src,
        generated_cert,
        app.status.mangled_observer_schema,
        "complete",
    )


@listen.on(HookType.ApplicationMangling)
async def shutdown(app, api_endpoint, ssl_context, config):
    """Executes an application shutdown hook defined by :class:`Shutdown`.
    The hook mangles the given application and injects shutdown hooks variables.

    Application shutdown hook is disabled by default.
    User enables this hook by the --hook-shutdown argument in rok cli.

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        config (krake.data.config.HooksConfiguration): Shutdown hook
            configuration.

    """
    if "shutdown" not in app.spec.hooks:
        return

    # Use the endpoint of the API only if the external endpoint has not been set.
    if config.shutdown.external_endpoint:
        api_endpoint = config.shutdown.external_endpoint

    app.status.shutdown_token = (
        app.status.shutdown_token if app.status.shutdown_token else token_urlsafe()
    )

    # Generate only once the certificate and key for a specific Application
    generated_cert = CertificatePair(
        cert=app.status.shutdown_cert, key=app.status.shutdown_key
    )
    if ssl_context and generated_cert == (None, None):
        generated_cert = generate_certificate(config.shutdown)
        app.status.shutdown_cert = generated_cert.cert
        app.status.shutdown_key = generated_cert.key

    hook = Shutdown(
        api_endpoint,
        ssl_context,
        hook_user=config.shutdown.hook_user,
        cert_dest=config.shutdown.cert_dest,
        env_token=config.shutdown.env_token,
        env_url=config.shutdown.env_url,
    )
    hook.mangle_app(
        app.metadata.name,
        app.metadata.namespace,
        app.status.shutdown_token,
        app.status.last_applied_manifest,
        config.shutdown.intermediate_src,
        generated_cert,
        app.status.mangled_observer_schema,
        "shutdown",
    )


@listen.on(HookType.ResourcePreDelete)
async def pre_shutdown(controller, app, **kwargs):
    """

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
    """
    if "shutdown" not in app.spec.hooks:
        return

    return


class SubResource(NamedTuple):
    group: str
    name: str
    body: dict
    path: tuple


class CertificatePair(NamedTuple):
    """Tuple which contains a certificate and its corresponding key.

    Attributes:
        cert (str): content of a certificate.
        key (str): content of the key that corresponds to the certificate.

    """

    cert: str
    key: str


class Hook(object):
    hook_resources = ()

    ca_name = "ca-bundle.pem"
    cert_name = "cert.pem"
    key_name = "key.pem"

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        self.api_endpoint = api_endpoint
        self.ssl_context = ssl_context
        self.hook_user = hook_user
        self.cert_dest = cert_dest
        self.env_token = env_token
        self.env_url = env_url

    def mangle_app(
        self,
        name,
        namespace,
        token,
        last_applied_manifest,
        intermediate_src,
        generated_cert,
        mangled_observer_schema,
        hook_type="",
    ):
        """Mangle a given application and inject complete hook resources and
        sub-resources into the :attr:`last_applied_manifest` object by :meth:`mangle`.
        Also mangle the observer_schema as new resources and sub-resources should
        be observed.

        :attr:`last_applied_manifest` is created as a deep copy of the desired
        application resources, as defined by user. It can be updated by custom hook
        resources or modified by custom hook sub-resources. It is used as a desired
        state for the Krake deployment process.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            token (str): Complete hook authentication token
            last_applied_manifest (list): Application resources
            intermediate_src (str): content of the certificate that is used to sign new
                certificates for the complete hook.
            generated_cert (CertificatePair): tuple that contains the content of the
                new signed certificate for the Application, and the content of its
                corresponding key.
            mangled_observer_schema (list): Observed fields
            hook_type (str, optional): Name of the hook the app should be mangled for

        """

        secret_certs_name = "-".join([name, "krake", hook_type, "secret", "certs"])
        secret_token_name = "-".join([name, "krake", hook_type, "secret", "token"])
        volume_name = "-".join([name, "krake", hook_type, "volume"])
        ca_certs = (
            self.ssl_context.get_ca_certs(binary_form=True)
            if self.ssl_context
            else None
        )

        # Extract all different namespaces
        # FIXME: too many assumptions here: do we create one ConfigMap for each
        #  namespace?
        resource_namespaces = {
            resource["metadata"].get("namespace", "default")
            for resource in last_applied_manifest
        }

        hook_resources = []
        hook_sub_resources = []
        if ca_certs:
            hook_resources.extend(
                [
                    self.secret_certs(
                        secret_certs_name,
                        resource_namespace,
                        intermediate_src=intermediate_src,
                        generated_cert=generated_cert,
                        ca_certs=ca_certs,
                    )
                    for resource_namespace in resource_namespaces
                ]
            )
            hook_sub_resources.extend(
                [*self.volumes(secret_certs_name, volume_name, self.cert_dest)]
            )

        hook_resources.extend(
            [
                self.secret_token(
                    secret_token_name,
                    name,
                    namespace,
                    resource_namespace,
                    self.api_endpoint,
                    token,
                )
                for resource_namespace in resource_namespaces
            ]
        )
        hook_sub_resources.extend(
            [
                *self.env_vars(secret_token_name),
            ]
        )

        self.mangle(
            hook_resources,
            last_applied_manifest,
            mangled_observer_schema,
        )
        self.mangle(
            hook_sub_resources,
            last_applied_manifest,
            mangled_observer_schema,
            is_sub_resource=True,
        )

    def mangle(
        self,
        items,
        last_applied_manifest,
        mangled_observer_schema,
        is_sub_resource=False,
    ):
        """Mangle applications desired state with custom hook resources or
        sub-resources.

        Example:
            .. code:: python

            last_applied_manifest = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Pod',
                    'metadata': {'name': 'test', 'namespace': 'default'},
                    'spec': {'containers': [{'name': 'test'}]}
                }
            ]
            mangled_observer_schema = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Pod',
                    'metadata': {'name': 'test', 'namespace': 'default'},
                    'spec': {
                        'containers': [
                            {'name': None},
                            {
                                'observer_schema_list_max_length': 1,
                                'observer_schema_list_min_length': 1,
                            },
                        ]
                    },
                }
            ]
            hook_resources = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Secret',
                    'metadata': {'name': 'sct', 'namespace': 'default'}
                }
            ]
            hook_sub_resources = [
                SubResource(
                    group='env', name='env', body={'name': 'test', 'value': 'test'},
                    path=(('spec', 'containers'),)
                )
            ]

            mangle(
                hook_resources,
                last_applied_manifest,
                mangled_observer_schema,
            )
            mangle(
                hook_sub_resources,
                last_applied_manifest,
                mangled_observer_schema,
                is_sub_resource=True
            )

            assert last_applied_manifest == [
                {
                    "apiVersion": "v1",
                    "kind": "Pod",
                    "metadata": {"name": "test", 'namespace': 'default'},
                    "spec": {
                        "containers": [
                            {
                                "name": "test",
                                "env": [{"name": "test", "value": "test"}]
                            }
                        ]
                    },
                },
                {"apiVersion": "v1", "kind": "Secret", "metadata": {"name": "sct"}},
            ]

            assert mangled_observer_schema == [
                {
                    "apiVersion": "v1",
                    "kind": "Pod",
                    "metadata": {"name": "test", "namespace": None},
                    "spec": {
                        "containers": [
                            {
                                "name": None,
                                "env": [
                                    {"name": None, "value": None},
                                    {
                                        "observer_schema_list_max_length": 1,
                                        "observer_schema_list_min_length": 1,
                                    },
                                ],
                            },
                            {
                                "observer_schema_list_max_length": 1,
                                "observer_schema_list_min_length": 1,
                            },
                        ]
                    },
                },
                {
                    "apiVersion": "v1",
                    "kind": "Secret",
                    "metadata": {"name": "sct", "namespace": None},
                },
            ]

        Args:
            items (list[SubResource]): Custom hook resources or sub-resources
            last_applied_manifest (list): Application resources
            mangled_observer_schema (list): Observed resources
            is_sub_resource (bool, optional): if False, the function only extend the
                list of Kubernetes resources defined in :attr:`last_applied_manifest`
                with new hook resources. Otherwise, the function injects each new hook
                sub-resource into the :attr:`last_applied_manifest` object
                sub-resources. Defaults to False.

        """

        if not items:
            return

        if not is_sub_resource:
            last_applied_manifest.extend(items)
            for sub_resource in items:
                # Generate the default observer schema for each resource
                mangled_observer_schema.append(
                    generate_default_observer_schema_dict(
                        sub_resource,
                        first_level=True,
                    )
                )
            return

        def inject(sub_resource, sub_resource_to_mangle, observed_resource_to_mangle):
            """Inject a hooks defined sub-resource into a Kubernetes sub-resource.

            Args:
                sub_resource (SubResource): Hook sub-resource that needs to be injected
                    into :attr:`last_applied_manifest`
                sub_resource_to_mangle (object): Kubernetes sub-resources from
                    :attr:`last_applied_manifest` which need to be processed
                observed_resource_to_mangle (dict): partial mangled_observer_schema
                    corresponding to the Kubernetes sub-resource.

            Raises:
                InvalidManifestError: if the sub-resource which will be mangled is not a
                    list or a dict.

            """

            # Create sub-resource group if not present in the Kubernetes sub-resource
            if sub_resource.group not in sub_resource_to_mangle:
                # FIXME: This assumes the subresource group contains a list
                sub_resource_to_mangle.update({sub_resource.group: []})

            # Create sub-resource group if not present in the observed fields
            if sub_resource.group not in observed_resource_to_mangle:
                observed_resource_to_mangle.update(
                    {
                        sub_resource.group: [
                            {
                                "observer_schema_list_min_length": 0,
                                "observer_schema_list_max_length": 0,
                            }
                        ]
                    }
                )

            # Inject sub-resource
            # If sub-resource name is already there update it, if not, append it
            if sub_resource.name in [
                g["name"] for g in sub_resource_to_mangle[sub_resource.group]
            ]:
                # FIXME: Assuming we are dealing with a list
                for idx, item in enumerate(sub_resource_to_mangle[sub_resource.group]):
                    if item["name"]:
                        if hasattr(item, "body"):
                            sub_resource_to_mangle[item.group][idx] = item["body"]
            else:
                sub_resource_to_mangle[sub_resource.group].append(sub_resource.body)

            # Make sure the value is observed
            if sub_resource.name not in [
                g["name"] for g in observed_resource_to_mangle[sub_resource.group][:-1]
            ]:
                observed_resource_to_mangle[sub_resource.group].insert(
                    -1, generate_default_observer_schema_dict(sub_resource.body)
                )
                observed_resource_to_mangle[sub_resource.group][-1][
                    "observer_schema_list_min_length"
                ] += 1
                observed_resource_to_mangle[sub_resource.group][-1][
                    "observer_schema_list_max_length"
                ] += 1

        for resource in last_applied_manifest:
            # Complete hook is applied only on defined Kubernetes resources
            if resource["kind"] not in self.hook_resources:
                continue

            for sub_resource in items:
                sub_resources_to_mangle = None
                idx_observed = get_kubernetes_resource_idx(
                    mangled_observer_schema, resource
                )
                for keys in sub_resource.path:
                    try:
                        sub_resources_to_mangle = reduce(getitem, keys, resource)
                    except KeyError:
                        continue

                    break

                # Create the path to the observed sub-resource, if it doesn't yet exist
                try:
                    observed_sub_resources = reduce(
                        getitem, keys, mangled_observer_schema[idx_observed]
                    )
                except KeyError:
                    Complete.create_path(
                        mangled_observer_schema[idx_observed], list(keys)
                    )
                    observed_sub_resources = reduce(
                        getitem, keys, mangled_observer_schema[idx_observed]
                    )

                if isinstance(sub_resources_to_mangle, list):
                    for idx, sub_resource_to_mangle in enumerate(
                        sub_resources_to_mangle
                    ):

                        # Ensure that each element of the list is observed.
                        idx_observed = idx
                        if idx >= len(observed_sub_resources[:-1]):
                            idx_observed = len(observed_sub_resources[:-1])
                            # FIXME: Assuming each element of the list contains a
                            # dictionary, therefore initializing new elements with an
                            # empty dict
                            observed_sub_resources.insert(-1, {})
                        observed_sub_resource = observed_sub_resources[idx_observed]

                        # FIXME: This is assuming a list always contains dict
                        inject(
                            sub_resource, sub_resource_to_mangle, observed_sub_resource
                        )

                elif isinstance(sub_resources_to_mangle, dict):
                    inject(
                        sub_resource, sub_resources_to_mangle, observed_sub_resources
                    )

                else:
                    message = (
                        f"The sub-resource to mangle {sub_resources_to_mangle!r} has an"
                        "invalid type, should be in '[dict, list]'"
                    )
                    raise InvalidManifestError(message)

    @staticmethod
    def attribute_map(obj):
        """Convert a Kubernetes object to dict based on its attribute mapping

        Example:
            .. code:: python

            from kubernetes_asyncio.client import V1VolumeMount

            d = attribute_map(
                    V1VolumeMount(name="name", mount_path="path")
            )
            assert d == {'mountPath': 'path', 'name': 'name'}

        Args:
            obj (object): Kubernetes object

        Returns:
            dict: Converted Kubernetes object

        """
        return {
            obj.attribute_map[attr]: getattr(obj, attr)
            for attr, _ in obj.to_dict().items()
            if getattr(obj, attr) is not None
        }

    @staticmethod
    def create_path(mangled_observer_schema, keys):
        """Create the path to the observed field in the observer schema.

        When a sub-resource is mangled, it should be observed. This function creates
        the path to the subresource to observe.

        Args:
            mangled_observer_schema (dict): Partial observer schema of a resource
            keys (list): list of keys forming the path to the sub-resource to
                observe

        FIXME: This assumes we are only adding keys to dict. We don't consider lists

        """

        # Unpack the first key first, as it contains the base directory
        key = keys.pop(0)

        # If the key is the last of the list, we reached the end of the path.
        if len(keys) == 0:
            mangled_observer_schema[key] = None
            return

        if key not in mangled_observer_schema:
            mangled_observer_schema[key] = {}
        Hook.create_path(mangled_observer_schema[key], keys)

    def secret_certs(
        self,
        secret_name,
        namespace,
        ca_certs=None,
        intermediate_src=None,
        generated_cert=None,
    ):
        """Create a complete hooks secret resource.

        Complete hook secret stores Krake CAs and client certificates to communicate
        with the Krake API.

        Args:
            secret_name (str): Secret name
            namespace (str): Kubernetes namespace where the Secret will be created.
            ca_certs (list): Krake CA list
            intermediate_src (str): content of the certificate that is used to sign new
                certificates for the complete hook.
            generated_cert (CertificatePair): tuple that contains the content of the
                new signed certificate for the Application, and the content of its
                corresponding key.

        Returns:
            dict: complete hook secret resource

        """
        ca_certs_pem = ""
        for ca_cert in ca_certs:
            x509 = crypto.load_certificate(crypto.FILETYPE_ASN1, ca_cert)
            ca_certs_pem += crypto.dump_certificate(crypto.FILETYPE_PEM, x509).decode()

        # Add the intermediate certificate into the chain
        with open(intermediate_src, "r") as f:
            intermediate_src_content = f.read()
        ca_certs_pem += intermediate_src_content

        data = {
            self.ca_name: self._encode_to_64(ca_certs_pem),
            self.cert_name: self._encode_to_64(generated_cert.cert),
            self.key_name: self._encode_to_64(generated_cert.key),
        }
        return self.secret(secret_name, data, namespace)

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create a hooks secret resource.

        The hook secret stores Krake authentication token
        and hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Complete hook authentication token

        Returns:
            dict: complete hook secret resource

        """
        pass

    def volumes(self, secret_name, volume_name, mount_path):
        """Create complete hooks volume and volume mount sub-resources

        Complete hook volume gives access to hook's secret, which stores
        Krake CAs and client certificates to communicate with the Krake API.
        Complete hook volume mount puts the volume into the application

        Args:
            secret_name (str): Secret name
            volume_name (str): Volume name
            mount_path (list): Volume mount path

        Returns:
            list: List of complete hook volume and volume mount sub-resources

        """
        volume = V1Volume(name=volume_name, secret={"secretName": secret_name})
        volume_mount = V1VolumeMount(name=volume_name, mount_path=mount_path)
        return [
            SubResource(
                group="volumes",
                name=volume.name,
                body=self.attribute_map(volume),
                path=(("spec", "template", "spec"), ("spec",)),
            ),
            SubResource(
                group="volumeMounts",
                name=volume_mount.name,
                body=self.attribute_map(volume_mount),
                path=(
                    ("spec", "template", "spec", "containers"),
                    ("spec", "containers"),  # kind: Pod
                ),
            ),
        ]

    @staticmethod
    def _encode_to_64(string):
        """Compute the base 64 encoding of a string.

        Args:
            string (str): the string to encode.

        Returns:
            str: the result of the encoding.

        """
        return b64encode(string.encode()).decode()

    def secret(self, secret_name, secret_data, namespace, _type="Opaque"):
        """Create a secret resource.

        Args:
            secret_name (str): Secret name
            secret_data (dict): Secret data
            namespace (str): Kubernetes namespace where the Secret will be created.
            _type (str, optional): Secret type. Defaults to Opaque.

        Returns:
            dict: secret resource

        """
        return self.attribute_map(
            V1Secret(
                api_version="v1",
                kind="Secret",
                data=secret_data,
                metadata={"name": secret_name, "namespace": namespace},
                type=_type,
            )
        )

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' hook URL.
        Function needs to be specified for each hook.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application shutdown url

        """
        pass

    def env_vars(self, secret_name):
        """Create the hooks' environment variables sub-resources.
        Function needs to be specified for each hook.

        Creates hook environment variables to store Krake authentication token
        and a hook URL for the given applications.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of shutdown hook environment variables sub-resources

        """
        pass


class Complete(Hook):
    """Mangle given application and inject complete hooks variables into it.

    Hook injects a Kubernetes secret, which stores Krake authentication token
    and the Krake complete hook URL for the given application. The variables
    from Kubernetes secret are imported as environment variables
    into the application resource definition. Only resources defined in
    :args:`hook_resources` can be modified.

    Names of environment variables are defined in the application controller
    configuration file.

    If TLS is enabled on the Krake API, the complete hook injects a Kubernetes secret,
    and it's corresponding volume and volume mount definitions for the Krake CA,
    the client certificate with the right CN, and its key. The directory where the
    secret is mounted is defined in the configuration.

    Args:
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        cert_dest (str, optional): Path of the directory where the CA, client
            certificate and key to the Krake API will be stored.
        env_token (str, optional): Name of the environment variable, which stores Krake
            authentication token.
        env_url (str, optional): Name of the environment variable,
            which stores Krake complete hook URL.

    """

    hook_resources = ("Pod", "Deployment", "ReplicationController")

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        super().__init__(
            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
        )
        self.env_url = env_url

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create complete hooks secret resource.

        Complete hook secret stores Krake authentication token
        and complete hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Complete hook authentication token

        Returns:
            dict: complete hook secret resource

        """
        complete_url = self.create_hook_url(name, namespace, api_endpoint)
        data = {
            self.env_token.lower(): self._encode_to_64(token),
            self.env_url.lower(): self._encode_to_64(complete_url),
        }
        return self.secret(secret_name, data, resource_namespace)

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' complete URL.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application complete url

        """
        api_url = URL(api_endpoint)
        return str(
            api_url.with_path(
                f"/kubernetes/namespaces/{namespace}/applications/{name}/complete"
            )
        )

    def env_vars(self, secret_name):
        """Create complete hooks environment variables sub-resources

        Create complete hook environment variables store Krake authentication token
        and complete hook URL for given application.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of complete hook environment variables sub-resources

        """
        sub_resources = []

        env_token = V1EnvVar(
            name=self.env_token,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(
                            name=secret_name, key=self.env_token.lower()
                        )
                    )
                )
            ),
        )
        env_url = V1EnvVar(
            name=self.env_url,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())
                    )
                )
            ),
        )

        for env in (env_token, env_url):
            sub_resources.append(
                SubResource(
                    group="env",
                    name=env.name,
                    body=self.attribute_map(env),
                    path=(
                        ("spec", "template", "spec", "containers"),
                        ("spec", "containers"),  # kind: Pod
                    ),
                )
            )
        return sub_resources


class Shutdown(Hook):
    """Mangle given application and inject shutdown hooks variables into it.

    Hook injects a Kubernetes secret, which stores Krake authentication token
    and the Krake complete hook URL for the given application. The variables
    from the Kubernetes secret are imported as environment variables
    into the application resource definition. Only resources defined in
    :args:`hook_resources` can be modified.

    Names of environment variables are defined in the application controller
    configuration file.

    If TLS is enabled on the Krake API, the shutdown hook injects a Kubernetes secret,
    and it's corresponding volume and volume mount definitions for the Krake CA,
    the client certificate with the right CN, and its key. The directory where the
    secret is mounted is defined in the configuration.

    Args:
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        cert_dest (str, optional): Path of the directory where the CA, client
            certificate and key to the Krake API will be stored.
        env_token (str, optional): Name of the environment variable, which stores Krake
            authentication token.
        env_url (str, optional): Name of the environment variable,
            which stores Krake complete hook URL.

    """

    hook_resources = ("Pod", "Deployment", "ReplicationController")

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        super().__init__(
            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
        )
        self.env_url = env_url

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create shutdown hooks secret resource.

        Shutdown hook secret stores Krake authentication token
        and shutdown hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Shutdown hook authentication token

        Returns:
            dict: shutdown hook secret resource

        """
        shutdown_url = self.create_hook_url(name, namespace, api_endpoint)
        data = {
            self.env_token.lower(): self._encode_to_64(token),
            self.env_url.lower(): self._encode_to_64(shutdown_url),
        }
        return self.secret(secret_name, data, resource_namespace)

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' shutdown URL.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application shutdown url

        """
        api_url = URL(api_endpoint)
        return str(
            api_url.with_path(
                f"/kubernetes/namespaces/{namespace}/applications/{name}/shutdown"
            )
        )

    def env_vars(self, secret_name):
        """Create shutdown hooks environment variables sub-resources.

        Creates shutdown hook environment variables to store Krake authentication token
        and a shutdown hook URL for given applications.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of shutdown hook environment variables sub-resources

        """
        sub_resources = []

        env_resources = []

        env_token = V1EnvVar(
            name=self.env_token,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(
                            name=secret_name, key=self.env_token.lower()
                        )
                    )
                )
            ),
        )
        env_resources.append(env_token)

        env_url = V1EnvVar(
            name=self.env_url,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())
                    )
                )
            ),
        )
        env_resources.append(env_url)

        for env in env_resources:
            sub_resources.append(
                SubResource(
                    group="env",
                    name=env.name,
                    body=self.attribute_map(env),
                    path=(
                        ("spec", "template", "spec", "containers"),
                        ("spec", "containers"),  # kind: Pod
                    ),
                )
            )
        return sub_resources

if __name__ == "__main__":
    isT = True
    args_1_1 = [{'name': 'nginx', 'image': 'nginx:1.7.9', 'ports': [{'containerPort': 80}]}]
    args_1_2 = [{'name': None, 'image': None, 'ports': [{'containerPort': None}, {'observer_schema_list_min_length': 1, 'observer_schema_list_max_length': 1}]}, {'observer_schema_list_min_length': 1, 'observer_schema_list_max_length': 1}]
    args_1_3 = [{'args': None, 'command': None, 'env': None, 'env_from': None, 'image': 'nginx:1.7.9', 'image_pull_policy': 'IfNotPresent', 'lifecycle': None, 'liveness_probe': None, 'name': 'nginx', 'ports': [{'container_port': 80, 'host_ip': None, 'host_port': None, 'name': None, 'protocol': 'TCP'}], 'readiness_probe': None, 'resources': {'limits': None, 'requests': None}, 'security_context': None, 'startup_probe': None, 'stdin': None, 'stdin_once': None, 'termination_message_path': '/dev/termination-log', 'termination_message_policy': 'File', 'tty': None, 'volume_devices': None, 'volume_mounts': None, 'working_dir': None}]
    args_2_1 = [{'containerPort': 80}]
    args_2_2 = [{'containerPort': None}, {'observer_schema_list_min_length': 1, 'observer_schema_list_max_length': 1}]
    args_2_3 = [{'container_port': 80, 'host_ip': None, 'host_port': None, 'name': None, 'protocol': 'TCP'}]

    args_3_1 = [{'port': 80, 'protocol': 'TCP', 'targetPort': 80}]
    args_3_2 = [{'port': None, 'targetPort': None}, {'observer_schema_list_min_length': 0, 'observer_schema_list_max_length': 2}]
    args_3_3 = [{'app_protocol': None, 'name': None, 'node_port': 32566, 'port': 80, 'protocol': 'TCP', 'target_port': 80}]
    update_last_applied_manifest_list_from_resp(args_1_1, args_1_2, args_1_3)
    update_last_applied_manifest_list_from_resp(args_2_1, args_2_2, args_2_3)
    update_last_applied_manifest_list_from_resp(args_3_1, args_3_2, args_3_3)
    ist1 = args_1_1 == [{'name': 'nginx', 'image': 'nginx:1.7.9', 'ports': [{'containerPort': 80}]}]
    ist2 = args_2_1 == [{'containerPort': 80}]
    ist3 = args_3_1 == [{'port': 80, 'protocol': 'TCP', 'targetPort': 80}]
    if not ist1 or not ist2 or not ist3:
        isT = False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/rak-n-rok---Krake/krake/krake/controller/kubernetes/hooks_update_last_applied_manifest_dict_from_resp_passk_validte.py
"""This module defines the Hook Dispatcher and listeners for registering and
executing hooks. Hook Dispatcher emits hooks based on :class:`Hook` attributes which
define when the hook will be executed.

"""
import asyncio
import logging
import random
from base64 import b64encode
from collections import defaultdict
from contextlib import suppress
from copy import deepcopy
from datetime import datetime
from functools import reduce
from operator import getitem
from enum import Enum, auto
from inspect import iscoroutinefunction
from OpenSSL import crypto
from typing import NamedTuple

import yarl
from aiohttp import ClientConnectorError
import sys
sys.path.append("/home/travis/builds/repos/rak-n-rok---Krake/krake")
from krake.controller import Observer
from krake.controller.kubernetes.client import KubernetesClient, InvalidManifestError
from krake.controller.kubernetes.tosca import ToscaParser, ToscaParserException
from krake.utils import camel_to_snake_case, get_kubernetes_resource_idx
from kubernetes_asyncio.client.rest import ApiException
from kubernetes_asyncio.client.api_client import ApiClient
from kubernetes_asyncio import client
from krake.data.kubernetes import (
    ClusterState,
    Application,
    ApplicationState,
    ContainerHealth,
    Cluster,
    ClusterNodeCondition,
    ClusterNode,
    ClusterNodeStatus,
    ClusterNodeMetadata,
)
from yarl import URL
from secrets import token_urlsafe

from kubernetes_asyncio.client import (
    Configuration,
    V1Secret,
    V1EnvVar,
    V1VolumeMount,
    V1Volume,
    V1SecretKeySelector,
    V1EnvVarSource,
)
from kubernetes_asyncio.config.kube_config import KubeConfigLoader

logger = logging.getLogger(__name__)


class HookType(Enum):
    ResourcePreCreate = auto()
    ResourcePostCreate = auto()
    ResourcePreUpdate = auto()
    ResourcePostUpdate = auto()
    ResourcePreDelete = auto()
    ResourcePostDelete = auto()
    ApplicationToscaTranslation = auto()
    ApplicationMangling = auto()
    ApplicationPreMigrate = auto()
    ApplicationPostMigrate = auto()
    ApplicationPreReconcile = auto()
    ApplicationPostReconcile = auto()
    ApplicationPreDelete = auto()
    ApplicationPostDelete = auto()
    ClusterCreation = auto()
    ClusterDeletion = auto()


class HookDispatcher(object):
    """Simple wrapper around a registry of handlers associated to :class:`Hook`
     attributes. Each :class:`Hook` attribute defines when the handler will be
     executed.

    Listeners for certain hooks can be registered via :meth:`on`. Registered
    listeners are executed via :meth:`hook`.

    Example:
        .. code:: python

        listen = HookDispatcher()

        @listen.on(HookType.PreApply)
        def to_perform_before_app_creation(app, cluster, resource, controller):
            # Do Stuff

        @listen.on(HookType.PostApply)
        def another_to_perform_after_app_creation(app, cluster, resource, resp):
            # Do Stuff

        @listen.on(HookType.PostDelete)
        def to_perform_after_app_deletion(app, cluster, resource, resp):
            # Do Stuff

    """

    def __init__(self):
        self.registry = defaultdict(list)

    def on(self, hook):
        """Decorator function to add a new handler to the registry.

        Args:
            hook (HookType): Hook attribute for which to register the handler.

        Returns:
            callable: Decorator for registering listeners for the specified
            hook.

        """

        def decorator(handler):
            self.registry[hook].append(handler)

            return handler

        return decorator

    async def hook(self, hook, **kwargs):
        """Execute the list of handlers associated to the provided :class:`Hook`
        attribute.

        Args:
            hook (HookType): The hook attribute for which to execute handlers.

        """
        try:
            handlers = self.registry[hook]
        except KeyError:
            pass
        else:
            for handler in handlers:
                if iscoroutinefunction(handler):
                    await handler(**kwargs)
                else:
                    handler(**kwargs)


listen = HookDispatcher()


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
async def register_service(app, cluster, resource, response):
    """Register endpoint of Kubernetes Service object on creation and update.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        cluster (krake.data.kubernetes.Cluster): The cluster on which the
            application is running
        resource (dict): Kubernetes object description as specified in the
            specification of the application.
        response (kubernetes_asyncio.client.V1Service): Response of the
            Kubernetes API

    """
    if resource["kind"] != "Service":
        return

    service_name = resource["metadata"]["name"]

    if response.spec and response.spec.type == "LoadBalancer":
        # For a "LoadBalancer" type of Service, an external IP is given in the cluster
        # by a load balancer controller to the service. In this case, the "port"
        # specified in the spec is reachable from the outside.
        if (
            not response.status.load_balancer
            or not response.status.load_balancer.ingress
        ):
            # When a "LoadBalancer" type of service is created, the IP is given by an
            # additional controller (e.g. a controller that requests a floating IP to an
            # OpenStack infrastructure). This process can take some time, but the
            # Service itself already exist before the IP is assigned. In the case of an
            # error with the controller, the IP is also not given. This "<pending>" IP
            # just expresses that the Service exists, but the IP is not ready yet.
            external_ip = "<pending>"
        else:
            external_ip = response.status.load_balancer.ingress[0].ip

        if not response.spec.ports:
            external_port = "<pending>"
        else:
            external_port = response.spec.ports[0].port
        app.status.services[service_name] = f"{external_ip}:{external_port}"
        return

    node_port = None
    # Ensure that ports are specified
    if response.spec and response.spec.ports:
        node_port = response.spec.ports[0].node_port

    # If the service does not have a node port, remove a potential reference
    # and return.
    if node_port is None:
        try:
            del app.status.services[service_name]
        except KeyError:
            pass
        return

    # Determine URL of Kubernetes cluster API
    loader = KubeConfigLoader(cluster.spec.kubeconfig)
    config = Configuration()
    await loader.load_and_set(config)
    cluster_url = yarl.URL(config.host)

    app.status.services[service_name] = f"{cluster_url.host}:{node_port}"


@listen.on(HookType.ResourcePostDelete)
async def unregister_service(app, resource, **kwargs):
    """Unregister endpoint of Kubernetes Service object on deletion.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        resource (dict): Kubernetes object description as specified in the
            specification of the application.

    """
    if resource["kind"] != "Service":
        return

    service_name = resource["metadata"]["name"]
    try:
        del app.status.services[service_name]
    except KeyError:
        pass


@listen.on(HookType.ResourcePostDelete)
async def remove_resource_from_last_observed_manifest(app, resource, **kwargs):
    """Remove a given resource from the last_observed_manifest after its deletion

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        resource (dict): Kubernetes object description as specified in the
            specification of the application.

    """
    try:
        idx = get_kubernetes_resource_idx(app.status.last_observed_manifest, resource)
    except IndexError:
        return

    app.status.last_observed_manifest.pop(idx)


def update_last_applied_manifest_dict_from_resp(
    last_applied_manifest, observer_schema, response
):
    """Together with :func:``update_last_applied_manifest_list_from_resp``, this
    function is called recursively to update a partial ``last_applied_manifest``
    from a partial Kubernetes response

    Args:
        last_applied_manifest (dict): partial ``last_applied_manifest`` being
            updated
        observer_schema (dict): partial ``observer_schema``
        response (dict): partial response from the Kubernetes API.

    Raises:
        KeyError: If the observed field is not present in the Kubernetes response

    This function go through all observed fields, and initialized their value in
    last_applied_manifest if they are not yet present

    """
    for key, value in observer_schema.items():

        # Keys in the response are in camelCase
        camel_key = camel_to_snake_case(key)

        if camel_key not in response:
            # An observed key should always be present in the k8s response
            raise KeyError(
                f"Observed key {camel_key} is not present in response {response}"
            )

        if isinstance(value, dict):
            if key not in last_applied_manifest:
                # The dictionary is observed, but not present in
                # last_applied_manifest
                last_applied_manifest[key] = {}

            update_last_applied_manifest_dict_from_resp(
                last_applied_manifest[key], observer_schema[key], response[camel_key]
            )

        elif isinstance(value, list):
            if key not in last_applied_manifest:
                # The list is observed, but not present in last_applied_manifest
                last_applied_manifest[key] = []

            update_last_applied_manifest_list_from_resp(
                last_applied_manifest[key], observer_schema[key], response[camel_key]
            )

        elif key not in last_applied_manifest:
            # If key not present in last_applied_manifest, and value is neither a
            # dict nor a list, simply add it.
            last_applied_manifest[key] = response[camel_key]


def update_last_applied_manifest_list_from_resp(
    last_applied_manifest, observer_schema, response
):
    """Together with :func:``update_last_applied_manifest_dict_from_resp``, this
    function is called recursively to update a partial ``last_applied_manifest``
    from a partial Kubernetes response

    Args:
        last_applied_manifest (list): partial ``last_applied_manifest`` being
            updated
        observer_schema (list): partial ``observer_schema``
        response (list): partial response from the Kubernetes API.

    This function go through all observed fields, and initialized their value in
    last_applied_manifest if they are not yet present

    """
    # Looping over the observed resource, except the last element which is the
    # special control dictionary
    for idx, val in enumerate(observer_schema[:-1]):

        if idx >= len(response):
            # Element is observed but not present in k8s response, so following
            # elements will also not exist.
            #
            # This doesn't raise an Exception as observing the element of a list
            # doesn't ensure its presence. The list length is controlled by the
            # special control dictionary
            return

        if isinstance(val, dict):
            if idx >= len(last_applied_manifest):
                # The dict is observed, but not present in last_applied_manifest
                last_applied_manifest.append({})

            update_last_applied_manifest_dict_from_resp(
                last_applied_manifest[idx], observer_schema[idx], response[idx]
            )

        elif isinstance(response[idx], list):
            if idx >= len(last_applied_manifest):
                # The list is observed, but not present in last_applied_manifest
                last_applied_manifest.append([])

            update_last_applied_manifest_list_from_resp(
                last_applied_manifest[idx], observer_schema[idx], response[idx]
            )

        elif idx >= len(last_applied_manifest):
            # Element is not yet present in last_applied_manifest. Adding it.
            last_applied_manifest.append(response[idx])


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
def update_last_applied_manifest_from_resp(app, response, **kwargs):
    """Hook run after the creation or update of an application in order to update the
    `status.last_applied_manifest` using the k8s response.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        response (kubernetes_asyncio.client.V1Status): Response of the Kubernetes API

    After a Kubernetes resource has been created/updated, the
    `status.last_applied_manifest` has to be updated. All fields already initialized
    (either from the mangling of `spec.manifest`, or by a previous call to this
    function) should be left untouched. Only observed fields which are not present in
    `status.last_applied_manifest` should be initialized.

    """

    if isinstance(response, dict):
        # The Kubernetes API couldn't deserialize the k8s response into an object
        resp = response
    else:
        # The Kubernetes API deserialized the k8s response into an object
        resp = response.to_dict()

    idx_applied = get_kubernetes_resource_idx(app.status.last_applied_manifest, resp)

    idx_observed = get_kubernetes_resource_idx(app.status.mangled_observer_schema, resp)

    update_last_applied_manifest_dict_from_resp(
        app.status.last_applied_manifest[idx_applied],
        app.status.mangled_observer_schema[idx_observed],
        resp,
    )


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
def update_last_observed_manifest_from_resp(app, response, **kwargs):
    """Handler to run after the creation or update of a Kubernetes resource to update
    the last_observed_manifest from the response of the Kubernetes API.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        response (kubernetes_asyncio.client.V1Service): Response of the
            Kubernetes API

    The target last_observed_manifest holds the value of all observed fields plus the
    special control dictionaries for the list length

    """
    if isinstance(response, dict):
        # The Kubernetes API couldn't deserialize the k8s response into an object
        resp = response
    else:
        # The Kubernetes API deserialized the k8s response into an object
        resp = response.to_dict()

    try:
        idx_observed = get_kubernetes_resource_idx(
            app.status.mangled_observer_schema,
            resp,
        )
    except IndexError:
        # All created resources should be observed
        raise

    try:
        idx_last_observed = get_kubernetes_resource_idx(
            app.status.last_observed_manifest,
            resp,
        )
    except IndexError:
        # If the resource is not yes present in last_observed_manifest, append it.
        idx_last_observed = len(app.status.last_observed_manifest)
        app.status.last_observed_manifest.append({})

    # Overwrite the last_observed_manifest for this resource
    app.status.last_observed_manifest[
        idx_last_observed
    ] = update_last_observed_manifest_dict(
        app.status.mangled_observer_schema[idx_observed], resp
    )


def update_last_observed_manifest_dict(observed_resource, response):
    """Together with :func:``update_last_observed_manifest_list``, recursively
    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.

    Args:
        observed_resource (dict): The schema to observe for the partial given resource
        response (dict): The partial Kubernetes response for this resource.

    Raises:
        KeyError: If an observed key is not present in the Kubernetes response

    Returns:
        dict: The dictionary of observed keys and their value

    Get the value of all observed fields from the Kubernetes response
    """
    res = {}
    for key, value in observed_resource.items():

        camel_key = camel_to_snake_case(key)
        if camel_key not in response:
            raise KeyError(
                f"Observed key {camel_key} is not present in response {response}"
            )

        if isinstance(value, dict):
            res[key] = update_last_observed_manifest_dict(value, response[camel_key])

        elif isinstance(value, list):
            res[key] = update_last_observed_manifest_list(value, response[camel_key])

        else:
            res[key] = response[camel_key]

    return res


def update_last_observed_manifest_list(observed_resource, response):
    """Together with :func:``update_last_observed_manifest_dict``, recursively
    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.

    Args:
        observed_resource (list): the schema to observe for the partial given resource
        response (list): the partial Kubernetes response for this resource.

    Returns:
        list: The list of observed elements, plus the special list length control
            dictionary

    Get the value of all observed elements from the Kubernetes response
    """

    if not response:
        return [{"observer_schema_list_current_length": 0}]

    res = []
    # Looping over the observed resource, except the last element which is the special
    # control dictionary
    for idx, val in enumerate(observed_resource[:-1]):

        if idx >= len(response):
            # Element is not present in the Kubernetes response, nothing more to do
            break

        if type(response[idx]) is dict:
            res.append(update_last_observed_manifest_dict(val, response[idx]))

        elif type(response[idx]) is list:
            res.append(update_last_observed_manifest_list(val, response[idx]))

        else:
            res.append(response[idx])

    # Append the special control dictionary to the list
    res.append({"observer_schema_list_current_length": len(response)})

    return res


def update_last_applied_manifest_dict_from_spec(
    resource_status_new, resource_status_old, resource_observed
):
    """Together with :func:``update_last_applied_manifest_list_from_spec``, this
    function is called recursively to update a partial ``last_applied_manifest``

    Args:
        resource_status_new (dict): partial ``last_applied_manifest`` being updated
        resource_status_old (dict): partial of the current ``last_applied_manifest``
        resource_observed (dict): partial observer_schema for the manifest file
            being updated

    """
    for key, value in resource_observed.items():

        if key not in resource_status_old:
            continue

        if key in resource_status_new:

            if isinstance(value, dict):
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            elif isinstance(value, list):
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

        else:
            # If the key is not present the spec.manifest, we first need to
            # initialize it

            if isinstance(value, dict):
                resource_status_new[key] = {}
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            elif isinstance(value, list):
                resource_status_new[key] = []
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            else:
                resource_status_new[key] = resource_status_old[key]


def update_last_applied_manifest_list_from_spec(
    resource_status_new, resource_status_old, resource_observed
):
    """Together with :func:``update_last_applied_manifest_dict_from_spec``, this
    function is called recursively to update a partial ``last_applied_manifest``

    Args:
        resource_status_new (list): partial ``last_applied_manifest`` being updated
        resource_status_old (list): partial of the current ``last_applied_manifest``
        resource_observed (list): partial observer_schema for the manifest file
            being updated

    """

    # Looping over the observed resource, except the last element which is the
    # special control dictionary
    for idx, val in enumerate(resource_observed[:-1]):

        if idx >= len(resource_status_old):
            # The element in not in the current last_applied_manifest, and neither
            # is the rest of the list
            break

        if idx < len(resource_status_new):
            # The element is present in spec.manifest and in the current
            # last_applied_manifest. Updating observed fields

            if isinstance(val, dict):
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            elif isinstance(val, list):
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

        else:
            # If the element is not present in the spec.manifest, we first have to
            # initialize it.

            if isinstance(val, dict):
                resource_status_new.append({})
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            elif isinstance(val, list):
                resource_status_new.append([])
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            else:
                resource_status_new.append(resource_status_old[idx])


def update_last_applied_manifest_from_spec(app):
    """Update the status.last_applied_manifest of an application from spec.manifests

    Args:
        app (krake.data.kubernetes.Application): Application to update

    This function is called on application creation and updates. The
    last_applied_manifest of an application is initialized as a copy of spec.manifest,
    and is augmented by all known observed fields not yet initialized (i.e. all observed
    fields or resources which are present in the current last_applied_manifest but not
    in the spec.manifest)

    """

    # The new last_applied_manifest is initialized as a copy of the spec.manifest, and
    # augmented by all observed fields which are present in the current
    # last_applied_manifest but not in the original spec.manifest
    new_last_applied_manifest = deepcopy(app.spec.manifest)

    # Loop over observed resources and observed fields, and check if they should be
    # added to the new last_applied_manifest (i.e. present in the current
    # last_applied_manifest but not in spec.manifest)
    for resource_observed in app.status.mangled_observer_schema:

        # If the resource is not present in the current last_applied_manifest, there is
        # nothing to do. Whether the resource was initialized by spec.manifest doesn't
        # matter.
        try:
            idx_status_old = get_kubernetes_resource_idx(
                app.status.last_applied_manifest, resource_observed
            )
        except IndexError:
            continue

        # As the resource is present in the current last_applied_manifest, we need to go
        # through it to check if observed fields should be set to their current value
        # (i.e. fields are present in the current last_applied_manifest, but not in
        # spec.manifest)
        try:
            # Check if the observed resource is present in spec.manifest
            idx_status_new = get_kubernetes_resource_idx(
                new_last_applied_manifest, resource_observed
            )
        except IndexError:
            # The resource is observed but is not present in the spec.manifest.
            # Create an empty resource, which will be augmented in
            # update_last_applied_manifest_dict_from_spec with the observed and known
            # fields.
            new_last_applied_manifest.append({})
            idx_status_new = len(new_last_applied_manifest) - 1

        update_last_applied_manifest_dict_from_spec(
            new_last_applied_manifest[idx_status_new],
            app.status.last_applied_manifest[idx_status_old],
            resource_observed,
        )

    app.status.last_applied_manifest = new_last_applied_manifest


class KubernetesApplicationObserver(Observer):
    """Observer specific for Kubernetes Applications. One observer is created for each
    Application managed by the Controller, but not one per Kubernetes resource
    (Deployment, Service...). If several resources are defined by an Application, they
    are all monitored by the same observer.

    The observer gets the actual status of the resources on the cluster using the
    Kubernetes API, and compare it to the status stored in the API.

    The observer is:
     * started at initial Krake resource creation;

     * deleted when a resource needs to be updated, then started again when it is done;

     * simply deleted on resource deletion.

    Args:
        cluster (krake.data.kubernetes.Cluster): the cluster on which the observed
            Application is created.
        resource (krake.data.kubernetes.Application): the application that will be
            observed.
        on_res_update (coroutine): a coroutine called when a resource's actual status
            differs from the status sent by the database. Its signature is:
            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of
            the resource that is up-to-date with the API. The Observer internal instance
            of the resource to observe will be updated. If the API cannot be contacted,
            ``None`` can be returned. In this case the internal instance of the Observer
            will not be updated.
        time_step (int, optional): how frequently the Observer should watch the actual
            status of the resources.

    """

    def __init__(self, cluster, resource, on_res_update, time_step=2):
        super().__init__(resource, on_res_update, time_step)
        self.cluster = cluster
        self.kubernetes_api = None

    def _set_container_health(self, resource, status):
        if status.container_health is None:
            status.container_health = ContainerHealth()

        if hasattr(resource, 'status') and resource.status is not None:
            if resource.kind == "Pod":
                container_state = resource.status.container_statuses[0].state
                status.container_health.desired_pods = 1
                if container_state.terminated is not None:
                    if resource.spec.restart_policy == "Never":
                        status.state = ApplicationState.DEGRADED
                    else:
                        status.state = ApplicationState.RESTARTING
                    status.container_health.running_pods = 0
                elif container_state.waiting is not None:
                    if container_state.waiting.reason == "CrashLoopBackOff":
                        status.state = ApplicationState.DEGRADED
                        status.container_health.running_pods = 0
                else:
                    status.state = ApplicationState.RUNNING
                    status.container_health.running_pods = 1

            elif (
                resource.kind == "Deployment" or
                resource.kind == "StatefulSet" or
                resource.kind == "ReplicaSet" or
                resource.kind == "DaemonSet"
            ):

                if resource.kind == "DaemonSet":
                    status.container_health.desired_pods = \
                        resource.status.current_number_scheduled
                    if isinstance(resource.status.desired_number_scheduled, int):
                        status.container_health.running_pods = \
                            resource.status.desired_number_scheduled
                    else:
                        status.container_health.running_pods = 0
                else:
                    status.container_health.desired_pods = resource.status.replicas
                    if isinstance(resource.status.ready_replicas, int):
                        status.container_health.running_pods = \
                            resource.status.ready_replicas
                    else:
                        status.container_health.running_pods = 0

                if status.container_health.running_pods != \
                   status.container_health.desired_pods:
                    status.state = ApplicationState.DEGRADED
                else:
                    status.state = ApplicationState.RUNNING

            elif resource.kind == "Job":
                if isinstance(resource.spec.completions, int):
                    status.container_health.desired_pods = resource.spec.completions
                else:
                    status.container_health.desired_pods = 1

                if isinstance(resource.status.active, int):
                    status.container_health.running_pods = resource.status.active
                else:
                    status.container_health.running_pods = 0

                if isinstance(resource.status.succeeded, int):
                    status.container_health.completed_pods = resource.status.succeeded
                else:
                    status.container_health.completed_pods = 0

                if isinstance(resource.status.failed, int):
                    status.container_health.failed_pods = resource.status.failed
                else:
                    status.container_health.failed_pods = 0

    async def poll_resource(self):
        """Fetch the current status of the Application monitored by the Observer.

        Returns:
            krake.data.core.Status: the status object created using information from the
                real world Applications resource.

        """
        app = self.resource

        status = deepcopy(app.status)
        status.last_observed_manifest = []

        # For each observed kubernetes resource of the Application,
        # get its current status on the cluster.
        for desired_resource in app.status.last_applied_manifest:
            kube = KubernetesClient(self.cluster.spec.kubeconfig)
            idx_observed = get_kubernetes_resource_idx(
                app.status.mangled_observer_schema, desired_resource
            )
            observed_resource = app.status.mangled_observer_schema[idx_observed]
            async with kube:
                try:
                    group, version, kind, name, namespace = kube.get_immutables(
                        desired_resource
                    )
                    resource_api = await kube.get_resource_api(group, version, kind)
                    resp = await resource_api.read(kind, name, namespace)
                except (ClientConnectorError, ApiException) as err:
                    if hasattr(err, "status") and err.status == 404:
                        # Resource does not exist
                        continue
                    # Otherwise, log the unexpected error and return the
                    # last known application status
                    logger.debug(err)
                    return app.status

                resource = resp
                self._set_container_health(resource, status)

            observed_manifest = update_last_observed_manifest_dict(
                observed_resource, resp.to_dict()
            )
            status.last_observed_manifest.append(observed_manifest)

        return status


class KubernetesClusterObserver(Observer):
    """Observer specific for Kubernetes Clusters. One observer is created for each
    Cluster managed by the Controller.

    The observer gets the actual status of the cluster using the
    Kubernetes API, and compare it to the status stored in the API.

    The observer is:
     * started at initial Krake resource creation;

     * deleted when a resource needs to be updated, then started again when it is done;

     * simply deleted on resource deletion.

    Args:
        resource (krake.data.kubernetes.Cluster): the cluster which will be observed.
        on_res_update (coroutine): a coroutine called when a resource's actual status
            differs from the status sent by the database. Its signature is:
            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of
            the resource that is up-to-date with the API. The Observer internal instance
            of the resource to observe will be updated. If the API cannot be contacted,
            ``None`` can be returned. In this case the internal instance of the Observer
            will not be updated.
        time_step (int, optional): how frequently the Observer should watch the actual
            status of the resources.

    """

    def __init__(self, resource, on_res_update, time_step=2):
        super().__init__(resource, on_res_update, time_step)

    async def poll_resource(self):
        """Fetch the current status of the Cluster monitored by the Observer.

        Note regarding exceptions handling:
          The current cluster status is fetched by :func:`poll_resource` from its API.
          If the cluster API is shutting down the API server responds with a 503
          (service unavailable, apiserver is shutting down) HTTP response which
          leads to the kubernetes client ApiException. If the cluster's API has been
          successfully shut down and there is an attempt to fetch cluster status,
          the ClientConnectorError is raised instead.
          Therefore, both exceptions should be handled.

        Returns:
            krake.data.core.Status: the status object created using information from the
                real world Cluster.

        """
        cluster = self.resource
        status = deepcopy(cluster.status)
        status.nodes = []
        # For each observed kubernetes cluster registered in Krake,
        # get its current node status.
        loader = KubeConfigLoader(cluster.spec.kubeconfig)
        config = Configuration()
        await loader.load_and_set(config)
        kube = ApiClient(config)

        async with kube as api:
            v1 = client.CoreV1Api(api)
            try:
                response = await v1.list_node()
            except (ClientConnectorError, ApiException) as err:
                # Log the error and set cluster state to OFFLINE
                logger.debug(err)
                status.state = ClusterState.OFFLINE
                return status

            # Fetch nodes conditions
            nodes = []
            for node in response.items:
                conditions = []
                for condition in node.status.conditions:
                    conditions.append(
                        ClusterNodeCondition(
                            message=condition.message,
                            reason=condition.reason,
                            status=condition.status,
                            type=condition.type,
                        )
                    )

                nodes.append(
                    ClusterNode(
                        metadata=ClusterNodeMetadata(name=node.metadata.name),
                        status=ClusterNodeStatus(conditions=conditions),
                    )
                )
            status.nodes = nodes

            # The scheduler is unable to fetch cluster metrics, hence
            # the cluster state should wait for it and the cluster
            # status should not be changed by the observer.
            if status.state == ClusterState.FAILING_METRICS:
                return status

            # Set the cluster state to CONNECTING if the previous state
            # was OFFLINE. It is due to smooth transition from
            # the OFFLINE to ONLINE state.
            if status.state == ClusterState.OFFLINE:
                status.state = ClusterState.CONNECTING
                return status

            for node in status.nodes:
                for condition in node.status.conditions:
                    if (
                        condition.type.lower().endswith("pressure")
                        and condition.status == "True"
                    ):
                        status.state = ClusterState.UNHEALTHY
                        return status

                    if condition.type.lower() == "ready" and condition.status != "True":
                        status.state = ClusterState.NOTREADY
                        return status

            status.state = ClusterState.ONLINE
            return status


@listen.on(HookType.ApplicationPostReconcile)
@listen.on(HookType.ApplicationPostMigrate)
@listen.on(HookType.ClusterCreation)
async def register_observer(controller, resource, start=True, **kwargs):
    """Create an observer for the given Application or Cluster, and start it as a
    background task if wanted.

    If an observer already existed for this Application or Cluster, it is stopped
    and deleted.

    Args:
        controller (KubernetesController): the controller for which the observer will be
            added in the list of working observers.
        resource (krake.data.kubernetes.Application): the Application to observe or
        resource (krake.data.kubernetes.Cluster): the Cluster to observe.
        start (bool, optional): if False, does not start the observer as background
            task.

    """
    if resource.kind == Application.kind:
        cluster = await controller.kubernetes_api.read_cluster(
            namespace=resource.status.running_on.namespace,
            name=resource.status.running_on.name,
        )
        observer = KubernetesApplicationObserver(
            cluster,
            resource,
            controller.on_status_update,
            time_step=controller.observer_time_step,
        )

    elif resource.kind == Cluster.kind:
        observer = KubernetesClusterObserver(
            resource,
            controller.on_status_update,
            time_step=controller.observer_time_step,
        )
    else:
        logger.debug("Unknown resource kind. No observer was registered.", resource)
        return

    logger.debug(f"Start observer for {resource.kind} %r", resource.metadata.name)
    task = None
    if start:
        task = controller.loop.create_task(observer.run())

    controller.observers[resource.metadata.uid] = (observer, task)


@listen.on(HookType.ApplicationPreReconcile)
@listen.on(HookType.ApplicationPreMigrate)
@listen.on(HookType.ApplicationPreDelete)
@listen.on(HookType.ClusterDeletion)
async def unregister_observer(controller, resource, **kwargs):
    """Stop and delete the observer for the given Application or Cluster. If no observer
    is started, do nothing.

    Args:
        controller (KubernetesController): the controller for which the observer will be
            removed from the list of working observers.
        resource (krake.data.kubernetes.Application): the Application whose observer
        will be stopped or
        resource (krake.data.kubernetes.Cluster): the Cluster whose observer will be
        stopped.

    """
    if resource.metadata.uid not in controller.observers:
        return

    logger.debug(f"Stop observer for {resource.kind} {resource.metadata.name}")
    _, task = controller.observers.pop(resource.metadata.uid)
    task.cancel()

    with suppress(asyncio.CancelledError):
        await task


@listen.on(HookType.ApplicationToscaTranslation)
async def translate_tosca(controller, app, **kwargs):
    """Translate a TOSCA template or CSAR archive into a Kubernetes manifest.

    Args:
        controller (KubernetesController): the controller that handles the application
            resource.
        app (krake.data.kubernetes.Application): the Application that could be defined
            by a TOSCA template or a CSAR archive.

    Raises:
        ToscaParserException: If the given application does not contain
         at least one from the following:
         - Kubernetes manifest
         - TOSCA template
         - CSAR archive

    """
    if app.spec.manifest:
        return

    if not app.spec.tosca and not app.spec.csar:
        raise ToscaParserException(
            "Application should be defined by a Kubernetes manifest,"
            " a TOSCA template or a CSAR archive: %r",
            app,
        )
    app.status.state = ApplicationState.TRANSLATING
    await controller.kubernetes_api.update_application_status(
        namespace=app.metadata.namespace, name=app.metadata.name, body=app
    )

    if app.spec.tosca and isinstance(app.spec.tosca, dict):

        manifest = ToscaParser.from_dict(app.spec.tosca).translate_to_manifests()
    else:
        manifest = ToscaParser.from_url(
            app.spec.tosca or app.spec.csar
        ).translate_to_manifests()

    app.spec.manifest = manifest
    await controller.kubernetes_api.update_application(
        namespace=app.metadata.namespace, name=app.metadata.name, body=app
    )


def utc_difference():
    """Get the difference in seconds between the current time and the current UTC time.

    Returns:
        int: the time difference in seconds.

    """
    delta = datetime.now() - datetime.utcnow()
    return delta.seconds


def generate_certificate(config):
    """Create and sign a new certificate using the one defined in the complete hook
    configuration as intermediate certificate.

    Args:
        config (krake.data.config.CompleteHookConfiguration): the configuration of the
            complete hook.

    Returns:
        CertificatePair: the content of the certificate created and its corresponding
            key.

    """
    with open(config.intermediate_src, "rb") as f:
        intermediate_src = crypto.load_certificate(crypto.FILETYPE_PEM, f.read())
    with open(config.intermediate_key_src, "rb") as f:
        intermediate_key_src = crypto.load_privatekey(crypto.FILETYPE_PEM, f.read())

    client_cert = crypto.X509()

    # Set general information
    client_cert.set_version(3)
    client_cert.set_serial_number(random.randint(50000000000000, 100000000000000))
    # If not set before, TLS will not accept to use this certificate in UTC cases, as
    # the server time may be earlier.
    time_offset = utc_difference() * -1
    client_cert.gmtime_adj_notBefore(time_offset)
    client_cert.gmtime_adj_notAfter(1 * 365 * 24 * 60 * 60)

    # Set issuer and subject
    intermediate_subject = intermediate_src.get_subject()
    client_cert.set_issuer(intermediate_subject)
    client_subj = crypto.X509Name(intermediate_subject)
    client_subj.CN = config.hook_user
    client_cert.set_subject(client_subj)

    # Create and set the private key
    client_key = crypto.PKey()
    client_key.generate_key(crypto.TYPE_RSA, 2048)
    client_cert.set_pubkey(client_key)

    client_cert.sign(intermediate_key_src, "sha256")

    cert_dump = crypto.dump_certificate(crypto.FILETYPE_PEM, client_cert).decode()
    key_dump = crypto.dump_privatekey(crypto.FILETYPE_PEM, client_key).decode()
    return CertificatePair(cert=cert_dump, key=key_dump)


def generate_default_observer_schema(app):
    """Generate the default observer schema for each Kubernetes resource present in
    ``spec.manifest`` for which a custom observer schema hasn't been specified.

    Args:
        app (krake.data.kubernetes.Application): The application for which to generate a
            default observer schema
    """

    app.status.mangled_observer_schema = deepcopy(app.spec.observer_schema)

    for resource_manifest in app.spec.manifest:
        try:
            get_kubernetes_resource_idx(
                app.status.mangled_observer_schema, resource_manifest
            )

        except IndexError:
            # Only create a default observer schema, if a custom observer schema hasn't
            # been set by the user.
            app.status.mangled_observer_schema.append(
                generate_default_observer_schema_dict(
                    resource_manifest,
                    first_level=True,
                )
            )


def generate_default_observer_schema_dict(manifest_dict, first_level=False):
    """Together with :func:``generate_default_observer_schema_list``, this function is
    called recursively to generate part of a default ``observer_schema`` from part of a
    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.

    Args:
        manifest_dict (dict): Partial Kubernetes resources
        first_level (bool, optional): If True, indicates that the dictionary represents
            the whole observer schema of a Kubernetes resource

    Returns:
        dict: Generated partial observer_schema

    This function creates a new dictionary from ``manifest_dict`` and replaces all
    non-list and non-dict values by ``None``.

    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a
    resource), the values of the identifying fields are copied from the manifest file.

    """
    observer_schema_dict = {}

    for key, value in manifest_dict.items():

        if isinstance(value, dict):
            observer_schema_dict[key] = generate_default_observer_schema_dict(value)

        elif isinstance(value, list):
            observer_schema_dict[key] = generate_default_observer_schema_list(value)

        else:
            observer_schema_dict[key] = None

    if first_level:
        observer_schema_dict["apiVersion"] = manifest_dict["apiVersion"]
        observer_schema_dict["kind"] = manifest_dict["kind"]
        observer_schema_dict["metadata"]["name"] = manifest_dict["metadata"]["name"]

        if (
            "spec" in manifest_dict
            and "type" in manifest_dict["spec"]
            and manifest_dict["spec"]["type"] == "LoadBalancer"
        ):
            observer_schema_dict["status"] = {"load_balancer": {"ingress": None}}

    return observer_schema_dict


def generate_default_observer_schema_list(manifest_list):
    """Together with :func:``generate_default_observer_schema_dict``, this function is
    called recursively to generate part of a default ``observer_schema`` from part of a
    Kubernetes resource, defined respectively by ``manifest_list`` or ``manifest_dict``.

    Args:
        manifest_list (list): Partial Kubernetes resources

    Returns:
        list: Generated partial observer_schema

    This function creates a new list from ``manifest_list`` and replaces all non-list
    and non-dict elements by ``None``.

    Additionally, it generates the default list control dictionary, using the current
    length of the list as default minimum and maximum values.

    """
    observer_schema_list = []

    for value in manifest_list:

        if isinstance(value, dict):
            observer_schema_list.append(generate_default_observer_schema_dict(value))

        elif isinstance(value, list):
            observer_schema_list.append(generate_default_observer_schema_list(value))

        else:
            observer_schema_list.append(None)

    observer_schema_list.append(
        {
            "observer_schema_list_min_length": len(manifest_list),
            "observer_schema_list_max_length": len(manifest_list),
        }
    )

    return observer_schema_list


@listen.on(HookType.ApplicationMangling)
async def complete(app, api_endpoint, ssl_context, config):
    """Execute application complete hook defined by :class:`Complete`.
    Hook mangles given application and injects complete hooks variables.

    Application complete hook is disabled by default.
    User enables this hook by the --hook-complete argument in rok cli.

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        config (krake.data.config.HooksConfiguration): Complete hook
            configuration.

    """
    if "complete" not in app.spec.hooks:
        return

    # Use the endpoint of the API only if the external endpoint has not been set.
    if config.complete.external_endpoint:
        api_endpoint = config.complete.external_endpoint

    app.status.complete_token = (
        app.status.complete_token if app.status.complete_token else token_urlsafe()
    )

    # Generate only once the certificate and key for a specific Application
    generated_cert = CertificatePair(
        cert=app.status.complete_cert, key=app.status.complete_key
    )
    if ssl_context and generated_cert == (None, None):
        generated_cert = generate_certificate(config.complete)
        app.status.complete_cert = generated_cert.cert
        app.status.complete_key = generated_cert.key

    hook = Complete(
        api_endpoint,
        ssl_context,
        hook_user=config.complete.hook_user,
        cert_dest=config.complete.cert_dest,
        env_token=config.complete.env_token,
        env_url=config.complete.env_url,
    )
    hook.mangle_app(
        app.metadata.name,
        app.metadata.namespace,
        app.status.complete_token,
        app.status.last_applied_manifest,
        config.complete.intermediate_src,
        generated_cert,
        app.status.mangled_observer_schema,
        "complete",
    )


@listen.on(HookType.ApplicationMangling)
async def shutdown(app, api_endpoint, ssl_context, config):
    """Executes an application shutdown hook defined by :class:`Shutdown`.
    The hook mangles the given application and injects shutdown hooks variables.

    Application shutdown hook is disabled by default.
    User enables this hook by the --hook-shutdown argument in rok cli.

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        config (krake.data.config.HooksConfiguration): Shutdown hook
            configuration.

    """
    if "shutdown" not in app.spec.hooks:
        return

    # Use the endpoint of the API only if the external endpoint has not been set.
    if config.shutdown.external_endpoint:
        api_endpoint = config.shutdown.external_endpoint

    app.status.shutdown_token = (
        app.status.shutdown_token if app.status.shutdown_token else token_urlsafe()
    )

    # Generate only once the certificate and key for a specific Application
    generated_cert = CertificatePair(
        cert=app.status.shutdown_cert, key=app.status.shutdown_key
    )
    if ssl_context and generated_cert == (None, None):
        generated_cert = generate_certificate(config.shutdown)
        app.status.shutdown_cert = generated_cert.cert
        app.status.shutdown_key = generated_cert.key

    hook = Shutdown(
        api_endpoint,
        ssl_context,
        hook_user=config.shutdown.hook_user,
        cert_dest=config.shutdown.cert_dest,
        env_token=config.shutdown.env_token,
        env_url=config.shutdown.env_url,
    )
    hook.mangle_app(
        app.metadata.name,
        app.metadata.namespace,
        app.status.shutdown_token,
        app.status.last_applied_manifest,
        config.shutdown.intermediate_src,
        generated_cert,
        app.status.mangled_observer_schema,
        "shutdown",
    )


@listen.on(HookType.ResourcePreDelete)
async def pre_shutdown(controller, app, **kwargs):
    """

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
    """
    if "shutdown" not in app.spec.hooks:
        return

    return


class SubResource(NamedTuple):
    group: str
    name: str
    body: dict
    path: tuple


class CertificatePair(NamedTuple):
    """Tuple which contains a certificate and its corresponding key.

    Attributes:
        cert (str): content of a certificate.
        key (str): content of the key that corresponds to the certificate.

    """

    cert: str
    key: str


class Hook(object):
    hook_resources = ()

    ca_name = "ca-bundle.pem"
    cert_name = "cert.pem"
    key_name = "key.pem"

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        self.api_endpoint = api_endpoint
        self.ssl_context = ssl_context
        self.hook_user = hook_user
        self.cert_dest = cert_dest
        self.env_token = env_token
        self.env_url = env_url

    def mangle_app(
        self,
        name,
        namespace,
        token,
        last_applied_manifest,
        intermediate_src,
        generated_cert,
        mangled_observer_schema,
        hook_type="",
    ):
        """Mangle a given application and inject complete hook resources and
        sub-resources into the :attr:`last_applied_manifest` object by :meth:`mangle`.
        Also mangle the observer_schema as new resources and sub-resources should
        be observed.

        :attr:`last_applied_manifest` is created as a deep copy of the desired
        application resources, as defined by user. It can be updated by custom hook
        resources or modified by custom hook sub-resources. It is used as a desired
        state for the Krake deployment process.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            token (str): Complete hook authentication token
            last_applied_manifest (list): Application resources
            intermediate_src (str): content of the certificate that is used to sign new
                certificates for the complete hook.
            generated_cert (CertificatePair): tuple that contains the content of the
                new signed certificate for the Application, and the content of its
                corresponding key.
            mangled_observer_schema (list): Observed fields
            hook_type (str, optional): Name of the hook the app should be mangled for

        """

        secret_certs_name = "-".join([name, "krake", hook_type, "secret", "certs"])
        secret_token_name = "-".join([name, "krake", hook_type, "secret", "token"])
        volume_name = "-".join([name, "krake", hook_type, "volume"])
        ca_certs = (
            self.ssl_context.get_ca_certs(binary_form=True)
            if self.ssl_context
            else None
        )

        # Extract all different namespaces
        # FIXME: too many assumptions here: do we create one ConfigMap for each
        #  namespace?
        resource_namespaces = {
            resource["metadata"].get("namespace", "default")
            for resource in last_applied_manifest
        }

        hook_resources = []
        hook_sub_resources = []
        if ca_certs:
            hook_resources.extend(
                [
                    self.secret_certs(
                        secret_certs_name,
                        resource_namespace,
                        intermediate_src=intermediate_src,
                        generated_cert=generated_cert,
                        ca_certs=ca_certs,
                    )
                    for resource_namespace in resource_namespaces
                ]
            )
            hook_sub_resources.extend(
                [*self.volumes(secret_certs_name, volume_name, self.cert_dest)]
            )

        hook_resources.extend(
            [
                self.secret_token(
                    secret_token_name,
                    name,
                    namespace,
                    resource_namespace,
                    self.api_endpoint,
                    token,
                )
                for resource_namespace in resource_namespaces
            ]
        )
        hook_sub_resources.extend(
            [
                *self.env_vars(secret_token_name),
            ]
        )

        self.mangle(
            hook_resources,
            last_applied_manifest,
            mangled_observer_schema,
        )
        self.mangle(
            hook_sub_resources,
            last_applied_manifest,
            mangled_observer_schema,
            is_sub_resource=True,
        )

    def mangle(
        self,
        items,
        last_applied_manifest,
        mangled_observer_schema,
        is_sub_resource=False,
    ):
        """Mangle applications desired state with custom hook resources or
        sub-resources.

        Example:
            .. code:: python

            last_applied_manifest = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Pod',
                    'metadata': {'name': 'test', 'namespace': 'default'},
                    'spec': {'containers': [{'name': 'test'}]}
                }
            ]
            mangled_observer_schema = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Pod',
                    'metadata': {'name': 'test', 'namespace': 'default'},
                    'spec': {
                        'containers': [
                            {'name': None},
                            {
                                'observer_schema_list_max_length': 1,
                                'observer_schema_list_min_length': 1,
                            },
                        ]
                    },
                }
            ]
            hook_resources = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Secret',
                    'metadata': {'name': 'sct', 'namespace': 'default'}
                }
            ]
            hook_sub_resources = [
                SubResource(
                    group='env', name='env', body={'name': 'test', 'value': 'test'},
                    path=(('spec', 'containers'),)
                )
            ]

            mangle(
                hook_resources,
                last_applied_manifest,
                mangled_observer_schema,
            )
            mangle(
                hook_sub_resources,
                last_applied_manifest,
                mangled_observer_schema,
                is_sub_resource=True
            )

            assert last_applied_manifest == [
                {
                    "apiVersion": "v1",
                    "kind": "Pod",
                    "metadata": {"name": "test", 'namespace': 'default'},
                    "spec": {
                        "containers": [
                            {
                                "name": "test",
                                "env": [{"name": "test", "value": "test"}]
                            }
                        ]
                    },
                },
                {"apiVersion": "v1", "kind": "Secret", "metadata": {"name": "sct"}},
            ]

            assert mangled_observer_schema == [
                {
                    "apiVersion": "v1",
                    "kind": "Pod",
                    "metadata": {"name": "test", "namespace": None},
                    "spec": {
                        "containers": [
                            {
                                "name": None,
                                "env": [
                                    {"name": None, "value": None},
                                    {
                                        "observer_schema_list_max_length": 1,
                                        "observer_schema_list_min_length": 1,
                                    },
                                ],
                            },
                            {
                                "observer_schema_list_max_length": 1,
                                "observer_schema_list_min_length": 1,
                            },
                        ]
                    },
                },
                {
                    "apiVersion": "v1",
                    "kind": "Secret",
                    "metadata": {"name": "sct", "namespace": None},
                },
            ]

        Args:
            items (list[SubResource]): Custom hook resources or sub-resources
            last_applied_manifest (list): Application resources
            mangled_observer_schema (list): Observed resources
            is_sub_resource (bool, optional): if False, the function only extend the
                list of Kubernetes resources defined in :attr:`last_applied_manifest`
                with new hook resources. Otherwise, the function injects each new hook
                sub-resource into the :attr:`last_applied_manifest` object
                sub-resources. Defaults to False.

        """

        if not items:
            return

        if not is_sub_resource:
            last_applied_manifest.extend(items)
            for sub_resource in items:
                # Generate the default observer schema for each resource
                mangled_observer_schema.append(
                    generate_default_observer_schema_dict(
                        sub_resource,
                        first_level=True,
                    )
                )
            return

        def inject(sub_resource, sub_resource_to_mangle, observed_resource_to_mangle):
            """Inject a hooks defined sub-resource into a Kubernetes sub-resource.

            Args:
                sub_resource (SubResource): Hook sub-resource that needs to be injected
                    into :attr:`last_applied_manifest`
                sub_resource_to_mangle (object): Kubernetes sub-resources from
                    :attr:`last_applied_manifest` which need to be processed
                observed_resource_to_mangle (dict): partial mangled_observer_schema
                    corresponding to the Kubernetes sub-resource.

            Raises:
                InvalidManifestError: if the sub-resource which will be mangled is not a
                    list or a dict.

            """

            # Create sub-resource group if not present in the Kubernetes sub-resource
            if sub_resource.group not in sub_resource_to_mangle:
                # FIXME: This assumes the subresource group contains a list
                sub_resource_to_mangle.update({sub_resource.group: []})

            # Create sub-resource group if not present in the observed fields
            if sub_resource.group not in observed_resource_to_mangle:
                observed_resource_to_mangle.update(
                    {
                        sub_resource.group: [
                            {
                                "observer_schema_list_min_length": 0,
                                "observer_schema_list_max_length": 0,
                            }
                        ]
                    }
                )

            # Inject sub-resource
            # If sub-resource name is already there update it, if not, append it
            if sub_resource.name in [
                g["name"] for g in sub_resource_to_mangle[sub_resource.group]
            ]:
                # FIXME: Assuming we are dealing with a list
                for idx, item in enumerate(sub_resource_to_mangle[sub_resource.group]):
                    if item["name"]:
                        if hasattr(item, "body"):
                            sub_resource_to_mangle[item.group][idx] = item["body"]
            else:
                sub_resource_to_mangle[sub_resource.group].append(sub_resource.body)

            # Make sure the value is observed
            if sub_resource.name not in [
                g["name"] for g in observed_resource_to_mangle[sub_resource.group][:-1]
            ]:
                observed_resource_to_mangle[sub_resource.group].insert(
                    -1, generate_default_observer_schema_dict(sub_resource.body)
                )
                observed_resource_to_mangle[sub_resource.group][-1][
                    "observer_schema_list_min_length"
                ] += 1
                observed_resource_to_mangle[sub_resource.group][-1][
                    "observer_schema_list_max_length"
                ] += 1

        for resource in last_applied_manifest:
            # Complete hook is applied only on defined Kubernetes resources
            if resource["kind"] not in self.hook_resources:
                continue

            for sub_resource in items:
                sub_resources_to_mangle = None
                idx_observed = get_kubernetes_resource_idx(
                    mangled_observer_schema, resource
                )
                for keys in sub_resource.path:
                    try:
                        sub_resources_to_mangle = reduce(getitem, keys, resource)
                    except KeyError:
                        continue

                    break

                # Create the path to the observed sub-resource, if it doesn't yet exist
                try:
                    observed_sub_resources = reduce(
                        getitem, keys, mangled_observer_schema[idx_observed]
                    )
                except KeyError:
                    Complete.create_path(
                        mangled_observer_schema[idx_observed], list(keys)
                    )
                    observed_sub_resources = reduce(
                        getitem, keys, mangled_observer_schema[idx_observed]
                    )

                if isinstance(sub_resources_to_mangle, list):
                    for idx, sub_resource_to_mangle in enumerate(
                        sub_resources_to_mangle
                    ):

                        # Ensure that each element of the list is observed.
                        idx_observed = idx
                        if idx >= len(observed_sub_resources[:-1]):
                            idx_observed = len(observed_sub_resources[:-1])
                            # FIXME: Assuming each element of the list contains a
                            # dictionary, therefore initializing new elements with an
                            # empty dict
                            observed_sub_resources.insert(-1, {})
                        observed_sub_resource = observed_sub_resources[idx_observed]

                        # FIXME: This is assuming a list always contains dict
                        inject(
                            sub_resource, sub_resource_to_mangle, observed_sub_resource
                        )

                elif isinstance(sub_resources_to_mangle, dict):
                    inject(
                        sub_resource, sub_resources_to_mangle, observed_sub_resources
                    )

                else:
                    message = (
                        f"The sub-resource to mangle {sub_resources_to_mangle!r} has an"
                        "invalid type, should be in '[dict, list]'"
                    )
                    raise InvalidManifestError(message)

    @staticmethod
    def attribute_map(obj):
        """Convert a Kubernetes object to dict based on its attribute mapping

        Example:
            .. code:: python

            from kubernetes_asyncio.client import V1VolumeMount

            d = attribute_map(
                    V1VolumeMount(name="name", mount_path="path")
            )
            assert d == {'mountPath': 'path', 'name': 'name'}

        Args:
            obj (object): Kubernetes object

        Returns:
            dict: Converted Kubernetes object

        """
        return {
            obj.attribute_map[attr]: getattr(obj, attr)
            for attr, _ in obj.to_dict().items()
            if getattr(obj, attr) is not None
        }

    @staticmethod
    def create_path(mangled_observer_schema, keys):
        """Create the path to the observed field in the observer schema.

        When a sub-resource is mangled, it should be observed. This function creates
        the path to the subresource to observe.

        Args:
            mangled_observer_schema (dict): Partial observer schema of a resource
            keys (list): list of keys forming the path to the sub-resource to
                observe

        FIXME: This assumes we are only adding keys to dict. We don't consider lists

        """

        # Unpack the first key first, as it contains the base directory
        key = keys.pop(0)

        # If the key is the last of the list, we reached the end of the path.
        if len(keys) == 0:
            mangled_observer_schema[key] = None
            return

        if key not in mangled_observer_schema:
            mangled_observer_schema[key] = {}
        Hook.create_path(mangled_observer_schema[key], keys)

    def secret_certs(
        self,
        secret_name,
        namespace,
        ca_certs=None,
        intermediate_src=None,
        generated_cert=None,
    ):
        """Create a complete hooks secret resource.

        Complete hook secret stores Krake CAs and client certificates to communicate
        with the Krake API.

        Args:
            secret_name (str): Secret name
            namespace (str): Kubernetes namespace where the Secret will be created.
            ca_certs (list): Krake CA list
            intermediate_src (str): content of the certificate that is used to sign new
                certificates for the complete hook.
            generated_cert (CertificatePair): tuple that contains the content of the
                new signed certificate for the Application, and the content of its
                corresponding key.

        Returns:
            dict: complete hook secret resource

        """
        ca_certs_pem = ""
        for ca_cert in ca_certs:
            x509 = crypto.load_certificate(crypto.FILETYPE_ASN1, ca_cert)
            ca_certs_pem += crypto.dump_certificate(crypto.FILETYPE_PEM, x509).decode()

        # Add the intermediate certificate into the chain
        with open(intermediate_src, "r") as f:
            intermediate_src_content = f.read()
        ca_certs_pem += intermediate_src_content

        data = {
            self.ca_name: self._encode_to_64(ca_certs_pem),
            self.cert_name: self._encode_to_64(generated_cert.cert),
            self.key_name: self._encode_to_64(generated_cert.key),
        }
        return self.secret(secret_name, data, namespace)

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create a hooks secret resource.

        The hook secret stores Krake authentication token
        and hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Complete hook authentication token

        Returns:
            dict: complete hook secret resource

        """
        pass

    def volumes(self, secret_name, volume_name, mount_path):
        """Create complete hooks volume and volume mount sub-resources

        Complete hook volume gives access to hook's secret, which stores
        Krake CAs and client certificates to communicate with the Krake API.
        Complete hook volume mount puts the volume into the application

        Args:
            secret_name (str): Secret name
            volume_name (str): Volume name
            mount_path (list): Volume mount path

        Returns:
            list: List of complete hook volume and volume mount sub-resources

        """
        volume = V1Volume(name=volume_name, secret={"secretName": secret_name})
        volume_mount = V1VolumeMount(name=volume_name, mount_path=mount_path)
        return [
            SubResource(
                group="volumes",
                name=volume.name,
                body=self.attribute_map(volume),
                path=(("spec", "template", "spec"), ("spec",)),
            ),
            SubResource(
                group="volumeMounts",
                name=volume_mount.name,
                body=self.attribute_map(volume_mount),
                path=(
                    ("spec", "template", "spec", "containers"),
                    ("spec", "containers"),  # kind: Pod
                ),
            ),
        ]

    @staticmethod
    def _encode_to_64(string):
        """Compute the base 64 encoding of a string.

        Args:
            string (str): the string to encode.

        Returns:
            str: the result of the encoding.

        """
        return b64encode(string.encode()).decode()

    def secret(self, secret_name, secret_data, namespace, _type="Opaque"):
        """Create a secret resource.

        Args:
            secret_name (str): Secret name
            secret_data (dict): Secret data
            namespace (str): Kubernetes namespace where the Secret will be created.
            _type (str, optional): Secret type. Defaults to Opaque.

        Returns:
            dict: secret resource

        """
        return self.attribute_map(
            V1Secret(
                api_version="v1",
                kind="Secret",
                data=secret_data,
                metadata={"name": secret_name, "namespace": namespace},
                type=_type,
            )
        )

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' hook URL.
        Function needs to be specified for each hook.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application shutdown url

        """
        pass

    def env_vars(self, secret_name):
        """Create the hooks' environment variables sub-resources.
        Function needs to be specified for each hook.

        Creates hook environment variables to store Krake authentication token
        and a hook URL for the given applications.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of shutdown hook environment variables sub-resources

        """
        pass


class Complete(Hook):
    """Mangle given application and inject complete hooks variables into it.

    Hook injects a Kubernetes secret, which stores Krake authentication token
    and the Krake complete hook URL for the given application. The variables
    from Kubernetes secret are imported as environment variables
    into the application resource definition. Only resources defined in
    :args:`hook_resources` can be modified.

    Names of environment variables are defined in the application controller
    configuration file.

    If TLS is enabled on the Krake API, the complete hook injects a Kubernetes secret,
    and it's corresponding volume and volume mount definitions for the Krake CA,
    the client certificate with the right CN, and its key. The directory where the
    secret is mounted is defined in the configuration.

    Args:
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        cert_dest (str, optional): Path of the directory where the CA, client
            certificate and key to the Krake API will be stored.
        env_token (str, optional): Name of the environment variable, which stores Krake
            authentication token.
        env_url (str, optional): Name of the environment variable,
            which stores Krake complete hook URL.

    """

    hook_resources = ("Pod", "Deployment", "ReplicationController")

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        super().__init__(
            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
        )
        self.env_url = env_url

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create complete hooks secret resource.

        Complete hook secret stores Krake authentication token
        and complete hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Complete hook authentication token

        Returns:
            dict: complete hook secret resource

        """
        complete_url = self.create_hook_url(name, namespace, api_endpoint)
        data = {
            self.env_token.lower(): self._encode_to_64(token),
            self.env_url.lower(): self._encode_to_64(complete_url),
        }
        return self.secret(secret_name, data, resource_namespace)

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' complete URL.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application complete url

        """
        api_url = URL(api_endpoint)
        return str(
            api_url.with_path(
                f"/kubernetes/namespaces/{namespace}/applications/{name}/complete"
            )
        )

    def env_vars(self, secret_name):
        """Create complete hooks environment variables sub-resources

        Create complete hook environment variables store Krake authentication token
        and complete hook URL for given application.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of complete hook environment variables sub-resources

        """
        sub_resources = []

        env_token = V1EnvVar(
            name=self.env_token,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(
                            name=secret_name, key=self.env_token.lower()
                        )
                    )
                )
            ),
        )
        env_url = V1EnvVar(
            name=self.env_url,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())
                    )
                )
            ),
        )

        for env in (env_token, env_url):
            sub_resources.append(
                SubResource(
                    group="env",
                    name=env.name,
                    body=self.attribute_map(env),
                    path=(
                        ("spec", "template", "spec", "containers"),
                        ("spec", "containers"),  # kind: Pod
                    ),
                )
            )
        return sub_resources


class Shutdown(Hook):
    """Mangle given application and inject shutdown hooks variables into it.

    Hook injects a Kubernetes secret, which stores Krake authentication token
    and the Krake complete hook URL for the given application. The variables
    from the Kubernetes secret are imported as environment variables
    into the application resource definition. Only resources defined in
    :args:`hook_resources` can be modified.

    Names of environment variables are defined in the application controller
    configuration file.

    If TLS is enabled on the Krake API, the shutdown hook injects a Kubernetes secret,
    and it's corresponding volume and volume mount definitions for the Krake CA,
    the client certificate with the right CN, and its key. The directory where the
    secret is mounted is defined in the configuration.

    Args:
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        cert_dest (str, optional): Path of the directory where the CA, client
            certificate and key to the Krake API will be stored.
        env_token (str, optional): Name of the environment variable, which stores Krake
            authentication token.
        env_url (str, optional): Name of the environment variable,
            which stores Krake complete hook URL.

    """

    hook_resources = ("Pod", "Deployment", "ReplicationController")

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        super().__init__(
            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
        )
        self.env_url = env_url

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create shutdown hooks secret resource.

        Shutdown hook secret stores Krake authentication token
        and shutdown hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Shutdown hook authentication token

        Returns:
            dict: shutdown hook secret resource

        """
        shutdown_url = self.create_hook_url(name, namespace, api_endpoint)
        data = {
            self.env_token.lower(): self._encode_to_64(token),
            self.env_url.lower(): self._encode_to_64(shutdown_url),
        }
        return self.secret(secret_name, data, resource_namespace)

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' shutdown URL.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application shutdown url

        """
        api_url = URL(api_endpoint)
        return str(
            api_url.with_path(
                f"/kubernetes/namespaces/{namespace}/applications/{name}/shutdown"
            )
        )

    def env_vars(self, secret_name):
        """Create shutdown hooks environment variables sub-resources.

        Creates shutdown hook environment variables to store Krake authentication token
        and a shutdown hook URL for given applications.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of shutdown hook environment variables sub-resources

        """
        sub_resources = []

        env_resources = []

        env_token = V1EnvVar(
            name=self.env_token,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(
                            name=secret_name, key=self.env_token.lower()
                        )
                    )
                )
            ),
        )
        env_resources.append(env_token)

        env_url = V1EnvVar(
            name=self.env_url,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())
                    )
                )
            ),
        )
        env_resources.append(env_url)

        for env in env_resources:
            sub_resources.append(
                SubResource(
                    group="env",
                    name=env.name,
                    body=self.attribute_map(env),
                    path=(
                        ("spec", "template", "spec", "containers"),
                        ("spec", "containers"),  # kind: Pod
                    ),
                )
            )
        return sub_resources

if __name__ == "__main__":
    isT=True
    args_0_1={'apiVersion': 'apps/v1', 'kind': 'Deployment', 'metadata': {'name': 'nginx-demo', 'namespace': 'secondary'}, 'spec': {'selector': {'matchLabels': {'app': 'nginx'}}, 'template': {'metadata': {'labels': {'app': 'nginx'}}, 'spec': {'containers': [{'name': 'nginx', 'image': 'nginx:1.7.9', 'ports': [{'containerPort': 80}]}]}}}}
    args_0_2={'apiVersion': 'apps/v1', 'kind': 'Deployment', 'metadata': {'name': 'nginx-demo', 'namespace': None}, 'spec': {'replicas': None, 'selector': {'matchLabels': {'app': None}}, 'template': {'metadata': {'labels': {'app': None}}, 'spec': {'containers': [{'name': None, 'image': None, 'ports': [{'containerPort': None}, {'observer_schema_list_min_length': 1, 'observer_schema_list_max_length': 1}]}, {'observer_schema_list_min_length': 1, 'observer_schema_list_max_length': 1}]}}}}
    args_0_3={'api_version': 'apps/v1', 'kind': 'Deployment', 'metadata': {'annotations': {'deployment.kubernetes.io/revision': '1'}, 'cluster_name': None, 'creation_timestamp': datetime(2019, 11, 11, 12, 1, 5), 'deletion_grace_period_seconds': None, 'deletion_timestamp': None, 'finalizers': None, 'generate_name': None, 'generation': 1, 'labels': None, 'managed_fields': None, 'name': 'nginx-demo', 'namespace': 'secondary', 'owner_references': None, 'resource_version': '8080030', 'self_link': '/apis/apps/v1/namespaces/secondary/deployments/nginx-demo', 'uid': '047686e4-af52-4264-b2a4-2f82b890e809'}, 'spec': {'min_ready_seconds': None, 'paused': None, 'progress_deadline_seconds': 600, 'replicas': 1, 'revision_history_limit': 10, 'selector': {'match_expressions': None, 'match_labels': {'app': 'nginx'}}, 'strategy': {'rolling_update': {'max_surge': '25%', 'max_unavailable': '25%'}, 'type': 'RollingUpdate'}, 'template': {'metadata': {'annotations': None, 'cluster_name': None, 'creation_timestamp': None, 'deletion_grace_period_seconds': None, 'deletion_timestamp': None, 'finalizers': None, 'generate_name': None, 'generation': None, 'labels': {'app': 'nginx'}, 'managed_fields': None, 'name': None, 'namespace': None, 'owner_references': None, 'resource_version': None, 'self_link': None, 'uid': None}, 'spec': {'active_deadline_seconds': None, 'affinity': None, 'automount_service_account_token': None, 'containers': [{'args': None, 'command': None, 'env': None, 'env_from': None, 'image': 'nginx:1.7.9', 'image_pull_policy': 'IfNotPresent', 'lifecycle': None, 'liveness_probe': None, 'name': 'nginx', 'ports': [{'container_port': 80, 'host_ip': None, 'host_port': None, 'name': None, 'protocol': 'TCP'}], 'readiness_probe': None, 'resources': {'limits': None, 'requests': None}, 'security_context': None, 'startup_probe': None, 'stdin': None, 'stdin_once': None, 'termination_message_path': '/dev/termination-log', 'termination_message_policy': 'File', 'tty': None, 'volume_devices': None, 'volume_mounts': None, 'working_dir': None}], 'dns_config': None, 'dns_policy': 'ClusterFirst', 'enable_service_links': None, 'ephemeral_containers': None, 'host_aliases': None, 'host_ipc': None, 'host_network': None, 'host_pid': None, 'hostname': None, 'image_pull_secrets': None, 'init_containers': None, 'node_name': None, 'node_selector': None, 'overhead': None, 'preemption_policy': None, 'priority': None, 'priority_class_name': None, 'readiness_gates': None, 'restart_policy': 'Always', 'runtime_class_name': None, 'scheduler_name': 'default-scheduler', 'security_context': {'fs_group': None, 'fs_group_change_policy': None, 'run_as_group': None, 'run_as_non_root': None, 'run_as_user': None, 'se_linux_options': None, 'seccomp_profile': None, 'supplemental_groups': None, 'sysctls': None, 'windows_options': None}, 'service_account': None, 'service_account_name': None, 'set_hostname_as_fqdn': None, 'share_process_namespace': None, 'subdomain': None, 'termination_grace_period_seconds': 30, 'tolerations': None, 'topology_spread_constraints': None, 'volumes': None}}}, 'status': {'available_replicas': 1, 'collision_count': None, 'conditions': [{'last_transition_time': datetime(2019, 11, 11, 12, 1, 7), 'last_update_time': datetime(2019, 11, 11, 12, 1, 7), 'message': 'Deployment has minimum availability.', 'reason': 'MinimumReplicasAvailable', 'status': 'True', 'type': 'Available'}, {'last_transition_time': datetime(2019, 11, 11, 12, 1, 5), 'last_update_time': datetime(2019, 11, 11, 12, 1, 7), 'message': 'ReplicaSet "nginx-demo-5754944d6c" has successfully progressed.', 'reason': 'NewReplicaSetAvailable', 'status': 'True', 'type': 'Progressing'}], 'observed_generation': 1, 'ready_replicas': 1, 'replicas': 1, 'unavailable_replicas': None, 'updated_replicas': 1}}
    args_1_1={'name': 'nginx-demo', 'namespace': 'secondary'}
    args_1_2={'name': 'nginx-demo', 'namespace': None}
    args_1_3 = {'annotations': {'deployment.kubernetes.io/revision': '1'}, 'cluster_name': None, 'creation_timestamp': datetime(2019, 11, 11, 12, 1, 5), 'deletion_grace_period_seconds': None, 'deletion_timestamp': None, 'finalizers': None, 'generate_name': None, 'generation': 1, 'labels': None, 'managed_fields': None, 'name': 'nginx-demo', 'namespace': 'secondary', 'owner_references': None, 'resource_version': '8080030', 'self_link': '/apis/apps/v1/namespaces/secondary/deployments/nginx-demo', 'uid': '047686e4-af52-4264-b2a4-2f82b890e809'}
    args_2_1={'selector': {'matchLabels': {'app': 'nginx'}}, 'template': {'metadata': {'labels': {'app': 'nginx'}}, 'spec': {'containers': [{'name': 'nginx', 'image': 'nginx:1.7.9', 'ports': [{'containerPort': 80}]}]}}}
    args_2_2={'replicas': None, 'selector': {'matchLabels': {'app': None}}, 'template': {'metadata': {'labels': {'app': None}}, 'spec': {'containers': [{'name': None, 'image': None, 'ports': [{'containerPort': None}, {'observer_schema_list_min_length': 1, 'observer_schema_list_max_length': 1}]}, {'observer_schema_list_min_length': 1, 'observer_schema_list_max_length': 1}]}}}
    args_2_3={'min_ready_seconds': None, 'paused': None, 'progress_deadline_seconds': 600, 'replicas': 1, 'revision_history_limit': 10, 'selector': {'match_expressions': None, 'match_labels': {'app': 'nginx'}}, 'strategy': {'rolling_update': {'max_surge': '25%', 'max_unavailable': '25%'}, 'type': 'RollingUpdate'}, 'template': {'metadata': {'annotations': None, 'cluster_name': None, 'creation_timestamp': None, 'deletion_grace_period_seconds': None, 'deletion_timestamp': None, 'finalizers': None, 'generate_name': None, 'generation': None, 'labels': {'app': 'nginx'}, 'managed_fields': None, 'name': None, 'namespace': None, 'owner_references': None, 'resource_version': None, 'self_link': None, 'uid': None}, 'spec': {'active_deadline_seconds': None, 'affinity': None, 'automount_service_account_token': None, 'containers': [{'args': None, 'command': None, 'env': None, 'env_from': None, 'image': 'nginx:1.7.9', 'image_pull_policy': 'IfNotPresent', 'lifecycle': None, 'liveness_probe': None, 'name': 'nginx', 'ports': [{'container_port': 80, 'host_ip': None, 'host_port': None, 'name': None, 'protocol': 'TCP'}], 'readiness_probe': None, 'resources': {'limits': None, 'requests': None}, 'security_context': None, 'startup_probe': None, 'stdin': None, 'stdin_once': None, 'termination_message_path': '/dev/termination-log', 'termination_message_policy': 'File', 'tty': None, 'volume_devices': None, 'volume_mounts': None, 'working_dir': None}], 'dns_config': None, 'dns_policy': 'ClusterFirst', 'enable_service_links': None, 'ephemeral_containers': None, 'host_aliases': None, 'host_ipc': None, 'host_network': None, 'host_pid': None, 'hostname': None, 'image_pull_secrets': None, 'init_containers': None, 'node_name': None, 'node_selector': None, 'overhead': None, 'preemption_policy': None, 'priority': None, 'priority_class_name': None, 'readiness_gates': None, 'restart_policy': 'Always', 'runtime_class_name': None, 'scheduler_name': 'default-scheduler', 'security_context': {'fs_group': None, 'fs_group_change_policy': None, 'run_as_group': None, 'run_as_non_root': None, 'run_as_user': None, 'se_linux_options': None, 'seccomp_profile': None, 'supplemental_groups': None, 'sysctls': None, 'windows_options': None}, 'service_account': None, 'service_account_name': None, 'set_hostname_as_fqdn': None, 'share_process_namespace': None, 'subdomain': None, 'termination_grace_period_seconds': 30, 'tolerations': None, 'topology_spread_constraints': None, 'volumes': None}}}

    args_3_1={'matchLabels': {'app': 'nginx'}}
    args_3_2={'matchLabels': {'app': None}}
    args_3_3={'match_expressions': None, 'match_labels': {'app': 'nginx'}}
    args_4_1={'app': 'nginx'}
    args_4_2={'app': None}
    args_4_3={'app': 'nginx'}
    update_last_applied_manifest_dict_from_resp(args_0_1,args_0_2,args_0_3)
    update_last_applied_manifest_dict_from_resp(args_1_1, args_1_2, args_1_3)
    update_last_applied_manifest_dict_from_resp(args_2_1, args_2_2, args_2_3)
    update_last_applied_manifest_dict_from_resp(args_3_1, args_3_2, args_3_3)
    ist1=args_0_1=={'apiVersion': 'apps/v1', 'kind': 'Deployment', 'metadata': {'name': 'nginx-demo', 'namespace': 'secondary'}, 'spec': {'selector': {'matchLabels': {'app': 'nginx'}}, 'template': {'metadata': {'labels': {'app': 'nginx'}}, 'spec': {'containers': [{'name': 'nginx', 'image': 'nginx:1.7.9', 'ports': [{'containerPort': 80}]}]}}, 'replicas': 1}}
    ist2=args_1_1=={'name': 'nginx-demo', 'namespace': 'secondary'}
    ist3=args_2_1=={'selector': {'matchLabels': {'app': 'nginx'}}, 'template': {'metadata': {'labels': {'app': 'nginx'}}, 'spec': {'containers': [{'name': 'nginx', 'image': 'nginx:1.7.9', 'ports': [{'containerPort': 80}]}]}}, 'replicas': 1}
    ist4=args_3_1=={'matchLabels': {'app': 'nginx'}}
    if not ist1 or not ist2 or not ist3 or not ist4:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/rak-n-rok---Krake/krake/krake/controller/kubernetes/hooks_generate_default_observer_schema_passk_validte.py
"""This module defines the Hook Dispatcher and listeners for registering and
executing hooks. Hook Dispatcher emits hooks based on :class:`Hook` attributes which
define when the hook will be executed.

"""
import asyncio
import logging
import random
from base64 import b64encode
from collections import defaultdict
from contextlib import suppress
from copy import deepcopy
from datetime import datetime
from functools import reduce
from operator import getitem
from enum import Enum, auto
from inspect import iscoroutinefunction
from OpenSSL import crypto
from typing import NamedTuple

import yarl
from aiohttp import ClientConnectorError
import sys
sys.path.append("/home/travis/builds/repos/rak-n-rok---Krake/krake")
from krake.controller import Observer
from krake.controller.kubernetes.client import KubernetesClient, InvalidManifestError
from krake.controller.kubernetes.tosca import ToscaParser, ToscaParserException
from krake.utils import camel_to_snake_case, get_kubernetes_resource_idx
from kubernetes_asyncio.client.rest import ApiException
from kubernetes_asyncio.client.api_client import ApiClient
from kubernetes_asyncio import client
from krake.data.kubernetes import (
    ClusterState,
    Application,
    ApplicationState,
    ContainerHealth,
    Cluster,
    ClusterNodeCondition,
    ClusterNode,
    ClusterNodeStatus,
    ClusterNodeMetadata,
)
from yarl import URL
from secrets import token_urlsafe

from kubernetes_asyncio.client import (
    Configuration,
    V1Secret,
    V1EnvVar,
    V1VolumeMount,
    V1Volume,
    V1SecretKeySelector,
    V1EnvVarSource,
)
from kubernetes_asyncio.config.kube_config import KubeConfigLoader

logger = logging.getLogger(__name__)


class HookType(Enum):
    ResourcePreCreate = auto()
    ResourcePostCreate = auto()
    ResourcePreUpdate = auto()
    ResourcePostUpdate = auto()
    ResourcePreDelete = auto()
    ResourcePostDelete = auto()
    ApplicationToscaTranslation = auto()
    ApplicationMangling = auto()
    ApplicationPreMigrate = auto()
    ApplicationPostMigrate = auto()
    ApplicationPreReconcile = auto()
    ApplicationPostReconcile = auto()
    ApplicationPreDelete = auto()
    ApplicationPostDelete = auto()
    ClusterCreation = auto()
    ClusterDeletion = auto()


class HookDispatcher(object):
    """Simple wrapper around a registry of handlers associated to :class:`Hook`
     attributes. Each :class:`Hook` attribute defines when the handler will be
     executed.

    Listeners for certain hooks can be registered via :meth:`on`. Registered
    listeners are executed via :meth:`hook`.

    Example:
        .. code:: python

        listen = HookDispatcher()

        @listen.on(HookType.PreApply)
        def to_perform_before_app_creation(app, cluster, resource, controller):
            # Do Stuff

        @listen.on(HookType.PostApply)
        def another_to_perform_after_app_creation(app, cluster, resource, resp):
            # Do Stuff

        @listen.on(HookType.PostDelete)
        def to_perform_after_app_deletion(app, cluster, resource, resp):
            # Do Stuff

    """

    def __init__(self):
        self.registry = defaultdict(list)

    def on(self, hook):
        """Decorator function to add a new handler to the registry.

        Args:
            hook (HookType): Hook attribute for which to register the handler.

        Returns:
            callable: Decorator for registering listeners for the specified
            hook.

        """

        def decorator(handler):
            self.registry[hook].append(handler)

            return handler

        return decorator

    async def hook(self, hook, **kwargs):
        """Execute the list of handlers associated to the provided :class:`Hook`
        attribute.

        Args:
            hook (HookType): The hook attribute for which to execute handlers.

        """
        try:
            handlers = self.registry[hook]
        except KeyError:
            pass
        else:
            for handler in handlers:
                if iscoroutinefunction(handler):
                    await handler(**kwargs)
                else:
                    handler(**kwargs)


listen = HookDispatcher()


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
async def register_service(app, cluster, resource, response):
    """Register endpoint of Kubernetes Service object on creation and update.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        cluster (krake.data.kubernetes.Cluster): The cluster on which the
            application is running
        resource (dict): Kubernetes object description as specified in the
            specification of the application.
        response (kubernetes_asyncio.client.V1Service): Response of the
            Kubernetes API

    """
    if resource["kind"] != "Service":
        return

    service_name = resource["metadata"]["name"]

    if response.spec and response.spec.type == "LoadBalancer":
        # For a "LoadBalancer" type of Service, an external IP is given in the cluster
        # by a load balancer controller to the service. In this case, the "port"
        # specified in the spec is reachable from the outside.
        if (
            not response.status.load_balancer
            or not response.status.load_balancer.ingress
        ):
            # When a "LoadBalancer" type of service is created, the IP is given by an
            # additional controller (e.g. a controller that requests a floating IP to an
            # OpenStack infrastructure). This process can take some time, but the
            # Service itself already exist before the IP is assigned. In the case of an
            # error with the controller, the IP is also not given. This "<pending>" IP
            # just expresses that the Service exists, but the IP is not ready yet.
            external_ip = "<pending>"
        else:
            external_ip = response.status.load_balancer.ingress[0].ip

        if not response.spec.ports:
            external_port = "<pending>"
        else:
            external_port = response.spec.ports[0].port
        app.status.services[service_name] = f"{external_ip}:{external_port}"
        return

    node_port = None
    # Ensure that ports are specified
    if response.spec and response.spec.ports:
        node_port = response.spec.ports[0].node_port

    # If the service does not have a node port, remove a potential reference
    # and return.
    if node_port is None:
        try:
            del app.status.services[service_name]
        except KeyError:
            pass
        return

    # Determine URL of Kubernetes cluster API
    loader = KubeConfigLoader(cluster.spec.kubeconfig)
    config = Configuration()
    await loader.load_and_set(config)
    cluster_url = yarl.URL(config.host)

    app.status.services[service_name] = f"{cluster_url.host}:{node_port}"


@listen.on(HookType.ResourcePostDelete)
async def unregister_service(app, resource, **kwargs):
    """Unregister endpoint of Kubernetes Service object on deletion.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        resource (dict): Kubernetes object description as specified in the
            specification of the application.

    """
    if resource["kind"] != "Service":
        return

    service_name = resource["metadata"]["name"]
    try:
        del app.status.services[service_name]
    except KeyError:
        pass


@listen.on(HookType.ResourcePostDelete)
async def remove_resource_from_last_observed_manifest(app, resource, **kwargs):
    """Remove a given resource from the last_observed_manifest after its deletion

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        resource (dict): Kubernetes object description as specified in the
            specification of the application.

    """
    try:
        idx = get_kubernetes_resource_idx(app.status.last_observed_manifest, resource)
    except IndexError:
        return

    app.status.last_observed_manifest.pop(idx)


def update_last_applied_manifest_dict_from_resp(
    last_applied_manifest, observer_schema, response
):
    """Together with :func:``update_last_applied_manifest_list_from_resp``, this
    function is called recursively to update a partial ``last_applied_manifest``
    from a partial Kubernetes response

    Args:
        last_applied_manifest (dict): partial ``last_applied_manifest`` being
            updated
        observer_schema (dict): partial ``observer_schema``
        response (dict): partial response from the Kubernetes API.

    Raises:
        KeyError: If the observed field is not present in the Kubernetes response

    This function go through all observed fields, and initialized their value in
    last_applied_manifest if they are not yet present

    """
    for key, value in observer_schema.items():

        # Keys in the response are in camelCase
        camel_key = camel_to_snake_case(key)

        if camel_key not in response:
            # An observed key should always be present in the k8s response
            raise KeyError(
                f"Observed key {camel_key} is not present in response {response}"
            )

        if isinstance(value, dict):
            if key not in last_applied_manifest:
                # The dictionary is observed, but not present in
                # last_applied_manifest
                last_applied_manifest[key] = {}

            update_last_applied_manifest_dict_from_resp(
                last_applied_manifest[key], observer_schema[key], response[camel_key]
            )

        elif isinstance(value, list):
            if key not in last_applied_manifest:
                # The list is observed, but not present in last_applied_manifest
                last_applied_manifest[key] = []

            update_last_applied_manifest_list_from_resp(
                last_applied_manifest[key], observer_schema[key], response[camel_key]
            )

        elif key not in last_applied_manifest:
            # If key not present in last_applied_manifest, and value is neither a
            # dict nor a list, simply add it.
            last_applied_manifest[key] = response[camel_key]


def update_last_applied_manifest_list_from_resp(
    last_applied_manifest, observer_schema, response
):
    """Together with :func:``update_last_applied_manifest_dict_from_resp``, this
    function is called recursively to update a partial ``last_applied_manifest``
    from a partial Kubernetes response

    Args:
        last_applied_manifest (list): partial ``last_applied_manifest`` being
            updated
        observer_schema (list): partial ``observer_schema``
        response (list): partial response from the Kubernetes API.

    This function go through all observed fields, and initialized their value in
    last_applied_manifest if they are not yet present

    """
    # Looping over the observed resource, except the last element which is the
    # special control dictionary
    for idx, val in enumerate(observer_schema[:-1]):

        if idx >= len(response):
            # Element is observed but not present in k8s response, so following
            # elements will also not exist.
            #
            # This doesn't raise an Exception as observing the element of a list
            # doesn't ensure its presence. The list length is controlled by the
            # special control dictionary
            return

        if isinstance(val, dict):
            if idx >= len(last_applied_manifest):
                # The dict is observed, but not present in last_applied_manifest
                last_applied_manifest.append({})

            update_last_applied_manifest_dict_from_resp(
                last_applied_manifest[idx], observer_schema[idx], response[idx]
            )

        elif isinstance(response[idx], list):
            if idx >= len(last_applied_manifest):
                # The list is observed, but not present in last_applied_manifest
                last_applied_manifest.append([])

            update_last_applied_manifest_list_from_resp(
                last_applied_manifest[idx], observer_schema[idx], response[idx]
            )

        elif idx >= len(last_applied_manifest):
            # Element is not yet present in last_applied_manifest. Adding it.
            last_applied_manifest.append(response[idx])


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
def update_last_applied_manifest_from_resp(app, response, **kwargs):
    """Hook run after the creation or update of an application in order to update the
    `status.last_applied_manifest` using the k8s response.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        response (kubernetes_asyncio.client.V1Status): Response of the Kubernetes API

    After a Kubernetes resource has been created/updated, the
    `status.last_applied_manifest` has to be updated. All fields already initialized
    (either from the mangling of `spec.manifest`, or by a previous call to this
    function) should be left untouched. Only observed fields which are not present in
    `status.last_applied_manifest` should be initialized.

    """

    if isinstance(response, dict):
        # The Kubernetes API couldn't deserialize the k8s response into an object
        resp = response
    else:
        # The Kubernetes API deserialized the k8s response into an object
        resp = response.to_dict()

    idx_applied = get_kubernetes_resource_idx(app.status.last_applied_manifest, resp)

    idx_observed = get_kubernetes_resource_idx(app.status.mangled_observer_schema, resp)

    update_last_applied_manifest_dict_from_resp(
        app.status.last_applied_manifest[idx_applied],
        app.status.mangled_observer_schema[idx_observed],
        resp,
    )


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
def update_last_observed_manifest_from_resp(app, response, **kwargs):
    """Handler to run after the creation or update of a Kubernetes resource to update
    the last_observed_manifest from the response of the Kubernetes API.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        response (kubernetes_asyncio.client.V1Service): Response of the
            Kubernetes API

    The target last_observed_manifest holds the value of all observed fields plus the
    special control dictionaries for the list length

    """
    if isinstance(response, dict):
        # The Kubernetes API couldn't deserialize the k8s response into an object
        resp = response
    else:
        # The Kubernetes API deserialized the k8s response into an object
        resp = response.to_dict()

    try:
        idx_observed = get_kubernetes_resource_idx(
            app.status.mangled_observer_schema,
            resp,
        )
    except IndexError:
        # All created resources should be observed
        raise

    try:
        idx_last_observed = get_kubernetes_resource_idx(
            app.status.last_observed_manifest,
            resp,
        )
    except IndexError:
        # If the resource is not yes present in last_observed_manifest, append it.
        idx_last_observed = len(app.status.last_observed_manifest)
        app.status.last_observed_manifest.append({})

    # Overwrite the last_observed_manifest for this resource
    app.status.last_observed_manifest[
        idx_last_observed
    ] = update_last_observed_manifest_dict(
        app.status.mangled_observer_schema[idx_observed], resp
    )


def update_last_observed_manifest_dict(observed_resource, response):
    """Together with :func:``update_last_observed_manifest_list``, recursively
    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.

    Args:
        observed_resource (dict): The schema to observe for the partial given resource
        response (dict): The partial Kubernetes response for this resource.

    Raises:
        KeyError: If an observed key is not present in the Kubernetes response

    Returns:
        dict: The dictionary of observed keys and their value

    Get the value of all observed fields from the Kubernetes response
    """
    res = {}
    for key, value in observed_resource.items():

        camel_key = camel_to_snake_case(key)
        if camel_key not in response:
            raise KeyError(
                f"Observed key {camel_key} is not present in response {response}"
            )

        if isinstance(value, dict):
            res[key] = update_last_observed_manifest_dict(value, response[camel_key])

        elif isinstance(value, list):
            res[key] = update_last_observed_manifest_list(value, response[camel_key])

        else:
            res[key] = response[camel_key]

    return res


def update_last_observed_manifest_list(observed_resource, response):
    """Together with :func:``update_last_observed_manifest_dict``, recursively
    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.

    Args:
        observed_resource (list): the schema to observe for the partial given resource
        response (list): the partial Kubernetes response for this resource.

    Returns:
        list: The list of observed elements, plus the special list length control
            dictionary

    Get the value of all observed elements from the Kubernetes response
    """

    if not response:
        return [{"observer_schema_list_current_length": 0}]

    res = []
    # Looping over the observed resource, except the last element which is the special
    # control dictionary
    for idx, val in enumerate(observed_resource[:-1]):

        if idx >= len(response):
            # Element is not present in the Kubernetes response, nothing more to do
            break

        if type(response[idx]) is dict:
            res.append(update_last_observed_manifest_dict(val, response[idx]))

        elif type(response[idx]) is list:
            res.append(update_last_observed_manifest_list(val, response[idx]))

        else:
            res.append(response[idx])

    # Append the special control dictionary to the list
    res.append({"observer_schema_list_current_length": len(response)})

    return res


def update_last_applied_manifest_dict_from_spec(
    resource_status_new, resource_status_old, resource_observed
):
    """Together with :func:``update_last_applied_manifest_list_from_spec``, this
    function is called recursively to update a partial ``last_applied_manifest``

    Args:
        resource_status_new (dict): partial ``last_applied_manifest`` being updated
        resource_status_old (dict): partial of the current ``last_applied_manifest``
        resource_observed (dict): partial observer_schema for the manifest file
            being updated

    """
    for key, value in resource_observed.items():

        if key not in resource_status_old:
            continue

        if key in resource_status_new:

            if isinstance(value, dict):
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            elif isinstance(value, list):
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

        else:
            # If the key is not present the spec.manifest, we first need to
            # initialize it

            if isinstance(value, dict):
                resource_status_new[key] = {}
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            elif isinstance(value, list):
                resource_status_new[key] = []
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            else:
                resource_status_new[key] = resource_status_old[key]


def update_last_applied_manifest_list_from_spec(
    resource_status_new, resource_status_old, resource_observed
):
    """Together with :func:``update_last_applied_manifest_dict_from_spec``, this
    function is called recursively to update a partial ``last_applied_manifest``

    Args:
        resource_status_new (list): partial ``last_applied_manifest`` being updated
        resource_status_old (list): partial of the current ``last_applied_manifest``
        resource_observed (list): partial observer_schema for the manifest file
            being updated

    """

    # Looping over the observed resource, except the last element which is the
    # special control dictionary
    for idx, val in enumerate(resource_observed[:-1]):

        if idx >= len(resource_status_old):
            # The element in not in the current last_applied_manifest, and neither
            # is the rest of the list
            break

        if idx < len(resource_status_new):
            # The element is present in spec.manifest and in the current
            # last_applied_manifest. Updating observed fields

            if isinstance(val, dict):
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            elif isinstance(val, list):
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

        else:
            # If the element is not present in the spec.manifest, we first have to
            # initialize it.

            if isinstance(val, dict):
                resource_status_new.append({})
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            elif isinstance(val, list):
                resource_status_new.append([])
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            else:
                resource_status_new.append(resource_status_old[idx])


def update_last_applied_manifest_from_spec(app):
    """Update the status.last_applied_manifest of an application from spec.manifests

    Args:
        app (krake.data.kubernetes.Application): Application to update

    This function is called on application creation and updates. The
    last_applied_manifest of an application is initialized as a copy of spec.manifest,
    and is augmented by all known observed fields not yet initialized (i.e. all observed
    fields or resources which are present in the current last_applied_manifest but not
    in the spec.manifest)

    """

    # The new last_applied_manifest is initialized as a copy of the spec.manifest, and
    # augmented by all observed fields which are present in the current
    # last_applied_manifest but not in the original spec.manifest
    new_last_applied_manifest = deepcopy(app.spec.manifest)

    # Loop over observed resources and observed fields, and check if they should be
    # added to the new last_applied_manifest (i.e. present in the current
    # last_applied_manifest but not in spec.manifest)
    for resource_observed in app.status.mangled_observer_schema:

        # If the resource is not present in the current last_applied_manifest, there is
        # nothing to do. Whether the resource was initialized by spec.manifest doesn't
        # matter.
        try:
            idx_status_old = get_kubernetes_resource_idx(
                app.status.last_applied_manifest, resource_observed
            )
        except IndexError:
            continue

        # As the resource is present in the current last_applied_manifest, we need to go
        # through it to check if observed fields should be set to their current value
        # (i.e. fields are present in the current last_applied_manifest, but not in
        # spec.manifest)
        try:
            # Check if the observed resource is present in spec.manifest
            idx_status_new = get_kubernetes_resource_idx(
                new_last_applied_manifest, resource_observed
            )
        except IndexError:
            # The resource is observed but is not present in the spec.manifest.
            # Create an empty resource, which will be augmented in
            # update_last_applied_manifest_dict_from_spec with the observed and known
            # fields.
            new_last_applied_manifest.append({})
            idx_status_new = len(new_last_applied_manifest) - 1

        update_last_applied_manifest_dict_from_spec(
            new_last_applied_manifest[idx_status_new],
            app.status.last_applied_manifest[idx_status_old],
            resource_observed,
        )

    app.status.last_applied_manifest = new_last_applied_manifest


class KubernetesApplicationObserver(Observer):
    """Observer specific for Kubernetes Applications. One observer is created for each
    Application managed by the Controller, but not one per Kubernetes resource
    (Deployment, Service...). If several resources are defined by an Application, they
    are all monitored by the same observer.

    The observer gets the actual status of the resources on the cluster using the
    Kubernetes API, and compare it to the status stored in the API.

    The observer is:
     * started at initial Krake resource creation;

     * deleted when a resource needs to be updated, then started again when it is done;

     * simply deleted on resource deletion.

    Args:
        cluster (krake.data.kubernetes.Cluster): the cluster on which the observed
            Application is created.
        resource (krake.data.kubernetes.Application): the application that will be
            observed.
        on_res_update (coroutine): a coroutine called when a resource's actual status
            differs from the status sent by the database. Its signature is:
            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of
            the resource that is up-to-date with the API. The Observer internal instance
            of the resource to observe will be updated. If the API cannot be contacted,
            ``None`` can be returned. In this case the internal instance of the Observer
            will not be updated.
        time_step (int, optional): how frequently the Observer should watch the actual
            status of the resources.

    """

    def __init__(self, cluster, resource, on_res_update, time_step=2):
        super().__init__(resource, on_res_update, time_step)
        self.cluster = cluster
        self.kubernetes_api = None

    def _set_container_health(self, resource, status):
        if status.container_health is None:
            status.container_health = ContainerHealth()

        if hasattr(resource, 'status') and resource.status is not None:
            if resource.kind == "Pod":
                container_state = resource.status.container_statuses[0].state
                status.container_health.desired_pods = 1
                if container_state.terminated is not None:
                    if resource.spec.restart_policy == "Never":
                        status.state = ApplicationState.DEGRADED
                    else:
                        status.state = ApplicationState.RESTARTING
                    status.container_health.running_pods = 0
                elif container_state.waiting is not None:
                    if container_state.waiting.reason == "CrashLoopBackOff":
                        status.state = ApplicationState.DEGRADED
                        status.container_health.running_pods = 0
                else:
                    status.state = ApplicationState.RUNNING
                    status.container_health.running_pods = 1

            elif (
                resource.kind == "Deployment" or
                resource.kind == "StatefulSet" or
                resource.kind == "ReplicaSet" or
                resource.kind == "DaemonSet"
            ):

                if resource.kind == "DaemonSet":
                    status.container_health.desired_pods = \
                        resource.status.current_number_scheduled
                    if isinstance(resource.status.desired_number_scheduled, int):
                        status.container_health.running_pods = \
                            resource.status.desired_number_scheduled
                    else:
                        status.container_health.running_pods = 0
                else:
                    status.container_health.desired_pods = resource.status.replicas
                    if isinstance(resource.status.ready_replicas, int):
                        status.container_health.running_pods = \
                            resource.status.ready_replicas
                    else:
                        status.container_health.running_pods = 0

                if status.container_health.running_pods != \
                   status.container_health.desired_pods:
                    status.state = ApplicationState.DEGRADED
                else:
                    status.state = ApplicationState.RUNNING

            elif resource.kind == "Job":
                if isinstance(resource.spec.completions, int):
                    status.container_health.desired_pods = resource.spec.completions
                else:
                    status.container_health.desired_pods = 1

                if isinstance(resource.status.active, int):
                    status.container_health.running_pods = resource.status.active
                else:
                    status.container_health.running_pods = 0

                if isinstance(resource.status.succeeded, int):
                    status.container_health.completed_pods = resource.status.succeeded
                else:
                    status.container_health.completed_pods = 0

                if isinstance(resource.status.failed, int):
                    status.container_health.failed_pods = resource.status.failed
                else:
                    status.container_health.failed_pods = 0

    async def poll_resource(self):
        """Fetch the current status of the Application monitored by the Observer.

        Returns:
            krake.data.core.Status: the status object created using information from the
                real world Applications resource.

        """
        app = self.resource

        status = deepcopy(app.status)
        status.last_observed_manifest = []

        # For each observed kubernetes resource of the Application,
        # get its current status on the cluster.
        for desired_resource in app.status.last_applied_manifest:
            kube = KubernetesClient(self.cluster.spec.kubeconfig)
            idx_observed = get_kubernetes_resource_idx(
                app.status.mangled_observer_schema, desired_resource
            )
            observed_resource = app.status.mangled_observer_schema[idx_observed]
            async with kube:
                try:
                    group, version, kind, name, namespace = kube.get_immutables(
                        desired_resource
                    )
                    resource_api = await kube.get_resource_api(group, version, kind)
                    resp = await resource_api.read(kind, name, namespace)
                except (ClientConnectorError, ApiException) as err:
                    if hasattr(err, "status") and err.status == 404:
                        # Resource does not exist
                        continue
                    # Otherwise, log the unexpected error and return the
                    # last known application status
                    logger.debug(err)
                    return app.status

                resource = resp
                self._set_container_health(resource, status)

            observed_manifest = update_last_observed_manifest_dict(
                observed_resource, resp.to_dict()
            )
            status.last_observed_manifest.append(observed_manifest)

        return status


class KubernetesClusterObserver(Observer):
    """Observer specific for Kubernetes Clusters. One observer is created for each
    Cluster managed by the Controller.

    The observer gets the actual status of the cluster using the
    Kubernetes API, and compare it to the status stored in the API.

    The observer is:
     * started at initial Krake resource creation;

     * deleted when a resource needs to be updated, then started again when it is done;

     * simply deleted on resource deletion.

    Args:
        resource (krake.data.kubernetes.Cluster): the cluster which will be observed.
        on_res_update (coroutine): a coroutine called when a resource's actual status
            differs from the status sent by the database. Its signature is:
            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of
            the resource that is up-to-date with the API. The Observer internal instance
            of the resource to observe will be updated. If the API cannot be contacted,
            ``None`` can be returned. In this case the internal instance of the Observer
            will not be updated.
        time_step (int, optional): how frequently the Observer should watch the actual
            status of the resources.

    """

    def __init__(self, resource, on_res_update, time_step=2):
        super().__init__(resource, on_res_update, time_step)

    async def poll_resource(self):
        """Fetch the current status of the Cluster monitored by the Observer.

        Note regarding exceptions handling:
          The current cluster status is fetched by :func:`poll_resource` from its API.
          If the cluster API is shutting down the API server responds with a 503
          (service unavailable, apiserver is shutting down) HTTP response which
          leads to the kubernetes client ApiException. If the cluster's API has been
          successfully shut down and there is an attempt to fetch cluster status,
          the ClientConnectorError is raised instead.
          Therefore, both exceptions should be handled.

        Returns:
            krake.data.core.Status: the status object created using information from the
                real world Cluster.

        """
        cluster = self.resource
        status = deepcopy(cluster.status)
        status.nodes = []
        # For each observed kubernetes cluster registered in Krake,
        # get its current node status.
        loader = KubeConfigLoader(cluster.spec.kubeconfig)
        config = Configuration()
        await loader.load_and_set(config)
        kube = ApiClient(config)

        async with kube as api:
            v1 = client.CoreV1Api(api)
            try:
                response = await v1.list_node()
            except (ClientConnectorError, ApiException) as err:
                # Log the error and set cluster state to OFFLINE
                logger.debug(err)
                status.state = ClusterState.OFFLINE
                return status

            # Fetch nodes conditions
            nodes = []
            for node in response.items:
                conditions = []
                for condition in node.status.conditions:
                    conditions.append(
                        ClusterNodeCondition(
                            message=condition.message,
                            reason=condition.reason,
                            status=condition.status,
                            type=condition.type,
                        )
                    )

                nodes.append(
                    ClusterNode(
                        metadata=ClusterNodeMetadata(name=node.metadata.name),
                        status=ClusterNodeStatus(conditions=conditions),
                    )
                )
            status.nodes = nodes

            # The scheduler is unable to fetch cluster metrics, hence
            # the cluster state should wait for it and the cluster
            # status should not be changed by the observer.
            if status.state == ClusterState.FAILING_METRICS:
                return status

            # Set the cluster state to CONNECTING if the previous state
            # was OFFLINE. It is due to smooth transition from
            # the OFFLINE to ONLINE state.
            if status.state == ClusterState.OFFLINE:
                status.state = ClusterState.CONNECTING
                return status

            for node in status.nodes:
                for condition in node.status.conditions:
                    if (
                        condition.type.lower().endswith("pressure")
                        and condition.status == "True"
                    ):
                        status.state = ClusterState.UNHEALTHY
                        return status

                    if condition.type.lower() == "ready" and condition.status != "True":
                        status.state = ClusterState.NOTREADY
                        return status

            status.state = ClusterState.ONLINE
            return status


@listen.on(HookType.ApplicationPostReconcile)
@listen.on(HookType.ApplicationPostMigrate)
@listen.on(HookType.ClusterCreation)
async def register_observer(controller, resource, start=True, **kwargs):
    """Create an observer for the given Application or Cluster, and start it as a
    background task if wanted.

    If an observer already existed for this Application or Cluster, it is stopped
    and deleted.

    Args:
        controller (KubernetesController): the controller for which the observer will be
            added in the list of working observers.
        resource (krake.data.kubernetes.Application): the Application to observe or
        resource (krake.data.kubernetes.Cluster): the Cluster to observe.
        start (bool, optional): if False, does not start the observer as background
            task.

    """
    if resource.kind == Application.kind:
        cluster = await controller.kubernetes_api.read_cluster(
            namespace=resource.status.running_on.namespace,
            name=resource.status.running_on.name,
        )
        observer = KubernetesApplicationObserver(
            cluster,
            resource,
            controller.on_status_update,
            time_step=controller.observer_time_step,
        )

    elif resource.kind == Cluster.kind:
        observer = KubernetesClusterObserver(
            resource,
            controller.on_status_update,
            time_step=controller.observer_time_step,
        )
    else:
        logger.debug("Unknown resource kind. No observer was registered.", resource)
        return

    logger.debug(f"Start observer for {resource.kind} %r", resource.metadata.name)
    task = None
    if start:
        task = controller.loop.create_task(observer.run())

    controller.observers[resource.metadata.uid] = (observer, task)


@listen.on(HookType.ApplicationPreReconcile)
@listen.on(HookType.ApplicationPreMigrate)
@listen.on(HookType.ApplicationPreDelete)
@listen.on(HookType.ClusterDeletion)
async def unregister_observer(controller, resource, **kwargs):
    """Stop and delete the observer for the given Application or Cluster. If no observer
    is started, do nothing.

    Args:
        controller (KubernetesController): the controller for which the observer will be
            removed from the list of working observers.
        resource (krake.data.kubernetes.Application): the Application whose observer
        will be stopped or
        resource (krake.data.kubernetes.Cluster): the Cluster whose observer will be
        stopped.

    """
    if resource.metadata.uid not in controller.observers:
        return

    logger.debug(f"Stop observer for {resource.kind} {resource.metadata.name}")
    _, task = controller.observers.pop(resource.metadata.uid)
    task.cancel()

    with suppress(asyncio.CancelledError):
        await task


@listen.on(HookType.ApplicationToscaTranslation)
async def translate_tosca(controller, app, **kwargs):
    """Translate a TOSCA template or CSAR archive into a Kubernetes manifest.

    Args:
        controller (KubernetesController): the controller that handles the application
            resource.
        app (krake.data.kubernetes.Application): the Application that could be defined
            by a TOSCA template or a CSAR archive.

    Raises:
        ToscaParserException: If the given application does not contain
         at least one from the following:
         - Kubernetes manifest
         - TOSCA template
         - CSAR archive

    """
    if app.spec.manifest:
        return

    if not app.spec.tosca and not app.spec.csar:
        raise ToscaParserException(
            "Application should be defined by a Kubernetes manifest,"
            " a TOSCA template or a CSAR archive: %r",
            app,
        )
    app.status.state = ApplicationState.TRANSLATING
    await controller.kubernetes_api.update_application_status(
        namespace=app.metadata.namespace, name=app.metadata.name, body=app
    )

    if app.spec.tosca and isinstance(app.spec.tosca, dict):

        manifest = ToscaParser.from_dict(app.spec.tosca).translate_to_manifests()
    else:
        manifest = ToscaParser.from_url(
            app.spec.tosca or app.spec.csar
        ).translate_to_manifests()

    app.spec.manifest = manifest
    await controller.kubernetes_api.update_application(
        namespace=app.metadata.namespace, name=app.metadata.name, body=app
    )


def utc_difference():
    """Get the difference in seconds between the current time and the current UTC time.

    Returns:
        int: the time difference in seconds.

    """
    delta = datetime.now() - datetime.utcnow()
    return delta.seconds


def generate_certificate(config):
    """Create and sign a new certificate using the one defined in the complete hook
    configuration as intermediate certificate.

    Args:
        config (krake.data.config.CompleteHookConfiguration): the configuration of the
            complete hook.

    Returns:
        CertificatePair: the content of the certificate created and its corresponding
            key.

    """
    with open(config.intermediate_src, "rb") as f:
        intermediate_src = crypto.load_certificate(crypto.FILETYPE_PEM, f.read())
    with open(config.intermediate_key_src, "rb") as f:
        intermediate_key_src = crypto.load_privatekey(crypto.FILETYPE_PEM, f.read())

    client_cert = crypto.X509()

    # Set general information
    client_cert.set_version(3)
    client_cert.set_serial_number(random.randint(50000000000000, 100000000000000))
    # If not set before, TLS will not accept to use this certificate in UTC cases, as
    # the server time may be earlier.
    time_offset = utc_difference() * -1
    client_cert.gmtime_adj_notBefore(time_offset)
    client_cert.gmtime_adj_notAfter(1 * 365 * 24 * 60 * 60)

    # Set issuer and subject
    intermediate_subject = intermediate_src.get_subject()
    client_cert.set_issuer(intermediate_subject)
    client_subj = crypto.X509Name(intermediate_subject)
    client_subj.CN = config.hook_user
    client_cert.set_subject(client_subj)

    # Create and set the private key
    client_key = crypto.PKey()
    client_key.generate_key(crypto.TYPE_RSA, 2048)
    client_cert.set_pubkey(client_key)

    client_cert.sign(intermediate_key_src, "sha256")

    cert_dump = crypto.dump_certificate(crypto.FILETYPE_PEM, client_cert).decode()
    key_dump = crypto.dump_privatekey(crypto.FILETYPE_PEM, client_key).decode()
    return CertificatePair(cert=cert_dump, key=key_dump)


def generate_default_observer_schema(app):
    """Generate the default observer schema for each Kubernetes resource present in
    ``spec.manifest`` for which a custom observer schema hasn't been specified.

    Args:
        app (krake.data.kubernetes.Application): The application for which to generate a
            default observer schema
    """

    app.status.mangled_observer_schema = deepcopy(app.spec.observer_schema)

    for resource_manifest in app.spec.manifest:
        try:
            get_kubernetes_resource_idx(
                app.status.mangled_observer_schema, resource_manifest
            )

        except IndexError:
            # Only create a default observer schema, if a custom observer schema hasn't
            # been set by the user.
            app.status.mangled_observer_schema.append(
                generate_default_observer_schema_dict(
                    resource_manifest,
                    first_level=True,
                )
            )


def generate_default_observer_schema_dict(manifest_dict, first_level=False):
    """Together with :func:``generate_default_observer_schema_list``, this function is
    called recursively to generate part of a default ``observer_schema`` from part of a
    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.

    Args:
        manifest_dict (dict): Partial Kubernetes resources
        first_level (bool, optional): If True, indicates that the dictionary represents
            the whole observer schema of a Kubernetes resource

    Returns:
        dict: Generated partial observer_schema

    This function creates a new dictionary from ``manifest_dict`` and replaces all
    non-list and non-dict values by ``None``.

    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a
    resource), the values of the identifying fields are copied from the manifest file.

    """
    observer_schema_dict = {}

    for key, value in manifest_dict.items():

        if isinstance(value, dict):
            observer_schema_dict[key] = generate_default_observer_schema_dict(value)

        elif isinstance(value, list):
            observer_schema_dict[key] = generate_default_observer_schema_list(value)

        else:
            observer_schema_dict[key] = None

    if first_level:
        observer_schema_dict["apiVersion"] = manifest_dict["apiVersion"]
        observer_schema_dict["kind"] = manifest_dict["kind"]
        observer_schema_dict["metadata"]["name"] = manifest_dict["metadata"]["name"]

        if (
            "spec" in manifest_dict
            and "type" in manifest_dict["spec"]
            and manifest_dict["spec"]["type"] == "LoadBalancer"
        ):
            observer_schema_dict["status"] = {"load_balancer": {"ingress": None}}

    return observer_schema_dict


def generate_default_observer_schema_list(manifest_list):
    """Together with :func:``generate_default_observer_schema_dict``, this function is
    called recursively to generate part of a default ``observer_schema`` from part of a
    Kubernetes resource, defined respectively by ``manifest_list`` or ``manifest_dict``.

    Args:
        manifest_list (list): Partial Kubernetes resources

    Returns:
        list: Generated partial observer_schema

    This function creates a new list from ``manifest_list`` and replaces all non-list
    and non-dict elements by ``None``.

    Additionally, it generates the default list control dictionary, using the current
    length of the list as default minimum and maximum values.

    """
    observer_schema_list = []

    for value in manifest_list:

        if isinstance(value, dict):
            observer_schema_list.append(generate_default_observer_schema_dict(value))

        elif isinstance(value, list):
            observer_schema_list.append(generate_default_observer_schema_list(value))

        else:
            observer_schema_list.append(None)

    observer_schema_list.append(
        {
            "observer_schema_list_min_length": len(manifest_list),
            "observer_schema_list_max_length": len(manifest_list),
        }
    )

    return observer_schema_list


@listen.on(HookType.ApplicationMangling)
async def complete(app, api_endpoint, ssl_context, config):
    """Execute application complete hook defined by :class:`Complete`.
    Hook mangles given application and injects complete hooks variables.

    Application complete hook is disabled by default.
    User enables this hook by the --hook-complete argument in rok cli.

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        config (krake.data.config.HooksConfiguration): Complete hook
            configuration.

    """
    if "complete" not in app.spec.hooks:
        return

    # Use the endpoint of the API only if the external endpoint has not been set.
    if config.complete.external_endpoint:
        api_endpoint = config.complete.external_endpoint

    app.status.complete_token = (
        app.status.complete_token if app.status.complete_token else token_urlsafe()
    )

    # Generate only once the certificate and key for a specific Application
    generated_cert = CertificatePair(
        cert=app.status.complete_cert, key=app.status.complete_key
    )
    if ssl_context and generated_cert == (None, None):
        generated_cert = generate_certificate(config.complete)
        app.status.complete_cert = generated_cert.cert
        app.status.complete_key = generated_cert.key

    hook = Complete(
        api_endpoint,
        ssl_context,
        hook_user=config.complete.hook_user,
        cert_dest=config.complete.cert_dest,
        env_token=config.complete.env_token,
        env_url=config.complete.env_url,
    )
    hook.mangle_app(
        app.metadata.name,
        app.metadata.namespace,
        app.status.complete_token,
        app.status.last_applied_manifest,
        config.complete.intermediate_src,
        generated_cert,
        app.status.mangled_observer_schema,
        "complete",
    )


@listen.on(HookType.ApplicationMangling)
async def shutdown(app, api_endpoint, ssl_context, config):
    """Executes an application shutdown hook defined by :class:`Shutdown`.
    The hook mangles the given application and injects shutdown hooks variables.

    Application shutdown hook is disabled by default.
    User enables this hook by the --hook-shutdown argument in rok cli.

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        config (krake.data.config.HooksConfiguration): Shutdown hook
            configuration.

    """
    if "shutdown" not in app.spec.hooks:
        return

    # Use the endpoint of the API only if the external endpoint has not been set.
    if config.shutdown.external_endpoint:
        api_endpoint = config.shutdown.external_endpoint

    app.status.shutdown_token = (
        app.status.shutdown_token if app.status.shutdown_token else token_urlsafe()
    )

    # Generate only once the certificate and key for a specific Application
    generated_cert = CertificatePair(
        cert=app.status.shutdown_cert, key=app.status.shutdown_key
    )
    if ssl_context and generated_cert == (None, None):
        generated_cert = generate_certificate(config.shutdown)
        app.status.shutdown_cert = generated_cert.cert
        app.status.shutdown_key = generated_cert.key

    hook = Shutdown(
        api_endpoint,
        ssl_context,
        hook_user=config.shutdown.hook_user,
        cert_dest=config.shutdown.cert_dest,
        env_token=config.shutdown.env_token,
        env_url=config.shutdown.env_url,
    )
    hook.mangle_app(
        app.metadata.name,
        app.metadata.namespace,
        app.status.shutdown_token,
        app.status.last_applied_manifest,
        config.shutdown.intermediate_src,
        generated_cert,
        app.status.mangled_observer_schema,
        "shutdown",
    )


@listen.on(HookType.ResourcePreDelete)
async def pre_shutdown(controller, app, **kwargs):
    """

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
    """
    if "shutdown" not in app.spec.hooks:
        return

    return


class SubResource(NamedTuple):
    group: str
    name: str
    body: dict
    path: tuple


class CertificatePair(NamedTuple):
    """Tuple which contains a certificate and its corresponding key.

    Attributes:
        cert (str): content of a certificate.
        key (str): content of the key that corresponds to the certificate.

    """

    cert: str
    key: str


class Hook(object):
    hook_resources = ()

    ca_name = "ca-bundle.pem"
    cert_name = "cert.pem"
    key_name = "key.pem"

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        self.api_endpoint = api_endpoint
        self.ssl_context = ssl_context
        self.hook_user = hook_user
        self.cert_dest = cert_dest
        self.env_token = env_token
        self.env_url = env_url

    def mangle_app(
        self,
        name,
        namespace,
        token,
        last_applied_manifest,
        intermediate_src,
        generated_cert,
        mangled_observer_schema,
        hook_type="",
    ):
        """Mangle a given application and inject complete hook resources and
        sub-resources into the :attr:`last_applied_manifest` object by :meth:`mangle`.
        Also mangle the observer_schema as new resources and sub-resources should
        be observed.

        :attr:`last_applied_manifest` is created as a deep copy of the desired
        application resources, as defined by user. It can be updated by custom hook
        resources or modified by custom hook sub-resources. It is used as a desired
        state for the Krake deployment process.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            token (str): Complete hook authentication token
            last_applied_manifest (list): Application resources
            intermediate_src (str): content of the certificate that is used to sign new
                certificates for the complete hook.
            generated_cert (CertificatePair): tuple that contains the content of the
                new signed certificate for the Application, and the content of its
                corresponding key.
            mangled_observer_schema (list): Observed fields
            hook_type (str, optional): Name of the hook the app should be mangled for

        """

        secret_certs_name = "-".join([name, "krake", hook_type, "secret", "certs"])
        secret_token_name = "-".join([name, "krake", hook_type, "secret", "token"])
        volume_name = "-".join([name, "krake", hook_type, "volume"])
        ca_certs = (
            self.ssl_context.get_ca_certs(binary_form=True)
            if self.ssl_context
            else None
        )

        # Extract all different namespaces
        # FIXME: too many assumptions here: do we create one ConfigMap for each
        #  namespace?
        resource_namespaces = {
            resource["metadata"].get("namespace", "default")
            for resource in last_applied_manifest
        }

        hook_resources = []
        hook_sub_resources = []
        if ca_certs:
            hook_resources.extend(
                [
                    self.secret_certs(
                        secret_certs_name,
                        resource_namespace,
                        intermediate_src=intermediate_src,
                        generated_cert=generated_cert,
                        ca_certs=ca_certs,
                    )
                    for resource_namespace in resource_namespaces
                ]
            )
            hook_sub_resources.extend(
                [*self.volumes(secret_certs_name, volume_name, self.cert_dest)]
            )

        hook_resources.extend(
            [
                self.secret_token(
                    secret_token_name,
                    name,
                    namespace,
                    resource_namespace,
                    self.api_endpoint,
                    token,
                )
                for resource_namespace in resource_namespaces
            ]
        )
        hook_sub_resources.extend(
            [
                *self.env_vars(secret_token_name),
            ]
        )

        self.mangle(
            hook_resources,
            last_applied_manifest,
            mangled_observer_schema,
        )
        self.mangle(
            hook_sub_resources,
            last_applied_manifest,
            mangled_observer_schema,
            is_sub_resource=True,
        )

    def mangle(
        self,
        items,
        last_applied_manifest,
        mangled_observer_schema,
        is_sub_resource=False,
    ):
        """Mangle applications desired state with custom hook resources or
        sub-resources.

        Example:
            .. code:: python

            last_applied_manifest = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Pod',
                    'metadata': {'name': 'test', 'namespace': 'default'},
                    'spec': {'containers': [{'name': 'test'}]}
                }
            ]
            mangled_observer_schema = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Pod',
                    'metadata': {'name': 'test', 'namespace': 'default'},
                    'spec': {
                        'containers': [
                            {'name': None},
                            {
                                'observer_schema_list_max_length': 1,
                                'observer_schema_list_min_length': 1,
                            },
                        ]
                    },
                }
            ]
            hook_resources = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Secret',
                    'metadata': {'name': 'sct', 'namespace': 'default'}
                }
            ]
            hook_sub_resources = [
                SubResource(
                    group='env', name='env', body={'name': 'test', 'value': 'test'},
                    path=(('spec', 'containers'),)
                )
            ]

            mangle(
                hook_resources,
                last_applied_manifest,
                mangled_observer_schema,
            )
            mangle(
                hook_sub_resources,
                last_applied_manifest,
                mangled_observer_schema,
                is_sub_resource=True
            )

            assert last_applied_manifest == [
                {
                    "apiVersion": "v1",
                    "kind": "Pod",
                    "metadata": {"name": "test", 'namespace': 'default'},
                    "spec": {
                        "containers": [
                            {
                                "name": "test",
                                "env": [{"name": "test", "value": "test"}]
                            }
                        ]
                    },
                },
                {"apiVersion": "v1", "kind": "Secret", "metadata": {"name": "sct"}},
            ]

            assert mangled_observer_schema == [
                {
                    "apiVersion": "v1",
                    "kind": "Pod",
                    "metadata": {"name": "test", "namespace": None},
                    "spec": {
                        "containers": [
                            {
                                "name": None,
                                "env": [
                                    {"name": None, "value": None},
                                    {
                                        "observer_schema_list_max_length": 1,
                                        "observer_schema_list_min_length": 1,
                                    },
                                ],
                            },
                            {
                                "observer_schema_list_max_length": 1,
                                "observer_schema_list_min_length": 1,
                            },
                        ]
                    },
                },
                {
                    "apiVersion": "v1",
                    "kind": "Secret",
                    "metadata": {"name": "sct", "namespace": None},
                },
            ]

        Args:
            items (list[SubResource]): Custom hook resources or sub-resources
            last_applied_manifest (list): Application resources
            mangled_observer_schema (list): Observed resources
            is_sub_resource (bool, optional): if False, the function only extend the
                list of Kubernetes resources defined in :attr:`last_applied_manifest`
                with new hook resources. Otherwise, the function injects each new hook
                sub-resource into the :attr:`last_applied_manifest` object
                sub-resources. Defaults to False.

        """

        if not items:
            return

        if not is_sub_resource:
            last_applied_manifest.extend(items)
            for sub_resource in items:
                # Generate the default observer schema for each resource
                mangled_observer_schema.append(
                    generate_default_observer_schema_dict(
                        sub_resource,
                        first_level=True,
                    )
                )
            return

        def inject(sub_resource, sub_resource_to_mangle, observed_resource_to_mangle):
            """Inject a hooks defined sub-resource into a Kubernetes sub-resource.

            Args:
                sub_resource (SubResource): Hook sub-resource that needs to be injected
                    into :attr:`last_applied_manifest`
                sub_resource_to_mangle (object): Kubernetes sub-resources from
                    :attr:`last_applied_manifest` which need to be processed
                observed_resource_to_mangle (dict): partial mangled_observer_schema
                    corresponding to the Kubernetes sub-resource.

            Raises:
                InvalidManifestError: if the sub-resource which will be mangled is not a
                    list or a dict.

            """

            # Create sub-resource group if not present in the Kubernetes sub-resource
            if sub_resource.group not in sub_resource_to_mangle:
                # FIXME: This assumes the subresource group contains a list
                sub_resource_to_mangle.update({sub_resource.group: []})

            # Create sub-resource group if not present in the observed fields
            if sub_resource.group not in observed_resource_to_mangle:
                observed_resource_to_mangle.update(
                    {
                        sub_resource.group: [
                            {
                                "observer_schema_list_min_length": 0,
                                "observer_schema_list_max_length": 0,
                            }
                        ]
                    }
                )

            # Inject sub-resource
            # If sub-resource name is already there update it, if not, append it
            if sub_resource.name in [
                g["name"] for g in sub_resource_to_mangle[sub_resource.group]
            ]:
                # FIXME: Assuming we are dealing with a list
                for idx, item in enumerate(sub_resource_to_mangle[sub_resource.group]):
                    if item["name"]:
                        if hasattr(item, "body"):
                            sub_resource_to_mangle[item.group][idx] = item["body"]
            else:
                sub_resource_to_mangle[sub_resource.group].append(sub_resource.body)

            # Make sure the value is observed
            if sub_resource.name not in [
                g["name"] for g in observed_resource_to_mangle[sub_resource.group][:-1]
            ]:
                observed_resource_to_mangle[sub_resource.group].insert(
                    -1, generate_default_observer_schema_dict(sub_resource.body)
                )
                observed_resource_to_mangle[sub_resource.group][-1][
                    "observer_schema_list_min_length"
                ] += 1
                observed_resource_to_mangle[sub_resource.group][-1][
                    "observer_schema_list_max_length"
                ] += 1

        for resource in last_applied_manifest:
            # Complete hook is applied only on defined Kubernetes resources
            if resource["kind"] not in self.hook_resources:
                continue

            for sub_resource in items:
                sub_resources_to_mangle = None
                idx_observed = get_kubernetes_resource_idx(
                    mangled_observer_schema, resource
                )
                for keys in sub_resource.path:
                    try:
                        sub_resources_to_mangle = reduce(getitem, keys, resource)
                    except KeyError:
                        continue

                    break

                # Create the path to the observed sub-resource, if it doesn't yet exist
                try:
                    observed_sub_resources = reduce(
                        getitem, keys, mangled_observer_schema[idx_observed]
                    )
                except KeyError:
                    Complete.create_path(
                        mangled_observer_schema[idx_observed], list(keys)
                    )
                    observed_sub_resources = reduce(
                        getitem, keys, mangled_observer_schema[idx_observed]
                    )

                if isinstance(sub_resources_to_mangle, list):
                    for idx, sub_resource_to_mangle in enumerate(
                        sub_resources_to_mangle
                    ):

                        # Ensure that each element of the list is observed.
                        idx_observed = idx
                        if idx >= len(observed_sub_resources[:-1]):
                            idx_observed = len(observed_sub_resources[:-1])
                            # FIXME: Assuming each element of the list contains a
                            # dictionary, therefore initializing new elements with an
                            # empty dict
                            observed_sub_resources.insert(-1, {})
                        observed_sub_resource = observed_sub_resources[idx_observed]

                        # FIXME: This is assuming a list always contains dict
                        inject(
                            sub_resource, sub_resource_to_mangle, observed_sub_resource
                        )

                elif isinstance(sub_resources_to_mangle, dict):
                    inject(
                        sub_resource, sub_resources_to_mangle, observed_sub_resources
                    )

                else:
                    message = (
                        f"The sub-resource to mangle {sub_resources_to_mangle!r} has an"
                        "invalid type, should be in '[dict, list]'"
                    )
                    raise InvalidManifestError(message)

    @staticmethod
    def attribute_map(obj):
        """Convert a Kubernetes object to dict based on its attribute mapping

        Example:
            .. code:: python

            from kubernetes_asyncio.client import V1VolumeMount

            d = attribute_map(
                    V1VolumeMount(name="name", mount_path="path")
            )
            assert d == {'mountPath': 'path', 'name': 'name'}

        Args:
            obj (object): Kubernetes object

        Returns:
            dict: Converted Kubernetes object

        """
        return {
            obj.attribute_map[attr]: getattr(obj, attr)
            for attr, _ in obj.to_dict().items()
            if getattr(obj, attr) is not None
        }

    @staticmethod
    def create_path(mangled_observer_schema, keys):
        """Create the path to the observed field in the observer schema.

        When a sub-resource is mangled, it should be observed. This function creates
        the path to the subresource to observe.

        Args:
            mangled_observer_schema (dict): Partial observer schema of a resource
            keys (list): list of keys forming the path to the sub-resource to
                observe

        FIXME: This assumes we are only adding keys to dict. We don't consider lists

        """

        # Unpack the first key first, as it contains the base directory
        key = keys.pop(0)

        # If the key is the last of the list, we reached the end of the path.
        if len(keys) == 0:
            mangled_observer_schema[key] = None
            return

        if key not in mangled_observer_schema:
            mangled_observer_schema[key] = {}
        Hook.create_path(mangled_observer_schema[key], keys)

    def secret_certs(
        self,
        secret_name,
        namespace,
        ca_certs=None,
        intermediate_src=None,
        generated_cert=None,
    ):
        """Create a complete hooks secret resource.

        Complete hook secret stores Krake CAs and client certificates to communicate
        with the Krake API.

        Args:
            secret_name (str): Secret name
            namespace (str): Kubernetes namespace where the Secret will be created.
            ca_certs (list): Krake CA list
            intermediate_src (str): content of the certificate that is used to sign new
                certificates for the complete hook.
            generated_cert (CertificatePair): tuple that contains the content of the
                new signed certificate for the Application, and the content of its
                corresponding key.

        Returns:
            dict: complete hook secret resource

        """
        ca_certs_pem = ""
        for ca_cert in ca_certs:
            x509 = crypto.load_certificate(crypto.FILETYPE_ASN1, ca_cert)
            ca_certs_pem += crypto.dump_certificate(crypto.FILETYPE_PEM, x509).decode()

        # Add the intermediate certificate into the chain
        with open(intermediate_src, "r") as f:
            intermediate_src_content = f.read()
        ca_certs_pem += intermediate_src_content

        data = {
            self.ca_name: self._encode_to_64(ca_certs_pem),
            self.cert_name: self._encode_to_64(generated_cert.cert),
            self.key_name: self._encode_to_64(generated_cert.key),
        }
        return self.secret(secret_name, data, namespace)

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create a hooks secret resource.

        The hook secret stores Krake authentication token
        and hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Complete hook authentication token

        Returns:
            dict: complete hook secret resource

        """
        pass

    def volumes(self, secret_name, volume_name, mount_path):
        """Create complete hooks volume and volume mount sub-resources

        Complete hook volume gives access to hook's secret, which stores
        Krake CAs and client certificates to communicate with the Krake API.
        Complete hook volume mount puts the volume into the application

        Args:
            secret_name (str): Secret name
            volume_name (str): Volume name
            mount_path (list): Volume mount path

        Returns:
            list: List of complete hook volume and volume mount sub-resources

        """
        volume = V1Volume(name=volume_name, secret={"secretName": secret_name})
        volume_mount = V1VolumeMount(name=volume_name, mount_path=mount_path)
        return [
            SubResource(
                group="volumes",
                name=volume.name,
                body=self.attribute_map(volume),
                path=(("spec", "template", "spec"), ("spec",)),
            ),
            SubResource(
                group="volumeMounts",
                name=volume_mount.name,
                body=self.attribute_map(volume_mount),
                path=(
                    ("spec", "template", "spec", "containers"),
                    ("spec", "containers"),  # kind: Pod
                ),
            ),
        ]

    @staticmethod
    def _encode_to_64(string):
        """Compute the base 64 encoding of a string.

        Args:
            string (str): the string to encode.

        Returns:
            str: the result of the encoding.

        """
        return b64encode(string.encode()).decode()

    def secret(self, secret_name, secret_data, namespace, _type="Opaque"):
        """Create a secret resource.

        Args:
            secret_name (str): Secret name
            secret_data (dict): Secret data
            namespace (str): Kubernetes namespace where the Secret will be created.
            _type (str, optional): Secret type. Defaults to Opaque.

        Returns:
            dict: secret resource

        """
        return self.attribute_map(
            V1Secret(
                api_version="v1",
                kind="Secret",
                data=secret_data,
                metadata={"name": secret_name, "namespace": namespace},
                type=_type,
            )
        )

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' hook URL.
        Function needs to be specified for each hook.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application shutdown url

        """
        pass

    def env_vars(self, secret_name):
        """Create the hooks' environment variables sub-resources.
        Function needs to be specified for each hook.

        Creates hook environment variables to store Krake authentication token
        and a hook URL for the given applications.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of shutdown hook environment variables sub-resources

        """
        pass


class Complete(Hook):
    """Mangle given application and inject complete hooks variables into it.

    Hook injects a Kubernetes secret, which stores Krake authentication token
    and the Krake complete hook URL for the given application. The variables
    from Kubernetes secret are imported as environment variables
    into the application resource definition. Only resources defined in
    :args:`hook_resources` can be modified.

    Names of environment variables are defined in the application controller
    configuration file.

    If TLS is enabled on the Krake API, the complete hook injects a Kubernetes secret,
    and it's corresponding volume and volume mount definitions for the Krake CA,
    the client certificate with the right CN, and its key. The directory where the
    secret is mounted is defined in the configuration.

    Args:
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        cert_dest (str, optional): Path of the directory where the CA, client
            certificate and key to the Krake API will be stored.
        env_token (str, optional): Name of the environment variable, which stores Krake
            authentication token.
        env_url (str, optional): Name of the environment variable,
            which stores Krake complete hook URL.

    """

    hook_resources = ("Pod", "Deployment", "ReplicationController")

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        super().__init__(
            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
        )
        self.env_url = env_url

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create complete hooks secret resource.

        Complete hook secret stores Krake authentication token
        and complete hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Complete hook authentication token

        Returns:
            dict: complete hook secret resource

        """
        complete_url = self.create_hook_url(name, namespace, api_endpoint)
        data = {
            self.env_token.lower(): self._encode_to_64(token),
            self.env_url.lower(): self._encode_to_64(complete_url),
        }
        return self.secret(secret_name, data, resource_namespace)

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' complete URL.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application complete url

        """
        api_url = URL(api_endpoint)
        return str(
            api_url.with_path(
                f"/kubernetes/namespaces/{namespace}/applications/{name}/complete"
            )
        )

    def env_vars(self, secret_name):
        """Create complete hooks environment variables sub-resources

        Create complete hook environment variables store Krake authentication token
        and complete hook URL for given application.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of complete hook environment variables sub-resources

        """
        sub_resources = []

        env_token = V1EnvVar(
            name=self.env_token,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(
                            name=secret_name, key=self.env_token.lower()
                        )
                    )
                )
            ),
        )
        env_url = V1EnvVar(
            name=self.env_url,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())
                    )
                )
            ),
        )

        for env in (env_token, env_url):
            sub_resources.append(
                SubResource(
                    group="env",
                    name=env.name,
                    body=self.attribute_map(env),
                    path=(
                        ("spec", "template", "spec", "containers"),
                        ("spec", "containers"),  # kind: Pod
                    ),
                )
            )
        return sub_resources


class Shutdown(Hook):
    """Mangle given application and inject shutdown hooks variables into it.

    Hook injects a Kubernetes secret, which stores Krake authentication token
    and the Krake complete hook URL for the given application. The variables
    from the Kubernetes secret are imported as environment variables
    into the application resource definition. Only resources defined in
    :args:`hook_resources` can be modified.

    Names of environment variables are defined in the application controller
    configuration file.

    If TLS is enabled on the Krake API, the shutdown hook injects a Kubernetes secret,
    and it's corresponding volume and volume mount definitions for the Krake CA,
    the client certificate with the right CN, and its key. The directory where the
    secret is mounted is defined in the configuration.

    Args:
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        cert_dest (str, optional): Path of the directory where the CA, client
            certificate and key to the Krake API will be stored.
        env_token (str, optional): Name of the environment variable, which stores Krake
            authentication token.
        env_url (str, optional): Name of the environment variable,
            which stores Krake complete hook URL.

    """

    hook_resources = ("Pod", "Deployment", "ReplicationController")

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        super().__init__(
            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
        )
        self.env_url = env_url

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create shutdown hooks secret resource.

        Shutdown hook secret stores Krake authentication token
        and shutdown hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Shutdown hook authentication token

        Returns:
            dict: shutdown hook secret resource

        """
        shutdown_url = self.create_hook_url(name, namespace, api_endpoint)
        data = {
            self.env_token.lower(): self._encode_to_64(token),
            self.env_url.lower(): self._encode_to_64(shutdown_url),
        }
        return self.secret(secret_name, data, resource_namespace)

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' shutdown URL.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application shutdown url

        """
        api_url = URL(api_endpoint)
        return str(
            api_url.with_path(
                f"/kubernetes/namespaces/{namespace}/applications/{name}/shutdown"
            )
        )

    def env_vars(self, secret_name):
        """Create shutdown hooks environment variables sub-resources.

        Creates shutdown hook environment variables to store Krake authentication token
        and a shutdown hook URL for given applications.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of shutdown hook environment variables sub-resources

        """
        sub_resources = []

        env_resources = []

        env_token = V1EnvVar(
            name=self.env_token,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(
                            name=secret_name, key=self.env_token.lower()
                        )
                    )
                )
            ),
        )
        env_resources.append(env_token)

        env_url = V1EnvVar(
            name=self.env_url,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())
                    )
                )
            ),
        )
        env_resources.append(env_url)

        for env in env_resources:
            sub_resources.append(
                SubResource(
                    group="env",
                    name=env.name,
                    body=self.attribute_map(env),
                    path=(
                        ("spec", "template", "spec", "containers"),
                        ("spec", "containers"),  # kind: Pod
                    ),
                )
            )
        return sub_resources

if __name__ == "__main__":
    import dill
    import os
    sys.path.append("/home/travis/builds/repos/rak-n-rok---Krake/krake/tests")

    isT = True
    from factories.kubernetes import (
        ApplicationFactory,
        ClusterFactory,
        make_kubeconfig,
    )
    from controller.kubernetes import (
        deployment_manifest,
        service_manifest,
        secret_manifest,
        nginx_manifest,
        custom_deployment_observer_schema,
        custom_service_observer_schema,
        custom_observer_schema,
        deployment_response,
        service_response,
        secret_response,
        initial_last_observed_manifest_deployment,
        initial_last_observed_manifest_service,
        initial_last_observed_manifest,
    )

    app = ApplicationFactory(
        spec__manifest=deepcopy(nginx_manifest),
        spec__observer_schema=deepcopy(custom_observer_schema),
    )

    generate_default_observer_schema(app)
    print(app.spec.observer_schema[0]["apiVersion"])



----------------------------
/home/travis/builds/repos/cpburnz---python-sql-parameters/sqlparams/__init___format_passk_validte.py
"""
:mod:`sqlparams` is a utility package for converting between various SQL
parameter styles.
"""
import sys
sys.path.append("/home/travis/builds/repos/cpburnz---python-sql-parameters/")
import re
from typing import (
    Any,
    Dict,
    Iterable,
    List,
    Optional,
    Pattern,
    Sequence,
    Tuple,
    Type,
    Union)

from sqlparams import _converting
from sqlparams import _styles
from sqlparams import _util

from sqlparams._meta import (
    __author__,
    __copyright__,
    __credits__,
    __license__,
    __version__)
from sqlparams._util import (
    SqlStr)

_BYTES_ENCODING = 'latin1'
"""
The encoding to use when parsing a byte query string.
"""

DEFAULT_COMMENTS: Sequence[Union[str, Tuple[str, str]]] = (
    ("/*", "*/"),
    "--",
)
"""
The default comment styles to strip. This strips single line comments
beginning with :data:`"--"` and multiline comments beginning with
:data:`"/*"` and ending with :data:`"*/"`.
"""


class SQLParams(object):
    """
    The :class:`.SQLParams` class is used to support named parameters in
    SQL queries where they are not otherwise supported (e.g., pyodbc).
    This is done by converting from one parameter style query to another
    parameter style query.

    By default, when converting to a numeric or ordinal style any
    :class:`tuple` parameter will be expanded into "(?,?,...)" to support
    the widely used "IN {tuple}" SQL expression without leaking any
    unescaped values.
    """

    def __init__(
            self,
            in_style: str,
            out_style: str,
            escape_char: Union[str, bool, None] = None,
            expand_tuples: Optional[bool] = None,
            strip_comments: Union[Sequence[Union[str, Tuple[str, str]]], bool, None] = None,
    ) -> None:
        """
        Instantiates the :class:`.SQLParams` instance.

        *in_style* (:class:`str`) is the parameter style that will be used
        in an SQL query before being parsed and converted to :attr:`.SQLParams.out_style`.

        *out_style* (:class:`str`) is the parameter style that the SQL query
        will be converted to.

        *escape_char* (:class:`str`, :class:`bool`, or :data:`None`) is the
        escape character used to prevent matching an in-style parameter. If
        :data:`True`, use the default escape character (repeat the initial
        character to escape it; e.g., "%%"). If :data:`False`, do not use an
        escape character. Default is :data:`None` for :data:`False`.

        *expand_tuples* (:class:`bool` or :data:`None`) is whether to
        expand tuples into a sequence of parameters. Default is :data:`None`
        to let it be determined by *out_style* (to maintain backward
        compatibility). If *out_style* is a numeric or ordinal style, expand
        tuples by default (:data:`True`). If *out_style* is a named style,
        do not expand tuples by default (:data:`False`).

        .. NOTE:: Empty tuples will be safely expanded to :data:`(NULL)` to
           prevent SQL syntax errors,

        *strip_comments* (:class:`Sequence`, :class:`bool`, or :data:`None`)
        whether to strip out comments and what style of comments to remove.
        If a :class:`Sequence`, this defines the comment styles. A single
        line comment is defined using a :class:`str` (e.g., :data:`"--"` or
        :data:`"#"`). A multiline comment is defined using a :class:`tuple`
        of :class:`str` (e.g., :data:`("/*", "*/")`). In order for a comment
        to be matched, it must be the first string of non-whitespace
        characters on the line. Trailing comments are not supported and will
        be ignored. A multiline comment will consume characters until the
        ending string is matched. If :data:`True`, :data:`DEFAULT_COMMENTS`
        will be used (:data:`"--"` and :data:`("/*", "*/")` styles). Default
        is :data:`None` to not remove comments.

        The following parameter styles are supported by both *in_style* and
        *out_style*:

        -	For all named styles the parameter keys must be valid `Python identifiers`_.
            They cannot start with a digit. This is to help prevent
            incorrectly matching common strings such as datetimes.

            Named styles:

            -	"named" indicates parameters will use the named style::

                    ... WHERE name = :name

            -	"named_dollar" indicates parameters will use the named dollar
                sign style::

                    ... WHERE name = $name

                .. NOTE:: This is not defined by `PEP 249`_.

            -	"pyformat" indicates parameters will use the named Python
                extended format style::

                    ... WHERE name = %(name)s

                .. NOTE:: Strictly speaking, `PEP 249`_ only specifies
                   "%(name)s" for the "pyformat" parameter style so only that
                   form (without any other conversions or flags) is supported.

        -	All numeric styles start at :data:`1`. When using a
            :class:`~collections.abc.Sequence` for the parameters, the 1st
            parameter (e.g., ":1") will correspond to the 1st element of the
            sequence (i.e., index :data:`0`). When using a :class:`~collections.abc.Mapping`
            for the parameters, the 1st parameter (e.g., ":1") will correspond
            to the matching key (i.e., :data:`1` or :data:`"1"`).

            Numeric styles:

            -	"numeric" indicates parameters will use the numeric style::

                    ... WHERE name = :1

            -	"numeric_dollar" indicates parameters will use the numeric
                dollar sign style (starts at :data:`1`)::

                    ... WHERE name = $1

                .. NOTE:: This is not defined by `PEP 249`_.

        - Ordinal styles:

            -	"format" indicates parameters will use the ordinal Python format
                style::

                    ... WHERE name = %s

                .. NOTE:: Strictly speaking, `PEP 249`_ only specifies "%s" for
                   the "format" parameter styles so only that form (without any
                   other conversions or flags) is supported.

            -	"qmark" indicates parameters will use the ordinal question mark
                style::

                    ... WHERE name = ?

        .. _`PEP 249`: http://www.python.org/dev/peps/pep-0249/

        .. _`Python identifiers`: https://docs.python.org/3/reference/lexical_analysis.html#identifiers
        """

        if not isinstance(in_style, str):
            raise TypeError("in_style:{!r} is not a string.".format(in_style))

        if not isinstance(out_style, str):
            raise TypeError("out_style:{!r} is not a string.".format(out_style))

        in_obj = _styles.STYLES[in_style]
        out_obj = _styles.STYLES[out_style]

        if escape_char is True:
            use_char = in_obj.escape_char
        elif not escape_char:
            use_char = None
        elif isinstance(escape_char, str):
            use_char = escape_char
        else:
            raise TypeError("escape_char:{!r} is not a string or bool.")

        if expand_tuples is None:
            expand_tuples = not isinstance(out_obj, _styles.NamedStyle)
        else:
            expand_tuples = bool(expand_tuples)

        if strip_comments is True:
            strip_comments = DEFAULT_COMMENTS
        elif strip_comments is False:
            strip_comments = None

        in_regex = self.__create_in_regex(
            escape_char=use_char,
            in_obj=in_obj,
            out_obj=out_obj,
        )

        self.__converter: _converting.Converter = self.__create_converter(
            escape_char=use_char,
            expand_tuples=expand_tuples,
            in_obj=in_obj,
            in_regex=in_regex,
            in_style=in_style,
            out_obj=out_obj,
            out_style=out_style,
        )
        """
        *__converter* (:class:`._converting.Converter`) is the parameter
        converter to use.
        """

        self.__escape_char: Optional[str] = use_char
        """
        *__escape_char* (:class:`str` or :data:`None`) is the escape
        character used to prevent matching an in-style parameter.
        """

        self.__expand_tuples: bool = expand_tuples
        """
        *__expand_tuples* (:class:`bool`) is whether to convert tuples into
        a sequence of parameters.
        """

        self.__in_obj: _styles.Style = in_obj
        """
        *__in_obj* (:class:`._styles.Style`) is the in-style parameter
        object.
        """

        self.__in_regex: Pattern = in_regex
        """
        *__in_regex* (:class:`re.Pattern`) is the regular expression used to
        extract the in-style parameters.
        """

        self.__in_style: str = in_style
        """
        *__in_style* (:class:`str`) is the parameter style that will be used
        in an SQL query before being parsed and converted to :attr:`.SQLParams.out_style`.
        """

        self.__out_obj: _styles.Style = out_obj
        """
        *__out_obj* (:class:`._styles.Style`) is the out-style parameter
        object.
        """

        self.__out_style: str = out_style
        """
        *__out_style* (:class:`str`) is the parameter style that the SQL
        query will be converted to.
        """

        self.__strip_comment_regexes: List[Pattern] = self.__create_strip_comment_regexes(
            strip_comments=strip_comments,
        )
        """
        *__strip_comment_regexes* (:class:`list` of :class:`Pattern`)
        contains the regular expressions to strip out comments.
        """

        self.__strip_comments: Optional[Sequence[Union[str, Tuple[str, str]]]] = strip_comments
        """
        *__strip_comments* (:class:`Sequence` or :data:`None`) contains the
        comment styles to remove.
        """

    def __repr__(self) -> str:
        """
        Returns the canonical string representation (:class:`str`) of this
        instance.
        """
        return "{}.{}({!r}, {!r})".format(
            self.__class__.__module__,
            self.__class__.__name__,
            self.__in_style,
            self.__out_style,
        )

    @staticmethod
    def __create_converter(
            escape_char: Optional[str],
            expand_tuples: bool,
            in_obj: _styles.Style,
            in_regex: Pattern,
            in_style: str,
            out_obj: _styles.Style,
            out_style: str,
    ) -> _converting.Converter:
        """
        Create the parameter style converter.

        *escape_char* (:class:`str` or :data:`None`) is the escape character
        used to prevent matching an in-style parameter.

        *expand_tuples* (:class:`bool`) is whether to convert tuples into a
        sequence of parameters.

        *in_obj* (:class:`._styles.Style`) is the in-style parameter object.

        *in_style* (:class:`str`) is the in-style name.

        *in_regex* (:class:`re.Pattern`) is the regular expression used to
        extract the in-style parameters.

        *out_obj* (:class:`._styles.Style`) is the out-style parameter
        object.

        *out_style* (:class:`str`) is the out-style name.

        Returns the parameter style converter (:class:`._converting.Converter`).
        """
        # Determine converter class.
        converter_class: Type[_converting.Converter]
        if isinstance(in_obj, _styles.NamedStyle):
            if isinstance(out_obj, _styles.NamedStyle):
                converter_class = _converting.NamedToNamedConverter
            elif isinstance(out_obj, _styles.NumericStyle):
                converter_class = _converting.NamedToNumericConverter
            elif isinstance(out_obj, _styles.OrdinalStyle):
                converter_class = _converting.NamedToOrdinalConverter
            else:
                raise TypeError("out_style:{!r} maps to an unexpected type: {!r}".format(
                    out_style,
                    out_obj,
                ))

        elif isinstance(in_obj, _styles.NumericStyle):
            if isinstance(out_obj, _styles.NamedStyle):
                converter_class = _converting.NumericToNamedConverter
            elif isinstance(out_obj, _styles.NumericStyle):
                converter_class = _converting.NumericToNumericConverter
            elif isinstance(out_obj, _styles.OrdinalStyle):
                converter_class = _converting.NumericToOrdinalConverter
            else:
                raise TypeError("out_style:{!r} maps to an unexpected type: {!r}".format(
                    out_style,
                    out_obj,
                ))

        elif isinstance(in_obj, _styles.OrdinalStyle):
            if isinstance(out_obj, _styles.NamedStyle):
                converter_class = _converting.OrdinalToNamedConverter
            elif isinstance(out_obj, _styles.NumericStyle):
                converter_class = _converting.OrdinalToNumericConverter
            elif isinstance(out_obj, _styles.OrdinalStyle):
                converter_class = _converting.OrdinalToOrdinalConverter
            else:
                raise TypeError("out_style:{!r} maps to an unexpected type: {!r}".format(
                    out_style,
                    out_obj,
                ))

        else:
            raise TypeError("in_style:{!r} maps to an unexpected type: {!r}".format(
                in_style,
                in_obj,
            ))

        # Create converter.
        converter = converter_class(
            escape_char=escape_char,
            expand_tuples=expand_tuples,
            in_regex=in_regex,
            in_style=in_obj,
            out_style=out_obj,
        )
        return converter

    @staticmethod
    def __create_in_regex(
            escape_char: str,
            in_obj: _styles.Style,
            out_obj: _styles.Style,
    ) -> Pattern:
        """
        Create the in-style parameter regular expression.

        *escape_char* (:class:`str` or :data:`None`) is the escape character
        sed to prevent matching an in-style parameter.

        *in_obj* (:class:`._styles.Style`) is the in-style parameter object.

        *out_obj* (:class:`._styles.Style`) is the out-style parameter
        object.

        Returns the in-style parameter regular expression (:class:`re.Pattern`).
        """
        regex_parts = []

        if in_obj.escape_char != "%" and out_obj.escape_char == "%":
            regex_parts.append("(?P<out_percent>%)")

        if escape_char:
            # Escaping is enabled.
            escape = in_obj.escape_regex.format(char=re.escape(escape_char))
            regex_parts.append(escape)

        regex_parts.append(in_obj.param_regex)

        return re.compile("|".join(regex_parts))

    @staticmethod
    def __create_strip_comment_regexes(
            strip_comments: Optional[Sequence[Union[str, Tuple[str, str]]]],
    ) -> List[Pattern]:
        """
        Create the regular expressions to strip comments.

        *strip_comments* (:class:`Sequence` or :data:`None`) contains the
        comment styles to remove.

        Returns the regular expressions (:class:`list` of :class:`re.Pattern`).
        """
        if strip_comments is None:
            return []

        out_regexes = []
        for i, comment_style in enumerate(strip_comments):
            if isinstance(comment_style, str):
                # Compile regular expression to strip single line comment.
                out_regexes.append(re.compile("^[ \t]*{comment}.*(?:\n|\r\n)?".format(
                    comment=re.escape(comment_style),
                ), re.M))

            elif _util.is_sequence(comment_style):
                # Compile regular expression to strip multiline comment.
                start_comment, end_comment = comment_style  # type: str
                out_regexes.append(re.compile("^[ \t]*{start}.*?{end}(?:\n|\r\n)?".format(
                    start=re.escape(start_comment),
                    end=re.escape(end_comment),
                ), re.DOTALL | re.M))
                pass

            else:
                raise TypeError("strip_comments[{}]:{!r} must be either a str or tuple.".format(
                    i,
                    comment_style,
                ))

        return out_regexes

    @property
    def escape_char(self) -> Optional[str]:
        """
        *escape_char* (:class:`str` or :data:`None`) is the escape character
        used to prevent matching an in-style parameter.
        """
        return self.__escape_char

    @property
    def expand_tuples(self) -> bool:
        """
        *expand_tuples* (:class:`bool`) is whether to convert tuples into a
        sequence of parameters.
        """
        return self.__expand_tuples

    def format(
            self,
            sql: SqlStr,
            params: Union[Dict[Union[str, int], Any], Sequence[Any]],
    ) -> Tuple[SqlStr, Union[Dict[str, Any], Sequence[Any]]]:
        """
        Convert the SQL query to use the out-style parameters instead of
        the in-style parameters.

        *sql* (:class:`LiteralString`, :class:`str`, or :class:`bytes`) is
        the SQL query.

        *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)
        contains the set of in-style parameters. It maps each parameter
        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`
        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.
        If :attr:`.SQLParams.in_style` is an ordinal parameter style, then
        *params* must be a :class:`~collections.abc.Sequence`.

        Returns a :class:`tuple` containing:

        -	The formatted SQL query (:class:`LiteralString`, :class:`str` or
            :class:`bytes`).

        -	The set of converted out-style parameters (:class:`dict` or
            :class:`list`).
        """
        # Normalize query encoding to simplify processing.
        if isinstance(sql, str):
            use_sql = sql
            string_type = str
        elif isinstance(sql, bytes):
            use_sql = sql.decode(_BYTES_ENCODING)
            string_type = bytes
        else:
            raise TypeError("sql:{!r} is not a unicode or byte string.".format(sql))

        # Strip comments.
        use_sql = self.__strip_comments_from_sql(use_sql)

        # Replace in-style with out-style parameters.
        use_sql, out_params = self.__converter.convert(use_sql, params)

        # Make sure the query is returned as the proper string type.
        if string_type is bytes:
            out_sql = use_sql.encode(_BYTES_ENCODING)
        else:
            out_sql = use_sql

        # Return converted SQL and out-parameters.
        return out_sql, out_params

    def formatmany(
            self,
            sql: SqlStr,
            many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],
    ) -> Tuple[SqlStr, Union[List[Dict[str, Any]], List[Sequence[Any]]]]:
        """
        Convert the SQL query to use the out-style parameters instead of the
        in-style parameters.

        *sql* (:class:`LiteralString`, :class:`str` or :class:`bytes`) is
        the SQL query.

        *many_params* (:class:`~collections.abc.Iterable`) contains each set
        of in-style parameters (*params*).

        -	*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)
            contains the set of in-style parameters. It maps each parameter
            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`
            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.
            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then
            *params* must be a :class:`~collections.abc.Sequence`.

        Returns a :class:`tuple` containing:

        -	The formatted SQL query (:class:`LiteralString`, :class:`str` or
            :class:`bytes`).

        -	A :class:`list` containing each set of converted out-style
            parameters (:class:`dict` or :class:`list`).
        """
        # Normalize query encoding to simplify processing.
        if isinstance(sql, str):
            use_sql = sql
            string_type = str
        elif isinstance(sql, bytes):
            use_sql = sql.decode(_BYTES_ENCODING)
            string_type = bytes
        else:
            raise TypeError("sql:{!r} is not a unicode or byte string.".format(sql))

        if not _util.is_iterable(many_params):
            raise TypeError("many_params:{!r} is not iterable.".format(many_params))

        # Strip comments.
        use_sql = self.__strip_comments_from_sql(use_sql)

        # Replace in-style with out-style parameters.
        use_sql, many_out_params = self.__converter.convert_many(use_sql, many_params)

        # Make sure the query is returned as the proper string type.
        if string_type is bytes:
            out_sql = use_sql.encode(_BYTES_ENCODING)
        else:
            out_sql = use_sql

        # Return converted SQL and out-parameters.
        return out_sql, many_out_params

    @property
    def in_style(self) -> str:
        """
        *in_style* (:class:`str`) is the parameter style to expect in an SQL
        query when being parsed.
        """
        return self.__in_style

    @property
    def out_style(self) -> str:
        """
        *out_style* (:class:`str`) is the parameter style that the SQL query
        will be converted to.
        """
        return self.__out_style

    @property
    def strip_comments(self) -> Optional[Sequence[Union[str, Tuple[str, str]]]]:
        """
        *strip_comments* (:class:`Sequence` or :data:`None`) contains the
        comment styles to remove.
        """
        return self.__strip_comments

    def __strip_comments_from_sql(self, sql: str) -> str:
        """
        Strip comments from the SQL.

        *sql* (:class:`str`) is the SQL query.

        Returns the stripped SQL query (:class:`str`).
        """
        out_sql = sql
        for comment_regex in self.__strip_comment_regexes:
            out_sql = comment_regex.sub("", out_sql)

        return out_sql


if __name__ == "__main__":
    isT = True
    query = SQLParams('numeric_dollar', 'format')
    src_sql = """
    			SELECT *
    			FROM users
    			WHERE id = $1 OR name = $2;
    		"""
    id, name = 1, "Dwalin"
    seq_params = [id, name]
    int_params = {1: id, 2: name}
    str_params = {'1': id, '2': name}
    listt=[]
    # Desired SQL and params.
    dest_sql = """
    			SELECT *
    			FROM users
    			WHERE id = %s OR name = %s;
    		"""
    dest_params = [id, name]

    for src_params, src in zip([seq_params, int_params, str_params], ['seq', 'int', 'str']):
        sql, params = query.format(src_sql, src_params)
        if sql!=dest_sql or params!=[1, 'Dwalin']:
            isT=False
            break
    if not isT:
        raise Exception("Result not True!!!")

----------------------------
/home/travis/builds/repos/cpburnz---python-sql-parameters/sqlparams/__init___formatmany_passk_validte.py
"""
:mod:`sqlparams` is a utility package for converting between various SQL
parameter styles.
"""
import sys
sys.path.append("/home/travis/builds/repos/cpburnz---python-sql-parameters/")
import re
from typing import (
    Any,
    Dict,
    Iterable,
    List,
    Optional,
    Pattern,
    Sequence,
    Tuple,
    Type,
    Union)

from sqlparams import _converting
from sqlparams import _styles
from sqlparams import _util

from sqlparams._meta import (
    __author__,
    __copyright__,
    __credits__,
    __license__,
    __version__)
from sqlparams._util import (
    SqlStr)

_BYTES_ENCODING = 'latin1'
"""
The encoding to use when parsing a byte query string.
"""

DEFAULT_COMMENTS: Sequence[Union[str, Tuple[str, str]]] = (
    ("/*", "*/"),
    "--",
)
"""
The default comment styles to strip. This strips single line comments
beginning with :data:`"--"` and multiline comments beginning with
:data:`"/*"` and ending with :data:`"*/"`.
"""


class SQLParams(object):
    """
    The :class:`.SQLParams` class is used to support named parameters in
    SQL queries where they are not otherwise supported (e.g., pyodbc).
    This is done by converting from one parameter style query to another
    parameter style query.

    By default, when converting to a numeric or ordinal style any
    :class:`tuple` parameter will be expanded into "(?,?,...)" to support
    the widely used "IN {tuple}" SQL expression without leaking any
    unescaped values.
    """

    def __init__(
            self,
            in_style: str,
            out_style: str,
            escape_char: Union[str, bool, None] = None,
            expand_tuples: Optional[bool] = None,
            strip_comments: Union[Sequence[Union[str, Tuple[str, str]]], bool, None] = None,
    ) -> None:
        """
        Instantiates the :class:`.SQLParams` instance.

        *in_style* (:class:`str`) is the parameter style that will be used
        in an SQL query before being parsed and converted to :attr:`.SQLParams.out_style`.

        *out_style* (:class:`str`) is the parameter style that the SQL query
        will be converted to.

        *escape_char* (:class:`str`, :class:`bool`, or :data:`None`) is the
        escape character used to prevent matching an in-style parameter. If
        :data:`True`, use the default escape character (repeat the initial
        character to escape it; e.g., "%%"). If :data:`False`, do not use an
        escape character. Default is :data:`None` for :data:`False`.

        *expand_tuples* (:class:`bool` or :data:`None`) is whether to
        expand tuples into a sequence of parameters. Default is :data:`None`
        to let it be determined by *out_style* (to maintain backward
        compatibility). If *out_style* is a numeric or ordinal style, expand
        tuples by default (:data:`True`). If *out_style* is a named style,
        do not expand tuples by default (:data:`False`).

        .. NOTE:: Empty tuples will be safely expanded to :data:`(NULL)` to
           prevent SQL syntax errors,

        *strip_comments* (:class:`Sequence`, :class:`bool`, or :data:`None`)
        whether to strip out comments and what style of comments to remove.
        If a :class:`Sequence`, this defines the comment styles. A single
        line comment is defined using a :class:`str` (e.g., :data:`"--"` or
        :data:`"#"`). A multiline comment is defined using a :class:`tuple`
        of :class:`str` (e.g., :data:`("/*", "*/")`). In order for a comment
        to be matched, it must be the first string of non-whitespace
        characters on the line. Trailing comments are not supported and will
        be ignored. A multiline comment will consume characters until the
        ending string is matched. If :data:`True`, :data:`DEFAULT_COMMENTS`
        will be used (:data:`"--"` and :data:`("/*", "*/")` styles). Default
        is :data:`None` to not remove comments.

        The following parameter styles are supported by both *in_style* and
        *out_style*:

        -	For all named styles the parameter keys must be valid `Python identifiers`_.
            They cannot start with a digit. This is to help prevent
            incorrectly matching common strings such as datetimes.

            Named styles:

            -	"named" indicates parameters will use the named style::

                    ... WHERE name = :name

            -	"named_dollar" indicates parameters will use the named dollar
                sign style::

                    ... WHERE name = $name

                .. NOTE:: This is not defined by `PEP 249`_.

            -	"pyformat" indicates parameters will use the named Python
                extended format style::

                    ... WHERE name = %(name)s

                .. NOTE:: Strictly speaking, `PEP 249`_ only specifies
                   "%(name)s" for the "pyformat" parameter style so only that
                   form (without any other conversions or flags) is supported.

        -	All numeric styles start at :data:`1`. When using a
            :class:`~collections.abc.Sequence` for the parameters, the 1st
            parameter (e.g., ":1") will correspond to the 1st element of the
            sequence (i.e., index :data:`0`). When using a :class:`~collections.abc.Mapping`
            for the parameters, the 1st parameter (e.g., ":1") will correspond
            to the matching key (i.e., :data:`1` or :data:`"1"`).

            Numeric styles:

            -	"numeric" indicates parameters will use the numeric style::

                    ... WHERE name = :1

            -	"numeric_dollar" indicates parameters will use the numeric
                dollar sign style (starts at :data:`1`)::

                    ... WHERE name = $1

                .. NOTE:: This is not defined by `PEP 249`_.

        - Ordinal styles:

            -	"format" indicates parameters will use the ordinal Python format
                style::

                    ... WHERE name = %s

                .. NOTE:: Strictly speaking, `PEP 249`_ only specifies "%s" for
                   the "format" parameter styles so only that form (without any
                   other conversions or flags) is supported.

            -	"qmark" indicates parameters will use the ordinal question mark
                style::

                    ... WHERE name = ?

        .. _`PEP 249`: http://www.python.org/dev/peps/pep-0249/

        .. _`Python identifiers`: https://docs.python.org/3/reference/lexical_analysis.html#identifiers
        """

        if not isinstance(in_style, str):
            raise TypeError("in_style:{!r} is not a string.".format(in_style))

        if not isinstance(out_style, str):
            raise TypeError("out_style:{!r} is not a string.".format(out_style))

        in_obj = _styles.STYLES[in_style]
        out_obj = _styles.STYLES[out_style]

        if escape_char is True:
            use_char = in_obj.escape_char
        elif not escape_char:
            use_char = None
        elif isinstance(escape_char, str):
            use_char = escape_char
        else:
            raise TypeError("escape_char:{!r} is not a string or bool.")

        if expand_tuples is None:
            expand_tuples = not isinstance(out_obj, _styles.NamedStyle)
        else:
            expand_tuples = bool(expand_tuples)

        if strip_comments is True:
            strip_comments = DEFAULT_COMMENTS
        elif strip_comments is False:
            strip_comments = None

        in_regex = self.__create_in_regex(
            escape_char=use_char,
            in_obj=in_obj,
            out_obj=out_obj,
        )

        self.__converter: _converting.Converter = self.__create_converter(
            escape_char=use_char,
            expand_tuples=expand_tuples,
            in_obj=in_obj,
            in_regex=in_regex,
            in_style=in_style,
            out_obj=out_obj,
            out_style=out_style,
        )
        """
        *__converter* (:class:`._converting.Converter`) is the parameter
        converter to use.
        """

        self.__escape_char: Optional[str] = use_char
        """
        *__escape_char* (:class:`str` or :data:`None`) is the escape
        character used to prevent matching an in-style parameter.
        """

        self.__expand_tuples: bool = expand_tuples
        """
        *__expand_tuples* (:class:`bool`) is whether to convert tuples into
        a sequence of parameters.
        """

        self.__in_obj: _styles.Style = in_obj
        """
        *__in_obj* (:class:`._styles.Style`) is the in-style parameter
        object.
        """

        self.__in_regex: Pattern = in_regex
        """
        *__in_regex* (:class:`re.Pattern`) is the regular expression used to
        extract the in-style parameters.
        """

        self.__in_style: str = in_style
        """
        *__in_style* (:class:`str`) is the parameter style that will be used
        in an SQL query before being parsed and converted to :attr:`.SQLParams.out_style`.
        """

        self.__out_obj: _styles.Style = out_obj
        """
        *__out_obj* (:class:`._styles.Style`) is the out-style parameter
        object.
        """

        self.__out_style: str = out_style
        """
        *__out_style* (:class:`str`) is the parameter style that the SQL
        query will be converted to.
        """

        self.__strip_comment_regexes: List[Pattern] = self.__create_strip_comment_regexes(
            strip_comments=strip_comments,
        )
        """
        *__strip_comment_regexes* (:class:`list` of :class:`Pattern`)
        contains the regular expressions to strip out comments.
        """

        self.__strip_comments: Optional[Sequence[Union[str, Tuple[str, str]]]] = strip_comments
        """
        *__strip_comments* (:class:`Sequence` or :data:`None`) contains the
        comment styles to remove.
        """

    def __repr__(self) -> str:
        """
        Returns the canonical string representation (:class:`str`) of this
        instance.
        """
        return "{}.{}({!r}, {!r})".format(
            self.__class__.__module__,
            self.__class__.__name__,
            self.__in_style,
            self.__out_style,
        )

    @staticmethod
    def __create_converter(
            escape_char: Optional[str],
            expand_tuples: bool,
            in_obj: _styles.Style,
            in_regex: Pattern,
            in_style: str,
            out_obj: _styles.Style,
            out_style: str,
    ) -> _converting.Converter:
        """
        Create the parameter style converter.

        *escape_char* (:class:`str` or :data:`None`) is the escape character
        used to prevent matching an in-style parameter.

        *expand_tuples* (:class:`bool`) is whether to convert tuples into a
        sequence of parameters.

        *in_obj* (:class:`._styles.Style`) is the in-style parameter object.

        *in_style* (:class:`str`) is the in-style name.

        *in_regex* (:class:`re.Pattern`) is the regular expression used to
        extract the in-style parameters.

        *out_obj* (:class:`._styles.Style`) is the out-style parameter
        object.

        *out_style* (:class:`str`) is the out-style name.

        Returns the parameter style converter (:class:`._converting.Converter`).
        """
        # Determine converter class.
        converter_class: Type[_converting.Converter]
        if isinstance(in_obj, _styles.NamedStyle):
            if isinstance(out_obj, _styles.NamedStyle):
                converter_class = _converting.NamedToNamedConverter
            elif isinstance(out_obj, _styles.NumericStyle):
                converter_class = _converting.NamedToNumericConverter
            elif isinstance(out_obj, _styles.OrdinalStyle):
                converter_class = _converting.NamedToOrdinalConverter
            else:
                raise TypeError("out_style:{!r} maps to an unexpected type: {!r}".format(
                    out_style,
                    out_obj,
                ))

        elif isinstance(in_obj, _styles.NumericStyle):
            if isinstance(out_obj, _styles.NamedStyle):
                converter_class = _converting.NumericToNamedConverter
            elif isinstance(out_obj, _styles.NumericStyle):
                converter_class = _converting.NumericToNumericConverter
            elif isinstance(out_obj, _styles.OrdinalStyle):
                converter_class = _converting.NumericToOrdinalConverter
            else:
                raise TypeError("out_style:{!r} maps to an unexpected type: {!r}".format(
                    out_style,
                    out_obj,
                ))

        elif isinstance(in_obj, _styles.OrdinalStyle):
            if isinstance(out_obj, _styles.NamedStyle):
                converter_class = _converting.OrdinalToNamedConverter
            elif isinstance(out_obj, _styles.NumericStyle):
                converter_class = _converting.OrdinalToNumericConverter
            elif isinstance(out_obj, _styles.OrdinalStyle):
                converter_class = _converting.OrdinalToOrdinalConverter
            else:
                raise TypeError("out_style:{!r} maps to an unexpected type: {!r}".format(
                    out_style,
                    out_obj,
                ))

        else:
            raise TypeError("in_style:{!r} maps to an unexpected type: {!r}".format(
                in_style,
                in_obj,
            ))

        # Create converter.
        converter = converter_class(
            escape_char=escape_char,
            expand_tuples=expand_tuples,
            in_regex=in_regex,
            in_style=in_obj,
            out_style=out_obj,
        )
        return converter

    @staticmethod
    def __create_in_regex(
            escape_char: str,
            in_obj: _styles.Style,
            out_obj: _styles.Style,
    ) -> Pattern:
        """
        Create the in-style parameter regular expression.

        *escape_char* (:class:`str` or :data:`None`) is the escape character
        sed to prevent matching an in-style parameter.

        *in_obj* (:class:`._styles.Style`) is the in-style parameter object.

        *out_obj* (:class:`._styles.Style`) is the out-style parameter
        object.

        Returns the in-style parameter regular expression (:class:`re.Pattern`).
        """
        regex_parts = []

        if in_obj.escape_char != "%" and out_obj.escape_char == "%":
            regex_parts.append("(?P<out_percent>%)")

        if escape_char:
            # Escaping is enabled.
            escape = in_obj.escape_regex.format(char=re.escape(escape_char))
            regex_parts.append(escape)

        regex_parts.append(in_obj.param_regex)

        return re.compile("|".join(regex_parts))

    @staticmethod
    def __create_strip_comment_regexes(
            strip_comments: Optional[Sequence[Union[str, Tuple[str, str]]]],
    ) -> List[Pattern]:
        """
        Create the regular expressions to strip comments.

        *strip_comments* (:class:`Sequence` or :data:`None`) contains the
        comment styles to remove.

        Returns the regular expressions (:class:`list` of :class:`re.Pattern`).
        """
        if strip_comments is None:
            return []

        out_regexes = []
        for i, comment_style in enumerate(strip_comments):
            if isinstance(comment_style, str):
                # Compile regular expression to strip single line comment.
                out_regexes.append(re.compile("^[ \t]*{comment}.*(?:\n|\r\n)?".format(
                    comment=re.escape(comment_style),
                ), re.M))

            elif _util.is_sequence(comment_style):
                # Compile regular expression to strip multiline comment.
                start_comment, end_comment = comment_style  # type: str
                out_regexes.append(re.compile("^[ \t]*{start}.*?{end}(?:\n|\r\n)?".format(
                    start=re.escape(start_comment),
                    end=re.escape(end_comment),
                ), re.DOTALL | re.M))
                pass

            else:
                raise TypeError("strip_comments[{}]:{!r} must be either a str or tuple.".format(
                    i,
                    comment_style,
                ))

        return out_regexes

    @property
    def escape_char(self) -> Optional[str]:
        """
        *escape_char* (:class:`str` or :data:`None`) is the escape character
        used to prevent matching an in-style parameter.
        """
        return self.__escape_char

    @property
    def expand_tuples(self) -> bool:
        """
        *expand_tuples* (:class:`bool`) is whether to convert tuples into a
        sequence of parameters.
        """
        return self.__expand_tuples

    def format(
            self,
            sql: SqlStr,
            params: Union[Dict[Union[str, int], Any], Sequence[Any]],
    ) -> Tuple[SqlStr, Union[Dict[str, Any], Sequence[Any]]]:
        """
        Convert the SQL query to use the out-style parameters instead of
        the in-style parameters.

        *sql* (:class:`LiteralString`, :class:`str`, or :class:`bytes`) is
        the SQL query.

        *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)
        contains the set of in-style parameters. It maps each parameter
        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`
        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.
        If :attr:`.SQLParams.in_style` is an ordinal parameter style, then
        *params* must be a :class:`~collections.abc.Sequence`.

        Returns a :class:`tuple` containing:

        -	The formatted SQL query (:class:`LiteralString`, :class:`str` or
            :class:`bytes`).

        -	The set of converted out-style parameters (:class:`dict` or
            :class:`list`).
        """
        # Normalize query encoding to simplify processing.
        if isinstance(sql, str):
            use_sql = sql
            string_type = str
        elif isinstance(sql, bytes):
            use_sql = sql.decode(_BYTES_ENCODING)
            string_type = bytes
        else:
            raise TypeError("sql:{!r} is not a unicode or byte string.".format(sql))

        # Strip comments.
        use_sql = self.__strip_comments_from_sql(use_sql)

        # Replace in-style with out-style parameters.
        use_sql, out_params = self.__converter.convert(use_sql, params)

        # Make sure the query is returned as the proper string type.
        if string_type is bytes:
            out_sql = use_sql.encode(_BYTES_ENCODING)
        else:
            out_sql = use_sql

        # Return converted SQL and out-parameters.
        return out_sql, out_params

    def formatmany(
            self,
            sql: SqlStr,
            many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],
    ) -> Tuple[SqlStr, Union[List[Dict[str, Any]], List[Sequence[Any]]]]:
        """
        Convert the SQL query to use the out-style parameters instead of the
        in-style parameters.

        *sql* (:class:`LiteralString`, :class:`str` or :class:`bytes`) is
        the SQL query.

        *many_params* (:class:`~collections.abc.Iterable`) contains each set
        of in-style parameters (*params*).

        -	*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)
            contains the set of in-style parameters. It maps each parameter
            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`
            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.
            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then
            *params* must be a :class:`~collections.abc.Sequence`.

        Returns a :class:`tuple` containing:

        -	The formatted SQL query (:class:`LiteralString`, :class:`str` or
            :class:`bytes`).

        -	A :class:`list` containing each set of converted out-style
            parameters (:class:`dict` or :class:`list`).
        """
        # Normalize query encoding to simplify processing.
        if isinstance(sql, str):
            use_sql = sql
            string_type = str
        elif isinstance(sql, bytes):
            use_sql = sql.decode(_BYTES_ENCODING)
            string_type = bytes
        else:
            raise TypeError("sql:{!r} is not a unicode or byte string.".format(sql))

        if not _util.is_iterable(many_params):
            raise TypeError("many_params:{!r} is not iterable.".format(many_params))

        # Strip comments.
        use_sql = self.__strip_comments_from_sql(use_sql)

        # Replace in-style with out-style parameters.
        use_sql, many_out_params = self.__converter.convert_many(use_sql, many_params)

        # Make sure the query is returned as the proper string type.
        if string_type is bytes:
            out_sql = use_sql.encode(_BYTES_ENCODING)
        else:
            out_sql = use_sql

        # Return converted SQL and out-parameters.
        return out_sql, many_out_params

    @property
    def in_style(self) -> str:
        """
        *in_style* (:class:`str`) is the parameter style to expect in an SQL
        query when being parsed.
        """
        return self.__in_style

    @property
    def out_style(self) -> str:
        """
        *out_style* (:class:`str`) is the parameter style that the SQL query
        will be converted to.
        """
        return self.__out_style

    @property
    def strip_comments(self) -> Optional[Sequence[Union[str, Tuple[str, str]]]]:
        """
        *strip_comments* (:class:`Sequence` or :data:`None`) contains the
        comment styles to remove.
        """
        return self.__strip_comments

    def __strip_comments_from_sql(self, sql: str) -> str:
        """
        Strip comments from the SQL.

        *sql* (:class:`str`) is the SQL query.

        Returns the stripped SQL query (:class:`str`).
        """
        out_sql = sql
        for comment_regex in self.__strip_comment_regexes:
            out_sql = comment_regex.sub("", out_sql)

        return out_sql


if __name__ == "__main__":
    isT = True
    query = SQLParams('numeric', 'qmark', expand_tuples=True)

    # Source SQL and params.
    src_sql = """
    			SELECT *
    			FROM users
    			WHERE race = :1 AND name IN :2;
    		"""
    base_params = [
        {'names': ("Dori", "Ori", "Nori"), 'race': "Dwarf"},
        {'names': ("Thorin",), 'race': "Dwarf"},
    ]
    seq_params = [[__row['race'], __row['names']] for __row in base_params]
    int_params = [{1: __row['race'], 2: __row['names']} for __row in base_params]
    str_params = [{'1': __row['race'], '2': __row['names']} for __row in base_params]

    for src_params, src in zip([seq_params, int_params, str_params], ['seq', 'int', 'str']):

        # Format SQL with params.
        # with self.assertRaisesRegex(ValueError, "length was expected to be 3.$"):
        try:
            query.formatmany(src_sql, src_params)
            isT=False
            break
        except ValueError as e:
            if str(e)!="many_params[1][1]:('Thorin',) length was expected to be 3.":
                isT=False
                break
    print(isT)
    if not isT:
        raise Exception("Result not True!!!")


----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/validator_validate_passk_validte.py
"""OCFL Validator.

Philosophy of this code is to keep it separate from the implementations
of Store, Object and Version used to build and manipulate OCFL data, but
to leverage lower level functions such as digest creation etc.. Code style
is plain/verbose with detailed and specific validation errors that might
help someone debug an implementation.

This code uses PyFilesystem (import fs) exclusively for access to files. This
should enable application beyond the operating system filesystem.
"""
import json
import re
import fs

from digest import file_digest, normalized_digest
from inventory_validator import InventoryValidator
from namaste import find_namastes
from pyfs import open_fs, ocfl_walk, ocfl_files_identical
from validation_logger import ValidationLogger


class ValidatorAbortException(Exception):
    """Exception class to bail out of validation."""


class Validator():
    """Class for OCFL Validator."""

    def __init__(self, log=None, show_warnings=False, show_errors=True, check_digests=True, lax_digests=False, lang='en'):
        """Initialize OCFL validator."""
        self.log = log
        self.check_digests = check_digests
        self.lax_digests = lax_digests
        if self.log is None:
            self.log = ValidationLogger(show_warnings=show_warnings, show_errors=show_errors, lang=lang)
        self.registered_extensions = [
            '0001-digest-algorithms', '0002-flat-direct-storage-layout',
            '0003-hash-and-id-n-tuple-storage-layout', '0004-hashed-n-tuple-storage-layout',
            '0005-mutable-head'
        ]
        # The following actually initialized in initialize() method
        self.id = None
        self.spec_version = None
        self.digest_algorithm = None
        self.content_directory = None
        self.inventory_digest_files = None
        self.root_inv_validator = None
        self.obj_fs = None
        self.initialize()

    def initialize(self):
        """Initialize object state.

        Must be called between attempts to validate objects.
        """
        self.id = None
        self.spec_version = '1.0'  # default to latest published version
        self.digest_algorithm = 'sha512'
        self.content_directory = 'content'
        self.inventory_digest_files = {}  # index by version_dir, algorithms may differ
        self.root_inv_validator = None
        self.obj_fs = None

    def status_str(self, prefix=''):
        """Return string representation of validation log, with optional prefix."""
        return self.log.status_str(prefix=prefix)

    def __str__(self):
        """Return string representation of validation log."""
        return self.status_str()

    def validate(self, path):
        """Validate OCFL object at path or pyfs root.

        Returns True if valid (warnings permitted), False otherwise.
        """
        self.initialize()
        try:
            if isinstance(path, str):
                self.obj_fs = open_fs(path)
            else:
                self.obj_fs = path
                path = self.obj_fs.desc('')
        except fs.errors.CreateFailed:
            self.log.error('E003e', path=path)
            return False
        # Object declaration, set spec version number. If there are multiple declarations,
        # look for the lastest object version then report any others as errors
        namastes = find_namastes(0, pyfs=self.obj_fs)
        if len(namastes) == 0:
            self.log.error('E003a', assumed_version=self.spec_version)
        else:
            spec_version = None
            for namaste in namastes:
                # Extract and check spec version number
                this_file_version = None
                for version in ('1.1', '1.0'):
                    if namaste.filename == '0=ocfl_object_' + version:
                        this_file_version = version
                        break
                if this_file_version is None:
                    self.log.error('E006', filename=namaste.filename)
                elif spec_version is None or this_file_version > spec_version:
                    spec_version = this_file_version
                    if not namaste.content_ok(pyfs=self.obj_fs):
                        self.log.error('E007', filename=namaste.filename)
            if spec_version is None:
                self.log.error('E003c', assumed_version=self.spec_version)
            else:
                self.spec_version = spec_version
                if len(namastes) > 1:
                    self.log.error('E003b', files=len(namastes), using_version=self.spec_version)
        # Object root inventory file
        inv_file = 'inventory.json'
        if not self.obj_fs.exists(inv_file):
            self.log.error('E063')
            return False
        try:
            inventory, inv_validator = self.validate_inventory(inv_file)
            inventory_is_valid = self.log.num_errors == 0
            self.root_inv_validator = inv_validator
            all_versions = inv_validator.all_versions
            self.id = inv_validator.id
            self.content_directory = inv_validator.content_directory
            self.digest_algorithm = inv_validator.digest_algorithm
            self.validate_inventory_digest(inv_file, self.digest_algorithm)
            # Object root
            self.validate_object_root(all_versions, already_checked=[namaste.filename for namaste in namastes])
            # Version inventory files
            (prior_manifest_digests, prior_fixity_digests) = self.validate_version_inventories(all_versions)
            if inventory_is_valid:
                # Object content
                self.validate_content(inventory, all_versions, prior_manifest_digests, prior_fixity_digests)
        except ValidatorAbortException:
            pass
        return self.log.num_errors == 0

    def validate_inventory(self, inv_file, where='root', extract_spec_version=False):
        """Validate a given inventory file, record errors with self.log.error().

        Returns inventory object for use in later validation
        of object content. Does not look at anything else in the
        object itself.

        where - used for reporting messages of where inventory is in object

        extract_spec_version - if set True will attempt to take spec_version from the
            inventory itself instead of using the spec_version provided
        """
        try:
            with self.obj_fs.openbin(inv_file, 'r') as fh:
                inventory = json.load(fh)
        except json.decoder.JSONDecodeError as e:
            self.log.error('E033', where=where, explanation=str(e))
            raise ValidatorAbortException
        inv_validator = InventoryValidator(log=self.log, where=where,
                                           lax_digests=self.lax_digests,
                                           spec_version=self.spec_version)
        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)
        return inventory, inv_validator

    def validate_inventory_digest(self, inv_file, digest_algorithm, where="root"):
        """Validate the appropriate inventory digest file in path."""
        inv_digest_file = inv_file + '.' + digest_algorithm
        if not self.obj_fs.exists(inv_digest_file):
            self.log.error('E058a', where=where, path=inv_digest_file)
        else:
            self.validate_inventory_digest_match(inv_file, inv_digest_file)

    def validate_inventory_digest_match(self, inv_file, inv_digest_file):
        """Validate a given inventory digest for a given inventory file.

        On error throws exception with debugging string intended to
        be presented to a user.
        """
        if not self.check_digests:
            return
        m = re.match(r'''.*\.(\w+)$''', inv_digest_file)
        if m:
            digest_algorithm = m.group(1)
            try:
                digest_recorded = self.read_inventory_digest(inv_digest_file)
                digest_actual = file_digest(inv_file, digest_algorithm, pyfs=self.obj_fs)
                if digest_actual != digest_recorded:
                    self.log.error("E060", inv_file=inv_file, actual=digest_actual, recorded=digest_recorded, inv_digest_file=inv_digest_file)
            except Exception as e:  # pylint: disable=broad-except
                self.log.error("E061", description=str(e))
        else:
            self.log.error("E058b", inv_digest_file=inv_digest_file)

    def validate_object_root(self, version_dirs, already_checked):
        """Validate object root.

        All expected_files must be present and no other files.
        All expected_dirs must be present and no other dirs.
        """
        expected_files = ['0=ocfl_object_' + self.spec_version, 'inventory.json',
                          'inventory.json.' + self.digest_algorithm]
        for entry in self.obj_fs.scandir(''):
            if entry.is_file:
                if entry.name not in expected_files and entry.name not in already_checked:
                    self.log.error('E001a', file=entry.name)
            elif entry.is_dir:
                if entry.name in version_dirs:
                    pass
                elif entry.name == 'extensions':
                    self.validate_extensions_dir()
                elif re.match(r'''v\d+$''', entry.name):
                    # Looks like a version directory so give more specific error
                    self.log.error('E046b', dir=entry.name)
                else:
                    # Simply an unexpected directory
                    self.log.error('E001b', dir=entry.name)
            else:
                self.log.error('E001c', entry=entry.name)

    def validate_extensions_dir(self):
        """Validate content of extensions directory inside object root.

        Validate the extensions directory by checking that there aren't any
        entries in the extensions directory that aren't directories themselves.
        Where there are extension directories they SHOULD be registered and
        this code relies up the registered_extensions property to list known
        extensions.
        """
        for entry in self.obj_fs.scandir('extensions'):
            if entry.is_dir:
                if entry.name not in self.registered_extensions:
                    self.log.warning('W013', entry=entry.name)
            else:
                self.log.error('E067', entry=entry.name)

    def validate_version_inventories(self, version_dirs):
        """Each version SHOULD have an inventory up to that point.

        Also keep a record of any content digests different from those in the root inventory
        so that we can also check them when validating the content.

        version_dirs is an array of version directory names and is assumed to be in
        version sequence (1, 2, 3...).
        """
        prior_manifest_digests = {}  # file -> algorithm -> digest -> [versions]
        prior_fixity_digests = {}  # file -> algorithm -> digest -> [versions]
        if len(version_dirs) == 0:
            return prior_manifest_digests, prior_fixity_digests
        last_version = version_dirs[-1]
        prev_version_dir = "NONE"  # will be set for first directory with inventory
        prev_spec_version = '1.0'  # lowest version
        for version_dir in version_dirs:
            inv_file = fs.path.join(version_dir, 'inventory.json')
            if not self.obj_fs.exists(inv_file):
                self.log.warning('W010', where=version_dir)
                continue
            # There is an inventory file for this version directory, check it
            if version_dir == last_version:
                # Don't validate in this case. Per the spec the inventory in the last version
                # MUST be identical to the copy in the object root, just check that
                root_inv_file = 'inventory.json'
                if not ocfl_files_identical(self.obj_fs, inv_file, root_inv_file):
                    self.log.error('E064', root_inv_file=root_inv_file, inv_file=inv_file)
                else:
                    # We could also just compare digest files but this gives a more helpful error for
                    # which file has the incorrect digest if they don't match
                    self.validate_inventory_digest(inv_file, self.digest_algorithm, where=version_dir)
                self.inventory_digest_files[version_dir] = 'inventory.json.' + self.digest_algorithm
                this_spec_version = self.spec_version
            else:
                # Note that inventories in prior versions may use different digest algorithms
                # from the current invenotory. Also,
                # an may accord with the same or earlier versions of the specification
                version_inventory, inv_validator = self.validate_inventory(inv_file, where=version_dir, extract_spec_version=True)
                this_spec_version = inv_validator.spec_version
                digest_algorithm = inv_validator.digest_algorithm
                self.validate_inventory_digest(inv_file, digest_algorithm, where=version_dir)
                self.inventory_digest_files[version_dir] = 'inventory.json.' + digest_algorithm
                if self.id and 'id' in version_inventory:
                    if version_inventory['id'] != self.id:
                        self.log.error('E037b', where=version_dir, root_id=self.id, version_id=version_inventory['id'])
                if 'manifest' in version_inventory:
                    # Check that all files listed in prior inventories are in manifest
                    not_seen = set(prior_manifest_digests.keys())
                    for digest in version_inventory['manifest']:
                        for filepath in version_inventory['manifest'][digest]:
                            # We rely on the validation to check that anything present is OK
                            if filepath in not_seen:
                                not_seen.remove(filepath)
                    if len(not_seen) > 0:
                        self.log.error('E023b', where=version_dir, missing_filepaths=', '.join(sorted(not_seen)))
                    # Record all prior digests
                    for unnormalized_digest in version_inventory['manifest']:
                        digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)
                        for filepath in version_inventory['manifest'][unnormalized_digest]:
                            if filepath not in prior_manifest_digests:
                                prior_manifest_digests[filepath] = {}
                            if digest_algorithm not in prior_manifest_digests[filepath]:
                                prior_manifest_digests[filepath][digest_algorithm] = {}
                            if digest not in prior_manifest_digests[filepath][digest_algorithm]:
                                prior_manifest_digests[filepath][digest_algorithm][digest] = []
                            prior_manifest_digests[filepath][digest_algorithm][digest].append(version_dir)
                # Is this inventory an appropriate prior version of the object root inventory?
                if self.root_inv_validator is not None:
                    self.root_inv_validator.validate_as_prior_version(inv_validator)
                # Fixity blocks are independent in each version. Record all values and the versions
                # they occur in for later checks against content
                if 'fixity' in version_inventory:
                    for digest_algorithm in version_inventory['fixity']:
                        for unnormalized_digest in version_inventory['fixity'][digest_algorithm]:
                            digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)
                            for filepath in version_inventory['fixity'][digest_algorithm][unnormalized_digest]:
                                if filepath not in prior_fixity_digests:
                                    prior_fixity_digests[filepath] = {}
                                if digest_algorithm not in prior_fixity_digests[filepath]:
                                    prior_fixity_digests[filepath][digest_algorithm] = {}
                                if digest not in prior_fixity_digests[filepath][digest_algorithm]:
                                    prior_fixity_digests[filepath][digest_algorithm][digest] = []
                                prior_fixity_digests[filepath][digest_algorithm][digest].append(version_dir)
            # We are validating the inventories in sequence and each new version must
            # follow the same or later spec version to previous inventories
            if prev_spec_version > this_spec_version:
                self.log.error('E103', where=version_dir, this_spec_version=this_spec_version,
                               prev_version_dir=prev_version_dir, prev_spec_version=prev_spec_version)
            prev_version_dir = version_dir
            prev_spec_version = this_spec_version
        return prior_manifest_digests, prior_fixity_digests

    def validate_content(self, inventory, version_dirs, prior_manifest_digests, prior_fixity_digests):
        """Validate file presence and content against inventory.

        The root inventory in `inventory` is assumed to be valid and safe to use
        for construction of file paths etc..
        """
        files_seen = set()
        # Check files in each version directory
        for version_dir in version_dirs:
            try:
                # Check contents of version directory except content_directory
                for entry in self.obj_fs.listdir(version_dir):
                    if ((entry == 'inventory.json')
                            or (version_dir in self.inventory_digest_files and entry == self.inventory_digest_files[version_dir])):
                        pass
                    elif entry == self.content_directory:
                        # Check content_directory
                        content_path = fs.path.join(version_dir, self.content_directory)
                        num_content_files_in_version = 0
                        for dirpath, dirs, files in ocfl_walk(self.obj_fs, content_path):
                            if dirpath != '/' + content_path and (len(dirs) + len(files)) == 0:
                                self.log.error("E024", where=version_dir, path=dirpath)
                            for file in files:
                                files_seen.add(fs.path.join(dirpath, file).lstrip('/'))
                                num_content_files_in_version += 1
                        if num_content_files_in_version == 0:
                            self.log.warning("W003", where=version_dir)
                    elif self.obj_fs.isdir(fs.path.join(version_dir, entry)):
                        self.log.warning("W002", where=version_dir, entry=entry)
                    else:
                        self.log.error("E015", where=version_dir, entry=entry)
            except (fs.errors.ResourceNotFound, fs.errors.DirectoryExpected):
                self.log.error('E046a', version_dir=version_dir)
        # Extract any digests in fixity and organize by filepath
        fixity_digests = {}
        if 'fixity' in inventory:
            for digest_algorithm in inventory['fixity']:
                for digest in inventory['fixity'][digest_algorithm]:
                    for filepath in inventory['fixity'][digest_algorithm][digest]:
                        if filepath in files_seen:
                            if filepath not in fixity_digests:
                                fixity_digests[filepath] = {}
                            if digest_algorithm not in fixity_digests[filepath]:
                                fixity_digests[filepath][digest_algorithm] = {}
                            if digest not in fixity_digests[filepath][digest_algorithm]:
                                fixity_digests[filepath][digest_algorithm][digest] = ['root']
                        else:
                            self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, digest=digest, content_path=filepath)
        # Check all files in root manifest
        if 'manifest' in inventory:
            for digest in inventory['manifest']:
                for filepath in inventory['manifest'][digest]:
                    if filepath not in files_seen:
                        self.log.error('E092b', where='root', content_path=filepath)
                    else:
                        if self.check_digests:
                            content_digest = file_digest(filepath, digest_type=self.digest_algorithm, pyfs=self.obj_fs)
                            if content_digest != normalized_digest(digest, digest_type=self.digest_algorithm):
                                self.log.error('E092a', where='root', digest_algorithm=self.digest_algorithm, digest=digest, content_path=filepath, content_digest=content_digest)
                            known_digests = {self.digest_algorithm: content_digest}
                            # Are there digest values in the fixity block?
                            self.check_additional_digests(filepath, known_digests, fixity_digests, 'E093a')
                            # Are there other digests for this same file from other inventories?
                            self.check_additional_digests(filepath, known_digests, prior_manifest_digests, 'E092a')
                            self.check_additional_digests(filepath, known_digests, prior_fixity_digests, 'E093a')
                        files_seen.discard(filepath)
        # Anything left in files_seen is not mentioned in the inventory
        if len(files_seen) > 0:
            self.log.error('E023a', where='root', extra_files=', '.join(sorted(files_seen)))

    def check_additional_digests(self, filepath, known_digests, additional_digests, error_code):
        """Check all the additional digests for filepath.

        This method is intended to be used both for manifest digests in prior versions and
        for fixity digests. The digests_seen dict is used to store any values calculated
        so that we don't recalculate digests that might appear multiple times. It is added to
        with any additional values calculated.

        Parameters:
            filepath - path of file in object (`v1/content/something` etc.)
            known_digests - dict of algorithm->digest that we have calculated
            additional_digests - dict: filepath -> algorithm -> digest -> [versions appears in]
            error_code - error code to log on mismatch (E092a for manifest, E093a for fixity)
        """
        if filepath in additional_digests:
            for digest_algorithm in additional_digests[filepath]:
                if digest_algorithm in known_digests:
                    # Don't recompute anything, just use it if we've seen it before
                    content_digest = known_digests[digest_algorithm]
                else:
                    content_digest = file_digest(filepath, digest_type=digest_algorithm, pyfs=self.obj_fs)
                    known_digests[digest_algorithm] = content_digest
                for digest in additional_digests[filepath][digest_algorithm]:
                    if content_digest != normalized_digest(digest, digest_type=digest_algorithm):
                        where = ','.join(additional_digests[filepath][digest_algorithm][digest])
                        self.log.error(error_code, where=where, digest_algorithm=digest_algorithm, digest=digest, content_path=filepath, content_digest=content_digest)

    def read_inventory_digest(self, inv_digest_file):
        """Read inventory digest from sidecar file.

        Raise exception if there is an error, else return digest.
        """
        with self.obj_fs.open(inv_digest_file, 'r') as fh:
            line = fh.readline()
            # we ignore any following lines, could raise exception
        m = re.match(r'''(\w+)\s+(\S+)\s*$''', line)
        if not m:
            raise Exception("Bad inventory digest file %s, wrong format" % (inv_digest_file))
        if m.group(2) != 'inventory.json':
            raise Exception("Bad inventory name in inventory digest file %s" % (inv_digest_file))
        return m.group(1)

if __name__ == "__main__":
    isT=True
    import os
    def extra_fixture_maybe_zip(filepath):
        """Filepath or URL for extra_fixture that may be a zip file."""
        if filepath.endswith('.zip'):
            zippath = filepath
        else:
            zippath = filepath + '.zip'
        if os.path.isfile(zippath):
            return 'zip://' + zippath
        return filepath

        """Check bad v1.0 objects fail."""
    for bad, codes in { 'E009_version_two_only': ['E009'],
                       'E033_inventory_bad_json': ['E033'],
                       'E041_manifest_not_object': ['E041c'],
                       'E042_bad_manifest_content_path': ['E042a'],
                       'E046_missing_version_dir': ['E046a'],
                       'E050_state_digest_different_case': ['E050f'],
                       'E050_state_repeated_digest': ['E050f'],
                       'E056_null_fixity': ['E056a'],
                       'E066_changed_v1_logical_path': ['E066b'],
                       'E066_changed_content_for_logical_path': ['E066c'],
                       'E092_bad_manifest_digest': ['E092a'],
                       'E093_fixity_digest_mismatch_in_v1': ['E093a'],
                       'E094_message_not_a_string': ['E094'],
                       'E096_manifest_repeated_digest': ['E096'],
                       'E097_fixity_repeated_digest': ['E097'],
                       'E099_bad_content_path_elements': ['E099']}.items():
        v = Validator()
        filepath = 'repos/zimeon---ocfl-py/extra_fixtures/1.0/bad-objects/' + bad
        if not os.path.isdir(filepath):
            filepath = extra_fixture_maybe_zip('repos/zimeon---ocfl-py/extra_fixtures/1.0/bad-objects/' + bad)
        if (v.validate(filepath)):
            
            isT=False
        for code in codes:
            if not(code in v.log.codes):
                
                isT=False


    """Check bad v1.1 objects fail."""
    for bad, codes in {'E003_two_declarations': ['E003b'],'E111_null_fixity': ['E111']}.items():
        v = Validator()
        filepath = 'repos/zimeon---ocfl-py/extra_fixtures/1.1/bad-objects/' + bad
        if not os.path.isdir(filepath):
            filepath = extra_fixture_maybe_zip('repos/zimeon---ocfl-py/extra_fixtures/1.1/bad-objects/' + bad)
        if (v.validate(filepath)):
            
            isT=False
        for code in codes:
            if not(code in v.log.codes):
                
                isT=False


    """Check warn v1.0 objects pass but give expected warnings."""
    for warn, codes in {'W003_empty_content_dir': ['W003']}.items():
        v = Validator()
        filepath = 'repos/zimeon---ocfl-py/extra_fixtures/1.0/warn-objects/' + warn
        if not os.path.isdir(filepath):
            filepath = extra_fixture_maybe_zip('repos/zimeon---ocfl-py/extra_fixtures/1.0/warn-objects/' + warn)
        if not(v.validate(filepath)):
            
            isT=False
        if not(set(codes)==set(v.log.codes)):
            
            isT=False



    """Check good objects (v1.0, v1.1 and extra) pass."""
    for base_dir in ['repos/zimeon---ocfl-py/extra_fixtures/1.0/good-objects',
                     'repos/zimeon---ocfl-py/extra_fixtures/1.1/good-objects']:
        for name in os.listdir(base_dir):
            filepath = extra_fixture_maybe_zip(os.path.join(base_dir, name))
            v = Validator()
            if not(v.validate(filepath)):
                isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/validation_logger_status_str_passk_validte.py
"""OCFL Validation Logger.

Handle logging of validation errors and warnings.
"""
import json
import os
import os.path
import re


class ValidationLogger():
    """Class for OCFL ValidationLogger."""

    validation_codes = None

    def __init__(self, show_warnings=False, show_errors=True,
                 lang='en', validation_codes=None):
        """Initialize OCFL validation logger."""
        self.show_warnings = show_warnings
        self.show_errors = show_errors
        self.lang = lang
        self.codes = {}
        self.messages = []
        self.num_errors = 0
        self.num_warnings = 0
        self.info = 0
        self.spec = 'https://ocfl.io/1.0/spec/'
        if validation_codes is not None:
            self.validation_codes = validation_codes
        elif self.validation_codes is None:
            with open(os.path.join(os.path.dirname(__file__), 'data/validation-errors.json'), 'r', encoding="utf-8") as fh:
                self.validation_codes = json.load(fh)

    def error_or_warning(self, code, severity='error', **args):
        """Add error or warning to self.codes."""
        if code in self.validation_codes and 'description' in self.validation_codes[code]:
            desc = self.validation_codes[code]['description']
            lang_desc = None
            if self.lang in desc:
                lang_desc = desc[self.lang]
            elif 'en' in desc:
                lang_desc = desc['en']
            elif len(desc) > 0:
                # first key alphabetically
                lang_desc = desc[sorted(list(desc.keys()))[0]]
            else:
                lang_desc = "Unknown " + severity + " without a description"
            # Add in any parameters
            if 'params' in self.validation_codes[code]:
                params = []
                for param in self.validation_codes[code]['params']:
                    params.append(str(args[param]) if param in args else '???')
                try:
                    lang_desc = lang_desc % tuple(params)
                except TypeError:
                    lang_desc += ' ' + str(args)
            message = '[' + code + '] ' + lang_desc
        else:
            message = "Unknown " + severity + ": %s - params (%s)" % (code, str(args))
        # Add link to spec
        m = re.match(r'''([EW](\d\d\d))''', code)
        if m and int(m.group(2)) < 200:
            message += ' (see ' + self.spec + '#' + m.group(1) + ')'
        # Store set of codes with last message for that code, and _full_ list of messages
        self.codes[code] = message
        if (severity == 'error' and self.show_errors) or (severity != 'error' and self.show_warnings):
            self.messages.append(message)

    def error(self, code, **args):
        """Add error code to self.codes."""
        self.error_or_warning(code, severity='error', **args)
        self.num_errors += 1

    def warning(self, code, **args):
        """Add warning code to self.codes."""
        self.error_or_warning(code, severity='warning', **args)
        self.num_warnings += 1

    def status_str(self, prefix=''):
        """Return string of validator status, with optional prefix."""
        s = ''
        for message in sorted(self.messages):
            s += prefix + message + '\n'
        return s[:-1]

    def __str__(self):
        """Return status string."""
        return self.status_str()

if __name__ == "__main__":
    # import dill
    # import os
    isT = True

    args1 = ['', '[[e9/ea/d1/32f80142-0149-4df3-a83a-6046d48cfcdc]]']
    temp_class = ValidationLogger()
    for arg in args1:
        res0 = temp_class.status_str(arg)
        if res0 != '':
            isT = False
    # for l in os.listdir("D:/fse/python_test/repos/zimeon---ocfl-py/data_passk_platform1/62b45df15108cfac7f2109dc/"):
    #     f = open("D:/fse/python_test/repos/zimeon---ocfl-py/data_passk_platform1/62b45df15108cfac7f2109dc/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     #object_class=dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class=ValidationLogger()
    #     print(args1)
    #     print(content["output"][0])
        # temp_class.__dict__.update(object_class)
        # res0 = temp_class.status_str(args1)
        # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
        #     isT=False
        #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/validator_status_str_passk_validte.py
"""OCFL Validator.

Philosophy of this code is to keep it separate from the implementations
of Store, Object and Version used to build and manipulate OCFL data, but
to leverage lower level functions such as digest creation etc.. Code style
is plain/verbose with detailed and specific validation errors that might
help someone debug an implementation.

This code uses PyFilesystem (import fs) exclusively for access to files. This
should enable application beyond the operating system filesystem.
"""
import json
import re
import fs

from digest import file_digest, normalized_digest
from inventory_validator import InventoryValidator
from namaste import find_namastes
from pyfs import open_fs, ocfl_walk, ocfl_files_identical
from validation_logger import ValidationLogger


class ValidatorAbortException(Exception):
    """Exception class to bail out of validation."""


class Validator():
    """Class for OCFL Validator."""

    def __init__(self, log=None, show_warnings=False, show_errors=True, check_digests=True, lax_digests=False, lang='en'):
        """Initialize OCFL validator."""
        self.log = log
        self.check_digests = check_digests
        self.lax_digests = lax_digests
        if self.log is None:
            self.log = ValidationLogger(show_warnings=show_warnings, show_errors=show_errors, lang=lang)
        self.registered_extensions = [
            '0001-digest-algorithms', '0002-flat-direct-storage-layout',
            '0003-hash-and-id-n-tuple-storage-layout', '0004-hashed-n-tuple-storage-layout',
            '0005-mutable-head'
        ]
        # The following actually initialized in initialize() method
        self.id = None
        self.spec_version = None
        self.digest_algorithm = None
        self.content_directory = None
        self.inventory_digest_files = None
        self.root_inv_validator = None
        self.obj_fs = None
        self.initialize()

    def initialize(self):
        """Initialize object state.

        Must be called between attempts to validate objects.
        """
        self.id = None
        self.spec_version = '1.0'  # default to latest published version
        self.digest_algorithm = 'sha512'
        self.content_directory = 'content'
        self.inventory_digest_files = {}  # index by version_dir, algorithms may differ
        self.root_inv_validator = None
        self.obj_fs = None

    def status_str(self, prefix=''):
        """Return string representation of validation log, with optional prefix."""
        return self.log.status_str(prefix=prefix)

    def __str__(self):
        """Return string representation of validation log."""
        return self.status_str()

    def validate(self, path):
        """Validate OCFL object at path or pyfs root.

        Returns True if valid (warnings permitted), False otherwise.
        """
        self.initialize()
        try:
            if isinstance(path, str):
                self.obj_fs = open_fs(path)
            else:
                self.obj_fs = path
                path = self.obj_fs.desc('')
        except fs.errors.CreateFailed:
            self.log.error('E003e', path=path)
            return False
        # Object declaration, set spec version number. If there are multiple declarations,
        # look for the lastest object version then report any others as errors
        namastes = find_namastes(0, pyfs=self.obj_fs)
        if len(namastes) == 0:
            self.log.error('E003a', assumed_version=self.spec_version)
        else:
            spec_version = None
            for namaste in namastes:
                # Extract and check spec version number
                this_file_version = None
                for version in ('1.1', '1.0'):
                    if namaste.filename == '0=ocfl_object_' + version:
                        this_file_version = version
                        break
                if this_file_version is None:
                    self.log.error('E006', filename=namaste.filename)
                elif spec_version is None or this_file_version > spec_version:
                    spec_version = this_file_version
                    if not namaste.content_ok(pyfs=self.obj_fs):
                        self.log.error('E007', filename=namaste.filename)
            if spec_version is None:
                self.log.error('E003c', assumed_version=self.spec_version)
            else:
                self.spec_version = spec_version
                if len(namastes) > 1:
                    self.log.error('E003b', files=len(namastes), using_version=self.spec_version)
        # Object root inventory file
        inv_file = 'inventory.json'
        if not self.obj_fs.exists(inv_file):
            self.log.error('E063')
            return False
        try:
            inventory, inv_validator = self.validate_inventory(inv_file)
            inventory_is_valid = self.log.num_errors == 0
            self.root_inv_validator = inv_validator
            all_versions = inv_validator.all_versions
            self.id = inv_validator.id
            self.content_directory = inv_validator.content_directory
            self.digest_algorithm = inv_validator.digest_algorithm
            self.validate_inventory_digest(inv_file, self.digest_algorithm)
            # Object root
            self.validate_object_root(all_versions, already_checked=[namaste.filename for namaste in namastes])
            # Version inventory files
            (prior_manifest_digests, prior_fixity_digests) = self.validate_version_inventories(all_versions)
            if inventory_is_valid:
                # Object content
                self.validate_content(inventory, all_versions, prior_manifest_digests, prior_fixity_digests)
        except ValidatorAbortException:
            pass
        return self.log.num_errors == 0

    def validate_inventory(self, inv_file, where='root', extract_spec_version=False):
        """Validate a given inventory file, record errors with self.log.error().

        Returns inventory object for use in later validation
        of object content. Does not look at anything else in the
        object itself.

        where - used for reporting messages of where inventory is in object

        extract_spec_version - if set True will attempt to take spec_version from the
            inventory itself instead of using the spec_version provided
        """
        try:
            with self.obj_fs.openbin(inv_file, 'r') as fh:
                inventory = json.load(fh)
        except json.decoder.JSONDecodeError as e:
            self.log.error('E033', where=where, explanation=str(e))
            raise ValidatorAbortException
        inv_validator = InventoryValidator(log=self.log, where=where,
                                           lax_digests=self.lax_digests,
                                           spec_version=self.spec_version)
        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)
        return inventory, inv_validator

    def validate_inventory_digest(self, inv_file, digest_algorithm, where="root"):
        """Validate the appropriate inventory digest file in path."""
        inv_digest_file = inv_file + '.' + digest_algorithm
        if not self.obj_fs.exists(inv_digest_file):
            self.log.error('E058a', where=where, path=inv_digest_file)
        else:
            self.validate_inventory_digest_match(inv_file, inv_digest_file)

    def validate_inventory_digest_match(self, inv_file, inv_digest_file):
        """Validate a given inventory digest for a given inventory file.

        On error throws exception with debugging string intended to
        be presented to a user.
        """
        if not self.check_digests:
            return
        m = re.match(r'''.*\.(\w+)$''', inv_digest_file)
        if m:
            digest_algorithm = m.group(1)
            try:
                digest_recorded = self.read_inventory_digest(inv_digest_file)
                digest_actual = file_digest(inv_file, digest_algorithm, pyfs=self.obj_fs)
                if digest_actual != digest_recorded:
                    self.log.error("E060", inv_file=inv_file, actual=digest_actual, recorded=digest_recorded, inv_digest_file=inv_digest_file)
            except Exception as e:  # pylint: disable=broad-except
                self.log.error("E061", description=str(e))
        else:
            self.log.error("E058b", inv_digest_file=inv_digest_file)

    def validate_object_root(self, version_dirs, already_checked):
        """Validate object root.

        All expected_files must be present and no other files.
        All expected_dirs must be present and no other dirs.
        """
        expected_files = ['0=ocfl_object_' + self.spec_version, 'inventory.json',
                          'inventory.json.' + self.digest_algorithm]
        for entry in self.obj_fs.scandir(''):
            if entry.is_file:
                if entry.name not in expected_files and entry.name not in already_checked:
                    self.log.error('E001a', file=entry.name)
            elif entry.is_dir:
                if entry.name in version_dirs:
                    pass
                elif entry.name == 'extensions':
                    self.validate_extensions_dir()
                elif re.match(r'''v\d+$''', entry.name):
                    # Looks like a version directory so give more specific error
                    self.log.error('E046b', dir=entry.name)
                else:
                    # Simply an unexpected directory
                    self.log.error('E001b', dir=entry.name)
            else:
                self.log.error('E001c', entry=entry.name)

    def validate_extensions_dir(self):
        """Validate content of extensions directory inside object root.

        Validate the extensions directory by checking that there aren't any
        entries in the extensions directory that aren't directories themselves.
        Where there are extension directories they SHOULD be registered and
        this code relies up the registered_extensions property to list known
        extensions.
        """
        for entry in self.obj_fs.scandir('extensions'):
            if entry.is_dir:
                if entry.name not in self.registered_extensions:
                    self.log.warning('W013', entry=entry.name)
            else:
                self.log.error('E067', entry=entry.name)

    def validate_version_inventories(self, version_dirs):
        """Each version SHOULD have an inventory up to that point.

        Also keep a record of any content digests different from those in the root inventory
        so that we can also check them when validating the content.

        version_dirs is an array of version directory names and is assumed to be in
        version sequence (1, 2, 3...).
        """
        prior_manifest_digests = {}  # file -> algorithm -> digest -> [versions]
        prior_fixity_digests = {}  # file -> algorithm -> digest -> [versions]
        if len(version_dirs) == 0:
            return prior_manifest_digests, prior_fixity_digests
        last_version = version_dirs[-1]
        prev_version_dir = "NONE"  # will be set for first directory with inventory
        prev_spec_version = '1.0'  # lowest version
        for version_dir in version_dirs:
            inv_file = fs.path.join(version_dir, 'inventory.json')
            if not self.obj_fs.exists(inv_file):
                self.log.warning('W010', where=version_dir)
                continue
            # There is an inventory file for this version directory, check it
            if version_dir == last_version:
                # Don't validate in this case. Per the spec the inventory in the last version
                # MUST be identical to the copy in the object root, just check that
                root_inv_file = 'inventory.json'
                if not ocfl_files_identical(self.obj_fs, inv_file, root_inv_file):
                    self.log.error('E064', root_inv_file=root_inv_file, inv_file=inv_file)
                else:
                    # We could also just compare digest files but this gives a more helpful error for
                    # which file has the incorrect digest if they don't match
                    self.validate_inventory_digest(inv_file, self.digest_algorithm, where=version_dir)
                self.inventory_digest_files[version_dir] = 'inventory.json.' + self.digest_algorithm
                this_spec_version = self.spec_version
            else:
                # Note that inventories in prior versions may use different digest algorithms
                # from the current invenotory. Also,
                # an may accord with the same or earlier versions of the specification
                version_inventory, inv_validator = self.validate_inventory(inv_file, where=version_dir, extract_spec_version=True)
                this_spec_version = inv_validator.spec_version
                digest_algorithm = inv_validator.digest_algorithm
                self.validate_inventory_digest(inv_file, digest_algorithm, where=version_dir)
                self.inventory_digest_files[version_dir] = 'inventory.json.' + digest_algorithm
                if self.id and 'id' in version_inventory:
                    if version_inventory['id'] != self.id:
                        self.log.error('E037b', where=version_dir, root_id=self.id, version_id=version_inventory['id'])
                if 'manifest' in version_inventory:
                    # Check that all files listed in prior inventories are in manifest
                    not_seen = set(prior_manifest_digests.keys())
                    for digest in version_inventory['manifest']:
                        for filepath in version_inventory['manifest'][digest]:
                            # We rely on the validation to check that anything present is OK
                            if filepath in not_seen:
                                not_seen.remove(filepath)
                    if len(not_seen) > 0:
                        self.log.error('E023b', where=version_dir, missing_filepaths=', '.join(sorted(not_seen)))
                    # Record all prior digests
                    for unnormalized_digest in version_inventory['manifest']:
                        digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)
                        for filepath in version_inventory['manifest'][unnormalized_digest]:
                            if filepath not in prior_manifest_digests:
                                prior_manifest_digests[filepath] = {}
                            if digest_algorithm not in prior_manifest_digests[filepath]:
                                prior_manifest_digests[filepath][digest_algorithm] = {}
                            if digest not in prior_manifest_digests[filepath][digest_algorithm]:
                                prior_manifest_digests[filepath][digest_algorithm][digest] = []
                            prior_manifest_digests[filepath][digest_algorithm][digest].append(version_dir)
                # Is this inventory an appropriate prior version of the object root inventory?
                if self.root_inv_validator is not None:
                    self.root_inv_validator.validate_as_prior_version(inv_validator)
                # Fixity blocks are independent in each version. Record all values and the versions
                # they occur in for later checks against content
                if 'fixity' in version_inventory:
                    for digest_algorithm in version_inventory['fixity']:
                        for unnormalized_digest in version_inventory['fixity'][digest_algorithm]:
                            digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)
                            for filepath in version_inventory['fixity'][digest_algorithm][unnormalized_digest]:
                                if filepath not in prior_fixity_digests:
                                    prior_fixity_digests[filepath] = {}
                                if digest_algorithm not in prior_fixity_digests[filepath]:
                                    prior_fixity_digests[filepath][digest_algorithm] = {}
                                if digest not in prior_fixity_digests[filepath][digest_algorithm]:
                                    prior_fixity_digests[filepath][digest_algorithm][digest] = []
                                prior_fixity_digests[filepath][digest_algorithm][digest].append(version_dir)
            # We are validating the inventories in sequence and each new version must
            # follow the same or later spec version to previous inventories
            if prev_spec_version > this_spec_version:
                self.log.error('E103', where=version_dir, this_spec_version=this_spec_version,
                               prev_version_dir=prev_version_dir, prev_spec_version=prev_spec_version)
            prev_version_dir = version_dir
            prev_spec_version = this_spec_version
        return prior_manifest_digests, prior_fixity_digests

    def validate_content(self, inventory, version_dirs, prior_manifest_digests, prior_fixity_digests):
        """Validate file presence and content against inventory.

        The root inventory in `inventory` is assumed to be valid and safe to use
        for construction of file paths etc..
        """
        files_seen = set()
        # Check files in each version directory
        for version_dir in version_dirs:
            try:
                # Check contents of version directory except content_directory
                for entry in self.obj_fs.listdir(version_dir):
                    if ((entry == 'inventory.json')
                            or (version_dir in self.inventory_digest_files and entry == self.inventory_digest_files[version_dir])):
                        pass
                    elif entry == self.content_directory:
                        # Check content_directory
                        content_path = fs.path.join(version_dir, self.content_directory)
                        num_content_files_in_version = 0
                        for dirpath, dirs, files in ocfl_walk(self.obj_fs, content_path):
                            if dirpath != '/' + content_path and (len(dirs) + len(files)) == 0:
                                self.log.error("E024", where=version_dir, path=dirpath)
                            for file in files:
                                files_seen.add(fs.path.join(dirpath, file).lstrip('/'))
                                num_content_files_in_version += 1
                        if num_content_files_in_version == 0:
                            self.log.warning("W003", where=version_dir)
                    elif self.obj_fs.isdir(fs.path.join(version_dir, entry)):
                        self.log.warning("W002", where=version_dir, entry=entry)
                    else:
                        self.log.error("E015", where=version_dir, entry=entry)
            except (fs.errors.ResourceNotFound, fs.errors.DirectoryExpected):
                self.log.error('E046a', version_dir=version_dir)
        # Extract any digests in fixity and organize by filepath
        fixity_digests = {}
        if 'fixity' in inventory:
            for digest_algorithm in inventory['fixity']:
                for digest in inventory['fixity'][digest_algorithm]:
                    for filepath in inventory['fixity'][digest_algorithm][digest]:
                        if filepath in files_seen:
                            if filepath not in fixity_digests:
                                fixity_digests[filepath] = {}
                            if digest_algorithm not in fixity_digests[filepath]:
                                fixity_digests[filepath][digest_algorithm] = {}
                            if digest not in fixity_digests[filepath][digest_algorithm]:
                                fixity_digests[filepath][digest_algorithm][digest] = ['root']
                        else:
                            self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, digest=digest, content_path=filepath)
        # Check all files in root manifest
        if 'manifest' in inventory:
            for digest in inventory['manifest']:
                for filepath in inventory['manifest'][digest]:
                    if filepath not in files_seen:
                        self.log.error('E092b', where='root', content_path=filepath)
                    else:
                        if self.check_digests:
                            content_digest = file_digest(filepath, digest_type=self.digest_algorithm, pyfs=self.obj_fs)
                            if content_digest != normalized_digest(digest, digest_type=self.digest_algorithm):
                                self.log.error('E092a', where='root', digest_algorithm=self.digest_algorithm, digest=digest, content_path=filepath, content_digest=content_digest)
                            known_digests = {self.digest_algorithm: content_digest}
                            # Are there digest values in the fixity block?
                            self.check_additional_digests(filepath, known_digests, fixity_digests, 'E093a')
                            # Are there other digests for this same file from other inventories?
                            self.check_additional_digests(filepath, known_digests, prior_manifest_digests, 'E092a')
                            self.check_additional_digests(filepath, known_digests, prior_fixity_digests, 'E093a')
                        files_seen.discard(filepath)
        # Anything left in files_seen is not mentioned in the inventory
        if len(files_seen) > 0:
            self.log.error('E023a', where='root', extra_files=', '.join(sorted(files_seen)))

    def check_additional_digests(self, filepath, known_digests, additional_digests, error_code):
        """Check all the additional digests for filepath.

        This method is intended to be used both for manifest digests in prior versions and
        for fixity digests. The digests_seen dict is used to store any values calculated
        so that we don't recalculate digests that might appear multiple times. It is added to
        with any additional values calculated.

        Parameters:
            filepath - path of file in object (`v1/content/something` etc.)
            known_digests - dict of algorithm->digest that we have calculated
            additional_digests - dict: filepath -> algorithm -> digest -> [versions appears in]
            error_code - error code to log on mismatch (E092a for manifest, E093a for fixity)
        """
        if filepath in additional_digests:
            for digest_algorithm in additional_digests[filepath]:
                if digest_algorithm in known_digests:
                    # Don't recompute anything, just use it if we've seen it before
                    content_digest = known_digests[digest_algorithm]
                else:
                    content_digest = file_digest(filepath, digest_type=digest_algorithm, pyfs=self.obj_fs)
                    known_digests[digest_algorithm] = content_digest
                for digest in additional_digests[filepath][digest_algorithm]:
                    if content_digest != normalized_digest(digest, digest_type=digest_algorithm):
                        where = ','.join(additional_digests[filepath][digest_algorithm][digest])
                        self.log.error(error_code, where=where, digest_algorithm=digest_algorithm, digest=digest, content_path=filepath, content_digest=content_digest)

    def read_inventory_digest(self, inv_digest_file):
        """Read inventory digest from sidecar file.

        Raise exception if there is an error, else return digest.
        """
        with self.obj_fs.open(inv_digest_file, 'r') as fh:
            line = fh.readline()
            # we ignore any following lines, could raise exception
        m = re.match(r'''(\w+)\s+(\S+)\s*$''', line)
        if not m:
            raise Exception("Bad inventory digest file %s, wrong format" % (inv_digest_file))
        if m.group(2) != 'inventory.json':
            raise Exception("Bad inventory name in inventory digest file %s" % (inv_digest_file))
        return m.group(1)

if __name__ == "__main__":
    # import dill
    # import os
    isT=True
    # for l in os.listdir("D:/fse/python_test/repos/zimeon---ocfl-py/data_passk_platform1/62b45df15108cfac7f2109dd/"):
    #     f = open("D:/fse/python_test/repos/zimeon---ocfl-py/data_passk_platform1/62b45df15108cfac7f2109dd/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     print(args1)
    #     print(content['output'][0])
    args1 = ['', '[[e9/ea/d1/32f80142-0149-4df3-a83a-6046d48cfcdc]]']
        # object_class=dill.loads(content["input"]["args"][0]["bytes"])
        # temp_class=Validator()
        # temp_class.__dict__.update(object_class)
    temp_class = Validator()
    for arg in args1:
        res0 = temp_class.status_str(arg)
        if res0 != '':
            isT=False
        # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
        #     isT=False
        #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/dispositor_is_valid_passk_validte.py
"""Base class for Dispositor objects."""
import os
import os.path
from urllib.parse import quote_plus, unquote_plus


class Dispositor:
    """Base class for disposition handlers -- let's call them Dispositors."""

    def strip_root(self, path, root):
        """Remove root from path, throw exception on failure."""
        root = root.rstrip(os.sep)  # ditch any trailing path separator
        if os.path.commonprefix((path, root)) == root:
            return os.path.relpath(path, start=root)
        raise Exception("Path %s is not in root %s" % (path, root))

    def is_valid(self, identifier):  # pylint: disable=unused-argument
        """Return True if identifier is valid, always True in this base implementation."""
        return True

    def encode(self, identifier):
        """Encode identifier to get rid of unsafe chars."""
        return quote_plus(identifier)

    def decode(self, identifier):
        """Decode identifier to put back unsafe chars."""
        return unquote_plus(identifier)

    def identifier_to_path(self, identifier):
        """Convert identifier to path relative to some root."""
        raise Exception("No yet implemented")

    def relative_path_to_identifier(self, path):
        """Convert relative path to identifier."""
        raise Exception("No yet implemented")

    def path_to_identifier(self, path, root=None):
        """Convert path relative to root to identifier."""
        if root is not None:
            path = self.strip_root(path, root)
        return self.relative_path_to_identifier(path)

if __name__ == "__main__":
    isT=True

    d = Dispositor()
    if not d.is_valid(''):
        isT=False
    if not d.is_valid('anything'):
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/inventory_validator_validate_passk_validte.py
"""OCFL Inventory Validator.

Code to validate the Python representation of an OCFL Inventory
as read with json.load(). Does not examine anything in storage.
"""
import re

from digest import digest_regex, normalized_digest
from validation_logger import ValidationLogger
from w3c_datetime import str_to_datetime


def get_logical_path_map(inventory, version):
    """Get a map of logical paths in state to files on disk for version in inventory.

    Returns a dictionary: logical_path_in_state -> set(content_files)

    The set of content_files may includes references to duplicate files in
    later versions than the version being described.
    """
    state = inventory['versions'][version]['state']
    manifest = inventory['manifest']
    file_map = {}
    for digest in state:
        if digest in manifest:
            for file in state[digest]:
                file_map[file] = set(manifest[digest])
    return file_map


class InventoryValidator():
    """Class for OCFL Inventory Validator."""

    def __init__(self, log=None, where='???',
                 lax_digests=False, spec_version='1.0'):
        """Initialize OCFL Inventory Validator."""
        self.log = ValidationLogger() if log is None else log
        self.where = where
        self.spec_version = spec_version
        # Object state
        self.inventory = None
        self.id = None
        self.digest_algorithm = 'sha512'
        self.content_directory = 'content'
        self.all_versions = []
        self.manifest_files = None
        self.unnormalized_digests = None
        self.head = 'UNKNOWN'
        # Validation control
        self.lax_digests = lax_digests
        # Configuration
        self.spec_versions_supported = ('1.0', '1.1')

    def error(self, code, **args):
        """Error with added context."""
        self.log.error(code, where=self.where, **args)

    def warning(self, code, **args):
        """Warning with added context."""
        self.log.warning(code, where=self.where, **args)

    def validate(self, inventory, extract_spec_version=False):
        """Validate a given inventory.

        If extract_spec_version is True then will look at the type value to determine
        the specification version. In the case that there is no type value or it isn't
        valid, then other tests will be based on the version given in self.spec_version.
        """
        # Basic structure
        self.inventory = inventory
        if 'id' in inventory:
            iid = inventory['id']
            if not isinstance(iid, str) or iid == '':
                self.error("E037a")
            else:
                # URI syntax https://www.rfc-editor.org/rfc/rfc3986.html#section-3.1 :
                # scheme = ALPHA *( ALPHA / DIGIT / "+" / "-" / "." )
                if not re.match(r'''[a-z][a-z\d\+\-\.]*:.+''', iid, re.IGNORECASE):
                    self.warning("W005", id=iid)
                self.id = iid
        else:
            self.error("E036a")
        if 'type' not in inventory:
            self.error("E036b")
        elif not isinstance(inventory['type'], str):
            self.error("E999")
        elif extract_spec_version:
            m = re.match(r'''https://ocfl.io/(\d+.\d)/spec/#inventory''', inventory['type'])
            if not m:
                self.error('E038b', got=inventory['type'], assumed_spec_version=self.spec_version)
            elif m.group(1) in self.spec_versions_supported:
                self.spec_version = m.group(1)
            else:
                self.error("E038c", got=m.group(1), assumed_spec_version=self.spec_version)
        elif inventory['type'] != 'https://ocfl.io/' + self.spec_version + '/spec/#inventory':
            self.error("E038a", expected='https://ocfl.io/' + self.spec_version + '/spec/#inventory', got=inventory['type'])
        if 'digestAlgorithm' not in inventory:
            self.error("E036c")
        elif inventory['digestAlgorithm'] == 'sha512':
            pass
        elif self.lax_digests:
            self.digest_algorithm = inventory['digestAlgorithm']
        elif inventory['digestAlgorithm'] == 'sha256':
            self.warning("W004")
            self.digest_algorithm = inventory['digestAlgorithm']
        else:
            self.error("E039", digest_algorithm=inventory['digestAlgorithm'])
        if 'contentDirectory' in inventory:
            # Careful only to set self.content_directory if value is safe
            cd = inventory['contentDirectory']
            if not isinstance(cd, str) or '/' in cd:
                self.error("E017")
            elif cd in ('.', '..'):
                self.error("E018")
            else:
                self.content_directory = cd
        manifest_files_correct_format = None
        if 'manifest' not in inventory:
            self.error("E041a")
        else:
            (self.manifest_files, manifest_files_correct_format, self.unnormalized_digests) = self.validate_manifest(inventory['manifest'])
        digests_used = []
        if 'versions' not in inventory:
            self.error("E041b")
        else:
            self.all_versions = self.validate_version_sequence(inventory['versions'])
            digests_used = self.validate_versions(inventory['versions'], self.all_versions, self.unnormalized_digests)
        if 'head' not in inventory:
            self.error("E036d")
        elif len(self.all_versions) > 0:
            self.head = self.all_versions[-1]
            if inventory['head'] != self.head:
                self.error("E040", got=inventory['head'], expected=self.head)
        if len(self.all_versions) == 0:
            # Abort tests is we don't have a valid version sequence, otherwise
            # there will likely be spurious subsequent error reports
            return
        if len(self.all_versions) > 0:
            if manifest_files_correct_format is not None:
                self.check_content_paths_map_to_versions(manifest_files_correct_format, self.all_versions)
            if self.manifest_files is not None:
                self.check_digests_present_and_used(self.manifest_files, digests_used)
        if 'fixity' in inventory:
            self.validate_fixity(inventory['fixity'], self.manifest_files)

    def validate_manifest(self, manifest):
        """Validate manifest block in inventory.

        Returns:
          * manifest_files - a mapping from file to digest for each file in
              the manifest
          * manifest_files_correct_format - a simple list of the manifest file
              path that passed initial checks. They need to be checked for valid
              version directories later, when we know what version directories
              are valid
          * unnormalized_digests - a set of the original digests in unnormalized
              form that MUST match exactly the values used in state blocks
        """
        manifest_files = {}
        manifest_files_correct_format = []
        unnormalized_digests = set()
        manifest_digests = set()
        if not isinstance(manifest, dict):
            self.error('E041c')
        else:
            content_paths = set()
            content_directories = set()
            for digest in manifest:
                m = re.match(self.digest_regex(), digest)
                if not m:
                    self.error('E025a', digest=digest, algorithm=self.digest_algorithm)  # wrong form of digest
                elif not isinstance(manifest[digest], list):
                    self.error('E092', digest=digest)  # must have path list value
                else:
                    unnormalized_digests.add(digest)
                    norm_digest = normalized_digest(digest, self.digest_algorithm)
                    if norm_digest in manifest_digests:
                        # We have already seen this in different un-normalized form!
                        self.error("E096", digest=norm_digest)
                    else:
                        manifest_digests.add(norm_digest)
                    for file in manifest[digest]:
                        manifest_files[file] = norm_digest
                        if self.check_content_path(file, content_paths, content_directories):
                            manifest_files_correct_format.append(file)
            # Check for conflicting content paths
            for path in content_directories:
                if path in content_paths:
                    self.error("E101b", path=path)
        return manifest_files, manifest_files_correct_format, unnormalized_digests

    def validate_fixity(self, fixity, manifest_files):
        """Validate fixity block in inventory.

        Check the structure of the fixity block and makes sure that only files
        listed in the manifest are referenced.
        """
        if not isinstance(fixity, dict):
            # The value of fixity must be a JSON object. In v1.0 I catch not an object
            # as part of E056 but this was clarified as E111 in v1.1. The value may
            # be an empty object in either case
            self.error('E056a' if self.spec_version == '1.0' else 'E111')
        else:
            for digest_algorithm in fixity:
                known_digest = True
                try:
                    regex = digest_regex(digest_algorithm)
                except ValueError:
                    if not self.lax_digests:
                        self.error('E056b', algorithm=self.digest_algorithm)
                        continue
                    # Match anything
                    regex = r'''^.*$'''
                    known_digest = False
                fixity_algoritm_block = fixity[digest_algorithm]
                if not isinstance(fixity_algoritm_block, dict):
                    self.error('E057a', algorithm=self.digest_algorithm)
                else:
                    digests_seen = set()
                    for digest in fixity_algoritm_block:
                        m = re.match(regex, digest)
                        if not m:
                            self.error('E057b', digest=digest, algorithm=digest_algorithm)  # wrong form of digest
                        elif not isinstance(fixity_algoritm_block[digest], list):
                            self.error('E057c', digest=digest, algorithm=digest_algorithm)  # must have path list value
                        else:
                            if known_digest:
                                norm_digest = normalized_digest(digest, digest_algorithm)
                            else:
                                norm_digest = digest
                            if norm_digest in digests_seen:
                                # We have already seen this in different un-normalized form!
                                self.error("E097", digest=norm_digest, algorithm=digest_algorithm)
                            else:
                                digests_seen.add(norm_digest)
                            for file in fixity_algoritm_block[digest]:
                                if file not in manifest_files:
                                    self.error("E057d", digest=norm_digest, algorithm=digest_algorithm, path=file)

    def validate_version_sequence(self, versions):
        """Validate sequence of version names in versions block in inventory.

        Returns an array of in-sequence version directories that are part
        of a valid sequences. May exclude other version directory names that are
        not part of the valid sequence if an error is thrown.
        """
        all_versions = []
        if not isinstance(versions, dict):
            self.error("E044")
            return all_versions
        if len(versions) == 0:
            self.error("E008")
            return all_versions
        # Validate version sequence
        # https://ocfl.io/draft/spec/#version-directories
        zero_padded = None
        max_version_num = 999999  # Excessive limit
        if 'v1' in versions:
            fmt = 'v%d'
            zero_padded = False
            all_versions.append('v1')
        else:  # Find padding size
            for n in range(2, 11):
                fmt = 'v%0' + str(n) + 'd'
                vkey = fmt % 1
                if vkey in versions:
                    all_versions.append(vkey)
                    zero_padded = n
                    max_version_num = (10 ** (n - 1)) - 1
                    break
            if not zero_padded:
                self.error("E009")
                return all_versions
        if zero_padded:
            self.warning("W001")
        # Have v1 and know format, work through to check sequence
        for n in range(2, max_version_num + 1):
            v = (fmt % n)
            if v in versions:
                all_versions.append(v)
            else:
                if len(versions) != (n - 1):
                    self.error("E010")  # Extra version dirs outside sequence
                return all_versions
        # We have now included all possible versions up to the zero padding
        # size, if there are more versions than this number then we must
        # have extra that violate the zero-padding rule or are out of
        # sequence
        if len(versions) > max_version_num:
            self.error("E011")
        return all_versions

    def validate_versions(self, versions, all_versions, unnormalized_digests):
        """Validate versions blocks in inventory.

        Requires as input two things which are assumed to be structurally correct
        from prior basic validation:

          * versions - which is the JSON object (dict) from the inventory
          * all_versions - an ordered list of the versions to look at in versions
                           (all other keys in versions will be ignored)

        Returns a list of digests_used which can then be checked against the
        manifest.
        """
        digests_used = []
        for v in all_versions:
            version = versions[v]
            if 'created' not in version:
                self.error('E048', version=v)  # No created
            elif not isinstance(versions[v]['created'], str):
                self.error('E049d', version=v)  # Bad created
            else:
                created = versions[v]['created']
                try:
                    str_to_datetime(created)  # catch ValueError if fails
                    if not re.search(r'''(Z|[+-]\d\d:\d\d)$''', created):  # FIXME - kludge
                        self.error('E049a', version=v)
                    if not re.search(r'''T\d\d:\d\d:\d\d''', created):  # FIXME - kludge
                        self.error('E049b', version=v)
                except ValueError as e:
                    self.error('E049c', version=v, description=str(e))
            if 'state' in version:
                digests_used += self.validate_state_block(version['state'], version=v, unnormalized_digests=unnormalized_digests)
            else:
                self.error('E048c', version=v)
            if 'message' not in version:
                self.warning('W007a', version=v)
            elif not isinstance(version['message'], str):
                self.error('E094', version=v)
            if 'user' not in version:
                self.warning('W007b', version=v)
            else:
                user = version['user']
                if not isinstance(user, dict):
                    self.error('E054a', version=v)
                else:
                    if 'name' not in user or not isinstance(user['name'], str):
                        self.error('E054b', version=v)
                    if 'address' not in user:
                        self.warning('W008', version=v)
                    elif not isinstance(user['address'], str):
                        self.error('E054c', version=v)
                    elif not re.match(r'''\w{3,6}:''', user['address']):
                        self.warning('W009', version=v)
        return digests_used

    def validate_state_block(self, state, version, unnormalized_digests):
        """Validate state block in a version in an inventory.

        The version is used only for error reporting.

        Returns a list of content digests referenced in the state block.
        """
        digests = []
        logical_paths = set()
        logical_directories = set()
        if not isinstance(state, dict):
            self.error('E050c', version=version)
        else:
            digest_re = re.compile(self.digest_regex())
            for digest in state:
                if not digest_re.match(digest):
                    self.error('E050d', version=version, digest=digest)
                elif not isinstance(state[digest], list):
                    self.error('E050e', version=version, digest=digest)
                else:
                    for path in state[digest]:
                        if path in logical_paths:
                            self.error("E095a", version=version, path=path)
                        else:
                            self.check_logical_path(path, version, logical_paths, logical_directories)
                    if digest not in unnormalized_digests:
                        # Exact string value must match, not just normalized
                        self.error("E050f", version=version, digest=digest)
                    norm_digest = normalized_digest(digest, self.digest_algorithm)
                    digests.append(norm_digest)
            # Check for conflicting logical paths
            for path in logical_directories:
                if path in logical_paths:
                    self.error("E095b", version=version, path=path)
        return digests

    def check_content_paths_map_to_versions(self, manifest_files, all_versions):
        """Check that every content path starts with a valid version.

        The content directory component has already been checked in
        check_content_path(). We have already tested all paths enough
        to know that they can be split into at least 2 components.
        """
        for path in manifest_files:
            version_dir, dummy_rest = path.split('/', 1)
            if version_dir not in all_versions:
                self.error('E042b', path=path)

    def check_digests_present_and_used(self, manifest_files, digests_used):
        """Check all digests in manifest that are needed are present and used."""
        in_manifest = set(manifest_files.values())
        in_state = set(digests_used)
        not_in_manifest = in_state.difference(in_manifest)
        if len(not_in_manifest) > 0:
            self.error("E050a", digests=", ".join(sorted(not_in_manifest)))
        not_in_state = in_manifest.difference(in_state)
        if len(not_in_state) > 0:
            self.error("E107", digests=", ".join(sorted(not_in_state)))

    def digest_regex(self):
        """Return regex for validating un-normalized digest format."""
        try:
            return digest_regex(self.digest_algorithm)
        except ValueError:
            if not self.lax_digests:
                self.error('E026a', digest=self.digest_algorithm)
        # Match anything
        return r'''^.*$'''

    def check_logical_path(self, path, version, logical_paths, logical_directories):
        """Check logical path and accumulate paths/directories for E095b check.

        logical_paths and logical_directories are expected to be sets.

        Only adds good paths to the accumulated paths/directories.
        """
        if path.startswith('/') or path.endswith('/'):
            self.error("E053", version=version, path=path)
        else:
            elements = path.split('/')
            for element in elements:
                if element in ['.', '..', '']:
                    self.error("E052", version=version, path=path)
                    return
            # Accumulate paths and directories
            logical_paths.add(path)
            logical_directories.add('/'.join(elements[0:-1]))

    def check_content_path(self, path, content_paths, content_directories):
        """Check logical path and accumulate paths/directories for E101 check.

        Returns True if valid, else False. Only adds good paths to the
        accumulated paths/directories. We don't yet know the set of valid
        version directories so the check here is just for 'v' + digits.
        """
        if path.startswith('/') or path.endswith('/'):
            self.error("E100", path=path)
            return False
        m = re.match(r'''^(v\d+/''' + self.content_directory + r''')/(.+)''', path)
        if not m:
            self.error("E042a", path=path)
            return False
        elements = m.group(2).split('/')
        for element in elements:
            if element in ('', '.', '..'):
                self.error("E099", path=path)
                return False
        # Accumulate paths and directories if not seen before
        if path in content_paths:
            self.error("E101a", path=path)
            return False
        content_paths.add(path)
        content_directories.add('/'.join([m.group(1)] + elements[0:-1]))
        return True

    def validate_as_prior_version(self, prior):
        """Check that prior is a valid prior version of the current inventory object.

        The input variable prior is also expected to be an InventoryValidator object
        and both self and prior inventories are assumed to have been checked for
        internal consistency.
        """
        # Must have a subset of versions which also checks zero padding format etc.
        if not set(prior.all_versions) < set(self.all_versions):
            self.error('E066a', prior_head=prior.head)
        else:
            # Check references to files but realize that there might be different
            # digest algorithms between versions
            version = 'no-version'
            for version in prior.all_versions:
                # If the digest algorithm is the same then we can make a
                # direct check on whether the state blocks match
                if prior.digest_algorithm == self.digest_algorithm:
                    self.compare_states_for_version(prior, version)
                # Now check the mappings from state to logical path, which must
                # be consistent even if the digestAlgorithm is different between
                # versions. Get maps from logical paths to files on disk:
                prior_map = get_logical_path_map(prior.inventory, version)
                self_map = get_logical_path_map(self.inventory, version)
                # Look first for differences in logical paths listed
                only_in_prior = prior_map.keys() - self_map.keys()
                only_in_self = self_map.keys() - prior_map.keys()
                if only_in_prior or only_in_self:
                    if only_in_prior:
                        self.error('E066b', version=version, prior_head=prior.head, only_in=prior.head, logical_paths=','.join(only_in_prior))
                    if only_in_self:
                        self.error('E066b', version=version, prior_head=prior.head, only_in=self.where, logical_paths=','.join(only_in_self))
                else:
                    # Check them all in details - digests must match
                    for logical_path, this_map in prior_map.items():
                        if not this_map.issubset(self_map[logical_path]):
                            self.error('E066c', version=version, prior_head=prior.head,
                                       logical_path=logical_path, prior_content=','.join(this_map),
                                       current_content=','.join(self_map[logical_path]))
                # Check metadata
                prior_version = prior.inventory['versions'][version]
                self_version = self.inventory['versions'][version]
                for key in ('created', 'message', 'user'):
                    if prior_version.get(key) != self_version.get(key):
                        self.warning('W011', version=version, prior_head=prior.head, key=key)

    def compare_states_for_version(self, prior, version):
        """Compare state blocks for version between self and prior.

        Assumes the same digest algorithm in both, do not call otherwise!

        Looks only for digests that appear in one but not in the other, the code
        in validate_as_prior_version(..) does a check for whether the same sets
        of logical files appear and we don't want to duplicate an error message
        about that.

        While the mapping checks in validate_as_prior_version(..) do all that is
        necessary to detect an error, the additional errors that may be generated
        here provide more detailed diagnostics in the case that the digest
        algorithm is the same across versions being compared.
        """
        self_state = self.inventory['versions'][version]['state']
        prior_state = prior.inventory['versions'][version]['state']
        for digest in set(self_state.keys()).union(prior_state.keys()):
            if digest not in prior_state:
                self.error('E066d', version=version, prior_head=prior.head,
                           digest=digest, logical_files=', '.join(self_state[digest]))
            elif digest not in self_state:
                self.error('E066e', version=version, prior_head=prior.head,
                           digest=digest, logical_files=', '.join(prior_state[digest]))

class TLogger():
    """Simplified logger to replace ValidationLogger."""


    def __init__(self):
        """Initialize."""
        self.clear()


    def error(self, code, **args):  # pylint: disable=unused-argument
        """Add error code, discard args."""
        self.errors.append(code)


    def warning(self, code, **args):  # pylint: disable=unused-argument
        """Add warn code, discard args."""
        self.warns.append(code)


    def clear(self):
        """Clear records."""
        self.errors = []
        self.warns = []
        
if __name__ == "__main__":
    isT=True
    
    log = TLogger()
    iv = InventoryValidator(log=log)
    iv.validate({})
    if not 'E036a' in log.errors or\
    not 'E036b' in log.errors or \
    not 'E036c' in log.errors or \
    not 'E036d' in log.errors or \
    not 'E041a' in log.errors or \
    not 'E041b' in log.errors:
        isT=False
    log.clear()
    iv.validate({"id": []})
    if not ('E037a' in log.errors):
        isT=False
    log.clear()
    # Valid and invalid URIs
    iv.validate({"id": "scheme:rest", "digestAlgorithm": "sha512"})
    if ('W005' in log.warns):
        isT=False
    log.clear()
    iv.validate({"id": "URN-3:rest", "digestAlgorithm": "sha512"})
    if 'W005' in log.warns:
        isT=False
    log.clear()
    iv.validate({"id": "a1+2-3z.:rest", "digestAlgorithm": "sha512"})
    if ('W005' in log.warns):
        isT=False
    if ('W004' in log.warns):
        isT=False
    log.clear()
    iv.validate({"id": "not_a_uri", "digestAlgorithm": "sha256"})
    if not ('W005' in log.warns):
        isT=False
    if not ('W004' in log.warns):
        isT=False
    log.clear()
    iv.validate({"id": "like:uri", "type": "wrong type", "digestAlgorithm": "my_digest"})
    if not ('E038a' in log.errors):
        isT=False
    if not ('E039' in log.errors):
        isT=False
    log.clear()
    iv.validate({"id": "like:uri", "type": "wrong type", "digestAlgorithm": "my_digest"}, extract_spec_version=True)
    if not ('E038b' in log.errors):
        isT=False
    log.clear()
    iv.validate({"id": "like:uri", "type": "https://ocfl.io/100.9/spec/#inventory", "digestAlgorithm": "my_digest"},
                extract_spec_version=True)
    if not ('E038c' in log.errors):
        isT=False
    iv = InventoryValidator(log=log, lax_digests=True)
    log.clear()
    iv.validate({"id": "like:uri", "type": "wrong type", "digestAlgorithm": "my_digest"})
    if ('E039' in log.errors):
        isT=False
    if (iv.digest_algorithm != "my_digest"):
        isT=False
    iv = InventoryValidator(log=log)
    log.clear()
    iv.validate({"id": "like:uri", "contentDirectory": "not/allowed"})
    if not ('E017' in log.errors):
        isT=False
    log.clear()
    iv.validate({"id": "like:uri", "contentDirectory": ["s"]})
    if not ('E017' in log.errors):
        isT=False
    log.clear()
    iv.validate({"id": "like:uri", "contentDirectory": ".."})
    if not ('E018' in log.errors):
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/inventory_validator_check_digests_present_and_used_passk_validte.py
"""OCFL Inventory Validator.

Code to validate the Python representation of an OCFL Inventory
as read with json.load(). Does not examine anything in storage.
"""
import re

from digest import digest_regex, normalized_digest
from validation_logger import ValidationLogger
from w3c_datetime import str_to_datetime


def get_logical_path_map(inventory, version):
    """Get a map of logical paths in state to files on disk for version in inventory.

    Returns a dictionary: logical_path_in_state -> set(content_files)

    The set of content_files may includes references to duplicate files in
    later versions than the version being described.
    """
    state = inventory['versions'][version]['state']
    manifest = inventory['manifest']
    file_map = {}
    for digest in state:
        if digest in manifest:
            for file in state[digest]:
                file_map[file] = set(manifest[digest])
    return file_map


class InventoryValidator():
    """Class for OCFL Inventory Validator."""

    def __init__(self, log=None, where='???',
                 lax_digests=False, spec_version='1.0'):
        """Initialize OCFL Inventory Validator."""
        self.log = ValidationLogger() if log is None else log
        self.where = where
        self.spec_version = spec_version
        # Object state
        self.inventory = None
        self.id = None
        self.digest_algorithm = 'sha512'
        self.content_directory = 'content'
        self.all_versions = []
        self.manifest_files = None
        self.unnormalized_digests = None
        self.head = 'UNKNOWN'
        # Validation control
        self.lax_digests = lax_digests
        # Configuration
        self.spec_versions_supported = ('1.0', '1.1')

    def error(self, code, **args):
        """Error with added context."""
        self.log.error(code, where=self.where, **args)

    def warning(self, code, **args):
        """Warning with added context."""
        self.log.warning(code, where=self.where, **args)

    def validate(self, inventory, extract_spec_version=False):
        """Validate a given inventory.

        If extract_spec_version is True then will look at the type value to determine
        the specification version. In the case that there is no type value or it isn't
        valid, then other tests will be based on the version given in self.spec_version.
        """
        # Basic structure
        self.inventory = inventory
        if 'id' in inventory:
            iid = inventory['id']
            if not isinstance(iid, str) or iid == '':
                self.error("E037a")
            else:
                # URI syntax https://www.rfc-editor.org/rfc/rfc3986.html#section-3.1 :
                # scheme = ALPHA *( ALPHA / DIGIT / "+" / "-" / "." )
                if not re.match(r'''[a-z][a-z\d\+\-\.]*:.+''', iid, re.IGNORECASE):
                    self.warning("W005", id=iid)
                self.id = iid
        else:
            self.error("E036a")
        if 'type' not in inventory:
            self.error("E036b")
        elif not isinstance(inventory['type'], str):
            self.error("E999")
        elif extract_spec_version:
            m = re.match(r'''https://ocfl.io/(\d+.\d)/spec/#inventory''', inventory['type'])
            if not m:
                self.error('E038b', got=inventory['type'], assumed_spec_version=self.spec_version)
            elif m.group(1) in self.spec_versions_supported:
                self.spec_version = m.group(1)
            else:
                self.error("E038c", got=m.group(1), assumed_spec_version=self.spec_version)
        elif inventory['type'] != 'https://ocfl.io/' + self.spec_version + '/spec/#inventory':
            self.error("E038a", expected='https://ocfl.io/' + self.spec_version + '/spec/#inventory', got=inventory['type'])
        if 'digestAlgorithm' not in inventory:
            self.error("E036c")
        elif inventory['digestAlgorithm'] == 'sha512':
            pass
        elif self.lax_digests:
            self.digest_algorithm = inventory['digestAlgorithm']
        elif inventory['digestAlgorithm'] == 'sha256':
            self.warning("W004")
            self.digest_algorithm = inventory['digestAlgorithm']
        else:
            self.error("E039", digest_algorithm=inventory['digestAlgorithm'])
        if 'contentDirectory' in inventory:
            # Careful only to set self.content_directory if value is safe
            cd = inventory['contentDirectory']
            if not isinstance(cd, str) or '/' in cd:
                self.error("E017")
            elif cd in ('.', '..'):
                self.error("E018")
            else:
                self.content_directory = cd
        manifest_files_correct_format = None
        if 'manifest' not in inventory:
            self.error("E041a")
        else:
            (self.manifest_files, manifest_files_correct_format, self.unnormalized_digests) = self.validate_manifest(inventory['manifest'])
        digests_used = []
        if 'versions' not in inventory:
            self.error("E041b")
        else:
            self.all_versions = self.validate_version_sequence(inventory['versions'])
            digests_used = self.validate_versions(inventory['versions'], self.all_versions, self.unnormalized_digests)
        if 'head' not in inventory:
            self.error("E036d")
        elif len(self.all_versions) > 0:
            self.head = self.all_versions[-1]
            if inventory['head'] != self.head:
                self.error("E040", got=inventory['head'], expected=self.head)
        if len(self.all_versions) == 0:
            # Abort tests is we don't have a valid version sequence, otherwise
            # there will likely be spurious subsequent error reports
            return
        if len(self.all_versions) > 0:
            if manifest_files_correct_format is not None:
                self.check_content_paths_map_to_versions(manifest_files_correct_format, self.all_versions)
            if self.manifest_files is not None:
                self.check_digests_present_and_used(self.manifest_files, digests_used)
        if 'fixity' in inventory:
            self.validate_fixity(inventory['fixity'], self.manifest_files)

    def validate_manifest(self, manifest):
        """Validate manifest block in inventory.

        Returns:
          * manifest_files - a mapping from file to digest for each file in
              the manifest
          * manifest_files_correct_format - a simple list of the manifest file
              path that passed initial checks. They need to be checked for valid
              version directories later, when we know what version directories
              are valid
          * unnormalized_digests - a set of the original digests in unnormalized
              form that MUST match exactly the values used in state blocks
        """
        manifest_files = {}
        manifest_files_correct_format = []
        unnormalized_digests = set()
        manifest_digests = set()
        if not isinstance(manifest, dict):
            self.error('E041c')
        else:
            content_paths = set()
            content_directories = set()
            for digest in manifest:
                m = re.match(self.digest_regex(), digest)
                if not m:
                    self.error('E025a', digest=digest, algorithm=self.digest_algorithm)  # wrong form of digest
                elif not isinstance(manifest[digest], list):
                    self.error('E092', digest=digest)  # must have path list value
                else:
                    unnormalized_digests.add(digest)
                    norm_digest = normalized_digest(digest, self.digest_algorithm)
                    if norm_digest in manifest_digests:
                        # We have already seen this in different un-normalized form!
                        self.error("E096", digest=norm_digest)
                    else:
                        manifest_digests.add(norm_digest)
                    for file in manifest[digest]:
                        manifest_files[file] = norm_digest
                        if self.check_content_path(file, content_paths, content_directories):
                            manifest_files_correct_format.append(file)
            # Check for conflicting content paths
            for path in content_directories:
                if path in content_paths:
                    self.error("E101b", path=path)
        return manifest_files, manifest_files_correct_format, unnormalized_digests

    def validate_fixity(self, fixity, manifest_files):
        """Validate fixity block in inventory.

        Check the structure of the fixity block and makes sure that only files
        listed in the manifest are referenced.
        """
        if not isinstance(fixity, dict):
            # The value of fixity must be a JSON object. In v1.0 I catch not an object
            # as part of E056 but this was clarified as E111 in v1.1. The value may
            # be an empty object in either case
            self.error('E056a' if self.spec_version == '1.0' else 'E111')
        else:
            for digest_algorithm in fixity:
                known_digest = True
                try:
                    regex = digest_regex(digest_algorithm)
                except ValueError:
                    if not self.lax_digests:
                        self.error('E056b', algorithm=self.digest_algorithm)
                        continue
                    # Match anything
                    regex = r'''^.*$'''
                    known_digest = False
                fixity_algoritm_block = fixity[digest_algorithm]
                if not isinstance(fixity_algoritm_block, dict):
                    self.error('E057a', algorithm=self.digest_algorithm)
                else:
                    digests_seen = set()
                    for digest in fixity_algoritm_block:
                        m = re.match(regex, digest)
                        if not m:
                            self.error('E057b', digest=digest, algorithm=digest_algorithm)  # wrong form of digest
                        elif not isinstance(fixity_algoritm_block[digest], list):
                            self.error('E057c', digest=digest, algorithm=digest_algorithm)  # must have path list value
                        else:
                            if known_digest:
                                norm_digest = normalized_digest(digest, digest_algorithm)
                            else:
                                norm_digest = digest
                            if norm_digest in digests_seen:
                                # We have already seen this in different un-normalized form!
                                self.error("E097", digest=norm_digest, algorithm=digest_algorithm)
                            else:
                                digests_seen.add(norm_digest)
                            for file in fixity_algoritm_block[digest]:
                                if file not in manifest_files:
                                    self.error("E057d", digest=norm_digest, algorithm=digest_algorithm, path=file)

    def validate_version_sequence(self, versions):
        """Validate sequence of version names in versions block in inventory.

        Returns an array of in-sequence version directories that are part
        of a valid sequences. May exclude other version directory names that are
        not part of the valid sequence if an error is thrown.
        """
        all_versions = []
        if not isinstance(versions, dict):
            self.error("E044")
            return all_versions
        if len(versions) == 0:
            self.error("E008")
            return all_versions
        # Validate version sequence
        # https://ocfl.io/draft/spec/#version-directories
        zero_padded = None
        max_version_num = 999999  # Excessive limit
        if 'v1' in versions:
            fmt = 'v%d'
            zero_padded = False
            all_versions.append('v1')
        else:  # Find padding size
            for n in range(2, 11):
                fmt = 'v%0' + str(n) + 'd'
                vkey = fmt % 1
                if vkey in versions:
                    all_versions.append(vkey)
                    zero_padded = n
                    max_version_num = (10 ** (n - 1)) - 1
                    break
            if not zero_padded:
                self.error("E009")
                return all_versions
        if zero_padded:
            self.warning("W001")
        # Have v1 and know format, work through to check sequence
        for n in range(2, max_version_num + 1):
            v = (fmt % n)
            if v in versions:
                all_versions.append(v)
            else:
                if len(versions) != (n - 1):
                    self.error("E010")  # Extra version dirs outside sequence
                return all_versions
        # We have now included all possible versions up to the zero padding
        # size, if there are more versions than this number then we must
        # have extra that violate the zero-padding rule or are out of
        # sequence
        if len(versions) > max_version_num:
            self.error("E011")
        return all_versions

    def validate_versions(self, versions, all_versions, unnormalized_digests):
        """Validate versions blocks in inventory.

        Requires as input two things which are assumed to be structurally correct
        from prior basic validation:

          * versions - which is the JSON object (dict) from the inventory
          * all_versions - an ordered list of the versions to look at in versions
                           (all other keys in versions will be ignored)

        Returns a list of digests_used which can then be checked against the
        manifest.
        """
        digests_used = []
        for v in all_versions:
            version = versions[v]
            if 'created' not in version:
                self.error('E048', version=v)  # No created
            elif not isinstance(versions[v]['created'], str):
                self.error('E049d', version=v)  # Bad created
            else:
                created = versions[v]['created']
                try:
                    str_to_datetime(created)  # catch ValueError if fails
                    if not re.search(r'''(Z|[+-]\d\d:\d\d)$''', created):  # FIXME - kludge
                        self.error('E049a', version=v)
                    if not re.search(r'''T\d\d:\d\d:\d\d''', created):  # FIXME - kludge
                        self.error('E049b', version=v)
                except ValueError as e:
                    self.error('E049c', version=v, description=str(e))
            if 'state' in version:
                digests_used += self.validate_state_block(version['state'], version=v, unnormalized_digests=unnormalized_digests)
            else:
                self.error('E048c', version=v)
            if 'message' not in version:
                self.warning('W007a', version=v)
            elif not isinstance(version['message'], str):
                self.error('E094', version=v)
            if 'user' not in version:
                self.warning('W007b', version=v)
            else:
                user = version['user']
                if not isinstance(user, dict):
                    self.error('E054a', version=v)
                else:
                    if 'name' not in user or not isinstance(user['name'], str):
                        self.error('E054b', version=v)
                    if 'address' not in user:
                        self.warning('W008', version=v)
                    elif not isinstance(user['address'], str):
                        self.error('E054c', version=v)
                    elif not re.match(r'''\w{3,6}:''', user['address']):
                        self.warning('W009', version=v)
        return digests_used

    def validate_state_block(self, state, version, unnormalized_digests):
        """Validate state block in a version in an inventory.

        The version is used only for error reporting.

        Returns a list of content digests referenced in the state block.
        """
        digests = []
        logical_paths = set()
        logical_directories = set()
        if not isinstance(state, dict):
            self.error('E050c', version=version)
        else:
            digest_re = re.compile(self.digest_regex())
            for digest in state:
                if not digest_re.match(digest):
                    self.error('E050d', version=version, digest=digest)
                elif not isinstance(state[digest], list):
                    self.error('E050e', version=version, digest=digest)
                else:
                    for path in state[digest]:
                        if path in logical_paths:
                            self.error("E095a", version=version, path=path)
                        else:
                            self.check_logical_path(path, version, logical_paths, logical_directories)
                    if digest not in unnormalized_digests:
                        # Exact string value must match, not just normalized
                        self.error("E050f", version=version, digest=digest)
                    norm_digest = normalized_digest(digest, self.digest_algorithm)
                    digests.append(norm_digest)
            # Check for conflicting logical paths
            for path in logical_directories:
                if path in logical_paths:
                    self.error("E095b", version=version, path=path)
        return digests

    def check_content_paths_map_to_versions(self, manifest_files, all_versions):
        """Check that every content path starts with a valid version.

        The content directory component has already been checked in
        check_content_path(). We have already tested all paths enough
        to know that they can be split into at least 2 components.
        """
        for path in manifest_files:
            version_dir, dummy_rest = path.split('/', 1)
            if version_dir not in all_versions:
                self.error('E042b', path=path)

    def check_digests_present_and_used(self, manifest_files, digests_used):
        """Check all digests in manifest that are needed are present and used."""
        in_manifest = set(manifest_files.values())
        in_state = set(digests_used)
        not_in_manifest = in_state.difference(in_manifest)
        if len(not_in_manifest) > 0:
            self.error("E050a", digests=", ".join(sorted(not_in_manifest)))
        not_in_state = in_manifest.difference(in_state)
        if len(not_in_state) > 0:
            self.error("E107", digests=", ".join(sorted(not_in_state)))

    def digest_regex(self):
        """Return regex for validating un-normalized digest format."""
        try:
            return digest_regex(self.digest_algorithm)
        except ValueError:
            if not self.lax_digests:
                self.error('E026a', digest=self.digest_algorithm)
        # Match anything
        return r'''^.*$'''

    def check_logical_path(self, path, version, logical_paths, logical_directories):
        """Check logical path and accumulate paths/directories for E095b check.

        logical_paths and logical_directories are expected to be sets.

        Only adds good paths to the accumulated paths/directories.
        """
        if path.startswith('/') or path.endswith('/'):
            self.error("E053", version=version, path=path)
        else:
            elements = path.split('/')
            for element in elements:
                if element in ['.', '..', '']:
                    self.error("E052", version=version, path=path)
                    return
            # Accumulate paths and directories
            logical_paths.add(path)
            logical_directories.add('/'.join(elements[0:-1]))

    def check_content_path(self, path, content_paths, content_directories):
        """Check logical path and accumulate paths/directories for E101 check.

        Returns True if valid, else False. Only adds good paths to the
        accumulated paths/directories. We don't yet know the set of valid
        version directories so the check here is just for 'v' + digits.
        """
        if path.startswith('/') or path.endswith('/'):
            self.error("E100", path=path)
            return False
        m = re.match(r'''^(v\d+/''' + self.content_directory + r''')/(.+)''', path)
        if not m:
            self.error("E042a", path=path)
            return False
        elements = m.group(2).split('/')
        for element in elements:
            if element in ('', '.', '..'):
                self.error("E099", path=path)
                return False
        # Accumulate paths and directories if not seen before
        if path in content_paths:
            self.error("E101a", path=path)
            return False
        content_paths.add(path)
        content_directories.add('/'.join([m.group(1)] + elements[0:-1]))
        return True

    def validate_as_prior_version(self, prior):
        """Check that prior is a valid prior version of the current inventory object.

        The input variable prior is also expected to be an InventoryValidator object
        and both self and prior inventories are assumed to have been checked for
        internal consistency.
        """
        # Must have a subset of versions which also checks zero padding format etc.
        if not set(prior.all_versions) < set(self.all_versions):
            self.error('E066a', prior_head=prior.head)
        else:
            # Check references to files but realize that there might be different
            # digest algorithms between versions
            version = 'no-version'
            for version in prior.all_versions:
                # If the digest algorithm is the same then we can make a
                # direct check on whether the state blocks match
                if prior.digest_algorithm == self.digest_algorithm:
                    self.compare_states_for_version(prior, version)
                # Now check the mappings from state to logical path, which must
                # be consistent even if the digestAlgorithm is different between
                # versions. Get maps from logical paths to files on disk:
                prior_map = get_logical_path_map(prior.inventory, version)
                self_map = get_logical_path_map(self.inventory, version)
                # Look first for differences in logical paths listed
                only_in_prior = prior_map.keys() - self_map.keys()
                only_in_self = self_map.keys() - prior_map.keys()
                if only_in_prior or only_in_self:
                    if only_in_prior:
                        self.error('E066b', version=version, prior_head=prior.head, only_in=prior.head, logical_paths=','.join(only_in_prior))
                    if only_in_self:
                        self.error('E066b', version=version, prior_head=prior.head, only_in=self.where, logical_paths=','.join(only_in_self))
                else:
                    # Check them all in details - digests must match
                    for logical_path, this_map in prior_map.items():
                        if not this_map.issubset(self_map[logical_path]):
                            self.error('E066c', version=version, prior_head=prior.head,
                                       logical_path=logical_path, prior_content=','.join(this_map),
                                       current_content=','.join(self_map[logical_path]))
                # Check metadata
                prior_version = prior.inventory['versions'][version]
                self_version = self.inventory['versions'][version]
                for key in ('created', 'message', 'user'):
                    if prior_version.get(key) != self_version.get(key):
                        self.warning('W011', version=version, prior_head=prior.head, key=key)

    def compare_states_for_version(self, prior, version):
        """Compare state blocks for version between self and prior.

        Assumes the same digest algorithm in both, do not call otherwise!

        Looks only for digests that appear in one but not in the other, the code
        in validate_as_prior_version(..) does a check for whether the same sets
        of logical files appear and we don't want to duplicate an error message
        about that.

        While the mapping checks in validate_as_prior_version(..) do all that is
        necessary to detect an error, the additional errors that may be generated
        here provide more detailed diagnostics in the case that the digest
        algorithm is the same across versions being compared.
        """
        self_state = self.inventory['versions'][version]['state']
        prior_state = prior.inventory['versions'][version]['state']
        for digest in set(self_state.keys()).union(prior_state.keys()):
            if digest not in prior_state:
                self.error('E066d', version=version, prior_head=prior.head,
                           digest=digest, logical_files=', '.join(self_state[digest]))
            elif digest not in self_state:
                self.error('E066e', version=version, prior_head=prior.head,
                           digest=digest, logical_files=', '.join(prior_state[digest]))

if __name__ == "__main__":
    isT=True


    class TLogger():
        """Simplified logger to replace ValidationLogger."""

        def __init__(self):
            """Initialize."""
            self.clear()

        def error(self, code, **args):  # pylint: disable=unused-argument
            """Add error code, discard args."""
            self.errors.append(code)

        def warning(self, code, **args):  # pylint: disable=unused-argument
            """Add warn code, discard args."""
            self.warns.append(code)

        def clear(self):
            """Clear records."""
            self.errors = []
            self.warns = []


    log = TLogger()
    iv = InventoryValidator(log=log)
    manifest = {'file_aaa1': 'aaa', 'file_aaa2': 'aaa', 'file_bbb': 'bbb'}
    iv.check_digests_present_and_used(manifest, ['aaa', 'bbb'])
    if len(log.errors) != 0:
        isT=False
    iv.check_digests_present_and_used(manifest, ['aaa'])
    if not 'E107' in log.errors:
        isT=False
    log.clear()
    iv.check_digests_present_and_used(manifest, ['aaa', 'bbb', 'ccc'])
    if not 'E050a' in log.errors:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/inventory_validator_validate_as_prior_version_passk_validte.py
"""OCFL Inventory Validator.

Code to validate the Python representation of an OCFL Inventory
as read with json.load(). Does not examine anything in storage.
"""
import re

from digest import digest_regex, normalized_digest
from validation_logger import ValidationLogger
from w3c_datetime import str_to_datetime


def get_logical_path_map(inventory, version):
    """Get a map of logical paths in state to files on disk for version in inventory.

    Returns a dictionary: logical_path_in_state -> set(content_files)

    The set of content_files may includes references to duplicate files in
    later versions than the version being described.
    """
    state = inventory['versions'][version]['state']
    manifest = inventory['manifest']
    file_map = {}
    for digest in state:
        if digest in manifest:
            for file in state[digest]:
                file_map[file] = set(manifest[digest])
    return file_map


class InventoryValidator():
    """Class for OCFL Inventory Validator."""

    def __init__(self, log=None, where='???',
                 lax_digests=False, spec_version='1.0'):
        """Initialize OCFL Inventory Validator."""
        self.log = ValidationLogger() if log is None else log
        self.where = where
        self.spec_version = spec_version
        # Object state
        self.inventory = None
        self.id = None
        self.digest_algorithm = 'sha512'
        self.content_directory = 'content'
        self.all_versions = []
        self.manifest_files = None
        self.unnormalized_digests = None
        self.head = 'UNKNOWN'
        # Validation control
        self.lax_digests = lax_digests
        # Configuration
        self.spec_versions_supported = ('1.0', '1.1')

    def error(self, code, **args):
        """Error with added context."""
        self.log.error(code, where=self.where, **args)

    def warning(self, code, **args):
        """Warning with added context."""
        self.log.warning(code, where=self.where, **args)

    def validate(self, inventory, extract_spec_version=False):
        """Validate a given inventory.

        If extract_spec_version is True then will look at the type value to determine
        the specification version. In the case that there is no type value or it isn't
        valid, then other tests will be based on the version given in self.spec_version.
        """
        # Basic structure
        self.inventory = inventory
        if 'id' in inventory:
            iid = inventory['id']
            if not isinstance(iid, str) or iid == '':
                self.error("E037a")
            else:
                # URI syntax https://www.rfc-editor.org/rfc/rfc3986.html#section-3.1 :
                # scheme = ALPHA *( ALPHA / DIGIT / "+" / "-" / "." )
                if not re.match(r'''[a-z][a-z\d\+\-\.]*:.+''', iid, re.IGNORECASE):
                    self.warning("W005", id=iid)
                self.id = iid
        else:
            self.error("E036a")
        if 'type' not in inventory:
            self.error("E036b")
        elif not isinstance(inventory['type'], str):
            self.error("E999")
        elif extract_spec_version:
            m = re.match(r'''https://ocfl.io/(\d+.\d)/spec/#inventory''', inventory['type'])
            if not m:
                self.error('E038b', got=inventory['type'], assumed_spec_version=self.spec_version)
            elif m.group(1) in self.spec_versions_supported:
                self.spec_version = m.group(1)
            else:
                self.error("E038c", got=m.group(1), assumed_spec_version=self.spec_version)
        elif inventory['type'] != 'https://ocfl.io/' + self.spec_version + '/spec/#inventory':
            self.error("E038a", expected='https://ocfl.io/' + self.spec_version + '/spec/#inventory', got=inventory['type'])
        if 'digestAlgorithm' not in inventory:
            self.error("E036c")
        elif inventory['digestAlgorithm'] == 'sha512':
            pass
        elif self.lax_digests:
            self.digest_algorithm = inventory['digestAlgorithm']
        elif inventory['digestAlgorithm'] == 'sha256':
            self.warning("W004")
            self.digest_algorithm = inventory['digestAlgorithm']
        else:
            self.error("E039", digest_algorithm=inventory['digestAlgorithm'])
        if 'contentDirectory' in inventory:
            # Careful only to set self.content_directory if value is safe
            cd = inventory['contentDirectory']
            if not isinstance(cd, str) or '/' in cd:
                self.error("E017")
            elif cd in ('.', '..'):
                self.error("E018")
            else:
                self.content_directory = cd
        manifest_files_correct_format = None
        if 'manifest' not in inventory:
            self.error("E041a")
        else:
            (self.manifest_files, manifest_files_correct_format, self.unnormalized_digests) = self.validate_manifest(inventory['manifest'])
        digests_used = []
        if 'versions' not in inventory:
            self.error("E041b")
        else:
            self.all_versions = self.validate_version_sequence(inventory['versions'])
            digests_used = self.validate_versions(inventory['versions'], self.all_versions, self.unnormalized_digests)
        if 'head' not in inventory:
            self.error("E036d")
        elif len(self.all_versions) > 0:
            self.head = self.all_versions[-1]
            if inventory['head'] != self.head:
                self.error("E040", got=inventory['head'], expected=self.head)
        if len(self.all_versions) == 0:
            # Abort tests is we don't have a valid version sequence, otherwise
            # there will likely be spurious subsequent error reports
            return
        if len(self.all_versions) > 0:
            if manifest_files_correct_format is not None:
                self.check_content_paths_map_to_versions(manifest_files_correct_format, self.all_versions)
            if self.manifest_files is not None:
                self.check_digests_present_and_used(self.manifest_files, digests_used)
        if 'fixity' in inventory:
            self.validate_fixity(inventory['fixity'], self.manifest_files)

    def validate_manifest(self, manifest):
        """Validate manifest block in inventory.

        Returns:
          * manifest_files - a mapping from file to digest for each file in
              the manifest
          * manifest_files_correct_format - a simple list of the manifest file
              path that passed initial checks. They need to be checked for valid
              version directories later, when we know what version directories
              are valid
          * unnormalized_digests - a set of the original digests in unnormalized
              form that MUST match exactly the values used in state blocks
        """
        manifest_files = {}
        manifest_files_correct_format = []
        unnormalized_digests = set()
        manifest_digests = set()
        if not isinstance(manifest, dict):
            self.error('E041c')
        else:
            content_paths = set()
            content_directories = set()
            for digest in manifest:
                m = re.match(self.digest_regex(), digest)
                if not m:
                    self.error('E025a', digest=digest, algorithm=self.digest_algorithm)  # wrong form of digest
                elif not isinstance(manifest[digest], list):
                    self.error('E092', digest=digest)  # must have path list value
                else:
                    unnormalized_digests.add(digest)
                    norm_digest = normalized_digest(digest, self.digest_algorithm)
                    if norm_digest in manifest_digests:
                        # We have already seen this in different un-normalized form!
                        self.error("E096", digest=norm_digest)
                    else:
                        manifest_digests.add(norm_digest)
                    for file in manifest[digest]:
                        manifest_files[file] = norm_digest
                        if self.check_content_path(file, content_paths, content_directories):
                            manifest_files_correct_format.append(file)
            # Check for conflicting content paths
            for path in content_directories:
                if path in content_paths:
                    self.error("E101b", path=path)
        return manifest_files, manifest_files_correct_format, unnormalized_digests

    def validate_fixity(self, fixity, manifest_files):
        """Validate fixity block in inventory.

        Check the structure of the fixity block and makes sure that only files
        listed in the manifest are referenced.
        """
        if not isinstance(fixity, dict):
            # The value of fixity must be a JSON object. In v1.0 I catch not an object
            # as part of E056 but this was clarified as E111 in v1.1. The value may
            # be an empty object in either case
            self.error('E056a' if self.spec_version == '1.0' else 'E111')
        else:
            for digest_algorithm in fixity:
                known_digest = True
                try:
                    regex = digest_regex(digest_algorithm)
                except ValueError:
                    if not self.lax_digests:
                        self.error('E056b', algorithm=self.digest_algorithm)
                        continue
                    # Match anything
                    regex = r'''^.*$'''
                    known_digest = False
                fixity_algoritm_block = fixity[digest_algorithm]
                if not isinstance(fixity_algoritm_block, dict):
                    self.error('E057a', algorithm=self.digest_algorithm)
                else:
                    digests_seen = set()
                    for digest in fixity_algoritm_block:
                        m = re.match(regex, digest)
                        if not m:
                            self.error('E057b', digest=digest, algorithm=digest_algorithm)  # wrong form of digest
                        elif not isinstance(fixity_algoritm_block[digest], list):
                            self.error('E057c', digest=digest, algorithm=digest_algorithm)  # must have path list value
                        else:
                            if known_digest:
                                norm_digest = normalized_digest(digest, digest_algorithm)
                            else:
                                norm_digest = digest
                            if norm_digest in digests_seen:
                                # We have already seen this in different un-normalized form!
                                self.error("E097", digest=norm_digest, algorithm=digest_algorithm)
                            else:
                                digests_seen.add(norm_digest)
                            for file in fixity_algoritm_block[digest]:
                                if file not in manifest_files:
                                    self.error("E057d", digest=norm_digest, algorithm=digest_algorithm, path=file)

    def validate_version_sequence(self, versions):
        """Validate sequence of version names in versions block in inventory.

        Returns an array of in-sequence version directories that are part
        of a valid sequences. May exclude other version directory names that are
        not part of the valid sequence if an error is thrown.
        """
        all_versions = []
        if not isinstance(versions, dict):
            self.error("E044")
            return all_versions
        if len(versions) == 0:
            self.error("E008")
            return all_versions
        # Validate version sequence
        # https://ocfl.io/draft/spec/#version-directories
        zero_padded = None
        max_version_num = 999999  # Excessive limit
        if 'v1' in versions:
            fmt = 'v%d'
            zero_padded = False
            all_versions.append('v1')
        else:  # Find padding size
            for n in range(2, 11):
                fmt = 'v%0' + str(n) + 'd'
                vkey = fmt % 1
                if vkey in versions:
                    all_versions.append(vkey)
                    zero_padded = n
                    max_version_num = (10 ** (n - 1)) - 1
                    break
            if not zero_padded:
                self.error("E009")
                return all_versions
        if zero_padded:
            self.warning("W001")
        # Have v1 and know format, work through to check sequence
        for n in range(2, max_version_num + 1):
            v = (fmt % n)
            if v in versions:
                all_versions.append(v)
            else:
                if len(versions) != (n - 1):
                    self.error("E010")  # Extra version dirs outside sequence
                return all_versions
        # We have now included all possible versions up to the zero padding
        # size, if there are more versions than this number then we must
        # have extra that violate the zero-padding rule or are out of
        # sequence
        if len(versions) > max_version_num:
            self.error("E011")
        return all_versions

    def validate_versions(self, versions, all_versions, unnormalized_digests):
        """Validate versions blocks in inventory.

        Requires as input two things which are assumed to be structurally correct
        from prior basic validation:

          * versions - which is the JSON object (dict) from the inventory
          * all_versions - an ordered list of the versions to look at in versions
                           (all other keys in versions will be ignored)

        Returns a list of digests_used which can then be checked against the
        manifest.
        """
        digests_used = []
        for v in all_versions:
            version = versions[v]
            if 'created' not in version:
                self.error('E048', version=v)  # No created
            elif not isinstance(versions[v]['created'], str):
                self.error('E049d', version=v)  # Bad created
            else:
                created = versions[v]['created']
                try:
                    str_to_datetime(created)  # catch ValueError if fails
                    if not re.search(r'''(Z|[+-]\d\d:\d\d)$''', created):  # FIXME - kludge
                        self.error('E049a', version=v)
                    if not re.search(r'''T\d\d:\d\d:\d\d''', created):  # FIXME - kludge
                        self.error('E049b', version=v)
                except ValueError as e:
                    self.error('E049c', version=v, description=str(e))
            if 'state' in version:
                digests_used += self.validate_state_block(version['state'], version=v, unnormalized_digests=unnormalized_digests)
            else:
                self.error('E048c', version=v)
            if 'message' not in version:
                self.warning('W007a', version=v)
            elif not isinstance(version['message'], str):
                self.error('E094', version=v)
            if 'user' not in version:
                self.warning('W007b', version=v)
            else:
                user = version['user']
                if not isinstance(user, dict):
                    self.error('E054a', version=v)
                else:
                    if 'name' not in user or not isinstance(user['name'], str):
                        self.error('E054b', version=v)
                    if 'address' not in user:
                        self.warning('W008', version=v)
                    elif not isinstance(user['address'], str):
                        self.error('E054c', version=v)
                    elif not re.match(r'''\w{3,6}:''', user['address']):
                        self.warning('W009', version=v)
        return digests_used

    def validate_state_block(self, state, version, unnormalized_digests):
        """Validate state block in a version in an inventory.

        The version is used only for error reporting.

        Returns a list of content digests referenced in the state block.
        """
        digests = []
        logical_paths = set()
        logical_directories = set()
        if not isinstance(state, dict):
            self.error('E050c', version=version)
        else:
            digest_re = re.compile(self.digest_regex())
            for digest in state:
                if not digest_re.match(digest):
                    self.error('E050d', version=version, digest=digest)
                elif not isinstance(state[digest], list):
                    self.error('E050e', version=version, digest=digest)
                else:
                    for path in state[digest]:
                        if path in logical_paths:
                            self.error("E095a", version=version, path=path)
                        else:
                            self.check_logical_path(path, version, logical_paths, logical_directories)
                    if digest not in unnormalized_digests:
                        # Exact string value must match, not just normalized
                        self.error("E050f", version=version, digest=digest)
                    norm_digest = normalized_digest(digest, self.digest_algorithm)
                    digests.append(norm_digest)
            # Check for conflicting logical paths
            for path in logical_directories:
                if path in logical_paths:
                    self.error("E095b", version=version, path=path)
        return digests

    def check_content_paths_map_to_versions(self, manifest_files, all_versions):
        """Check that every content path starts with a valid version.

        The content directory component has already been checked in
        check_content_path(). We have already tested all paths enough
        to know that they can be split into at least 2 components.
        """
        for path in manifest_files:
            version_dir, dummy_rest = path.split('/', 1)
            if version_dir not in all_versions:
                self.error('E042b', path=path)

    def check_digests_present_and_used(self, manifest_files, digests_used):
        """Check all digests in manifest that are needed are present and used."""
        in_manifest = set(manifest_files.values())
        in_state = set(digests_used)
        not_in_manifest = in_state.difference(in_manifest)
        if len(not_in_manifest) > 0:
            self.error("E050a", digests=", ".join(sorted(not_in_manifest)))
        not_in_state = in_manifest.difference(in_state)
        if len(not_in_state) > 0:
            self.error("E107", digests=", ".join(sorted(not_in_state)))

    def digest_regex(self):
        """Return regex for validating un-normalized digest format."""
        try:
            return digest_regex(self.digest_algorithm)
        except ValueError:
            if not self.lax_digests:
                self.error('E026a', digest=self.digest_algorithm)
        # Match anything
        return r'''^.*$'''

    def check_logical_path(self, path, version, logical_paths, logical_directories):
        """Check logical path and accumulate paths/directories for E095b check.

        logical_paths and logical_directories are expected to be sets.

        Only adds good paths to the accumulated paths/directories.
        """
        if path.startswith('/') or path.endswith('/'):
            self.error("E053", version=version, path=path)
        else:
            elements = path.split('/')
            for element in elements:
                if element in ['.', '..', '']:
                    self.error("E052", version=version, path=path)
                    return
            # Accumulate paths and directories
            logical_paths.add(path)
            logical_directories.add('/'.join(elements[0:-1]))

    def check_content_path(self, path, content_paths, content_directories):
        """Check logical path and accumulate paths/directories for E101 check.

        Returns True if valid, else False. Only adds good paths to the
        accumulated paths/directories. We don't yet know the set of valid
        version directories so the check here is just for 'v' + digits.
        """
        if path.startswith('/') or path.endswith('/'):
            self.error("E100", path=path)
            return False
        m = re.match(r'''^(v\d+/''' + self.content_directory + r''')/(.+)''', path)
        if not m:
            self.error("E042a", path=path)
            return False
        elements = m.group(2).split('/')
        for element in elements:
            if element in ('', '.', '..'):
                self.error("E099", path=path)
                return False
        # Accumulate paths and directories if not seen before
        if path in content_paths:
            self.error("E101a", path=path)
            return False
        content_paths.add(path)
        content_directories.add('/'.join([m.group(1)] + elements[0:-1]))
        return True

    def validate_as_prior_version(self, prior):
        """Check that prior is a valid prior version of the current inventory object.

        The input variable prior is also expected to be an InventoryValidator object
        and both self and prior inventories are assumed to have been checked for
        internal consistency.
        """
        # Must have a subset of versions which also checks zero padding format etc.
        if not set(prior.all_versions) < set(self.all_versions):
            self.error('E066a', prior_head=prior.head)
        else:
            # Check references to files but realize that there might be different
            # digest algorithms between versions
            version = 'no-version'
            for version in prior.all_versions:
                # If the digest algorithm is the same then we can make a
                # direct check on whether the state blocks match
                if prior.digest_algorithm == self.digest_algorithm:
                    self.compare_states_for_version(prior, version)
                # Now check the mappings from state to logical path, which must
                # be consistent even if the digestAlgorithm is different between
                # versions. Get maps from logical paths to files on disk:
                prior_map = get_logical_path_map(prior.inventory, version)
                self_map = get_logical_path_map(self.inventory, version)
                # Look first for differences in logical paths listed
                only_in_prior = prior_map.keys() - self_map.keys()
                only_in_self = self_map.keys() - prior_map.keys()
                if only_in_prior or only_in_self:
                    if only_in_prior:
                        self.error('E066b', version=version, prior_head=prior.head, only_in=prior.head, logical_paths=','.join(only_in_prior))
                    if only_in_self:
                        self.error('E066b', version=version, prior_head=prior.head, only_in=self.where, logical_paths=','.join(only_in_self))
                else:
                    # Check them all in details - digests must match
                    for logical_path, this_map in prior_map.items():
                        if not this_map.issubset(self_map[logical_path]):
                            self.error('E066c', version=version, prior_head=prior.head,
                                       logical_path=logical_path, prior_content=','.join(this_map),
                                       current_content=','.join(self_map[logical_path]))
                # Check metadata
                prior_version = prior.inventory['versions'][version]
                self_version = self.inventory['versions'][version]
                for key in ('created', 'message', 'user'):
                    if prior_version.get(key) != self_version.get(key):
                        self.warning('W011', version=version, prior_head=prior.head, key=key)

    def compare_states_for_version(self, prior, version):
        """Compare state blocks for version between self and prior.

        Assumes the same digest algorithm in both, do not call otherwise!

        Looks only for digests that appear in one but not in the other, the code
        in validate_as_prior_version(..) does a check for whether the same sets
        of logical files appear and we don't want to duplicate an error message
        about that.

        While the mapping checks in validate_as_prior_version(..) do all that is
        necessary to detect an error, the additional errors that may be generated
        here provide more detailed diagnostics in the case that the digest
        algorithm is the same across versions being compared.
        """
        self_state = self.inventory['versions'][version]['state']
        prior_state = prior.inventory['versions'][version]['state']
        for digest in set(self_state.keys()).union(prior_state.keys()):
            if digest not in prior_state:
                self.error('E066d', version=version, prior_head=prior.head,
                           digest=digest, logical_files=', '.join(self_state[digest]))
            elif digest not in self_state:
                self.error('E066e', version=version, prior_head=prior.head,
                           digest=digest, logical_files=', '.join(prior_state[digest]))

if __name__ == "__main__":
    isT=True


    class TLogger():
        """Simplified logger to replace ValidationLogger."""

        def __init__(self):
            """Initialize."""
            self.clear()

        def error(self, code, **args):  # pylint: disable=unused-argument
            """Add error code, discard args."""
            self.errors.append(code)

        def warning(self, code, **args):  # pylint: disable=unused-argument
            """Add warn code, discard args."""
            self.warns.append(code)

        def clear(self):
            """Clear records."""
            self.errors = []
            self.warns = []


    log = TLogger()
    iv = InventoryValidator(log=log)
    prior = InventoryValidator(log=TLogger())
    # Same versions won't work...
    iv.all_versions = ['v1']
    prior.all_versions = ['v1']
    iv.validate_as_prior_version(prior)
    if log.errors != ['E066a']:
        isT=False
    log.clear()
    # Good inventory in spite of diferent digests
    iv.all_versions = ['v1', 'v2']
    iv.digest_algorithm = 'a1'
    iv.inventory = {"manifest": {"a1d1": ["v1/content/f1"],
                                 "a1d2": ["v1/content/f2"],
                                 "a1d3": ["v2/content/f3"]},
                    "versions": {"v1": {"state": {"a1d1": ["f1"], "a1d2": ["f2"]}},
                                 "v2": {"state": {"a1d1": ["f1"], "a1d3": ["f3"]}}}}
    prior.digest_algorithm = 'a2'
    prior.inventory = {"manifest": {"a2d1": ["v1/content/f1"],
                                    "a2d2": ["v1/content/f2"]},
                       "versions": {"v1": {"state": {"a2d1": ["f1"], "a2d2": ["f2"]}}}}
    iv.validate_as_prior_version(prior)
    if log.errors!=[]:
        isT=False
    log.clear()
    # Now let's add a copy file in the state in prior so as not to match
    prior.inventory["versions"]["v1"]["state"]["a2d2"] = ["f2", "f2-copy"]
    iv.validate_as_prior_version(prior)
    if log.errors!=["E066b"]:
        isT=False
    log.clear()
    # Now move that back but change a a manifest location
    prior.inventory["versions"]["v1"]["state"]["a2d2"] = ["f2"]
    prior.inventory["manifest"]["a2d2"] = ["v1/content/f2--moved"]
    iv.validate_as_prior_version(prior)
    if log.errors!=["E066c"]:
        isT=False

    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/inventory_validator_get_logical_path_map_passk_validte.py
"""OCFL Inventory Validator.

Code to validate the Python representation of an OCFL Inventory
as read with json.load(). Does not examine anything in storage.
"""
import re

from digest import digest_regex, normalized_digest
from validation_logger import ValidationLogger
from w3c_datetime import str_to_datetime


def get_logical_path_map(inventory, version):
    """Get a map of logical paths in state to files on disk for version in inventory.

    Returns a dictionary: logical_path_in_state -> set(content_files)

    The set of content_files may includes references to duplicate files in
    later versions than the version being described.
    """
    state = inventory['versions'][version]['state']
    manifest = inventory['manifest']
    file_map = {}
    for digest in state:
        if digest in manifest:
            for file in state[digest]:
                file_map[file] = set(manifest[digest])
    return file_map


class InventoryValidator():
    """Class for OCFL Inventory Validator."""

    def __init__(self, log=None, where='???',
                 lax_digests=False, spec_version='1.0'):
        """Initialize OCFL Inventory Validator."""
        self.log = ValidationLogger() if log is None else log
        self.where = where
        self.spec_version = spec_version
        # Object state
        self.inventory = None
        self.id = None
        self.digest_algorithm = 'sha512'
        self.content_directory = 'content'
        self.all_versions = []
        self.manifest_files = None
        self.unnormalized_digests = None
        self.head = 'UNKNOWN'
        # Validation control
        self.lax_digests = lax_digests
        # Configuration
        self.spec_versions_supported = ('1.0', '1.1')

    def error(self, code, **args):
        """Error with added context."""
        self.log.error(code, where=self.where, **args)

    def warning(self, code, **args):
        """Warning with added context."""
        self.log.warning(code, where=self.where, **args)

    def validate(self, inventory, extract_spec_version=False):
        """Validate a given inventory.

        If extract_spec_version is True then will look at the type value to determine
        the specification version. In the case that there is no type value or it isn't
        valid, then other tests will be based on the version given in self.spec_version.
        """
        # Basic structure
        self.inventory = inventory
        if 'id' in inventory:
            iid = inventory['id']
            if not isinstance(iid, str) or iid == '':
                self.error("E037a")
            else:
                # URI syntax https://www.rfc-editor.org/rfc/rfc3986.html#section-3.1 :
                # scheme = ALPHA *( ALPHA / DIGIT / "+" / "-" / "." )
                if not re.match(r'''[a-z][a-z\d\+\-\.]*:.+''', iid, re.IGNORECASE):
                    self.warning("W005", id=iid)
                self.id = iid
        else:
            self.error("E036a")
        if 'type' not in inventory:
            self.error("E036b")
        elif not isinstance(inventory['type'], str):
            self.error("E999")
        elif extract_spec_version:
            m = re.match(r'''https://ocfl.io/(\d+.\d)/spec/#inventory''', inventory['type'])
            if not m:
                self.error('E038b', got=inventory['type'], assumed_spec_version=self.spec_version)
            elif m.group(1) in self.spec_versions_supported:
                self.spec_version = m.group(1)
            else:
                self.error("E038c", got=m.group(1), assumed_spec_version=self.spec_version)
        elif inventory['type'] != 'https://ocfl.io/' + self.spec_version + '/spec/#inventory':
            self.error("E038a", expected='https://ocfl.io/' + self.spec_version + '/spec/#inventory', got=inventory['type'])
        if 'digestAlgorithm' not in inventory:
            self.error("E036c")
        elif inventory['digestAlgorithm'] == 'sha512':
            pass
        elif self.lax_digests:
            self.digest_algorithm = inventory['digestAlgorithm']
        elif inventory['digestAlgorithm'] == 'sha256':
            self.warning("W004")
            self.digest_algorithm = inventory['digestAlgorithm']
        else:
            self.error("E039", digest_algorithm=inventory['digestAlgorithm'])
        if 'contentDirectory' in inventory:
            # Careful only to set self.content_directory if value is safe
            cd = inventory['contentDirectory']
            if not isinstance(cd, str) or '/' in cd:
                self.error("E017")
            elif cd in ('.', '..'):
                self.error("E018")
            else:
                self.content_directory = cd
        manifest_files_correct_format = None
        if 'manifest' not in inventory:
            self.error("E041a")
        else:
            (self.manifest_files, manifest_files_correct_format, self.unnormalized_digests) = self.validate_manifest(inventory['manifest'])
        digests_used = []
        if 'versions' not in inventory:
            self.error("E041b")
        else:
            self.all_versions = self.validate_version_sequence(inventory['versions'])
            digests_used = self.validate_versions(inventory['versions'], self.all_versions, self.unnormalized_digests)
        if 'head' not in inventory:
            self.error("E036d")
        elif len(self.all_versions) > 0:
            self.head = self.all_versions[-1]
            if inventory['head'] != self.head:
                self.error("E040", got=inventory['head'], expected=self.head)
        if len(self.all_versions) == 0:
            # Abort tests is we don't have a valid version sequence, otherwise
            # there will likely be spurious subsequent error reports
            return
        if len(self.all_versions) > 0:
            if manifest_files_correct_format is not None:
                self.check_content_paths_map_to_versions(manifest_files_correct_format, self.all_versions)
            if self.manifest_files is not None:
                self.check_digests_present_and_used(self.manifest_files, digests_used)
        if 'fixity' in inventory:
            self.validate_fixity(inventory['fixity'], self.manifest_files)

    def validate_manifest(self, manifest):
        """Validate manifest block in inventory.

        Returns:
          * manifest_files - a mapping from file to digest for each file in
              the manifest
          * manifest_files_correct_format - a simple list of the manifest file
              path that passed initial checks. They need to be checked for valid
              version directories later, when we know what version directories
              are valid
          * unnormalized_digests - a set of the original digests in unnormalized
              form that MUST match exactly the values used in state blocks
        """
        manifest_files = {}
        manifest_files_correct_format = []
        unnormalized_digests = set()
        manifest_digests = set()
        if not isinstance(manifest, dict):
            self.error('E041c')
        else:
            content_paths = set()
            content_directories = set()
            for digest in manifest:
                m = re.match(self.digest_regex(), digest)
                if not m:
                    self.error('E025a', digest=digest, algorithm=self.digest_algorithm)  # wrong form of digest
                elif not isinstance(manifest[digest], list):
                    self.error('E092', digest=digest)  # must have path list value
                else:
                    unnormalized_digests.add(digest)
                    norm_digest = normalized_digest(digest, self.digest_algorithm)
                    if norm_digest in manifest_digests:
                        # We have already seen this in different un-normalized form!
                        self.error("E096", digest=norm_digest)
                    else:
                        manifest_digests.add(norm_digest)
                    for file in manifest[digest]:
                        manifest_files[file] = norm_digest
                        if self.check_content_path(file, content_paths, content_directories):
                            manifest_files_correct_format.append(file)
            # Check for conflicting content paths
            for path in content_directories:
                if path in content_paths:
                    self.error("E101b", path=path)
        return manifest_files, manifest_files_correct_format, unnormalized_digests

    def validate_fixity(self, fixity, manifest_files):
        """Validate fixity block in inventory.

        Check the structure of the fixity block and makes sure that only files
        listed in the manifest are referenced.
        """
        if not isinstance(fixity, dict):
            # The value of fixity must be a JSON object. In v1.0 I catch not an object
            # as part of E056 but this was clarified as E111 in v1.1. The value may
            # be an empty object in either case
            self.error('E056a' if self.spec_version == '1.0' else 'E111')
        else:
            for digest_algorithm in fixity:
                known_digest = True
                try:
                    regex = digest_regex(digest_algorithm)
                except ValueError:
                    if not self.lax_digests:
                        self.error('E056b', algorithm=self.digest_algorithm)
                        continue
                    # Match anything
                    regex = r'''^.*$'''
                    known_digest = False
                fixity_algoritm_block = fixity[digest_algorithm]
                if not isinstance(fixity_algoritm_block, dict):
                    self.error('E057a', algorithm=self.digest_algorithm)
                else:
                    digests_seen = set()
                    for digest in fixity_algoritm_block:
                        m = re.match(regex, digest)
                        if not m:
                            self.error('E057b', digest=digest, algorithm=digest_algorithm)  # wrong form of digest
                        elif not isinstance(fixity_algoritm_block[digest], list):
                            self.error('E057c', digest=digest, algorithm=digest_algorithm)  # must have path list value
                        else:
                            if known_digest:
                                norm_digest = normalized_digest(digest, digest_algorithm)
                            else:
                                norm_digest = digest
                            if norm_digest in digests_seen:
                                # We have already seen this in different un-normalized form!
                                self.error("E097", digest=norm_digest, algorithm=digest_algorithm)
                            else:
                                digests_seen.add(norm_digest)
                            for file in fixity_algoritm_block[digest]:
                                if file not in manifest_files:
                                    self.error("E057d", digest=norm_digest, algorithm=digest_algorithm, path=file)

    def validate_version_sequence(self, versions):
        """Validate sequence of version names in versions block in inventory.

        Returns an array of in-sequence version directories that are part
        of a valid sequences. May exclude other version directory names that are
        not part of the valid sequence if an error is thrown.
        """
        all_versions = []
        if not isinstance(versions, dict):
            self.error("E044")
            return all_versions
        if len(versions) == 0:
            self.error("E008")
            return all_versions
        # Validate version sequence
        # https://ocfl.io/draft/spec/#version-directories
        zero_padded = None
        max_version_num = 999999  # Excessive limit
        if 'v1' in versions:
            fmt = 'v%d'
            zero_padded = False
            all_versions.append('v1')
        else:  # Find padding size
            for n in range(2, 11):
                fmt = 'v%0' + str(n) + 'd'
                vkey = fmt % 1
                if vkey in versions:
                    all_versions.append(vkey)
                    zero_padded = n
                    max_version_num = (10 ** (n - 1)) - 1
                    break
            if not zero_padded:
                self.error("E009")
                return all_versions
        if zero_padded:
            self.warning("W001")
        # Have v1 and know format, work through to check sequence
        for n in range(2, max_version_num + 1):
            v = (fmt % n)
            if v in versions:
                all_versions.append(v)
            else:
                if len(versions) != (n - 1):
                    self.error("E010")  # Extra version dirs outside sequence
                return all_versions
        # We have now included all possible versions up to the zero padding
        # size, if there are more versions than this number then we must
        # have extra that violate the zero-padding rule or are out of
        # sequence
        if len(versions) > max_version_num:
            self.error("E011")
        return all_versions

    def validate_versions(self, versions, all_versions, unnormalized_digests):
        """Validate versions blocks in inventory.

        Requires as input two things which are assumed to be structurally correct
        from prior basic validation:

          * versions - which is the JSON object (dict) from the inventory
          * all_versions - an ordered list of the versions to look at in versions
                           (all other keys in versions will be ignored)

        Returns a list of digests_used which can then be checked against the
        manifest.
        """
        digests_used = []
        for v in all_versions:
            version = versions[v]
            if 'created' not in version:
                self.error('E048', version=v)  # No created
            elif not isinstance(versions[v]['created'], str):
                self.error('E049d', version=v)  # Bad created
            else:
                created = versions[v]['created']
                try:
                    str_to_datetime(created)  # catch ValueError if fails
                    if not re.search(r'''(Z|[+-]\d\d:\d\d)$''', created):  # FIXME - kludge
                        self.error('E049a', version=v)
                    if not re.search(r'''T\d\d:\d\d:\d\d''', created):  # FIXME - kludge
                        self.error('E049b', version=v)
                except ValueError as e:
                    self.error('E049c', version=v, description=str(e))
            if 'state' in version:
                digests_used += self.validate_state_block(version['state'], version=v, unnormalized_digests=unnormalized_digests)
            else:
                self.error('E048c', version=v)
            if 'message' not in version:
                self.warning('W007a', version=v)
            elif not isinstance(version['message'], str):
                self.error('E094', version=v)
            if 'user' not in version:
                self.warning('W007b', version=v)
            else:
                user = version['user']
                if not isinstance(user, dict):
                    self.error('E054a', version=v)
                else:
                    if 'name' not in user or not isinstance(user['name'], str):
                        self.error('E054b', version=v)
                    if 'address' not in user:
                        self.warning('W008', version=v)
                    elif not isinstance(user['address'], str):
                        self.error('E054c', version=v)
                    elif not re.match(r'''\w{3,6}:''', user['address']):
                        self.warning('W009', version=v)
        return digests_used

    def validate_state_block(self, state, version, unnormalized_digests):
        """Validate state block in a version in an inventory.

        The version is used only for error reporting.

        Returns a list of content digests referenced in the state block.
        """
        digests = []
        logical_paths = set()
        logical_directories = set()
        if not isinstance(state, dict):
            self.error('E050c', version=version)
        else:
            digest_re = re.compile(self.digest_regex())
            for digest in state:
                if not digest_re.match(digest):
                    self.error('E050d', version=version, digest=digest)
                elif not isinstance(state[digest], list):
                    self.error('E050e', version=version, digest=digest)
                else:
                    for path in state[digest]:
                        if path in logical_paths:
                            self.error("E095a", version=version, path=path)
                        else:
                            self.check_logical_path(path, version, logical_paths, logical_directories)
                    if digest not in unnormalized_digests:
                        # Exact string value must match, not just normalized
                        self.error("E050f", version=version, digest=digest)
                    norm_digest = normalized_digest(digest, self.digest_algorithm)
                    digests.append(norm_digest)
            # Check for conflicting logical paths
            for path in logical_directories:
                if path in logical_paths:
                    self.error("E095b", version=version, path=path)
        return digests

    def check_content_paths_map_to_versions(self, manifest_files, all_versions):
        """Check that every content path starts with a valid version.

        The content directory component has already been checked in
        check_content_path(). We have already tested all paths enough
        to know that they can be split into at least 2 components.
        """
        for path in manifest_files:
            version_dir, dummy_rest = path.split('/', 1)
            if version_dir not in all_versions:
                self.error('E042b', path=path)

    def check_digests_present_and_used(self, manifest_files, digests_used):
        """Check all digests in manifest that are needed are present and used."""
        in_manifest = set(manifest_files.values())
        in_state = set(digests_used)
        not_in_manifest = in_state.difference(in_manifest)
        if len(not_in_manifest) > 0:
            self.error("E050a", digests=", ".join(sorted(not_in_manifest)))
        not_in_state = in_manifest.difference(in_state)
        if len(not_in_state) > 0:
            self.error("E107", digests=", ".join(sorted(not_in_state)))

    def digest_regex(self):
        """Return regex for validating un-normalized digest format."""
        try:
            return digest_regex(self.digest_algorithm)
        except ValueError:
            if not self.lax_digests:
                self.error('E026a', digest=self.digest_algorithm)
        # Match anything
        return r'''^.*$'''

    def check_logical_path(self, path, version, logical_paths, logical_directories):
        """Check logical path and accumulate paths/directories for E095b check.

        logical_paths and logical_directories are expected to be sets.

        Only adds good paths to the accumulated paths/directories.
        """
        if path.startswith('/') or path.endswith('/'):
            self.error("E053", version=version, path=path)
        else:
            elements = path.split('/')
            for element in elements:
                if element in ['.', '..', '']:
                    self.error("E052", version=version, path=path)
                    return
            # Accumulate paths and directories
            logical_paths.add(path)
            logical_directories.add('/'.join(elements[0:-1]))

    def check_content_path(self, path, content_paths, content_directories):
        """Check logical path and accumulate paths/directories for E101 check.

        Returns True if valid, else False. Only adds good paths to the
        accumulated paths/directories. We don't yet know the set of valid
        version directories so the check here is just for 'v' + digits.
        """
        if path.startswith('/') or path.endswith('/'):
            self.error("E100", path=path)
            return False
        m = re.match(r'''^(v\d+/''' + self.content_directory + r''')/(.+)''', path)
        if not m:
            self.error("E042a", path=path)
            return False
        elements = m.group(2).split('/')
        for element in elements:
            if element in ('', '.', '..'):
                self.error("E099", path=path)
                return False
        # Accumulate paths and directories if not seen before
        if path in content_paths:
            self.error("E101a", path=path)
            return False
        content_paths.add(path)
        content_directories.add('/'.join([m.group(1)] + elements[0:-1]))
        return True

    def validate_as_prior_version(self, prior):
        """Check that prior is a valid prior version of the current inventory object.

        The input variable prior is also expected to be an InventoryValidator object
        and both self and prior inventories are assumed to have been checked for
        internal consistency.
        """
        # Must have a subset of versions which also checks zero padding format etc.
        if not set(prior.all_versions) < set(self.all_versions):
            self.error('E066a', prior_head=prior.head)
        else:
            # Check references to files but realize that there might be different
            # digest algorithms between versions
            version = 'no-version'
            for version in prior.all_versions:
                # If the digest algorithm is the same then we can make a
                # direct check on whether the state blocks match
                if prior.digest_algorithm == self.digest_algorithm:
                    self.compare_states_for_version(prior, version)
                # Now check the mappings from state to logical path, which must
                # be consistent even if the digestAlgorithm is different between
                # versions. Get maps from logical paths to files on disk:
                prior_map = get_logical_path_map(prior.inventory, version)
                self_map = get_logical_path_map(self.inventory, version)
                # Look first for differences in logical paths listed
                only_in_prior = prior_map.keys() - self_map.keys()
                only_in_self = self_map.keys() - prior_map.keys()
                if only_in_prior or only_in_self:
                    if only_in_prior:
                        self.error('E066b', version=version, prior_head=prior.head, only_in=prior.head, logical_paths=','.join(only_in_prior))
                    if only_in_self:
                        self.error('E066b', version=version, prior_head=prior.head, only_in=self.where, logical_paths=','.join(only_in_self))
                else:
                    # Check them all in details - digests must match
                    for logical_path, this_map in prior_map.items():
                        if not this_map.issubset(self_map[logical_path]):
                            self.error('E066c', version=version, prior_head=prior.head,
                                       logical_path=logical_path, prior_content=','.join(this_map),
                                       current_content=','.join(self_map[logical_path]))
                # Check metadata
                prior_version = prior.inventory['versions'][version]
                self_version = self.inventory['versions'][version]
                for key in ('created', 'message', 'user'):
                    if prior_version.get(key) != self_version.get(key):
                        self.warning('W011', version=version, prior_head=prior.head, key=key)

    def compare_states_for_version(self, prior, version):
        """Compare state blocks for version between self and prior.

        Assumes the same digest algorithm in both, do not call otherwise!

        Looks only for digests that appear in one but not in the other, the code
        in validate_as_prior_version(..) does a check for whether the same sets
        of logical files appear and we don't want to duplicate an error message
        about that.

        While the mapping checks in validate_as_prior_version(..) do all that is
        necessary to detect an error, the additional errors that may be generated
        here provide more detailed diagnostics in the case that the digest
        algorithm is the same across versions being compared.
        """
        self_state = self.inventory['versions'][version]['state']
        prior_state = prior.inventory['versions'][version]['state']
        for digest in set(self_state.keys()).union(prior_state.keys()):
            if digest not in prior_state:
                self.error('E066d', version=version, prior_head=prior.head,
                           digest=digest, logical_files=', '.join(self_state[digest]))
            elif digest not in self_state:
                self.error('E066e', version=version, prior_head=prior.head,
                           digest=digest, logical_files=', '.join(prior_state[digest]))

if __name__ == "__main__":
    import dill
    import os
    isT=True
    args0_ls = [{'manifest': {'a2d1': ['v1/content/f1'], 'a2d2': ['v1/content/f2']},
     'versions': {'v1': {'state': {'a2d1': ['f1'], 'a2d2': ['f2']}}}},

    {'manifest': {'a1d1': ['v1/content/f1'], 'a1d2': ['v1/content/f2'], 'a1d3': ['v2/content/f3']},
     'versions': {'v1': {'state': {'a1d1': ['f1'], 'a1d2': ['f2']}}, 'v2': {'state': {'a1d1': ['f1'], 'a1d3': ['f3']}}}},

    {'manifest': {'a2d1': ['v1/content/f1'], 'a2d2': ['v1/content/f2']},
     'versions': {'v1': {'state': {'a2d1': ['f1'], 'a2d2': ['f2', 'f2-copy']}}}},

    {'digestAlgorithm': 'sha512', 'head': 'v2', 'id': 'info:bb123cd4567', 'manifest': {
        '69f54f2e9f4568f7df4a4c3b07e4cbda4ba3bba7913c5218add6dea891817a80ce829b877d7a84ce47f93cbad8aa522bf7dd8eda2778e16bdf3c47cf49ee3bdf': [
            'v1/content/my_content/poe.txt'],
        'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
            'v1/content/my_content/dracula.txt']}, 'type': 'https://ocfl.io/1.0/spec/#inventory', 'versions': {
        'v1': {'created': '2020-01-01T00:00:00Z', 'message': 'First version', 'state': {
            '69f54f2e9f4568f7df4a4c3b07e4cbda4ba3bba7913c5218add6dea891817a80ce829b877d7a84ce47f93cbad8aa522bf7dd8eda2778e16bdf3c47cf49ee3bdf': [
                'my_content/poe.txt'],
            'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
                'my_content/dracula.txt']},
               'user': {'address': 'mailto:all_seeing_spheres@miskatonic.edu', 'name': 'Yog-Sothoth'}},
        'v2': {'created': '2020-01-02T00:00:00Z', 'message': 'Second version', 'state': {
            '69f54f2e9f4568f7df4a4c3b07e4cbda4ba3bba7913c5218add6dea891817a80ce829b877d7a84ce47f93cbad8aa522bf7dd8eda2778e16bdf3c47cf49ee3bdf': [
                'my_content/poe-nevermore.txt'],
            'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
                'my_content/a_second_copy_of_dracula.txt', 'my_content/another_directory/a_third_copy_of_dracula.txt',
                'my_content/dracula.txt']},
               'user': {'address': 'mailto:all_seeing_spheres@miskatonic.edu', 'name': 'Yog-Sothoth'}}}},

    {'digestAlgorithm': 'sha512', 'head': 'v2', 'id': 'info:bb123cd4567', 'manifest': {
        '69f54f2e9f4568f7df4a4c3b07e4cbda4ba3bba7913c5218add6dea891817a80ce829b877d7a84ce47f93cbad8aa522bf7dd8eda2778e16bdf3c47cf49ee3bdf': [
            'v1/content/my_content/poe.txt'],
        'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
            'v1/content/my_content/dracula.txt']}, 'type': 'https://ocfl.io/1.0/spec/#inventory', 'versions': {
        'v1': {'created': '2020-01-01T00:00:00Z', 'message': 'First version', 'state': {
            '69f54f2e9f4568f7df4a4c3b07e4cbda4ba3bba7913c5218add6dea891817a80ce829b877d7a84ce47f93cbad8aa522bf7dd8eda2778e16bdf3c47cf49ee3bdf': [
                'my_content/poe.txt'],
            'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
                'my_content/dracula.txt']},
               'user': {'address': 'mailto:all_seeing_spheres@miskatonic.edu', 'name': 'Yog-Sothoth'}},
        'v2': {'created': '2020-01-02T00:00:00Z', 'message': 'Second version', 'state': {
            '69f54f2e9f4568f7df4a4c3b07e4cbda4ba3bba7913c5218add6dea891817a80ce829b877d7a84ce47f93cbad8aa522bf7dd8eda2778e16bdf3c47cf49ee3bdf': [
                'my_content/poe-nevermore.txt'],
            'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
                'my_content/a_second_copy_of_dracula.txt', 'my_content/another_directory/a_third_copy_of_dracula.txt',
                'my_content/dracula.txt']},
               'user': {'address': 'mailto:all_seeing_spheres@miskatonic.edu', 'name': 'Yog-Sothoth'}}}},

    {'digestAlgorithm': 'sha512', 'head': 'v3', 'id': 'info:bb123cd4567', 'manifest': {
        '242a60b18a716f1e88ebbb3a546a119009671dc210317be1cca206650db471c8d84769d495b4e169bfe8200b4d6d60520aa75fe99e401bd7738107b7b0ca0bcd': [
            'v3/content/my_content/poe-nevermore.txt'],
        '69f54f2e9f4568f7df4a4c3b07e4cbda4ba3bba7913c5218add6dea891817a80ce829b877d7a84ce47f93cbad8aa522bf7dd8eda2778e16bdf3c47cf49ee3bdf': [
            'v1/content/my_content/poe.txt'],
        'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
            'v1/content/my_content/dracula.txt']}, 'type': 'https://ocfl.io/1.0/spec/#inventory', 'versions': {
        'v1': {'created': '2020-01-01T00:00:00Z', 'message': 'First version', 'state': {
            '69f54f2e9f4568f7df4a4c3b07e4cbda4ba3bba7913c5218add6dea891817a80ce829b877d7a84ce47f93cbad8aa522bf7dd8eda2778e16bdf3c47cf49ee3bdf': [
                'my_content/poe.txt'],
            'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
                'my_content/dracula.txt']},
               'user': {'address': 'mailto:all_seeing_spheres@miskatonic.edu', 'name': 'Yog-Sothoth'}},
        'v2': {'created': '2020-01-02T00:00:00Z', 'message': 'Second version', 'state': {
            '69f54f2e9f4568f7df4a4c3b07e4cbda4ba3bba7913c5218add6dea891817a80ce829b877d7a84ce47f93cbad8aa522bf7dd8eda2778e16bdf3c47cf49ee3bdf': [
                'my_content/poe-nevermore.txt'],
            'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
                'my_content/a_second_copy_of_dracula.txt', 'my_content/another_directory/a_third_copy_of_dracula.txt',
                'my_content/dracula.txt']},
               'user': {'address': 'mailto:all_seeing_spheres@miskatonic.edu', 'name': 'Yog-Sothoth'}},
        'v3': {'created': '2020-01-03T00:00:00Z', 'message': 'Third version', 'state': {
            '242a60b18a716f1e88ebbb3a546a119009671dc210317be1cca206650db471c8d84769d495b4e169bfe8200b4d6d60520aa75fe99e401bd7738107b7b0ca0bcd': [
                'my_content/poe-nevermore.txt'],
            'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
                'my_content/another_directory/a_third_copy_of_dracula.txt', 'my_content/dracula.txt']},
               'user': {'address': 'mailto:all_seeing_spheres@miskatonic.edu', 'name': 'Yog-Sothoth'}}}},

    {'digestAlgorithm': 'sha512', 'head': 'v3', 'id': 'info:bb123cd4567', 'manifest': {
        '242a60b18a716f1e88ebbb3a546a119009671dc210317be1cca206650db471c8d84769d495b4e169bfe8200b4d6d60520aa75fe99e401bd7738107b7b0ca0bcd': [
            'v3/content/my_content/poe-nevermore.txt'],
        '69f54f2e9f4568f7df4a4c3b07e4cbda4ba3bba7913c5218add6dea891817a80ce829b877d7a84ce47f93cbad8aa522bf7dd8eda2778e16bdf3c47cf49ee3bdf': [
            'v1/content/my_content/poe.txt'],
        'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
            'v1/content/my_content/dracula.txt']}, 'type': 'https://ocfl.io/1.0/spec/#inventory', 'versions': {
        'v1': {'created': '2020-01-01T00:00:00Z', 'message': 'First version', 'state': {
            '69f54f2e9f4568f7df4a4c3b07e4cbda4ba3bba7913c5218add6dea891817a80ce829b877d7a84ce47f93cbad8aa522bf7dd8eda2778e16bdf3c47cf49ee3bdf': [
                'my_content/poe.txt'],
            'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
                'my_content/dracula.txt']},
               'user': {'address': 'mailto:all_seeing_spheres@miskatonic.edu', 'name': 'Yog-Sothoth'}},
        'v2': {'created': '2020-01-02T00:00:00Z', 'message': 'Second version', 'state': {
            '69f54f2e9f4568f7df4a4c3b07e4cbda4ba3bba7913c5218add6dea891817a80ce829b877d7a84ce47f93cbad8aa522bf7dd8eda2778e16bdf3c47cf49ee3bdf': [
                'my_content/poe-nevermore.txt'],
            'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
                'my_content/a_second_copy_of_dracula.txt', 'my_content/another_directory/a_third_copy_of_dracula.txt',
                'my_content/dracula.txt']},
               'user': {'address': 'mailto:all_seeing_spheres@miskatonic.edu', 'name': 'Yog-Sothoth'}},
        'v3': {'created': '2020-01-03T00:00:00Z', 'message': 'Third version', 'state': {
            '242a60b18a716f1e88ebbb3a546a119009671dc210317be1cca206650db471c8d84769d495b4e169bfe8200b4d6d60520aa75fe99e401bd7738107b7b0ca0bcd': [
                'my_content/poe-nevermore.txt'],
            'ffc150e7944b5cf5ddb899b2f48efffbd490f97632fc258434aefc4afb92aef2e3441ddcceae11404e5805e1b6c804083c9398c28f061c9ba42dd4bac53d5a2e': [
                'my_content/another_directory/a_third_copy_of_dracula.txt', 'my_content/dracula.txt']},
               'user': {'address': 'mailto:all_seeing_spheres@miskatonic.edu', 'name': 'Yog-Sothoth'}}}}
    ]
    args1 = 'v1'
    res_ls = [{'f1': {'v1/content/f1'}, 'f2': {'v1/content/f2'}},
{'f1': {'v1/content/f1'}, 'f2': {'v1/content/f2'}},
{'f1': {'v1/content/f1'}, 'f2': {'v1/content/f2'}, 'f2-copy': {'v1/content/f2'}},
{'my_content/poe.txt': {'v1/content/my_content/poe.txt'}, 'my_content/dracula.txt': {'v1/content/my_content/dracula.txt'}},
{'my_content/poe.txt': {'v1/content/my_content/poe.txt'}, 'my_content/dracula.txt': {'v1/content/my_content/dracula.txt'}},
{'my_content/poe.txt': {'v1/content/my_content/poe.txt'}, 'my_content/dracula.txt': {'v1/content/my_content/dracula.txt'}},
{'my_content/poe.txt': {'v1/content/my_content/poe.txt'}, 'my_content/dracula.txt': {'v1/content/my_content/dracula.txt'}}]

    for args0, target in zip(args0_ls, res_ls):
        res0 = get_logical_path_map(args0, args1)
        if res0 != target:
            isT=False
    # for l in os.listdir("D:/fse/python_test/repos/zimeon---ocfl-py/data_passk_platform1/62b45e165108cfac7f210a17/"):
    #     f = open("D:/fse/python_test/repos/zimeon---ocfl-py/data_passk_platform1/62b45e165108cfac7f210a17/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     # print(args0)
    #     # #print(args1)
    #     args1 = 'v1'
    #     args0 = {'manifest': {'a2d1': ['v1/content/f1'], 'a2d2': ['v1/content/f2']}, 'versions': {'v1': {'state': {'a2d1': ['f1'], 'a2d2': ['f2']}}}}
    #     print(content["output"][0]['bytes'])
    #     res0 = get_logical_path_map(args0,args1)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
            #break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/inventory_validator_validate_fixity_passk_validte.py
"""OCFL Inventory Validator.

Code to validate the Python representation of an OCFL Inventory
as read with json.load(). Does not examine anything in storage.
"""
import re

from digest import digest_regex, normalized_digest
from validation_logger import ValidationLogger
from w3c_datetime import str_to_datetime


def get_logical_path_map(inventory, version):
    """Get a map of logical paths in state to files on disk for version in inventory.

    Returns a dictionary: logical_path_in_state -> set(content_files)

    The set of content_files may includes references to duplicate files in
    later versions than the version being described.
    """
    state = inventory['versions'][version]['state']
    manifest = inventory['manifest']
    file_map = {}
    for digest in state:
        if digest in manifest:
            for file in state[digest]:
                file_map[file] = set(manifest[digest])
    return file_map


class InventoryValidator():
    """Class for OCFL Inventory Validator."""

    def __init__(self, log=None, where='???',
                 lax_digests=False, spec_version='1.0'):
        """Initialize OCFL Inventory Validator."""
        self.log = ValidationLogger() if log is None else log
        self.where = where
        self.spec_version = spec_version
        # Object state
        self.inventory = None
        self.id = None
        self.digest_algorithm = 'sha512'
        self.content_directory = 'content'
        self.all_versions = []
        self.manifest_files = None
        self.unnormalized_digests = None
        self.head = 'UNKNOWN'
        # Validation control
        self.lax_digests = lax_digests
        # Configuration
        self.spec_versions_supported = ('1.0', '1.1')

    def error(self, code, **args):
        """Error with added context."""
        self.log.error(code, where=self.where, **args)

    def warning(self, code, **args):
        """Warning with added context."""
        self.log.warning(code, where=self.where, **args)

    def validate(self, inventory, extract_spec_version=False):
        """Validate a given inventory.

        If extract_spec_version is True then will look at the type value to determine
        the specification version. In the case that there is no type value or it isn't
        valid, then other tests will be based on the version given in self.spec_version.
        """
        # Basic structure
        self.inventory = inventory
        if 'id' in inventory:
            iid = inventory['id']
            if not isinstance(iid, str) or iid == '':
                self.error("E037a")
            else:
                # URI syntax https://www.rfc-editor.org/rfc/rfc3986.html#section-3.1 :
                # scheme = ALPHA *( ALPHA / DIGIT / "+" / "-" / "." )
                if not re.match(r'''[a-z][a-z\d\+\-\.]*:.+''', iid, re.IGNORECASE):
                    self.warning("W005", id=iid)
                self.id = iid
        else:
            self.error("E036a")
        if 'type' not in inventory:
            self.error("E036b")
        elif not isinstance(inventory['type'], str):
            self.error("E999")
        elif extract_spec_version:
            m = re.match(r'''https://ocfl.io/(\d+.\d)/spec/#inventory''', inventory['type'])
            if not m:
                self.error('E038b', got=inventory['type'], assumed_spec_version=self.spec_version)
            elif m.group(1) in self.spec_versions_supported:
                self.spec_version = m.group(1)
            else:
                self.error("E038c", got=m.group(1), assumed_spec_version=self.spec_version)
        elif inventory['type'] != 'https://ocfl.io/' + self.spec_version + '/spec/#inventory':
            self.error("E038a", expected='https://ocfl.io/' + self.spec_version + '/spec/#inventory', got=inventory['type'])
        if 'digestAlgorithm' not in inventory:
            self.error("E036c")
        elif inventory['digestAlgorithm'] == 'sha512':
            pass
        elif self.lax_digests:
            self.digest_algorithm = inventory['digestAlgorithm']
        elif inventory['digestAlgorithm'] == 'sha256':
            self.warning("W004")
            self.digest_algorithm = inventory['digestAlgorithm']
        else:
            self.error("E039", digest_algorithm=inventory['digestAlgorithm'])
        if 'contentDirectory' in inventory:
            # Careful only to set self.content_directory if value is safe
            cd = inventory['contentDirectory']
            if not isinstance(cd, str) or '/' in cd:
                self.error("E017")
            elif cd in ('.', '..'):
                self.error("E018")
            else:
                self.content_directory = cd
        manifest_files_correct_format = None
        if 'manifest' not in inventory:
            self.error("E041a")
        else:
            (self.manifest_files, manifest_files_correct_format, self.unnormalized_digests) = self.validate_manifest(inventory['manifest'])
        digests_used = []
        if 'versions' not in inventory:
            self.error("E041b")
        else:
            self.all_versions = self.validate_version_sequence(inventory['versions'])
            digests_used = self.validate_versions(inventory['versions'], self.all_versions, self.unnormalized_digests)
        if 'head' not in inventory:
            self.error("E036d")
        elif len(self.all_versions) > 0:
            self.head = self.all_versions[-1]
            if inventory['head'] != self.head:
                self.error("E040", got=inventory['head'], expected=self.head)
        if len(self.all_versions) == 0:
            # Abort tests is we don't have a valid version sequence, otherwise
            # there will likely be spurious subsequent error reports
            return
        if len(self.all_versions) > 0:
            if manifest_files_correct_format is not None:
                self.check_content_paths_map_to_versions(manifest_files_correct_format, self.all_versions)
            if self.manifest_files is not None:
                self.check_digests_present_and_used(self.manifest_files, digests_used)
        if 'fixity' in inventory:
            self.validate_fixity(inventory['fixity'], self.manifest_files)

    def validate_manifest(self, manifest):
        """Validate manifest block in inventory.

        Returns:
          * manifest_files - a mapping from file to digest for each file in
              the manifest
          * manifest_files_correct_format - a simple list of the manifest file
              path that passed initial checks. They need to be checked for valid
              version directories later, when we know what version directories
              are valid
          * unnormalized_digests - a set of the original digests in unnormalized
              form that MUST match exactly the values used in state blocks
        """
        manifest_files = {}
        manifest_files_correct_format = []
        unnormalized_digests = set()
        manifest_digests = set()
        if not isinstance(manifest, dict):
            self.error('E041c')
        else:
            content_paths = set()
            content_directories = set()
            for digest in manifest:
                m = re.match(self.digest_regex(), digest)
                if not m:
                    self.error('E025a', digest=digest, algorithm=self.digest_algorithm)  # wrong form of digest
                elif not isinstance(manifest[digest], list):
                    self.error('E092', digest=digest)  # must have path list value
                else:
                    unnormalized_digests.add(digest)
                    norm_digest = normalized_digest(digest, self.digest_algorithm)
                    if norm_digest in manifest_digests:
                        # We have already seen this in different un-normalized form!
                        self.error("E096", digest=norm_digest)
                    else:
                        manifest_digests.add(norm_digest)
                    for file in manifest[digest]:
                        manifest_files[file] = norm_digest
                        if self.check_content_path(file, content_paths, content_directories):
                            manifest_files_correct_format.append(file)
            # Check for conflicting content paths
            for path in content_directories:
                if path in content_paths:
                    self.error("E101b", path=path)
        return manifest_files, manifest_files_correct_format, unnormalized_digests

    def validate_fixity(self, fixity, manifest_files):
        """Validate fixity block in inventory.

        Check the structure of the fixity block and makes sure that only files
        listed in the manifest are referenced.
        """
        if not isinstance(fixity, dict):
            # The value of fixity must be a JSON object. In v1.0 I catch not an object
            # as part of E056 but this was clarified as E111 in v1.1. The value may
            # be an empty object in either case
            self.error('E056a' if self.spec_version == '1.0' else 'E111')
        else:
            for digest_algorithm in fixity:
                known_digest = True
                try:
                    regex = digest_regex(digest_algorithm)
                except ValueError:
                    if not self.lax_digests:
                        self.error('E056b', algorithm=self.digest_algorithm)
                        continue
                    # Match anything
                    regex = r'''^.*$'''
                    known_digest = False
                fixity_algoritm_block = fixity[digest_algorithm]
                if not isinstance(fixity_algoritm_block, dict):
                    self.error('E057a', algorithm=self.digest_algorithm)
                else:
                    digests_seen = set()
                    for digest in fixity_algoritm_block:
                        m = re.match(regex, digest)
                        if not m:
                            self.error('E057b', digest=digest, algorithm=digest_algorithm)  # wrong form of digest
                        elif not isinstance(fixity_algoritm_block[digest], list):
                            self.error('E057c', digest=digest, algorithm=digest_algorithm)  # must have path list value
                        else:
                            if known_digest:
                                norm_digest = normalized_digest(digest, digest_algorithm)
                            else:
                                norm_digest = digest
                            if norm_digest in digests_seen:
                                # We have already seen this in different un-normalized form!
                                self.error("E097", digest=norm_digest, algorithm=digest_algorithm)
                            else:
                                digests_seen.add(norm_digest)
                            for file in fixity_algoritm_block[digest]:
                                if file not in manifest_files:
                                    self.error("E057d", digest=norm_digest, algorithm=digest_algorithm, path=file)

    def validate_version_sequence(self, versions):
        """Validate sequence of version names in versions block in inventory.

        Returns an array of in-sequence version directories that are part
        of a valid sequences. May exclude other version directory names that are
        not part of the valid sequence if an error is thrown.
        """
        all_versions = []
        if not isinstance(versions, dict):
            self.error("E044")
            return all_versions
        if len(versions) == 0:
            self.error("E008")
            return all_versions
        # Validate version sequence
        # https://ocfl.io/draft/spec/#version-directories
        zero_padded = None
        max_version_num = 999999  # Excessive limit
        if 'v1' in versions:
            fmt = 'v%d'
            zero_padded = False
            all_versions.append('v1')
        else:  # Find padding size
            for n in range(2, 11):
                fmt = 'v%0' + str(n) + 'd'
                vkey = fmt % 1
                if vkey in versions:
                    all_versions.append(vkey)
                    zero_padded = n
                    max_version_num = (10 ** (n - 1)) - 1
                    break
            if not zero_padded:
                self.error("E009")
                return all_versions
        if zero_padded:
            self.warning("W001")
        # Have v1 and know format, work through to check sequence
        for n in range(2, max_version_num + 1):
            v = (fmt % n)
            if v in versions:
                all_versions.append(v)
            else:
                if len(versions) != (n - 1):
                    self.error("E010")  # Extra version dirs outside sequence
                return all_versions
        # We have now included all possible versions up to the zero padding
        # size, if there are more versions than this number then we must
        # have extra that violate the zero-padding rule or are out of
        # sequence
        if len(versions) > max_version_num:
            self.error("E011")
        return all_versions

    def validate_versions(self, versions, all_versions, unnormalized_digests):
        """Validate versions blocks in inventory.

        Requires as input two things which are assumed to be structurally correct
        from prior basic validation:

          * versions - which is the JSON object (dict) from the inventory
          * all_versions - an ordered list of the versions to look at in versions
                           (all other keys in versions will be ignored)

        Returns a list of digests_used which can then be checked against the
        manifest.
        """
        digests_used = []
        for v in all_versions:
            version = versions[v]
            if 'created' not in version:
                self.error('E048', version=v)  # No created
            elif not isinstance(versions[v]['created'], str):
                self.error('E049d', version=v)  # Bad created
            else:
                created = versions[v]['created']
                try:
                    str_to_datetime(created)  # catch ValueError if fails
                    if not re.search(r'''(Z|[+-]\d\d:\d\d)$''', created):  # FIXME - kludge
                        self.error('E049a', version=v)
                    if not re.search(r'''T\d\d:\d\d:\d\d''', created):  # FIXME - kludge
                        self.error('E049b', version=v)
                except ValueError as e:
                    self.error('E049c', version=v, description=str(e))
            if 'state' in version:
                digests_used += self.validate_state_block(version['state'], version=v, unnormalized_digests=unnormalized_digests)
            else:
                self.error('E048c', version=v)
            if 'message' not in version:
                self.warning('W007a', version=v)
            elif not isinstance(version['message'], str):
                self.error('E094', version=v)
            if 'user' not in version:
                self.warning('W007b', version=v)
            else:
                user = version['user']
                if not isinstance(user, dict):
                    self.error('E054a', version=v)
                else:
                    if 'name' not in user or not isinstance(user['name'], str):
                        self.error('E054b', version=v)
                    if 'address' not in user:
                        self.warning('W008', version=v)
                    elif not isinstance(user['address'], str):
                        self.error('E054c', version=v)
                    elif not re.match(r'''\w{3,6}:''', user['address']):
                        self.warning('W009', version=v)
        return digests_used

    def validate_state_block(self, state, version, unnormalized_digests):
        """Validate state block in a version in an inventory.

        The version is used only for error reporting.

        Returns a list of content digests referenced in the state block.
        """
        digests = []
        logical_paths = set()
        logical_directories = set()
        if not isinstance(state, dict):
            self.error('E050c', version=version)
        else:
            digest_re = re.compile(self.digest_regex())
            for digest in state:
                if not digest_re.match(digest):
                    self.error('E050d', version=version, digest=digest)
                elif not isinstance(state[digest], list):
                    self.error('E050e', version=version, digest=digest)
                else:
                    for path in state[digest]:
                        if path in logical_paths:
                            self.error("E095a", version=version, path=path)
                        else:
                            self.check_logical_path(path, version, logical_paths, logical_directories)
                    if digest not in unnormalized_digests:
                        # Exact string value must match, not just normalized
                        self.error("E050f", version=version, digest=digest)
                    norm_digest = normalized_digest(digest, self.digest_algorithm)
                    digests.append(norm_digest)
            # Check for conflicting logical paths
            for path in logical_directories:
                if path in logical_paths:
                    self.error("E095b", version=version, path=path)
        return digests

    def check_content_paths_map_to_versions(self, manifest_files, all_versions):
        """Check that every content path starts with a valid version.

        The content directory component has already been checked in
        check_content_path(). We have already tested all paths enough
        to know that they can be split into at least 2 components.
        """
        for path in manifest_files:
            version_dir, dummy_rest = path.split('/', 1)
            if version_dir not in all_versions:
                self.error('E042b', path=path)

    def check_digests_present_and_used(self, manifest_files, digests_used):
        """Check all digests in manifest that are needed are present and used."""
        in_manifest = set(manifest_files.values())
        in_state = set(digests_used)
        not_in_manifest = in_state.difference(in_manifest)
        if len(not_in_manifest) > 0:
            self.error("E050a", digests=", ".join(sorted(not_in_manifest)))
        not_in_state = in_manifest.difference(in_state)
        if len(not_in_state) > 0:
            self.error("E107", digests=", ".join(sorted(not_in_state)))

    def digest_regex(self):
        """Return regex for validating un-normalized digest format."""
        try:
            return digest_regex(self.digest_algorithm)
        except ValueError:
            if not self.lax_digests:
                self.error('E026a', digest=self.digest_algorithm)
        # Match anything
        return r'''^.*$'''

    def check_logical_path(self, path, version, logical_paths, logical_directories):
        """Check logical path and accumulate paths/directories for E095b check.

        logical_paths and logical_directories are expected to be sets.

        Only adds good paths to the accumulated paths/directories.
        """
        if path.startswith('/') or path.endswith('/'):
            self.error("E053", version=version, path=path)
        else:
            elements = path.split('/')
            for element in elements:
                if element in ['.', '..', '']:
                    self.error("E052", version=version, path=path)
                    return
            # Accumulate paths and directories
            logical_paths.add(path)
            logical_directories.add('/'.join(elements[0:-1]))

    def check_content_path(self, path, content_paths, content_directories):
        """Check logical path and accumulate paths/directories for E101 check.

        Returns True if valid, else False. Only adds good paths to the
        accumulated paths/directories. We don't yet know the set of valid
        version directories so the check here is just for 'v' + digits.
        """
        if path.startswith('/') or path.endswith('/'):
            self.error("E100", path=path)
            return False
        m = re.match(r'''^(v\d+/''' + self.content_directory + r''')/(.+)''', path)
        if not m:
            self.error("E042a", path=path)
            return False
        elements = m.group(2).split('/')
        for element in elements:
            if element in ('', '.', '..'):
                self.error("E099", path=path)
                return False
        # Accumulate paths and directories if not seen before
        if path in content_paths:
            self.error("E101a", path=path)
            return False
        content_paths.add(path)
        content_directories.add('/'.join([m.group(1)] + elements[0:-1]))
        return True

    def validate_as_prior_version(self, prior):
        """Check that prior is a valid prior version of the current inventory object.

        The input variable prior is also expected to be an InventoryValidator object
        and both self and prior inventories are assumed to have been checked for
        internal consistency.
        """
        # Must have a subset of versions which also checks zero padding format etc.
        if not set(prior.all_versions) < set(self.all_versions):
            self.error('E066a', prior_head=prior.head)
        else:
            # Check references to files but realize that there might be different
            # digest algorithms between versions
            version = 'no-version'
            for version in prior.all_versions:
                # If the digest algorithm is the same then we can make a
                # direct check on whether the state blocks match
                if prior.digest_algorithm == self.digest_algorithm:
                    self.compare_states_for_version(prior, version)
                # Now check the mappings from state to logical path, which must
                # be consistent even if the digestAlgorithm is different between
                # versions. Get maps from logical paths to files on disk:
                prior_map = get_logical_path_map(prior.inventory, version)
                self_map = get_logical_path_map(self.inventory, version)
                # Look first for differences in logical paths listed
                only_in_prior = prior_map.keys() - self_map.keys()
                only_in_self = self_map.keys() - prior_map.keys()
                if only_in_prior or only_in_self:
                    if only_in_prior:
                        self.error('E066b', version=version, prior_head=prior.head, only_in=prior.head, logical_paths=','.join(only_in_prior))
                    if only_in_self:
                        self.error('E066b', version=version, prior_head=prior.head, only_in=self.where, logical_paths=','.join(only_in_self))
                else:
                    # Check them all in details - digests must match
                    for logical_path, this_map in prior_map.items():
                        if not this_map.issubset(self_map[logical_path]):
                            self.error('E066c', version=version, prior_head=prior.head,
                                       logical_path=logical_path, prior_content=','.join(this_map),
                                       current_content=','.join(self_map[logical_path]))
                # Check metadata
                prior_version = prior.inventory['versions'][version]
                self_version = self.inventory['versions'][version]
                for key in ('created', 'message', 'user'):
                    if prior_version.get(key) != self_version.get(key):
                        self.warning('W011', version=version, prior_head=prior.head, key=key)

    def compare_states_for_version(self, prior, version):
        """Compare state blocks for version between self and prior.

        Assumes the same digest algorithm in both, do not call otherwise!

        Looks only for digests that appear in one but not in the other, the code
        in validate_as_prior_version(..) does a check for whether the same sets
        of logical files appear and we don't want to duplicate an error message
        about that.

        While the mapping checks in validate_as_prior_version(..) do all that is
        necessary to detect an error, the additional errors that may be generated
        here provide more detailed diagnostics in the case that the digest
        algorithm is the same across versions being compared.
        """
        self_state = self.inventory['versions'][version]['state']
        prior_state = prior.inventory['versions'][version]['state']
        for digest in set(self_state.keys()).union(prior_state.keys()):
            if digest not in prior_state:
                self.error('E066d', version=version, prior_head=prior.head,
                           digest=digest, logical_files=', '.join(self_state[digest]))
            elif digest not in self_state:
                self.error('E066e', version=version, prior_head=prior.head,
                           digest=digest, logical_files=', '.join(prior_state[digest]))

class TLogger():
    """Simplified logger to replace ValidationLogger."""


    def __init__(self):
        """Initialize."""
        self.clear()


    def error(self, code, **args):  # pylint: disable=unused-argument
        """Add error code, discard args."""
        self.errors.append(code)


    def warning(self, code, **args):  # pylint: disable=unused-argument
        """Add warn code, discard args."""
        self.warns.append(code)


    def clear(self):
        """Clear records."""
        self.errors = []
        self.warns = []

if __name__ == "__main__":
    isT=True

    log = TLogger()
    iv = InventoryValidator(log=log)
    iv.validate_fixity("not a fixity block", [])
    if not 'E056a' in log.errors:
        isT=False
    log.clear()
    iv.validate_fixity({'a': 'b'}, [])
    if not 'E056b' in log.errors:
        isT=False
    log.clear()
    iv.validate_fixity({'md5': 'f1'}, [])
    if not 'E057a' in log.errors:
        isT=False
    log.clear()
    iv.validate_fixity({'md5': {'d1': 'f1'}}, [])
    if not 'E057b' in log.errors:
        isT=False
    log.clear()
    iv.validate_fixity({'md5': {'68b329da9893e34099c7d8ad5cb9c940': 'f1'}}, [])
    if not 'E057c' in log.errors:
        isT=False
    log.clear()
    iv.validate_fixity({'md5': {'68b329da9893e34099c7d8ad5cb9c940': ['f1']}}, [])
    if not 'E057d' in log.errors:
        isT=False
    log.clear()
    iv.validate_fixity({'md5': {'68b329da9893e34099c7d8ad5cb9c940': ['f1'],
                                '68B329DA9893e34099c7d8ad5cb9c940': ['f2']}}, [])
    if not 'E097' in log.errors:
        isT=False
    log.clear()
    # Good case
    iv.validate_fixity({'md5': {'68b329da9893e34099c7d8ad5cb9c940': ['f1a', 'f1b'],
                                '06c7aa0ab7739f5fde7cb8504af3e851': ['f2']},
                        'sha1': {'adc83b19e793491b1c6ea0fd8b46cd9f32e592fc': ['f1a', 'f1b'],
                                 '00be977f5f719e87c17704954341f50d929bc070': ['f2']}},
                       ['f1a', 'f1b', 'f2'])
    if len(log.errors) != 0:
        isT=False
    # Good case when lax_digests
    iv.lax_digests = True
    iv.validate_fixity({'XXX': {'digest1': ['f1a', 'f1b'],
                                'digest2': ['f2']}},
                       ['f1a', 'f1b', 'f2'])
    if len(log.errors) != 0:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/scieloorg---packtools/packtools/file_utils_files_list_passk_validte.py
import os
import logging
import re
import shutil
import tempfile

from zipfile import ZipFile, ZIP_DEFLATED


logger = logging.getLogger(__name__)


def is_folder(source):
    return os.path.isdir(source)


def is_zipfile(source):
    return os.path.isfile(source) and source.endswith(".zip")


def xml_files_list(path):
    """
    Return the XML files found in `path`
    """
    return (f for f in os.listdir(path) if f.endswith(".xml"))


def files_list(path):
    """
    Return the files in `path`
    """
    return os.listdir(path)


def read_file(path, encoding="utf-8", mode="r"):
    with open(path, mode=mode, encoding=encoding) as f:
        text = f.read()
    return text


def read_from_zipfile(zip_path, filename):
    with ZipFile(zip_path) as zf:
        return zf.read(filename)


def xml_files_list_from_zipfile(zip_path):
    with ZipFile(zip_path) as zf:
        xmls_filenames = [
            xml_filename
            for xml_filename in zf.namelist()
            if os.path.splitext(xml_filename)[-1] == ".xml"
        ]
    return xmls_filenames


def files_list_from_zipfile(zip_path):
    """
    Return the files in `zip_path`

    Example:

    ```
    [
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',
    ]
    ```
    """
    with ZipFile(zip_path) as zf:
        return zf.namelist()


def write_file(path, source, mode="w"):
    dirname = os.path.dirname(path)
    if not os.path.isdir(dirname):
        os.makedirs(dirname)
    logger.debug("Gravando arquivo: %s", path)
    if "b" in mode:
        with open(path, mode) as f:
            f.write(source)
        return

    with open(path, mode, encoding="utf-8") as f:
        f.write(source)


def create_zip_file(files, zip_name, zip_folder=None):
    zip_folder = zip_folder or tempfile.mkdtemp()

    zip_path = os.path.join(zip_folder, zip_name)
    with ZipFile(zip_path, 'w', ZIP_DEFLATED) as myzip:
        for f in files:
            myzip.write(f, os.path.basename(f))
    return zip_path


def delete_folder(path):
    try:
        shutil.rmtree(path)
    except:
        pass


def create_temp_file(filename, content=None, mode='w'):
    file_path = tempfile.mkdtemp()
    file_path = os.path.join(file_path, filename)
    write_file(file_path, content or '', mode)
    return file_path


def copy_file(source, target):
    tmpdir = tempfile.mkdtemp()
    fullpath_target = os.path.join(tmpdir, target)

    logger.info(f'Copying file {source} to {fullpath_target}')
    return shutil.copyfile(source, fullpath_target)


def size(file_path):
    return os.path.getsize(file_path)


def get_prefix_by_xml_filename(xml_filename):
    """
    Obtém o prefixo associado a um arquivo xml

    Parameters
    ----------
    xml_filename : str
        Nome de arquivo xml

    Returns
    -------
    str
        Prefixo associado ao arquivo xml
    """
    file, ext = os.path.splitext(xml_filename)
    return file


def get_file_role(file_path, prefix, pdf_langs):
    """
    Obtém o papel/função de um arquivo (xml, renditions ou assets) no contexto de um documento

    Parameters
    ----------
    file_path : str
        Nome de um arquivo
    prefix: str
        Prefixo associado ao arquivo
    pdf_langs: list
        Idiomas dos PDFs do documento

    Returns
    -------
    str
        Papel/função de arquivo (xml, rendition ou assets) no contexto de um documento
    """
    file, ext = os.path.splitext(file_path)

    if ext == '.xml':
        return 'xml'
    elif ext == '.pdf':
        if file == prefix:
            return 'renditions'

        for lang in pdf_langs:
            if file == f'{prefix}-{lang}':
                return 'renditions'
    return 'assets'


def extract_issn_from_zip_uri(zip_uri):
    """
    Extrai código ISSN a partir do endereço de um arquivo zip

    Parameters
    ----------
    zip_uri : str
        Endereço de um arquivo zip

    Returns
    -------
    str
        ISSN
    """
    match = re.search(r'.*/ingress/packages/(\d{4}-\d{4})/.*.zip', zip_uri)
    if match:
        return match.group(1)


def get_filename(file_path):
    return os.path.basename(file_path)

if __name__ == "__main__":
    import dill
    import os
    if '__init__.py' not in files_list("/home/travis/builds/repos/scieloorg---packtools/packtools") or "domain.py" not in files_list("/home/travis/builds/repos/scieloorg---packtools/packtools") or "webapp" not in files_list("/home/travis/builds/repos/scieloorg---packtools/packtools"):
        raise Exception("Result not True!!!")



----------------------------
/home/travis/builds/repos/scieloorg---packtools/packtools/sps/models/packages__group_files_by_xml_filename_passk_validte.py
import logging
import os
import sys
sys.path.append("/home/travis/builds/repos/scieloorg---packtools/")
from packtools import file_utils
from zipfile import ZipFile


logger = logging.getLogger(__name__)


class Package:
    def __init__(self, source, name):
        self._source = source
        self._xml = None
        self._assets = {}
        self._renditions = {}
        self._name = name
        self.zip_file_path = file_utils.is_zipfile(source) and source

    @property
    def assets(self):
        return self._assets

    @property
    def name(self):
        return self._name

    def file_path(self, file_path):
        if file_utils.is_folder(self._source):
            return os.path.join(self._source, file_path)
        return file_path

    def add_asset(self, basename, file_path):
        """
        "{
            "artigo02-gf03.tiff": "/path/artigo02-gf03.tiff",
            "artigo02-gf03.jpg": "/path/artigo02-gf03.jpg",
            "artigo02-gf03.png": "/path/artigo02-gf03.png",
        }
        """
        self._assets[basename] = self.file_path(file_path)

    def get_asset(self, basename):
        try:
            return self._assets[basename]
        except KeyError:
            return

    def add_rendition(self, lang, file_path):
        """
        {
            "original": "artigo02.pdf",
            "en": "artigo02-en.pdf",
        }
        """
        self._renditions[lang] = self.file_path(file_path)

    def get_rendition(self, lang):
        try:
            return self._renditions[lang]
        except KeyError:
            return

    @property
    def source(self):
        return self._source

    @property
    def xml(self):
        return self.file_path(self._xml)

    @xml.setter
    def xml(self, value):
        self._xml = value

    @property
    def renditions(self):
        return self._renditions

    @property
    def xml_content(self):
        if file_utils.is_folder(self._source):
            with open(self.xml, "rb") as fp:
                return fp.read()
        with ZipFile(self._source) as zf:
            return zf.read(self.xml)


def select_filenames_by_prefix(prefix, files):
    """
    Get files which belongs to a document package.

    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`

    Parameters
    ----------
    prefix : str
        Filename prefix
    files : str list
        Files paths
    Returns
    -------
    list
        files paths which basename files matches to prefix
    """
    return [
        item
        for item in files
        if match_file_by_prefix(prefix, item)
    ]


def match_file_by_prefix(prefix, file_path):
    """
    Identify if a `file_path` belongs to a document package by a given `prefix`

    Retorna `True` para documentos pertencentes a um pacote.

    Parameters
    ----------
    prefix : str
        Filename prefix
    file_path : str
        File path
    Returns
    -------
    bool
        True - file belongs to the package
    """
    basename = os.path.basename(file_path)
    if basename.startswith(prefix + "-"):
        return True
    if basename.startswith(prefix + "."):
        return True
    return False


def explore_source(source):
    packages = _explore_zipfile(source)
    if not packages:
        packages = _explore_folder(source)
    if not packages:
        raise ValueError("%s: Invalid value for `source`" % source)
    return packages


def _explore_folder(folder):
    """
    Get packages' data from folder

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    folder : str
        Folder of the package
    Returns
    -------
    dict
    """
    if file_utils.is_folder(folder):
        data = _group_files_by_xml_filename(
            folder,
            file_utils.xml_files_list(folder),
            file_utils.files_list(folder),
        )
        return data


def _explore_zipfile(zip_path):
    """
    Get packages' data from zip_path

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    zip_path : str
        zip file path
    Returns
    -------
    dict
    """
    if file_utils.is_zipfile(zip_path):
        with ZipFile(zip_path, 'r'):
            data = _group_files_by_xml_filename(
                zip_path,
                file_utils.xml_files_list_from_zipfile(zip_path),
                file_utils.files_list_from_zipfile(zip_path),
            )
            return data


def _group_files_by_xml_filename(source, xmls, files):
    """
    Group files by their XML basename

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    xml_filename : str
        XML filenames
    files : list
        list of files in the folder or zipfile

    Returns
    -------
    dict
        key: name of the XML files
        value: Package
    """
    docs = {}
    for xml in xmls:
        basename = os.path.basename(xml)
        prefix, ext = os.path.splitext(basename)

        docs.setdefault(prefix, Package(source, prefix))

        # XML
        docs[prefix].xml = xml

        for file in select_filenames_by_prefix(prefix, files):
            # avalia arquivo do pacote, se é asset ou rendition
            component = _eval_file(prefix, file)
            if not component:
                continue

            # resultado do avaliação do pacote
            ftype = component.get("ftype")
            file_path = component["file_path"]
            comp_id = component["component_id"]

            if ftype:
                docs[prefix].add_asset(comp_id, file_path)
            else:
                docs[prefix].add_rendition(comp_id, file_path)
            files.remove(file)
    return docs


def _eval_file(prefix, file_path):
    """
    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.

    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e
    o endereço do arquivo em análise.

    Parameters
    ----------
    prefix : str
        nome do arquivo XML sem extensão
    filename : str
        filename
    file_folder : str
        file folder

    Returns
    -------
    dict
    """
    if not match_file_by_prefix(prefix, file_path):
        # ignore files which name does not match
        return
    if file_path.endswith(".xml"):
        # ignore XML files
        return

    # it matches
    filename = os.path.basename(file_path)
    fname, ext = os.path.splitext(filename)

    lang = None
    if ext == ".pdf":
        suffix = fname.replace(prefix, "")
        if fname == prefix:
            lang = "original"
        elif len(suffix) == 3 and suffix[0] == "-":
            # it is a rendition
            lang = suffix[1:]

    if lang:
        return dict(
            component_id=lang,
            file_path=file_path,
        )
    else:
        return dict(
            component_id=filename,
            component_name=fname,
            ftype=ext[1:],
            file_path=file_path,
        )


if __name__ == "__main__":
    isT=True
    pkg1 = Package("source", "name")
    pkg11 = Package("source", "name")

    pkg11.xml = "a11.xml"
    pkg11._assets = {
        "a11-gf01-es.tiff": "a11-gf01-es.tiff",
        "a11-gf02-es.tiff": "a11-gf02-es.tiff",
        "a11-suppl-es.pdf": "a11-suppl-es.pdf",
    }
    pkg11._renditions = {
        "es": "a11-es.pdf",
        "original": "a11.pdf",
    }

    pkg1.xml = "a1.xml"
    pkg1._assets = {
        "a1-gf01.tiff": "a1-gf01.tiff",
        "a1-gf01.jpg": "a1-gf01.jpg",
        "a1-gf02.tiff": "a1-gf02.tiff",
    }
    pkg1._renditions = {
        "en": "a1-en.pdf",
        "original": "a1.pdf",
    }
    xmls = [
        "a1.xml", "a11.xml",
    ]
    files = [
        "a1-en.pdf",
        "a1-gf01.jpg",
        "a1-gf01.tiff",
        "a1-gf02.tiff",
        "a1.pdf",
        "a1.xml",
        "a11-es.pdf",
        "a11-gf01-es.tiff",
        "a11-gf02-es.tiff",
        "a11-suppl-es.pdf",
        "a11.pdf",
        "a11.xml",
    ]
    result = _group_files_by_xml_filename("source", xmls, files)
    ist1=pkg11.xml== result["a11"].xml
    ist2=pkg11._assets==result["a11"]._assets
    ist3=pkg11._renditions== result["a11"]._renditions
    ist4=pkg1.xml== result["a1"].xml
    ist5=pkg1._assets== result["a1"]._assets
    ist6=pkg1._renditions== result["a1"]._renditions

    if not ist1 or not ist2 or not ist3 or not ist4 or not ist5 or not ist6:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463153879012d1948149a/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463153879012d1948149a/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"],bytes):
    #         args2=dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2=content["input"]["args"][2]["bytes"]
    #     print(args0,args1,args2)
    # #     res0 = _group_files_by_xml_filename(args0,args1,args2)
    # #     if res0=={}:
    # #         continue
    # #     if "2318-0889-tinf-33-e200071" not in res0.keys():
    # #         isT=False
    # #         break
    # #     if res0["2318-0889-tinf-33-e200071"].source!="./tests/sps/fixtures/package.zip" or res0["2318-0889-tinf-33-e200071"].name!="2318-0889-tinf-33-e200071":
    # #         isT=False
    # #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/scieloorg---packtools/packtools/sps/models/packages_match_file_by_prefix_passk_validte.py
import logging
import os
import sys
sys.path.append("/home/travis/builds/repos/scieloorg---packtools/")
from packtools import file_utils
from zipfile import ZipFile


logger = logging.getLogger(__name__)


class Package:
    def __init__(self, source, name):
        self._source = source
        self._xml = None
        self._assets = {}
        self._renditions = {}
        self._name = name
        self.zip_file_path = file_utils.is_zipfile(source) and source

    @property
    def assets(self):
        return self._assets

    @property
    def name(self):
        return self._name

    def file_path(self, file_path):
        if file_utils.is_folder(self._source):
            return os.path.join(self._source, file_path)
        return file_path

    def add_asset(self, basename, file_path):
        """
        "{
            "artigo02-gf03.tiff": "/path/artigo02-gf03.tiff",
            "artigo02-gf03.jpg": "/path/artigo02-gf03.jpg",
            "artigo02-gf03.png": "/path/artigo02-gf03.png",
        }
        """
        self._assets[basename] = self.file_path(file_path)

    def get_asset(self, basename):
        try:
            return self._assets[basename]
        except KeyError:
            return

    def add_rendition(self, lang, file_path):
        """
        {
            "original": "artigo02.pdf",
            "en": "artigo02-en.pdf",
        }
        """
        self._renditions[lang] = self.file_path(file_path)

    def get_rendition(self, lang):
        try:
            return self._renditions[lang]
        except KeyError:
            return

    @property
    def source(self):
        return self._source

    @property
    def xml(self):
        return self.file_path(self._xml)

    @xml.setter
    def xml(self, value):
        self._xml = value

    @property
    def renditions(self):
        return self._renditions

    @property
    def xml_content(self):
        if file_utils.is_folder(self._source):
            with open(self.xml, "rb") as fp:
                return fp.read()
        with ZipFile(self._source) as zf:
            return zf.read(self.xml)


def select_filenames_by_prefix(prefix, files):
    """
    Get files which belongs to a document package.

    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`

    Parameters
    ----------
    prefix : str
        Filename prefix
    files : str list
        Files paths
    Returns
    -------
    list
        files paths which basename files matches to prefix
    """
    return [
        item
        for item in files
        if match_file_by_prefix(prefix, item)
    ]


def match_file_by_prefix(prefix, file_path):
    """
    Identify if a `file_path` belongs to a document package by a given `prefix`

    Retorna `True` para documentos pertencentes a um pacote.

    Parameters
    ----------
    prefix : str
        Filename prefix
    file_path : str
        File path
    Returns
    -------
    bool
        True - file belongs to the package
    """
    basename = os.path.basename(file_path)
    if basename.startswith(prefix + "-"):
        return True
    if basename.startswith(prefix + "."):
        return True
    return False


def explore_source(source):
    packages = _explore_zipfile(source)
    if not packages:
        packages = _explore_folder(source)
    if not packages:
        raise ValueError("%s: Invalid value for `source`" % source)
    return packages


def _explore_folder(folder):
    """
    Get packages' data from folder

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    folder : str
        Folder of the package
    Returns
    -------
    dict
    """
    if file_utils.is_folder(folder):
        data = _group_files_by_xml_filename(
            folder,
            file_utils.xml_files_list(folder),
            file_utils.files_list(folder),
        )
        return data


def _explore_zipfile(zip_path):
    """
    Get packages' data from zip_path

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    zip_path : str
        zip file path
    Returns
    -------
    dict
    """
    if file_utils.is_zipfile(zip_path):
        with ZipFile(zip_path, 'r'):
            data = _group_files_by_xml_filename(
                zip_path,
                file_utils.xml_files_list_from_zipfile(zip_path),
                file_utils.files_list_from_zipfile(zip_path),
            )
            return data


def _group_files_by_xml_filename(source, xmls, files):
    """
    Group files by their XML basename

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    xml_filename : str
        XML filenames
    files : list
        list of files in the folder or zipfile

    Returns
    -------
    dict
        key: name of the XML files
        value: Package
    """
    docs = {}
    for xml in xmls:
        basename = os.path.basename(xml)
        prefix, ext = os.path.splitext(basename)

        docs.setdefault(prefix, Package(source, prefix))

        # XML
        docs[prefix].xml = xml

        for file in select_filenames_by_prefix(prefix, files):
            # avalia arquivo do pacote, se é asset ou rendition
            component = _eval_file(prefix, file)
            if not component:
                continue

            # resultado do avaliação do pacote
            ftype = component.get("ftype")
            file_path = component["file_path"]
            comp_id = component["component_id"]

            if ftype:
                docs[prefix].add_asset(comp_id, file_path)
            else:
                docs[prefix].add_rendition(comp_id, file_path)
            files.remove(file)
    return docs


def _eval_file(prefix, file_path):
    """
    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.

    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e
    o endereço do arquivo em análise.

    Parameters
    ----------
    prefix : str
        nome do arquivo XML sem extensão
    filename : str
        filename
    file_folder : str
        file folder

    Returns
    -------
    dict
    """
    if not match_file_by_prefix(prefix, file_path):
        # ignore files which name does not match
        return
    if file_path.endswith(".xml"):
        # ignore XML files
        return

    # it matches
    filename = os.path.basename(file_path)
    fname, ext = os.path.splitext(filename)

    lang = None
    if ext == ".pdf":
        suffix = fname.replace(prefix, "")
        if fname == prefix:
            lang = "original"
        elif len(suffix) == 3 and suffix[0] == "-":
            # it is a rendition
            lang = suffix[1:]

    if lang:
        return dict(
            component_id=lang,
            file_path=file_path,
        )
    else:
        return dict(
            component_id=filename,
            component_name=fname,
            ftype=ext[1:],
            file_path=file_path,
        )


if __name__ == "__main__":
    isT=True
    pkg1 = Package("source", "name")
    pkg11 = Package("source", "name")

    pkg11.xml = "a11.xml"
    pkg11._assets = {
        "a11-gf01-es.tiff": "a11-gf01-es.tiff",
        "a11-gf02-es.tiff": "a11-gf02-es.tiff",
        "a11-suppl-es.pdf": "a11-suppl-es.pdf",
    }
    pkg11._renditions = {
        "es": "a11-es.pdf",
        "original": "a11.pdf",
    }

    pkg1.xml = "a1.xml"
    pkg1._assets = {
        "a1-gf01.tiff": "a1-gf01.tiff",
        "a1-gf01.jpg": "a1-gf01.jpg",
        "a1-gf02.tiff": "a1-gf02.tiff",
    }
    pkg1._renditions = {
        "en": "a1-en.pdf",
        "original": "a1.pdf",
    }
    xmls = [
        "a1.xml", "a11.xml",
    ]
    files = [
        "a1-en.pdf",
        "a1-gf01.jpg",
        "a1-gf01.tiff",
        "a1-gf02.tiff",
        "a1.pdf",
        "a1.xml",
        "a11-es.pdf",
        "a11-gf01-es.tiff",
        "a11-gf02-es.tiff",
        "a11-suppl-es.pdf",
        "a11.pdf",
        "a11.xml",
    ]
    result = _group_files_by_xml_filename("source", xmls, files)
    ist1=pkg11.xml== result["a11"].xml
    ist2=pkg11._assets==result["a11"]._assets
    ist3=pkg11._renditions== result["a11"]._renditions
    ist4=pkg1.xml== result["a1"].xml
    ist5=pkg1._assets== result["a1"]._assets
    ist6=pkg1._renditions== result["a1"]._renditions

    if not ist1 or not ist2 or not ist3 or not ist4 or not ist5 or not ist6:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463153879012d1948149a/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463153879012d1948149a/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"],bytes):
    #         args2=dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2=content["input"]["args"][2]["bytes"]
    #     print(args0,args1,args2)
    # #     res0 = _group_files_by_xml_filename(args0,args1,args2)
    # #     if res0=={}:
    # #         continue
    # #     if "2318-0889-tinf-33-e200071" not in res0.keys():
    # #         isT=False
    # #         break
    # #     if res0["2318-0889-tinf-33-e200071"].source!="./tests/sps/fixtures/package.zip" or res0["2318-0889-tinf-33-e200071"].name!="2318-0889-tinf-33-e200071":
    # #         isT=False
    # #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/scieloorg---packtools/packtools/sps/models/packages_select_filenames_by_prefix_passk_validte.py
import logging
import os
import sys
sys.path.append("/home/travis/builds/repos/scieloorg---packtools/")
from packtools import file_utils
from zipfile import ZipFile


logger = logging.getLogger(__name__)


class Package:
    def __init__(self, source, name):
        self._source = source
        self._xml = None
        self._assets = {}
        self._renditions = {}
        self._name = name
        self.zip_file_path = file_utils.is_zipfile(source) and source

    @property
    def assets(self):
        return self._assets

    @property
    def name(self):
        return self._name

    def file_path(self, file_path):
        if file_utils.is_folder(self._source):
            return os.path.join(self._source, file_path)
        return file_path

    def add_asset(self, basename, file_path):
        """
        "{
            "artigo02-gf03.tiff": "/path/artigo02-gf03.tiff",
            "artigo02-gf03.jpg": "/path/artigo02-gf03.jpg",
            "artigo02-gf03.png": "/path/artigo02-gf03.png",
        }
        """
        self._assets[basename] = self.file_path(file_path)

    def get_asset(self, basename):
        try:
            return self._assets[basename]
        except KeyError:
            return

    def add_rendition(self, lang, file_path):
        """
        {
            "original": "artigo02.pdf",
            "en": "artigo02-en.pdf",
        }
        """
        self._renditions[lang] = self.file_path(file_path)

    def get_rendition(self, lang):
        try:
            return self._renditions[lang]
        except KeyError:
            return

    @property
    def source(self):
        return self._source

    @property
    def xml(self):
        return self.file_path(self._xml)

    @xml.setter
    def xml(self, value):
        self._xml = value

    @property
    def renditions(self):
        return self._renditions

    @property
    def xml_content(self):
        if file_utils.is_folder(self._source):
            with open(self.xml, "rb") as fp:
                return fp.read()
        with ZipFile(self._source) as zf:
            return zf.read(self.xml)


def select_filenames_by_prefix(prefix, files):
    """
    Get files which belongs to a document package.

    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`

    Parameters
    ----------
    prefix : str
        Filename prefix
    files : str list
        Files paths
    Returns
    -------
    list
        files paths which basename files matches to prefix
    """
    return [
        item
        for item in files
        if match_file_by_prefix(prefix, item)
    ]


def match_file_by_prefix(prefix, file_path):
    """
    Identify if a `file_path` belongs to a document package by a given `prefix`

    Retorna `True` para documentos pertencentes a um pacote.

    Parameters
    ----------
    prefix : str
        Filename prefix
    file_path : str
        File path
    Returns
    -------
    bool
        True - file belongs to the package
    """
    basename = os.path.basename(file_path)
    if basename.startswith(prefix + "-"):
        return True
    if basename.startswith(prefix + "."):
        return True
    return False


def explore_source(source):
    packages = _explore_zipfile(source)
    if not packages:
        packages = _explore_folder(source)
    if not packages:
        raise ValueError("%s: Invalid value for `source`" % source)
    return packages


def _explore_folder(folder):
    """
    Get packages' data from folder

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    folder : str
        Folder of the package
    Returns
    -------
    dict
    """
    if file_utils.is_folder(folder):
        data = _group_files_by_xml_filename(
            folder,
            file_utils.xml_files_list(folder),
            file_utils.files_list(folder),
        )
        return data


def _explore_zipfile(zip_path):
    """
    Get packages' data from zip_path

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    zip_path : str
        zip file path
    Returns
    -------
    dict
    """
    if file_utils.is_zipfile(zip_path):
        with ZipFile(zip_path, 'r'):
            data = _group_files_by_xml_filename(
                zip_path,
                file_utils.xml_files_list_from_zipfile(zip_path),
                file_utils.files_list_from_zipfile(zip_path),
            )
            return data


def _group_files_by_xml_filename(source, xmls, files):
    """
    Group files by their XML basename

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    xml_filename : str
        XML filenames
    files : list
        list of files in the folder or zipfile

    Returns
    -------
    dict
        key: name of the XML files
        value: Package
    """
    docs = {}
    for xml in xmls:
        basename = os.path.basename(xml)
        prefix, ext = os.path.splitext(basename)

        docs.setdefault(prefix, Package(source, prefix))

        # XML
        docs[prefix].xml = xml

        for file in select_filenames_by_prefix(prefix, files):
            # avalia arquivo do pacote, se é asset ou rendition
            component = _eval_file(prefix, file)
            if not component:
                continue

            # resultado do avaliação do pacote
            ftype = component.get("ftype")
            file_path = component["file_path"]
            comp_id = component["component_id"]

            if ftype:
                docs[prefix].add_asset(comp_id, file_path)
            else:
                docs[prefix].add_rendition(comp_id, file_path)
            files.remove(file)
    return docs


def _eval_file(prefix, file_path):
    """
    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.

    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e
    o endereço do arquivo em análise.

    Parameters
    ----------
    prefix : str
        nome do arquivo XML sem extensão
    filename : str
        filename
    file_folder : str
        file folder

    Returns
    -------
    dict
    """
    if not match_file_by_prefix(prefix, file_path):
        # ignore files which name does not match
        return
    if file_path.endswith(".xml"):
        # ignore XML files
        return

    # it matches
    filename = os.path.basename(file_path)
    fname, ext = os.path.splitext(filename)

    lang = None
    if ext == ".pdf":
        suffix = fname.replace(prefix, "")
        if fname == prefix:
            lang = "original"
        elif len(suffix) == 3 and suffix[0] == "-":
            # it is a rendition
            lang = suffix[1:]

    if lang:
        return dict(
            component_id=lang,
            file_path=file_path,
        )
    else:
        return dict(
            component_id=filename,
            component_name=fname,
            ftype=ext[1:],
            file_path=file_path,
        )


if __name__ == "__main__":
    isT=True
    pkg1 = Package("source", "name")
    pkg11 = Package("source", "name")

    pkg11.xml = "a11.xml"
    pkg11._assets = {
        "a11-gf01-es.tiff": "a11-gf01-es.tiff",
        "a11-gf02-es.tiff": "a11-gf02-es.tiff",
        "a11-suppl-es.pdf": "a11-suppl-es.pdf",
    }
    pkg11._renditions = {
        "es": "a11-es.pdf",
        "original": "a11.pdf",
    }

    pkg1.xml = "a1.xml"
    pkg1._assets = {
        "a1-gf01.tiff": "a1-gf01.tiff",
        "a1-gf01.jpg": "a1-gf01.jpg",
        "a1-gf02.tiff": "a1-gf02.tiff",
    }
    pkg1._renditions = {
        "en": "a1-en.pdf",
        "original": "a1.pdf",
    }
    xmls = [
        "a1.xml", "a11.xml",
    ]
    files = [
        "a1-en.pdf",
        "a1-gf01.jpg",
        "a1-gf01.tiff",
        "a1-gf02.tiff",
        "a1.pdf",
        "a1.xml",
        "a11-es.pdf",
        "a11-gf01-es.tiff",
        "a11-gf02-es.tiff",
        "a11-suppl-es.pdf",
        "a11.pdf",
        "a11.xml",
    ]
    result = _group_files_by_xml_filename("source", xmls, files)
    ist1=pkg11.xml== result["a11"].xml
    ist2=pkg11._assets==result["a11"]._assets
    ist3=pkg11._renditions== result["a11"]._renditions
    ist4=pkg1.xml== result["a1"].xml
    ist5=pkg1._assets== result["a1"]._assets
    ist6=pkg1._renditions== result["a1"]._renditions

    if not ist1 or not ist2 or not ist3 or not ist4 or not ist5 or not ist6:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463153879012d1948149a/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463153879012d1948149a/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"],bytes):
    #         args2=dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2=content["input"]["args"][2]["bytes"]
    #     print(args0,args1,args2)
    # #     res0 = _group_files_by_xml_filename(args0,args1,args2)
    # #     if res0=={}:
    # #         continue
    # #     if "2318-0889-tinf-33-e200071" not in res0.keys():
    # #         isT=False
    # #         break
    # #     if res0["2318-0889-tinf-33-e200071"].source!="./tests/sps/fixtures/package.zip" or res0["2318-0889-tinf-33-e200071"].name!="2318-0889-tinf-33-e200071":
    # #         isT=False
    # #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/scieloorg---packtools/packtools/sps/models/packages__explore_folder_passk_validte.py
import logging
import os
import sys
sys.path.append("/home/travis/builds/repos/scieloorg---packtools/")
from packtools import file_utils
from zipfile import ZipFile


logger = logging.getLogger(__name__)


class Package:
    def __init__(self, source, name):
        self._source = source
        self._xml = None
        self._assets = {}
        self._renditions = {}
        self._name = name
        self.zip_file_path = file_utils.is_zipfile(source) and source

    @property
    def assets(self):
        return self._assets

    @property
    def name(self):
        return self._name

    def file_path(self, file_path):
        if file_utils.is_folder(self._source):
            return os.path.join(self._source, file_path)
        return file_path

    def add_asset(self, basename, file_path):
        """
        "{
            "artigo02-gf03.tiff": "/path/artigo02-gf03.tiff",
            "artigo02-gf03.jpg": "/path/artigo02-gf03.jpg",
            "artigo02-gf03.png": "/path/artigo02-gf03.png",
        }
        """
        self._assets[basename] = self.file_path(file_path)

    def get_asset(self, basename):
        try:
            return self._assets[basename]
        except KeyError:
            return

    def add_rendition(self, lang, file_path):
        """
        {
            "original": "artigo02.pdf",
            "en": "artigo02-en.pdf",
        }
        """
        self._renditions[lang] = self.file_path(file_path)

    def get_rendition(self, lang):
        try:
            return self._renditions[lang]
        except KeyError:
            return

    @property
    def source(self):
        return self._source

    @property
    def xml(self):
        return self.file_path(self._xml)

    @xml.setter
    def xml(self, value):
        self._xml = value

    @property
    def renditions(self):
        return self._renditions

    @property
    def xml_content(self):
        if file_utils.is_folder(self._source):
            with open(self.xml, "rb") as fp:
                return fp.read()
        with ZipFile(self._source) as zf:
            return zf.read(self.xml)


def select_filenames_by_prefix(prefix, files):
    """
    Get files which belongs to a document package.

    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`

    Parameters
    ----------
    prefix : str
        Filename prefix
    files : str list
        Files paths
    Returns
    -------
    list
        files paths which basename files matches to prefix
    """
    return [
        item
        for item in files
        if match_file_by_prefix(prefix, item)
    ]


def match_file_by_prefix(prefix, file_path):
    """
    Identify if a `file_path` belongs to a document package by a given `prefix`

    Retorna `True` para documentos pertencentes a um pacote.

    Parameters
    ----------
    prefix : str
        Filename prefix
    file_path : str
        File path
    Returns
    -------
    bool
        True - file belongs to the package
    """
    basename = os.path.basename(file_path)
    if basename.startswith(prefix + "-"):
        return True
    if basename.startswith(prefix + "."):
        return True
    return False


def explore_source(source):
    packages = _explore_zipfile(source)
    if not packages:
        packages = _explore_folder(source)
    if not packages:
        raise ValueError("%s: Invalid value for `source`" % source)
    return packages


def _explore_folder(folder):
    """
    Get packages' data from folder

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    folder : str
        Folder of the package
    Returns
    -------
    dict
    """
    if file_utils.is_folder(folder):
        data = _group_files_by_xml_filename(
            folder,
            file_utils.xml_files_list(folder),
            file_utils.files_list(folder),
        )
        return data


def _explore_zipfile(zip_path):
    """
    Get packages' data from zip_path

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    zip_path : str
        zip file path
    Returns
    -------
    dict
    """
    if file_utils.is_zipfile(zip_path):
        with ZipFile(zip_path, 'r'):
            data = _group_files_by_xml_filename(
                zip_path,
                file_utils.xml_files_list_from_zipfile(zip_path),
                file_utils.files_list_from_zipfile(zip_path),
            )
            return data


def _group_files_by_xml_filename(source, xmls, files):
    """
    Group files by their XML basename

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    xml_filename : str
        XML filenames
    files : list
        list of files in the folder or zipfile

    Returns
    -------
    dict
        key: name of the XML files
        value: Package
    """
    docs = {}
    for xml in xmls:
        basename = os.path.basename(xml)
        prefix, ext = os.path.splitext(basename)

        docs.setdefault(prefix, Package(source, prefix))

        # XML
        docs[prefix].xml = xml

        for file in select_filenames_by_prefix(prefix, files):
            # avalia arquivo do pacote, se é asset ou rendition
            component = _eval_file(prefix, file)
            if not component:
                continue

            # resultado do avaliação do pacote
            ftype = component.get("ftype")
            file_path = component["file_path"]
            comp_id = component["component_id"]

            if ftype:
                docs[prefix].add_asset(comp_id, file_path)
            else:
                docs[prefix].add_rendition(comp_id, file_path)
            files.remove(file)
    return docs


def _eval_file(prefix, file_path):
    """
    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.

    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e
    o endereço do arquivo em análise.

    Parameters
    ----------
    prefix : str
        nome do arquivo XML sem extensão
    filename : str
        filename
    file_folder : str
        file folder

    Returns
    -------
    dict
    """
    if not match_file_by_prefix(prefix, file_path):
        # ignore files which name does not match
        return
    if file_path.endswith(".xml"):
        # ignore XML files
        return

    # it matches
    filename = os.path.basename(file_path)
    fname, ext = os.path.splitext(filename)

    lang = None
    if ext == ".pdf":
        suffix = fname.replace(prefix, "")
        if fname == prefix:
            lang = "original"
        elif len(suffix) == 3 and suffix[0] == "-":
            # it is a rendition
            lang = suffix[1:]

    if lang:
        return dict(
            component_id=lang,
            file_path=file_path,
        )
    else:
        return dict(
            component_id=filename,
            component_name=fname,
            ftype=ext[1:],
            file_path=file_path,
        )


if __name__ == "__main__":
    # isT=True
    # result = _explore_folder("/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder")
    # pkg1 = Package("/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder", "folder")
    # pkg1.xml = "2318-0889-tinf-33-e200057.xml"
    # pkg1._renditions = {
    #     "original":
    #         "/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder\\2318-0889-tinf-33-e200057.pdf"}
    #
    # pkg2 = Package("/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder", "folder")
    # pkg2.xml = "2318-0889-tinf-33-e200068.xml"
    # pkg2._renditions = {
    #     "original":
    #         "/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder\\2318-0889-tinf-33-e200068.pdf",
    #     "es":
    #         "/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder\\2318-0889-tinf-33-e200068-es.pdf",
    # }
    # pkg2._assets = {
    #     "2318-0889-tinf-33-e200068-gf01.tif":
    #         "/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder\\"
    #         "2318-0889-tinf-33-e200068-gf01.tif",
    #     "2318-0889-tinf-33-e200068-gf02.tif":
    #         "/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder\\"
    #         "2318-0889-tinf-33-e200068-gf02.tif"
    # }
    #
    # if pkg1.xml != result["2318-0889-tinf-33-e200057"].xml:
    #     isT=False
    #     print(pkg1.xml)
    #     print(result["2318-0889-tinf-33-e200057"].xml)
    #
    # if pkg1._assets != result["2318-0889-tinf-33-e200057"]._assets:
    #     isT=False
    #     print(pkg1._assets)
    #     print(result["2318-0889-tinf-33-e200057"]._assets)
    #
    #
    # if pkg1._renditions !=result["2318-0889-tinf-33-e200057"]._renditions:
    #     isT=False
    #     # print(pkg1._renditions)
    #     # print(result["2318-0889-tinf-33-e200057"]._renditions)
    #
    #
    # if pkg2.xml !=result["2318-0889-tinf-33-e200068"].xml:
    #     isT=False
    #     # print(pkg2.xml)
    #     # print(result["2318-0889-tinf-33-e200068"].xml)
    # if pkg2._assets != result["2318-0889-tinf-33-e200068"]._assets:
    #     isT=False
    #     # print(pkg2._assets)
    #     # print(result["2318-0889-tinf-33-e200068"]._assets)
    # if pkg2._renditions != result["2318-0889-tinf-33-e200068"]._renditions:
    #     isT=False
    # if not isT:
    #     raise Exception("Result not True!!!")

    isT = True
    result = _explore_folder(
        "/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder")
    pkg1 = Package("/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder",
                   "folder")
    pkg1.xml = "2318-0889-tinf-33-e200057.xml"
    pkg1._renditions = {
        "original":
            "/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder/2318-0889-tinf-33-e200057.pdf"}

    pkg2 = Package("/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder",
                   "folder")
    pkg2.xml = "2318-0889-tinf-33-e200068.xml"
    pkg2._renditions = {
        "original":
            "/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder/2318-0889-tinf-33-e200068.pdf",
        "es":
            "/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder/2318-0889-tinf-33-e200068-es.pdf",
    }
    pkg2._assets = {
        "2318-0889-tinf-33-e200068-gf01.tif":
            "/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder/"
            "2318-0889-tinf-33-e200068-gf01.tif",
        "2318-0889-tinf-33-e200068-gf02.tif":
            "/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_folder/"
            "2318-0889-tinf-33-e200068-gf02.tif"
    }

    if pkg1.xml != result["2318-0889-tinf-33-e200057"].xml:
        isT = False
        print(pkg1.xml)
        print(result["2318-0889-tinf-33-e200057"].xml)

    if pkg1._assets != result["2318-0889-tinf-33-e200057"]._assets:
        isT = False
        print(pkg1._assets)
        print(result["2318-0889-tinf-33-e200057"]._assets)

    if pkg1._renditions != result["2318-0889-tinf-33-e200057"]._renditions:
        isT = False
        # print(pkg1._renditions)
        # print(result["2318-0889-tinf-33-e200057"]._renditions)

    if pkg2.xml != result["2318-0889-tinf-33-e200068"].xml:
        isT = False
        # print(pkg2.xml)
        # print(result["2318-0889-tinf-33-e200068"].xml)
    if pkg2._assets != result["2318-0889-tinf-33-e200068"]._assets:
        isT = False
        # print(pkg2._assets)
        # print(result["2318-0889-tinf-33-e200068"]._assets)
    if pkg2._renditions != result["2318-0889-tinf-33-e200068"]._renditions:
        isT = False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/scieloorg---packtools/packtools/sps/models/packages__eval_file_passk_validte.py
import logging
import os
import sys
sys.path.append("/home/travis/builds/repos/scieloorg---packtools/")
from packtools import file_utils
from zipfile import ZipFile


logger = logging.getLogger(__name__)


class Package:
    def __init__(self, source, name):
        self._source = source
        self._xml = None
        self._assets = {}
        self._renditions = {}
        self._name = name
        self.zip_file_path = file_utils.is_zipfile(source) and source

    @property
    def assets(self):
        return self._assets

    @property
    def name(self):
        return self._name

    def file_path(self, file_path):
        if file_utils.is_folder(self._source):
            return os.path.join(self._source, file_path)
        return file_path

    def add_asset(self, basename, file_path):
        """
        "{
            "artigo02-gf03.tiff": "/path/artigo02-gf03.tiff",
            "artigo02-gf03.jpg": "/path/artigo02-gf03.jpg",
            "artigo02-gf03.png": "/path/artigo02-gf03.png",
        }
        """
        self._assets[basename] = self.file_path(file_path)

    def get_asset(self, basename):
        try:
            return self._assets[basename]
        except KeyError:
            return

    def add_rendition(self, lang, file_path):
        """
        {
            "original": "artigo02.pdf",
            "en": "artigo02-en.pdf",
        }
        """
        self._renditions[lang] = self.file_path(file_path)

    def get_rendition(self, lang):
        try:
            return self._renditions[lang]
        except KeyError:
            return

    @property
    def source(self):
        return self._source

    @property
    def xml(self):
        return self.file_path(self._xml)

    @xml.setter
    def xml(self, value):
        self._xml = value

    @property
    def renditions(self):
        return self._renditions

    @property
    def xml_content(self):
        if file_utils.is_folder(self._source):
            with open(self.xml, "rb") as fp:
                return fp.read()
        with ZipFile(self._source) as zf:
            return zf.read(self.xml)


def select_filenames_by_prefix(prefix, files):
    """
    Get files which belongs to a document package.

    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`

    Parameters
    ----------
    prefix : str
        Filename prefix
    files : str list
        Files paths
    Returns
    -------
    list
        files paths which basename files matches to prefix
    """
    return [
        item
        for item in files
        if match_file_by_prefix(prefix, item)
    ]


def match_file_by_prefix(prefix, file_path):
    """
    Identify if a `file_path` belongs to a document package by a given `prefix`

    Retorna `True` para documentos pertencentes a um pacote.

    Parameters
    ----------
    prefix : str
        Filename prefix
    file_path : str
        File path
    Returns
    -------
    bool
        True - file belongs to the package
    """
    basename = os.path.basename(file_path)
    if basename.startswith(prefix + "-"):
        return True
    if basename.startswith(prefix + "."):
        return True
    return False


def explore_source(source):
    packages = _explore_zipfile(source)
    if not packages:
        packages = _explore_folder(source)
    if not packages:
        raise ValueError("%s: Invalid value for `source`" % source)
    return packages


def _explore_folder(folder):
    """
    Get packages' data from folder

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    folder : str
        Folder of the package
    Returns
    -------
    dict
    """
    if file_utils.is_folder(folder):
        data = _group_files_by_xml_filename(
            folder,
            file_utils.xml_files_list(folder),
            file_utils.files_list(folder),
        )
        return data


def _explore_zipfile(zip_path):
    """
    Get packages' data from zip_path

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    zip_path : str
        zip file path
    Returns
    -------
    dict
    """
    if file_utils.is_zipfile(zip_path):
        with ZipFile(zip_path, 'r'):
            data = _group_files_by_xml_filename(
                zip_path,
                file_utils.xml_files_list_from_zipfile(zip_path),
                file_utils.files_list_from_zipfile(zip_path),
            )
            return data


def _group_files_by_xml_filename(source, xmls, files):
    """
    Group files by their XML basename

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    xml_filename : str
        XML filenames
    files : list
        list of files in the folder or zipfile

    Returns
    -------
    dict
        key: name of the XML files
        value: Package
    """
    docs = {}
    for xml in xmls:
        basename = os.path.basename(xml)
        prefix, ext = os.path.splitext(basename)

        docs.setdefault(prefix, Package(source, prefix))

        # XML
        docs[prefix].xml = xml

        for file in select_filenames_by_prefix(prefix, files):
            # avalia arquivo do pacote, se é asset ou rendition
            component = _eval_file(prefix, file)
            if not component:
                continue

            # resultado do avaliação do pacote
            ftype = component.get("ftype")
            file_path = component["file_path"]
            comp_id = component["component_id"]

            if ftype:
                docs[prefix].add_asset(comp_id, file_path)
            else:
                docs[prefix].add_rendition(comp_id, file_path)
            files.remove(file)
    return docs


def _eval_file(prefix, file_path):
    """
    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.

    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e
    o endereço do arquivo em análise.

    Parameters
    ----------
    prefix : str
        nome do arquivo XML sem extensão
    filename : str
        filename
    file_folder : str
        file folder

    Returns
    -------
    dict
    """
    if not match_file_by_prefix(prefix, file_path):
        # ignore files which name does not match
        return
    if file_path.endswith(".xml"):
        # ignore XML files
        return

    # it matches
    filename = os.path.basename(file_path)
    fname, ext = os.path.splitext(filename)

    lang = None
    if ext == ".pdf":
        suffix = fname.replace(prefix, "")
        if fname == prefix:
            lang = "original"
        elif len(suffix) == 3 and suffix[0] == "-":
            # it is a rendition
            lang = suffix[1:]

    if lang:
        return dict(
            component_id=lang,
            file_path=file_path,
        )
    else:
        return dict(
            component_id=filename,
            component_name=fname,
            ftype=ext[1:],
            file_path=file_path,
        )


if __name__ == "__main__":
    isT=True
    def _eval_file_as_pdf():
        expected = {
            "component_id": "abcd-suppl01.pdf",
            "component_name": "abcd-suppl01",
            "file_path": "abcd-suppl01.pdf",
            "ftype": "pdf",
        }
        result =_eval_file(
            prefix="abcd", file_path="abcd-suppl01.pdf")
        if expected== result:
            return True
        else:
            return False


    def _eval_file_as_es_pdf():
        expected = {
            "component_id": "es",
            "file_path": "abcd-es.pdf",
        }
        result = _eval_file(
            prefix="abcd", file_path="abcd-es.pdf")
        if expected == result:
            return True
        else:
            return False

    def _eval_file_as_original_pdf():
        expected = {
            "component_id": "original",
            "file_path": "abcd.pdf",
        }
        result = _eval_file(
            prefix="abcd", file_path="abcd.pdf")
        if expected == result:
            return True
        else:
            return False

    def _eval_file_as_jpg():
        expected = {
            "component_id": "abcd-gf01.jpg",
            "component_name": "abcd-gf01",
            "file_path": "abcd-gf01.jpg",
            "ftype": "jpg",
        }
        result = _eval_file(
            prefix="abcd", file_path="abcd-gf01.jpg")
        if expected == result:
            return True
        else:
            return False

    def _eval_file_as_png():
        expected = {
            "component_id": "abcd-gf01.png",
            "component_name": "abcd-gf01",
            "file_path": "abcd-gf01.png",
            "ftype": "png",
        }
        result = _eval_file(
            prefix="abcd", file_path="abcd-gf01.png")
        if expected == result:
            return True
        else:
            return False



    def _eval_file_as_tif():
        expected = {
            "component_id": "abcd-gf01.tif",
            "component_name": "abcd-gf01",
            "file_path": "abcd-gf01.tif",
            "ftype": "tif",
        }
        result = _eval_file(
            prefix="abcd", file_path="abcd-gf01.tif")
        if expected == result:
            return True
        else:
            return False

    def _eval_file_returns_none():
        expected = None
        result = _eval_file(
            prefix="abcd", file_path="abcd-gf01.xml")
        if expected == result:
            return True
        else:
            return False

    def _eval_file_returns_none_because_prefix_doesnot_match():
        expected = None
        result = _eval_file(
            prefix="cd", file_path="abcd-gf01.xml")
        if expected == result:
            return True
        else:
            return False

    if not _eval_file_returns_none_because_prefix_doesnot_match() or not _eval_file_returns_none() or not _eval_file_as_tif() or not _eval_file_as_pdf() or not _eval_file_as_es_pdf() or not _eval_file_as_original_pdf() or not _eval_file_as_jpg() or not _eval_file_as_png():
        isT=False


    # import dill
    # import os
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463153879012d1948149f/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463153879012d1948149f/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     print(args0,args1)
    #     res0 = _eval_file(args0,args1)
    #     print(res0)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/scieloorg---packtools/packtools/sps/models/packages_add_rendition_passk_validte.py
import logging
import os
import sys
sys.path.append("/home/travis/builds/repos/scieloorg---packtools/")
from packtools import file_utils
from zipfile import ZipFile


logger = logging.getLogger(__name__)


class Package:
    def __init__(self, source, name):
        self._source = source
        self._xml = None
        self._assets = {}
        self._renditions = {}
        self._name = name
        self.zip_file_path = file_utils.is_zipfile(source) and source

    @property
    def assets(self):
        return self._assets

    @property
    def name(self):
        return self._name

    def file_path(self, file_path):
        if file_utils.is_folder(self._source):
            return os.path.join(self._source, file_path)
        return file_path

    def add_asset(self, basename, file_path):
        """
        "{
            "artigo02-gf03.tiff": "/path/artigo02-gf03.tiff",
            "artigo02-gf03.jpg": "/path/artigo02-gf03.jpg",
            "artigo02-gf03.png": "/path/artigo02-gf03.png",
        }
        """
        self._assets[basename] = self.file_path(file_path)

    def get_asset(self, basename):
        try:
            return self._assets[basename]
        except KeyError:
            return

    def add_rendition(self, lang, file_path):
        """
        {
            "original": "artigo02.pdf",
            "en": "artigo02-en.pdf",
        }
        """
        self._renditions[lang] = self.file_path(file_path)

    def get_rendition(self, lang):
        try:
            return self._renditions[lang]
        except KeyError:
            return

    @property
    def source(self):
        return self._source

    @property
    def xml(self):
        return self.file_path(self._xml)

    @xml.setter
    def xml(self, value):
        self._xml = value

    @property
    def renditions(self):
        return self._renditions

    @property
    def xml_content(self):
        if file_utils.is_folder(self._source):
            with open(self.xml, "rb") as fp:
                return fp.read()
        with ZipFile(self._source) as zf:
            return zf.read(self.xml)


def select_filenames_by_prefix(prefix, files):
    """
    Get files which belongs to a document package.

    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`

    Parameters
    ----------
    prefix : str
        Filename prefix
    files : str list
        Files paths
    Returns
    -------
    list
        files paths which basename files matches to prefix
    """
    return [
        item
        for item in files
        if match_file_by_prefix(prefix, item)
    ]


def match_file_by_prefix(prefix, file_path):
    """
    Identify if a `file_path` belongs to a document package by a given `prefix`

    Retorna `True` para documentos pertencentes a um pacote.

    Parameters
    ----------
    prefix : str
        Filename prefix
    file_path : str
        File path
    Returns
    -------
    bool
        True - file belongs to the package
    """
    basename = os.path.basename(file_path)
    if basename.startswith(prefix + "-"):
        return True
    if basename.startswith(prefix + "."):
        return True
    return False


def explore_source(source):
    packages = _explore_zipfile(source)
    if not packages:
        packages = _explore_folder(source)
    if not packages:
        raise ValueError("%s: Invalid value for `source`" % source)
    return packages


def _explore_folder(folder):
    """
    Get packages' data from folder

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    folder : str
        Folder of the package
    Returns
    -------
    dict
    """
    if file_utils.is_folder(folder):
        data = _group_files_by_xml_filename(
            folder,
            file_utils.xml_files_list(folder),
            file_utils.files_list(folder),
        )
        return data


def _explore_zipfile(zip_path):
    """
    Get packages' data from zip_path

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    zip_path : str
        zip file path
    Returns
    -------
    dict
    """
    if file_utils.is_zipfile(zip_path):
        with ZipFile(zip_path, 'r'):
            data = _group_files_by_xml_filename(
                zip_path,
                file_utils.xml_files_list_from_zipfile(zip_path),
                file_utils.files_list_from_zipfile(zip_path),
            )
            return data


def _group_files_by_xml_filename(source, xmls, files):
    """
    Group files by their XML basename

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    xml_filename : str
        XML filenames
    files : list
        list of files in the folder or zipfile

    Returns
    -------
    dict
        key: name of the XML files
        value: Package
    """
    docs = {}
    for xml in xmls:
        basename = os.path.basename(xml)
        prefix, ext = os.path.splitext(basename)

        docs.setdefault(prefix, Package(source, prefix))

        # XML
        docs[prefix].xml = xml

        for file in select_filenames_by_prefix(prefix, files):
            # avalia arquivo do pacote, se é asset ou rendition
            component = _eval_file(prefix, file)
            if not component:
                continue

            # resultado do avaliação do pacote
            ftype = component.get("ftype")
            file_path = component["file_path"]
            comp_id = component["component_id"]

            if ftype:
                docs[prefix].add_asset(comp_id, file_path)
            else:
                docs[prefix].add_rendition(comp_id, file_path)
            files.remove(file)
    return docs


def _eval_file(prefix, file_path):
    """
    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.

    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e
    o endereço do arquivo em análise.

    Parameters
    ----------
    prefix : str
        nome do arquivo XML sem extensão
    filename : str
        filename
    file_folder : str
        file folder

    Returns
    -------
    dict
    """
    if not match_file_by_prefix(prefix, file_path):
        # ignore files which name does not match
        return
    if file_path.endswith(".xml"):
        # ignore XML files
        return

    # it matches
    filename = os.path.basename(file_path)
    fname, ext = os.path.splitext(filename)

    lang = None
    if ext == ".pdf":
        suffix = fname.replace(prefix, "")
        if fname == prefix:
            lang = "original"
        elif len(suffix) == 3 and suffix[0] == "-":
            # it is a rendition
            lang = suffix[1:]

    if lang:
        return dict(
            component_id=lang,
            file_path=file_path,
        )
    else:
        return dict(
            component_id=filename,
            component_name=fname,
            ftype=ext[1:],
            file_path=file_path,
        )


if __name__ == "__main__":
    isT=True
    pkg1 = Package("source", "name")
    pkg11 = Package("source", "name")

    pkg11.xml = "a11.xml"
    pkg11._assets = {
        "a11-gf01-es.tiff": "a11-gf01-es.tiff",
        "a11-gf02-es.tiff": "a11-gf02-es.tiff",
        "a11-suppl-es.pdf": "a11-suppl-es.pdf",
    }
    pkg11._renditions = {
        "es": "a11-es.pdf",
        "original": "a11.pdf",
    }

    pkg1.xml = "a1.xml"
    pkg1._assets = {
        "a1-gf01.tiff": "a1-gf01.tiff",
        "a1-gf01.jpg": "a1-gf01.jpg",
        "a1-gf02.tiff": "a1-gf02.tiff",
    }
    pkg1._renditions = {
        "en": "a1-en.pdf",
        "original": "a1.pdf",
    }
    xmls = [
        "a1.xml", "a11.xml",
    ]
    files = [
        "a1-en.pdf",
        "a1-gf01.jpg",
        "a1-gf01.tiff",
        "a1-gf02.tiff",
        "a1.pdf",
        "a1.xml",
        "a11-es.pdf",
        "a11-gf01-es.tiff",
        "a11-gf02-es.tiff",
        "a11-suppl-es.pdf",
        "a11.pdf",
        "a11.xml",
    ]
    result = _group_files_by_xml_filename("source", xmls, files)
    ist1=pkg11.xml== result["a11"].xml
    ist2=pkg11._assets==result["a11"]._assets
    ist3=pkg11._renditions== result["a11"]._renditions
    ist4=pkg1.xml== result["a1"].xml
    ist5=pkg1._assets== result["a1"]._assets
    ist6=pkg1._renditions== result["a1"]._renditions

    if not ist1 or not ist2 or not ist3 or not ist4 or not ist5 or not ist6:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463153879012d1948149a/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463153879012d1948149a/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"],bytes):
    #         args2=dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2=content["input"]["args"][2]["bytes"]
    #     print(args0,args1,args2)
    # #     res0 = _group_files_by_xml_filename(args0,args1,args2)
    # #     if res0=={}:
    # #         continue
    # #     if "2318-0889-tinf-33-e200071" not in res0.keys():
    # #         isT=False
    # #         break
    # #     if res0["2318-0889-tinf-33-e200071"].source!="./tests/sps/fixtures/package.zip" or res0["2318-0889-tinf-33-e200071"].name!="2318-0889-tinf-33-e200071":
    # #         isT=False
    # #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/scieloorg---packtools/packtools/sps/models/packages_add_asset_passk_validte.py
import logging
import os
import sys
sys.path.append("/home/travis/builds/repos/scieloorg---packtools/")
from packtools import file_utils
from zipfile import ZipFile


logger = logging.getLogger(__name__)


class Package:
    def __init__(self, source, name):
        self._source = source
        self._xml = None
        self._assets = {}
        self._renditions = {}
        self._name = name
        self.zip_file_path = file_utils.is_zipfile(source) and source

    @property
    def assets(self):
        return self._assets

    @property
    def name(self):
        return self._name

    def file_path(self, file_path):
        if file_utils.is_folder(self._source):
            return os.path.join(self._source, file_path)
        return file_path

    def add_asset(self, basename, file_path):
        """
        "{
            "artigo02-gf03.tiff": "/path/artigo02-gf03.tiff",
            "artigo02-gf03.jpg": "/path/artigo02-gf03.jpg",
            "artigo02-gf03.png": "/path/artigo02-gf03.png",
        }
        """
        self._assets[basename] = self.file_path(file_path)

    def get_asset(self, basename):
        try:
            return self._assets[basename]
        except KeyError:
            return

    def add_rendition(self, lang, file_path):
        """
        {
            "original": "artigo02.pdf",
            "en": "artigo02-en.pdf",
        }
        """
        self._renditions[lang] = self.file_path(file_path)

    def get_rendition(self, lang):
        try:
            return self._renditions[lang]
        except KeyError:
            return

    @property
    def source(self):
        return self._source

    @property
    def xml(self):
        return self.file_path(self._xml)

    @xml.setter
    def xml(self, value):
        self._xml = value

    @property
    def renditions(self):
        return self._renditions

    @property
    def xml_content(self):
        if file_utils.is_folder(self._source):
            with open(self.xml, "rb") as fp:
                return fp.read()
        with ZipFile(self._source) as zf:
            return zf.read(self.xml)


def select_filenames_by_prefix(prefix, files):
    """
    Get files which belongs to a document package.

    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`

    Parameters
    ----------
    prefix : str
        Filename prefix
    files : str list
        Files paths
    Returns
    -------
    list
        files paths which basename files matches to prefix
    """
    return [
        item
        for item in files
        if match_file_by_prefix(prefix, item)
    ]


def match_file_by_prefix(prefix, file_path):
    """
    Identify if a `file_path` belongs to a document package by a given `prefix`

    Retorna `True` para documentos pertencentes a um pacote.

    Parameters
    ----------
    prefix : str
        Filename prefix
    file_path : str
        File path
    Returns
    -------
    bool
        True - file belongs to the package
    """
    basename = os.path.basename(file_path)
    if basename.startswith(prefix + "-"):
        return True
    if basename.startswith(prefix + "."):
        return True
    return False


def explore_source(source):
    packages = _explore_zipfile(source)
    if not packages:
        packages = _explore_folder(source)
    if not packages:
        raise ValueError("%s: Invalid value for `source`" % source)
    return packages


def _explore_folder(folder):
    """
    Get packages' data from folder

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    folder : str
        Folder of the package
    Returns
    -------
    dict
    """
    if file_utils.is_folder(folder):
        data = _group_files_by_xml_filename(
            folder,
            file_utils.xml_files_list(folder),
            file_utils.files_list(folder),
        )
        return data


def _explore_zipfile(zip_path):
    """
    Get packages' data from zip_path

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    zip_path : str
        zip file path
    Returns
    -------
    dict
    """
    if file_utils.is_zipfile(zip_path):
        with ZipFile(zip_path, 'r'):
            data = _group_files_by_xml_filename(
                zip_path,
                file_utils.xml_files_list_from_zipfile(zip_path),
                file_utils.files_list_from_zipfile(zip_path),
            )
            return data


def _group_files_by_xml_filename(source, xmls, files):
    """
    Group files by their XML basename

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    xml_filename : str
        XML filenames
    files : list
        list of files in the folder or zipfile

    Returns
    -------
    dict
        key: name of the XML files
        value: Package
    """
    docs = {}
    for xml in xmls:
        basename = os.path.basename(xml)
        prefix, ext = os.path.splitext(basename)

        docs.setdefault(prefix, Package(source, prefix))

        # XML
        docs[prefix].xml = xml

        for file in select_filenames_by_prefix(prefix, files):
            # avalia arquivo do pacote, se é asset ou rendition
            component = _eval_file(prefix, file)
            if not component:
                continue

            # resultado do avaliação do pacote
            ftype = component.get("ftype")
            file_path = component["file_path"]
            comp_id = component["component_id"]

            if ftype:
                docs[prefix].add_asset(comp_id, file_path)
            else:
                docs[prefix].add_rendition(comp_id, file_path)
            files.remove(file)
    return docs


def _eval_file(prefix, file_path):
    """
    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.

    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e
    o endereço do arquivo em análise.

    Parameters
    ----------
    prefix : str
        nome do arquivo XML sem extensão
    filename : str
        filename
    file_folder : str
        file folder

    Returns
    -------
    dict
    """
    if not match_file_by_prefix(prefix, file_path):
        # ignore files which name does not match
        return
    if file_path.endswith(".xml"):
        # ignore XML files
        return

    # it matches
    filename = os.path.basename(file_path)
    fname, ext = os.path.splitext(filename)

    lang = None
    if ext == ".pdf":
        suffix = fname.replace(prefix, "")
        if fname == prefix:
            lang = "original"
        elif len(suffix) == 3 and suffix[0] == "-":
            # it is a rendition
            lang = suffix[1:]

    if lang:
        return dict(
            component_id=lang,
            file_path=file_path,
        )
    else:
        return dict(
            component_id=filename,
            component_name=fname,
            ftype=ext[1:],
            file_path=file_path,
        )


if __name__ == "__main__":
    isT=True
    pkg1 = Package("source", "name")
    pkg11 = Package("source", "name")

    pkg11.xml = "a11.xml"
    pkg11._assets = {
        "a11-gf01-es.tiff": "a11-gf01-es.tiff",
        "a11-gf02-es.tiff": "a11-gf02-es.tiff",
        "a11-suppl-es.pdf": "a11-suppl-es.pdf",
    }
    pkg11._renditions = {
        "es": "a11-es.pdf",
        "original": "a11.pdf",
    }

    pkg1.xml = "a1.xml"
    pkg1._assets = {
        "a1-gf01.tiff": "a1-gf01.tiff",
        "a1-gf01.jpg": "a1-gf01.jpg",
        "a1-gf02.tiff": "a1-gf02.tiff",
    }
    pkg1._renditions = {
        "en": "a1-en.pdf",
        "original": "a1.pdf",
    }
    xmls = [
        "a1.xml", "a11.xml",
    ]
    files = [
        "a1-en.pdf",
        "a1-gf01.jpg",
        "a1-gf01.tiff",
        "a1-gf02.tiff",
        "a1.pdf",
        "a1.xml",
        "a11-es.pdf",
        "a11-gf01-es.tiff",
        "a11-gf02-es.tiff",
        "a11-suppl-es.pdf",
        "a11.pdf",
        "a11.xml",
    ]
    result = _group_files_by_xml_filename("source", xmls, files)
    ist1=pkg11.xml== result["a11"].xml
    ist2=pkg11._assets==result["a11"]._assets
    ist3=pkg11._renditions== result["a11"]._renditions
    ist4=pkg1.xml== result["a1"].xml
    ist5=pkg1._assets== result["a1"]._assets
    ist6=pkg1._renditions== result["a1"]._renditions

    if not ist1 or not ist2 or not ist3 or not ist4 or not ist5 or not ist6:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463153879012d1948149a/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463153879012d1948149a/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"],bytes):
    #         args2=dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2=content["input"]["args"][2]["bytes"]
    #     print(args0,args1,args2)
    # #     res0 = _group_files_by_xml_filename(args0,args1,args2)
    # #     if res0=={}:
    # #         continue
    # #     if "2318-0889-tinf-33-e200071" not in res0.keys():
    # #         isT=False
    # #         break
    # #     if res0["2318-0889-tinf-33-e200071"].source!="./tests/sps/fixtures/package.zip" or res0["2318-0889-tinf-33-e200071"].name!="2318-0889-tinf-33-e200071":
    # #         isT=False
    # #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/scieloorg---packtools/packtools/sps/models/packages__explore_zipfile_passk_validte.py
import logging
import os
import sys
sys.path.append("/home/travis/builds/repos/scieloorg---packtools/")
from packtools import file_utils
from zipfile import ZipFile


logger = logging.getLogger(__name__)


class Package:
    def __init__(self, source, name):
        self._source = source
        self._xml = None
        self._assets = {}
        self._renditions = {}
        self._name = name
        self.zip_file_path = file_utils.is_zipfile(source) and source

    @property
    def assets(self):
        return self._assets

    @property
    def name(self):
        return self._name

    def file_path(self, file_path):
        if file_utils.is_folder(self._source):
            return os.path.join(self._source, file_path)
        return file_path

    def add_asset(self, basename, file_path):
        """
        "{
            "artigo02-gf03.tiff": "/path/artigo02-gf03.tiff",
            "artigo02-gf03.jpg": "/path/artigo02-gf03.jpg",
            "artigo02-gf03.png": "/path/artigo02-gf03.png",
        }
        """
        self._assets[basename] = self.file_path(file_path)

    def get_asset(self, basename):
        try:
            return self._assets[basename]
        except KeyError:
            return

    def add_rendition(self, lang, file_path):
        """
        {
            "original": "artigo02.pdf",
            "en": "artigo02-en.pdf",
        }
        """
        self._renditions[lang] = self.file_path(file_path)

    def get_rendition(self, lang):
        try:
            return self._renditions[lang]
        except KeyError:
            return

    @property
    def source(self):
        return self._source

    @property
    def xml(self):
        return self.file_path(self._xml)

    @xml.setter
    def xml(self, value):
        self._xml = value

    @property
    def renditions(self):
        return self._renditions

    @property
    def xml_content(self):
        if file_utils.is_folder(self._source):
            with open(self.xml, "rb") as fp:
                return fp.read()
        with ZipFile(self._source) as zf:
            return zf.read(self.xml)


def select_filenames_by_prefix(prefix, files):
    """
    Get files which belongs to a document package.

    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`

    Parameters
    ----------
    prefix : str
        Filename prefix
    files : str list
        Files paths
    Returns
    -------
    list
        files paths which basename files matches to prefix
    """
    return [
        item
        for item in files
        if match_file_by_prefix(prefix, item)
    ]


def match_file_by_prefix(prefix, file_path):
    """
    Identify if a `file_path` belongs to a document package by a given `prefix`

    Retorna `True` para documentos pertencentes a um pacote.

    Parameters
    ----------
    prefix : str
        Filename prefix
    file_path : str
        File path
    Returns
    -------
    bool
        True - file belongs to the package
    """
    basename = os.path.basename(file_path)
    if basename.startswith(prefix + "-"):
        return True
    if basename.startswith(prefix + "."):
        return True
    return False


def explore_source(source):
    packages = _explore_zipfile(source)
    if not packages:
        packages = _explore_folder(source)
    if not packages:
        raise ValueError("%s: Invalid value for `source`" % source)
    return packages


def _explore_folder(folder):
    """
    Get packages' data from folder

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    folder : str
        Folder of the package
    Returns
    -------
    dict
    """
    if file_utils.is_folder(folder):
        data = _group_files_by_xml_filename(
            folder,
            file_utils.xml_files_list(folder),
            file_utils.files_list(folder),
        )
        return data


def _explore_zipfile(zip_path):
    """
    Get packages' data from zip_path

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    zip_path : str
        zip file path
    Returns
    -------
    dict
    """
    if file_utils.is_zipfile(zip_path):
        with ZipFile(zip_path, 'r'):
            data = _group_files_by_xml_filename(
                zip_path,
                file_utils.xml_files_list_from_zipfile(zip_path),
                file_utils.files_list_from_zipfile(zip_path),
            )
            return data


def _group_files_by_xml_filename(source, xmls, files):
    """
    Group files by their XML basename

    Groups files by their XML basename and returns data in dict format.

    Parameters
    ----------
    xml_filename : str
        XML filenames
    files : list
        list of files in the folder or zipfile

    Returns
    -------
    dict
        key: name of the XML files
        value: Package
    """
    docs = {}
    for xml in xmls:
        basename = os.path.basename(xml)
        prefix, ext = os.path.splitext(basename)

        docs.setdefault(prefix, Package(source, prefix))

        # XML
        docs[prefix].xml = xml

        for file in select_filenames_by_prefix(prefix, files):
            # avalia arquivo do pacote, se é asset ou rendition
            component = _eval_file(prefix, file)
            if not component:
                continue

            # resultado do avaliação do pacote
            ftype = component.get("ftype")
            file_path = component["file_path"]
            comp_id = component["component_id"]

            if ftype:
                docs[prefix].add_asset(comp_id, file_path)
            else:
                docs[prefix].add_rendition(comp_id, file_path)
            files.remove(file)
    return docs


def _eval_file(prefix, file_path):
    """
    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.

    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e
    o endereço do arquivo em análise.

    Parameters
    ----------
    prefix : str
        nome do arquivo XML sem extensão
    filename : str
        filename
    file_folder : str
        file folder

    Returns
    -------
    dict
    """
    if not match_file_by_prefix(prefix, file_path):
        # ignore files which name does not match
        return
    if file_path.endswith(".xml"):
        # ignore XML files
        return

    # it matches
    filename = os.path.basename(file_path)
    fname, ext = os.path.splitext(filename)

    lang = None
    if ext == ".pdf":
        suffix = fname.replace(prefix, "")
        if fname == prefix:
            lang = "original"
        elif len(suffix) == 3 and suffix[0] == "-":
            # it is a rendition
            lang = suffix[1:]

    if lang:
        return dict(
            component_id=lang,
            file_path=file_path,
        )
    else:
        return dict(
            component_id=filename,
            component_name=fname,
            ftype=ext[1:],
            file_path=file_path,
        )


if __name__ == "__main__":
    isT=True
    input1='/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package.zip'
    input2 = '/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/package_with_subdir.zip'

    out1=_explore_zipfile(input1)
    ist1=len(out1)==1 and "2318-0889-tinf-33-e200071" in out1.keys()
    out2=_explore_zipfile(input2)
    # print(len(out2))
    ist2=len(out2)==8 and "2318-0889-tinf-33-e200050" in out2.keys() and "._2318-0889-tinf-33-e200050" in out2.keys()
    if not ist1 or not ist2:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463163879012d194814a4/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\scieloorg---packtools/data_passk_platform1/62b463163879012d194814a4/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     print(args0)
    #     res0 = _explore_zipfile(args0)
    #     # print(res0)
    #     # if res0==None:
    #     #     continue
    #     # print(res0["2318-0889-tinf-33-e200071"].source)
    #     # print(res0["2318-0889-tinf-33-e200071"].name)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/scieloorg---packtools/packtools/file_utils_files_list_from_zipfile_passk_validte.py
import os
import logging
import re
import shutil
import tempfile

from zipfile import ZipFile, ZIP_DEFLATED


logger = logging.getLogger(__name__)


def is_folder(source):
    return os.path.isdir(source)


def is_zipfile(source):
    return os.path.isfile(source) and source.endswith(".zip")


def xml_files_list(path):
    """
    Return the XML files found in `path`
    """
    return (f for f in os.listdir(path) if f.endswith(".xml"))


def files_list(path):
    """
    Return the files in `path`
    """
    return os.listdir(path)


def read_file(path, encoding="utf-8", mode="r"):
    with open(path, mode=mode, encoding=encoding) as f:
        text = f.read()
    return text


def read_from_zipfile(zip_path, filename):
    with ZipFile(zip_path) as zf:
        return zf.read(filename)


def xml_files_list_from_zipfile(zip_path):
    with ZipFile(zip_path) as zf:
        xmls_filenames = [
            xml_filename
            for xml_filename in zf.namelist()
            if os.path.splitext(xml_filename)[-1] == ".xml"
        ]
    return xmls_filenames


def files_list_from_zipfile(zip_path):
    """
    Return the files in `zip_path`

    Example:

    ```
    [
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',
        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',
    ]
    ```
    """
    with ZipFile(zip_path) as zf:
        return zf.namelist()


def write_file(path, source, mode="w"):
    dirname = os.path.dirname(path)
    if not os.path.isdir(dirname):
        os.makedirs(dirname)
    logger.debug("Gravando arquivo: %s", path)
    if "b" in mode:
        with open(path, mode) as f:
            f.write(source)
        return

    with open(path, mode, encoding="utf-8") as f:
        f.write(source)


def create_zip_file(files, zip_name, zip_folder=None):
    zip_folder = zip_folder or tempfile.mkdtemp()

    zip_path = os.path.join(zip_folder, zip_name)
    with ZipFile(zip_path, 'w', ZIP_DEFLATED) as myzip:
        for f in files:
            myzip.write(f, os.path.basename(f))
    return zip_path


def delete_folder(path):
    try:
        shutil.rmtree(path)
    except:
        pass


def create_temp_file(filename, content=None, mode='w'):
    file_path = tempfile.mkdtemp()
    file_path = os.path.join(file_path, filename)
    write_file(file_path, content or '', mode)
    return file_path


def copy_file(source, target):
    tmpdir = tempfile.mkdtemp()
    fullpath_target = os.path.join(tmpdir, target)

    logger.info(f'Copying file {source} to {fullpath_target}')
    return shutil.copyfile(source, fullpath_target)


def size(file_path):
    return os.path.getsize(file_path)


def get_prefix_by_xml_filename(xml_filename):
    """
    Obtém o prefixo associado a um arquivo xml

    Parameters
    ----------
    xml_filename : str
        Nome de arquivo xml

    Returns
    -------
    str
        Prefixo associado ao arquivo xml
    """
    file, ext = os.path.splitext(xml_filename)
    return file


def get_file_role(file_path, prefix, pdf_langs):
    """
    Obtém o papel/função de um arquivo (xml, renditions ou assets) no contexto de um documento

    Parameters
    ----------
    file_path : str
        Nome de um arquivo
    prefix: str
        Prefixo associado ao arquivo
    pdf_langs: list
        Idiomas dos PDFs do documento

    Returns
    -------
    str
        Papel/função de arquivo (xml, rendition ou assets) no contexto de um documento
    """
    file, ext = os.path.splitext(file_path)

    if ext == '.xml':
        return 'xml'
    elif ext == '.pdf':
        if file == prefix:
            return 'renditions'

        for lang in pdf_langs:
            if file == f'{prefix}-{lang}':
                return 'renditions'
    return 'assets'


def extract_issn_from_zip_uri(zip_uri):
    """
    Extrai código ISSN a partir do endereço de um arquivo zip

    Parameters
    ----------
    zip_uri : str
        Endereço de um arquivo zip

    Returns
    -------
    str
        ISSN
    """
    match = re.search(r'.*/ingress/packages/(\d{4}-\d{4})/.*.zip', zip_uri)
    if match:
        return match.group(1)


def get_filename(file_path):
    return os.path.basename(file_path)

if __name__ == "__main__":
    import dill
    import os
    if str(files_list_from_zipfile("/home/travis/builds/repos/scieloorg---packtools/HumanEval.zip"))!="['HumanEval/', 'HumanEval/CodeGen_on_HumanEval.jsonl', 'HumanEval/Pangu-Coder_on_HumanEval.jsonl', 'HumanEval/Pangu-FT_on_HumanEval.jsonl', 'HumanEval/WenwangCoder_on_HumanEval.jsonl']":
        raise Exception("Result not True!!!")

    # print(files_list_from_zipfile("./HumanEval.zip"))




----------------------------
/home/travis/builds/repos/scieloorg---packtools/packtools/sps/utils/xml_utils_fix_namespace_prefix_w_passk_validte.py
import logging
import re
import sys
sys.path.append("/home/travis/builds/repos/scieloorg---packtools/")
from copy import deepcopy
from lxml import etree
from packtools import validations
from packtools.sps import exceptions
from packtools import file_utils
from packtools.sps.models.article_and_subarticles import ArticleAndSubArticles

logger = logging.getLogger(__name__)


def get_nodes_with_lang(xmltree, lang_xpath, node_xpath=None):
    _items = []
    for node in xmltree.xpath(lang_xpath):
        _item = {}
        if node_xpath:
            _item["node"] = node.find(node_xpath)
        else:
            _item["node"] = node
        _item["lang"] = node.get('{http://www.w3.org/XML/1998/namespace}lang')
        _items.append(_item)
    return _items


def node_plain_text(node):
    """
    Função que retorna texto de nó, sem subtags e com espaços padronizados

    Entrada:
    ```xml
    <node>
        <italic>Duguetia leucotricha</italic> (Annonaceae)<xref>1</xref>
    </node>
    ```

    Saída:
    Duguetia leucotricha (Annonaceae)
    """
    if node is None:
        return
    for xref in node.findall(".//xref"):
        for child in xref.findall(".//*"):
            child.text = ""
        xref.text = ""
    text = "".join([text for text in node.xpath(".//text()") if text.strip()])
    return " ".join(text.split())


def node_text_without_xref(node):
    """
    Retorna text com subtags, exceto `xref`
    """
    if node is None:
        return

    node = deepcopy(node)

    for xref in node.findall(".//xref"):
        if xref.tail:
            _next = xref.getnext()
            if _next is None or _next.tag != "xref":
                e = etree.Element("EMPTYTAGTOKEEPXREFTAIL")
                xref.addnext(e)
    for xref in node.findall(".//xref"):
        parent = xref.getparent()
        parent.remove(xref)
    etree.strip_tags(node, "EMPTYTAGTOKEEPXREFTAIL")
    return node_text(node)


def formatted_text(title_node):
    # FIXME substituir `formatted_text` por `node_text_without_xref`
    # por ser mais explícito
    return node_text_without_xref(title_node)


def fix_xml(xml_str):
    return fix_namespace_prefix_w(xml_str)


def fix_namespace_prefix_w(content):
    """
    Convert os textos cujo padrão é `w:st="` em `w-st="`
    """
    pattern = r"\bw:[a-z]{1,}=\""
    found_items = re.findall(pattern, content)
    logger.debug("Found %i namespace prefix w", len(found_items))
    for item in set(found_items):
        new_namespace = item.replace(":", "-")
        logger.debug("%s -> %s" % (item, new_namespace))
        content = content.replace(item, new_namespace)
    return content


def _get_xml_content(xml):
    if isinstance(xml, str):
        try:
            content = file_utils.read_file(xml)
        except (FileNotFoundError, OSError):
            content = xml
        content = fix_xml(content)
        return content.encode("utf-8")
    return xml


def get_xml_tree(content):
    parser = etree.XMLParser(remove_blank_text=True, no_network=True)
    try:
        content = _get_xml_content(content)
        xml_tree = etree.XML(content, parser)
    except etree.XMLSyntaxError as exc:
        raise exceptions.SPSLoadToXMLError(str(exc)) from None
    else:
        return xml_tree


def tostring(node, doctype=None, pretty_print=False):
    return etree.tostring(
        node,
        doctype=doctype,
        xml_declaration=True,
        method="xml",
        encoding="utf-8",
        pretty_print=pretty_print,
    ).decode("utf-8")


def node_text(node):
    """
    Retorna todos os node.text, incluindo a subtags
    Para <title>Text <bold>text</bold> Text</title>, retorna
    Text <bold>text</bold> Text
    """
    items = [node.text or ""]
    for child in node.getchildren():
        items.append(
            etree.tostring(child, encoding="utf-8").decode("utf-8")
        )
    return "".join(items)


def get_node_without_subtag(node, remove_extra_spaces=False):
    """
        Função que retorna nó sem subtags. 
    """
    if remove_extra_spaces:
        return " ".join([text.strip()for text in node.xpath(".//text()") if text.strip()])
    return "".join(node.xpath(".//text()"))


def get_year_month_day(node):
    """
    Retorna os valores respectivos dos elementos "year", "month", "day".

    Parameters
    ----------
    node : lxml.etree.Element
        Elemento do tipo _date_, que tem os elementos "year", "month", "day".

    Returns
    -------
    tuple of strings
        ("YYYY", "MM", "DD")
    None se node is None

    """
    if node is not None:
        return tuple(
            [(node.findtext(item) or "").zfill(2)
             for item in ["year", "month", "day"]]
        )


def create_alternatives(node, assets_data):
    """
    ```xml
    <alternatives>
        <graphic
            xlink:href="https://minio.scielo.br/documentstore/1678-2674/
            rQRTPbt6jkrncZTsPdCyXsn/
            6d6b2cfaa2dc5bd1fb84644218506cbfbc4dfb1e.tif"/>
        <graphic
            xlink:href="https://minio.scielo.br/documentstore/1678-2674/
            rQRTPbt6jkrncZTsPdCyXsn/
            b810735a45beb5f829d4eb07e4cf68842f57313f.png"
            specific-use="scielo-web"/>
        <graphic
            xlink:href="https://minio.scielo.br/documentstore/1678-2674/
            rQRTPbt6jkrncZTsPdCyXsn/
            e9d0cd6430c85a125e7490629ce43f227d00ef5e.jpg"
            specific-use="scielo-web"
            content-type="scielo-267x140"/>
    </alternatives>
    ```
    """
    if node is None or not assets_data:
        return
    parent = node.getparent()
    if parent is None:
        return
    if len(assets_data) == 1:
        for extension, uri in assets_data.items():
            node.set("{http://www.w3.org/1999/xlink}href", uri)
            if extension in [".tif", ".tiff"]:
                pass
            elif extension in [".png"]:
                node.set("specific-use", "scielo-web")
            else:
                node.set("specific-use", "scielo-web")
                node.set("content-type", "scielo-267x140")
    else:
        alternative_node = etree.Element("alternatives")
        for extension, uri in assets_data.items():
            _node = etree.Element("graphic")
            _node.set("{http://www.w3.org/1999/xlink}href", uri)
            alternative_node.append(_node)
            if extension in [".tif", ".tiff"]:
                pass
            elif extension in [".png"]:
                _node.set("specific-use", "scielo-web")
            else:
                _node.set("specific-use", "scielo-web")
                _node.set("content-type", "scielo-267x140")
        parent.replace(node, alternative_node)


def parse_value(value):
    value = value.lower()
    if value.isdigit():
        return value.zfill(2)
    if "spe" in value:
        return "spe"
    if "sup" in value:
        return "s"
    return value


def parse_issue(issue):
    issue = " ".join([item for item in issue.split()])
    parts = issue.split()
    parts = [parse_value(item) for item in parts]
    s = "-".join(parts)
    s = s.replace("spe-", "spe")
    s = s.replace("s-", "s")
    if s.endswith("s"):
        s += "0"
    return s


def is_allowed_to_update(xml_sps, attr_name, attr_new_value):
    """
    Se há uma função de validação associada com o atributo,
    verificar se é permitido atualizar o atributo, dados seus valores
    atual e/ou novo
    """
    validate_function = validations.VALIDATE_FUNCTIONS.get(attr_name)
    if validate_function is None:
        # não há nenhuma validação, então é permitido fazer a atualização
        return True

    curr_value = getattr(xml_sps, attr_name)

    if attr_new_value == curr_value:
        # desnecessario atualizar
        return False

    try:
        # valida o valor atual do atributo
        validate_function(curr_value)

    except (ValueError, exceptions.InvalidValueForOrderError):
        # o valor atual do atributo é inválido,
        # então continuar para verificar o valor "novo"
        pass

    else:
        # o valor atual do atributo é válido,
        # então não permitir atualização
        raise exceptions.NotAllowedtoChangeAttributeValueError(
            "Not allowed to update %s (%s) with %s, "
            "because current is valid" %
            (attr_name, curr_value, attr_new_value))

    try:
        # valida o valor novo para o atributo
        validate_function(attr_new_value)

    except (ValueError, exceptions.InvalidValueForOrderError):
        # o valor novo é inválido, então não permitir atualização
        raise exceptions.InvalidAttributeValueError(
            "Not allowed to update %s (%s) with %s, "
            "because new value is invalid" %
            (attr_name, curr_value, attr_new_value))

    else:
        # o valor novo é válido, então não permitir atualização
        return True


def match_pubdate(node, pubdate_xpaths):
    """
    Retorna o primeiro match da lista de pubdate_xpaths
    """
    for xpath in pubdate_xpaths:
        pubdate = node.find(xpath)
        if pubdate is not None:
            return pubdate
if __name__ == "__main__":
    isT=True
    data = """<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en"></article>"""
    xmltree = get_xml_tree(data)

    expected = 'en'
    obtained = ArticleAndSubArticles(xmltree).main_lang
    ist1=expected== obtained
    data = open('/home/travis/builds/repos/scieloorg---packtools/tests/samples/article-abstract-en-sub-articles-pt-es.xml',encoding="utf-8").read()
    xmltree = get_xml_tree(data)

    expected = ['en', 'pt', 'es']
    obtained = [d['lang'] for d in ArticleAndSubArticles(xmltree).data]

    ist2=expected== obtained
    #

    # print(fix_namespace_prefix_w("fdsjkfsdkjf:r2ry2ruewf:dfshdkjf-sjdfs"))
    # # if not isT:
    # #     raise Exception("Result not True!!!")
    # print(fix_namespace_prefix_w(r'&&&.w:aaaaa="&wow&.w:a="'))
    ist3='&&&.w-aaaaa="&wow&.w-a="'==fix_namespace_prefix_w(r'&&&.w:aaaaa="&wow&.w:a="')
    # print(ist3)
    if not ist1 or not ist2 or not ist3:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/scieloorg---packtools/packtools/sps/utils/xml_utils_match_pubdate_passk_validte.py
import logging
import re
import sys
sys.path.append("/home/travis/builds/repos/scieloorg---packtools/")
from copy import deepcopy
from lxml import etree
from packtools import validations
from packtools.sps import exceptions
from packtools import file_utils


logger = logging.getLogger(__name__)


def get_nodes_with_lang(xmltree, lang_xpath, node_xpath=None):
    _items = []
    for node in xmltree.xpath(lang_xpath):
        _item = {}
        if node_xpath:
            _item["node"] = node.find(node_xpath)
        else:
            _item["node"] = node
        _item["lang"] = node.get('{http://www.w3.org/XML/1998/namespace}lang')
        _items.append(_item)
    return _items


def node_plain_text(node):
    """
    Função que retorna texto de nó, sem subtags e com espaços padronizados

    Entrada:
    ```xml
    <node>
        <italic>Duguetia leucotricha</italic> (Annonaceae)<xref>1</xref>
    </node>
    ```

    Saída:
    Duguetia leucotricha (Annonaceae)
    """
    if node is None:
        return
    for xref in node.findall(".//xref"):
        for child in xref.findall(".//*"):
            child.text = ""
        xref.text = ""
    text = "".join([text for text in node.xpath(".//text()") if text.strip()])
    return " ".join(text.split())


def node_text_without_xref(node):
    """
    Retorna text com subtags, exceto `xref`
    """
    if node is None:
        return

    node = deepcopy(node)

    for xref in node.findall(".//xref"):
        if xref.tail:
            _next = xref.getnext()
            if _next is None or _next.tag != "xref":
                e = etree.Element("EMPTYTAGTOKEEPXREFTAIL")
                xref.addnext(e)
    for xref in node.findall(".//xref"):
        parent = xref.getparent()
        parent.remove(xref)
    etree.strip_tags(node, "EMPTYTAGTOKEEPXREFTAIL")
    return node_text(node)


def formatted_text(title_node):
    # FIXME substituir `formatted_text` por `node_text_without_xref`
    # por ser mais explícito
    return node_text_without_xref(title_node)


def fix_xml(xml_str):
    return fix_namespace_prefix_w(xml_str)


def fix_namespace_prefix_w(content):
    """
    Convert os textos cujo padrão é `w:st="` em `w-st="`
    """
    pattern = r"\bw:[a-z]{1,}=\""
    found_items = re.findall(pattern, content)
    logger.debug("Found %i namespace prefix w", len(found_items))
    for item in set(found_items):
        new_namespace = item.replace(":", "-")
        logger.debug("%s -> %s" % (item, new_namespace))
        content = content.replace(item, new_namespace)
    return content


def _get_xml_content(xml):
    if isinstance(xml, str):
        try:
            content = file_utils.read_file(xml)
        except (FileNotFoundError, OSError):
            content = xml
        content = fix_xml(content)
        return content.encode("utf-8")
    return xml


def get_xml_tree(content):
    parser = etree.XMLParser(remove_blank_text=True, no_network=True)
    try:
        content = _get_xml_content(content)
        xml_tree = etree.XML(content, parser)
    except etree.XMLSyntaxError as exc:
        raise exceptions.SPSLoadToXMLError(str(exc)) from None
    else:
        return xml_tree


def tostring(node, doctype=None, pretty_print=False):
    return etree.tostring(
        node,
        doctype=doctype,
        xml_declaration=True,
        method="xml",
        encoding="utf-8",
        pretty_print=pretty_print,
    ).decode("utf-8")


def node_text(node):
    """
    Retorna todos os node.text, incluindo a subtags
    Para <title>Text <bold>text</bold> Text</title>, retorna
    Text <bold>text</bold> Text
    """
    items = [node.text or ""]
    for child in node.getchildren():
        items.append(
            etree.tostring(child, encoding="utf-8").decode("utf-8")
        )
    return "".join(items)


def get_node_without_subtag(node, remove_extra_spaces=False):
    """
        Função que retorna nó sem subtags. 
    """
    if remove_extra_spaces:
        return " ".join([text.strip()for text in node.xpath(".//text()") if text.strip()])
    return "".join(node.xpath(".//text()"))


def get_year_month_day(node):
    """
    Retorna os valores respectivos dos elementos "year", "month", "day".

    Parameters
    ----------
    node : lxml.etree.Element
        Elemento do tipo _date_, que tem os elementos "year", "month", "day".

    Returns
    -------
    tuple of strings
        ("YYYY", "MM", "DD")
    None se node is None

    """
    if node is not None:
        return tuple(
            [(node.findtext(item) or "").zfill(2)
             for item in ["year", "month", "day"]]
        )


def create_alternatives(node, assets_data):
    """
    ```xml
    <alternatives>
        <graphic
            xlink:href="https://minio.scielo.br/documentstore/1678-2674/
            rQRTPbt6jkrncZTsPdCyXsn/
            6d6b2cfaa2dc5bd1fb84644218506cbfbc4dfb1e.tif"/>
        <graphic
            xlink:href="https://minio.scielo.br/documentstore/1678-2674/
            rQRTPbt6jkrncZTsPdCyXsn/
            b810735a45beb5f829d4eb07e4cf68842f57313f.png"
            specific-use="scielo-web"/>
        <graphic
            xlink:href="https://minio.scielo.br/documentstore/1678-2674/
            rQRTPbt6jkrncZTsPdCyXsn/
            e9d0cd6430c85a125e7490629ce43f227d00ef5e.jpg"
            specific-use="scielo-web"
            content-type="scielo-267x140"/>
    </alternatives>
    ```
    """
    if node is None or not assets_data:
        return
    parent = node.getparent()
    if parent is None:
        return
    if len(assets_data) == 1:
        for extension, uri in assets_data.items():
            node.set("{http://www.w3.org/1999/xlink}href", uri)
            if extension in [".tif", ".tiff"]:
                pass
            elif extension in [".png"]:
                node.set("specific-use", "scielo-web")
            else:
                node.set("specific-use", "scielo-web")
                node.set("content-type", "scielo-267x140")
    else:
        alternative_node = etree.Element("alternatives")
        for extension, uri in assets_data.items():
            _node = etree.Element("graphic")
            _node.set("{http://www.w3.org/1999/xlink}href", uri)
            alternative_node.append(_node)
            if extension in [".tif", ".tiff"]:
                pass
            elif extension in [".png"]:
                _node.set("specific-use", "scielo-web")
            else:
                _node.set("specific-use", "scielo-web")
                _node.set("content-type", "scielo-267x140")
        parent.replace(node, alternative_node)


def parse_value(value):
    value = value.lower()
    if value.isdigit():
        return value.zfill(2)
    if "spe" in value:
        return "spe"
    if "sup" in value:
        return "s"
    return value


def parse_issue(issue):
    issue = " ".join([item for item in issue.split()])
    parts = issue.split()
    parts = [parse_value(item) for item in parts]
    s = "-".join(parts)
    s = s.replace("spe-", "spe")
    s = s.replace("s-", "s")
    if s.endswith("s"):
        s += "0"
    return s


def is_allowed_to_update(xml_sps, attr_name, attr_new_value):
    """
    Se há uma função de validação associada com o atributo,
    verificar se é permitido atualizar o atributo, dados seus valores
    atual e/ou novo
    """
    validate_function = validations.VALIDATE_FUNCTIONS.get(attr_name)
    if validate_function is None:
        # não há nenhuma validação, então é permitido fazer a atualização
        return True

    curr_value = getattr(xml_sps, attr_name)

    if attr_new_value == curr_value:
        # desnecessario atualizar
        return False

    try:
        # valida o valor atual do atributo
        validate_function(curr_value)

    except (ValueError, exceptions.InvalidValueForOrderError):
        # o valor atual do atributo é inválido,
        # então continuar para verificar o valor "novo"
        pass

    else:
        # o valor atual do atributo é válido,
        # então não permitir atualização
        raise exceptions.NotAllowedtoChangeAttributeValueError(
            "Not allowed to update %s (%s) with %s, "
            "because current is valid" %
            (attr_name, curr_value, attr_new_value))

    try:
        # valida o valor novo para o atributo
        validate_function(attr_new_value)

    except (ValueError, exceptions.InvalidValueForOrderError):
        # o valor novo é inválido, então não permitir atualização
        raise exceptions.InvalidAttributeValueError(
            "Not allowed to update %s (%s) with %s, "
            "because new value is invalid" %
            (attr_name, curr_value, attr_new_value))

    else:
        # o valor novo é válido, então não permitir atualização
        return True


def match_pubdate(node, pubdate_xpaths):
    """
    Retorna o primeiro match da lista de pubdate_xpaths
    """
    for xpath in pubdate_xpaths:
        pubdate = node.find(xpath)
        if pubdate is not None:
            return pubdate
from packtools.sps.models.sps_package import SPS_Package
if __name__ == "__main__":
    expected = ("1997", "01", "00")
    xml_sps = SPS_Package("/home/travis/builds/repos/scieloorg---packtools/tests/sps/fixtures/document.xml")
    result = xml_sps.document_pubdate
    isT=expected== result
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/scieloorg---packtools/data_passk_platform/62b463283879012d1948153d/"):
    #     f = open("/home/travis/builds/repos/scieloorg---packtools/data_passk_platform/62b463283879012d1948153d/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     res0 = match_pubdate(args0,args1)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/scieloorg---packtools/packtools/sps/models/front_articlemeta_issue__extract_number_and_supplment_from_issue_element_passk_validte.py
"""<article>
<front>
    <article-meta>
      <pub-date publication-format="electronic" date-type="collection">
        <year>2003</year>
      </pub-date>
      <volume>4</volume>
      <issue>1</issue>
      <fpage>108</fpage>
      <lpage>123</lpage>
    </article-meta>
  </front>
</article>
"""
import sys
sys.path.append("/home/travis/builds/repos/scieloorg---packtools/")
from packtools.sps.models.dates import ArticleDates
from packtools.sps.models.article_ids import ArticleIds


def _extract_number_and_supplment_from_issue_element(issue):
    """
    Extrai do conteúdo de <issue>xxxx</issue>, os valores number e suppl.
    Valores possíveis
    5 (suppl), 5 Suppl, 5 Suppl 1, 5 spe, 5 suppl, 5 suppl 1, 5 suppl. 1,
    25 Suppl 1, 2-5 suppl 1, 2spe, Spe, Supl. 1, Suppl, Suppl 12,
    s2, spe, spe 1, spe pr, spe2, spe.2, spepr, supp 1, supp5 1, suppl,
    suppl 1, suppl 5 pr, suppl 12, suppl 1-2, suppl. 1
    """
    if not issue:
        return None, None
    issue = issue.strip().replace(".", "")
    splitted = [s for s in issue.split() if s]

    splitted = ["spe"
                if "spe" in s.lower() and s.isalpha() else s
                for s in splitted
                ]
    if len(splitted) == 1:
        issue = splitted[0]
        if issue.isdigit():
            return issue, None
        if "sup" in issue.lower():
            # match como sup*
            return None, "0"
        if issue.startswith("s"):
            if issue[1:].isdigit():
                return None, issue[1:]
        # match com spe, 2-5, 3B
        return issue, None

    if len(splitted) == 2:
        if "sup" in splitted[0].lower():
            return None, splitted[1]
        if "sup" in splitted[1].lower():
            return splitted[0], "0"
        # match spe 4 -> spe4
        return "".join(splitted), None

    if len(splitted) == 3:
        if "sup" in splitted[1].lower():
            return splitted[0], splitted[2]
    # match ????
    return "".join(splitted), None


class ArticleMetaIssue:

    def __init__(self, xmltree):
        self.xmltree = xmltree

    @property
    def data(self):
        attr_names = (
            "volume", "number", "suppl",
            "fpage", "fpage_seq", "lpage",
            "elocation_id",
        )
        _data = {}
        for k in attr_names:
            try:
                value = getattr(self, k)
            except AttributeError:
                continue
            else:
                if value:
                    _data[k] = value
        try:
            _data["pub_year"] = self.collection_date["year"]
        except (KeyError, TypeError):
            pass
        return _data

    @property
    def collection_date(self):
        _date = ArticleDates(self.xmltree)
        return _date.collection_date

    @property
    def volume(self):
        return self.xmltree.findtext(".//front/article-meta/volume")

    @property
    def issue(self):
        return self.xmltree.findtext(".//front/article-meta/issue")

    @property
    def number(self):
        _issue = self.issue
        if _issue:
            n, s = _extract_number_and_supplment_from_issue_element(_issue)
            return n

    @property
    def suppl(self):
        _suppl = self.xmltree.findtext(".//front/article-meta/supplement")
        if _suppl:
            return _suppl
        _issue = self.issue
        if _issue:
            n, s = _extract_number_and_supplment_from_issue_element(_issue)
            return s

    @property
    def elocation_id(self):
        return self.xmltree.findtext(".//front/article-meta/elocation-id")

    @property
    def fpage(self):
        return self.xmltree.findtext(".//front/article-meta/fpage")

    @property
    def fpage_seq(self):
        try:
            return self.xmltree.xpath(".//front/article-meta/fpage")[0].get("seq")
        except IndexError:
            return None

    @property
    def lpage(self):
        return self.xmltree.findtext(".//front/article-meta/lpage")

    @property
    def order(self):
        _order = self.xmltree.findtext('.//article-id[@pub-id-type="other"]')
        if _order is None:
            _order = ArticleIds(self.xmltree).v2
        return int(_order)

if __name__ == "__main__":
    isT=True
    def t1():
        expected = "5", "0"
        result = _extract_number_and_supplment_from_issue_element("5 (suppl)")
        return expected==result


    def t2():
        expected = "5", "0"
        result = _extract_number_and_supplment_from_issue_element("5 Suppl")
        return expected== result


    def t3():
        expected = "5", "1"
        result = _extract_number_and_supplment_from_issue_element("5 Suppl 1")
        return expected== result


    def t4():
        expected = "5spe", None
        result = _extract_number_and_supplment_from_issue_element("5 spe")
        return expected== result


    def t5():
        expected = "5", "0"
        result = _extract_number_and_supplment_from_issue_element("5 suppl")
        return expected== result


    def t6():
        expected = "5", "1"
        result = _extract_number_and_supplment_from_issue_element("5 suppl 1")
        return expected== result


    def t7():
        expected = "5", "1"
        result = _extract_number_and_supplment_from_issue_element("5 suppl. 1")
        return expected== result


    def t8():
        expected = "25", "1"
        result = _extract_number_and_supplment_from_issue_element("25 Suppl 1")
        return expected== result


    def t9():
        expected = "2-5", "1"
        result = _extract_number_and_supplment_from_issue_element("2-5 suppl 1")
        return expected== result


    def t10():
        expected = "2spe", None
        result = _extract_number_and_supplment_from_issue_element("2spe")
        return expected== result


    def t11():
        expected = "spe", None
        result = _extract_number_and_supplment_from_issue_element("Spe")
        return expected== result


    def t12():
        expected = None, "1"
        result = _extract_number_and_supplment_from_issue_element("Supl. 1")
        return expected== result


    def t13():
        expected = None, "0"
        result = _extract_number_and_supplment_from_issue_element("Suppl")
        return expected== result


    def t14():
        expected = None, "12"
        result = _extract_number_and_supplment_from_issue_element("Suppl 12")
        return expected== result


    def t15():
        expected = None, "2"
        result = _extract_number_and_supplment_from_issue_element("s2")
        return expected== result


    def t16():
        expected = "spe", None
        result = _extract_number_and_supplment_from_issue_element("spe")
        return expected== result


    def t17():
        expected = "spe", None
        result = _extract_number_and_supplment_from_issue_element("Especial")
        return expected== result


    def t18():
        expected = "spe1", None
        result = _extract_number_and_supplment_from_issue_element("spe 1")
        return expected== result


    def t19():
        expected = "spepr", None
        result = _extract_number_and_supplment_from_issue_element("spe pr")
        return expected == result


    def t20():
        expected = "spe2", None
        result = _extract_number_and_supplment_from_issue_element("spe2")
        return expected== result


    def t21():
        expected = "spe2", None
        result = _extract_number_and_supplment_from_issue_element("spe.2")
        return expected== result


    def t22():
        expected = None, "1"
        result = _extract_number_and_supplment_from_issue_element("supp 1")
        return expected== result


    def t23():
        expected = None, "0"
        result = _extract_number_and_supplment_from_issue_element("suppl")
        return expected== result


    def t24():
        expected = None, "1"
        result = _extract_number_and_supplment_from_issue_element("suppl 1")
        return expected== result


    def t25():
        expected = None, "12"
        result = _extract_number_and_supplment_from_issue_element("suppl 12")
        return expected== result


    def t26():
        expected = None, "1-2"
        result = _extract_number_and_supplment_from_issue_element("suppl 1-2")
        return expected== result


    def t27():
        expected = None, "1"
        result = _extract_number_and_supplment_from_issue_element("suppl. 1")
        return expected== result


    if not t1() or not t2() or not t3() or not t4() or not t5() \
          or  not t6() or not t7() or not t8() or not t9() or not t10() \
               or     not t11() or not t12() or not t13() or not t14() or not t15()\
  or  not t16() or not t17() or not t18() or not t19() or not t20() \
            or not t21() or not t22() or not t23() or not t24() or not t25() or not t26() or not t27():
        isT=False

if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/bastikr---boolean/boolean/boolean_pretty_passk_validte.py
"""
Boolean expressions algebra.

This module defines a Boolean algebra over the set {TRUE, FALSE} with boolean
variables called Symbols and the boolean functions AND, OR, NOT.

Some basic logic comparison is supported: two expressions can be
compared for equivalence or containment. Furthermore you can simplify
an expression and obtain its normal form.

You can create expressions in Python using familiar boolean operators
or parse expressions from strings. The parsing can be extended with
your own tokenizer.  You can also customize how expressions behave and
how they are presented.

For extensive documentation look either into the docs directory or view it
online, at https://booleanpy.readthedocs.org/en/latest/.

Copyright (c) Sebastian Kraemer, basti.kr@gmail.com and others

SPDX-License-Identifier: BSD-2-Clause
"""

import inspect
import itertools
from functools import reduce  # NOQA
from operator import and_ as and_operator
from operator import or_ as or_operator

# Set to True to enable tracing for parsing
TRACE_PARSE = False

# Token types for standard operators and parens
TOKEN_AND = 1
TOKEN_OR = 2
TOKEN_NOT = 3
TOKEN_LPAR = 4
TOKEN_RPAR = 5
TOKEN_TRUE = 6
TOKEN_FALSE = 7
TOKEN_SYMBOL = 8

TOKEN_TYPES = {
    TOKEN_AND: "AND",
    TOKEN_OR: "OR",
    TOKEN_NOT: "NOT",
    TOKEN_LPAR: "(",
    TOKEN_RPAR: ")",
    TOKEN_TRUE: "TRUE",
    TOKEN_FALSE: "FALSE",
    TOKEN_SYMBOL: "SYMBOL",
}

# parsing error code and messages
PARSE_UNKNOWN_TOKEN = 1
PARSE_UNBALANCED_CLOSING_PARENS = 2
PARSE_INVALID_EXPRESSION = 3
PARSE_INVALID_NESTING = 4
PARSE_INVALID_SYMBOL_SEQUENCE = 5
PARSE_INVALID_OPERATOR_SEQUENCE = 6

PARSE_ERRORS = {
    PARSE_UNKNOWN_TOKEN: "Unknown token",
    PARSE_UNBALANCED_CLOSING_PARENS: "Unbalanced parenthesis",
    PARSE_INVALID_EXPRESSION: "Invalid expression",
    PARSE_INVALID_NESTING: "Invalid expression nesting such as (AND xx)",
    PARSE_INVALID_SYMBOL_SEQUENCE: "Invalid symbols sequence such as (A B)",
    PARSE_INVALID_OPERATOR_SEQUENCE: "Invalid operator sequence without symbols such as AND OR or OR OR",
}


class ParseError(Exception):
    """
    Raised when the parser or tokenizer encounters a syntax error. Instances of
    this class have attributes token_type, token_string, position, error_code to
    access the details of the error. str() of the exception instance returns a
    formatted message.
    """

    def __init__(self, token_type=None, token_string="", position=-1, error_code=0):
        self.token_type = token_type
        self.token_string = token_string
        self.position = position
        self.error_code = error_code

    def __str__(self, *args, **kwargs):
        emsg = PARSE_ERRORS.get(self.error_code, "Unknown parsing error")

        tstr = ""
        if self.token_string:
            tstr = f' for token: "{self.token_string}"'

        pos = ""
        if self.position > 0:
            pos = f" at position: {self.position}"

        return f"{emsg}{tstr}{pos}"


class BooleanAlgebra(object):
    """
    An algebra is defined by:

    - the types of its operations and Symbol.
    - the tokenizer used when parsing expressions from strings.

    This class also serves as a base class for all boolean expressions,
    including base elements, functions and variable symbols.
    """

    def __init__(
        self,
        TRUE_class=None,
        FALSE_class=None,
        Symbol_class=None,
        NOT_class=None,
        AND_class=None,
        OR_class=None,
        allowed_in_token=(".", ":", "_"),
    ):
        """
        The types for TRUE, FALSE, NOT, AND, OR and Symbol define the boolean
        algebra elements, operations and Symbol variable. They default to the
        standard classes if not provided.

        You can customize an algebra by providing alternative subclasses of the
        standard types.
        """
        # TRUE and FALSE base elements are algebra-level "singleton" instances
        self.TRUE = TRUE_class or _TRUE
        self.TRUE = self.TRUE()

        self.FALSE = FALSE_class or _FALSE
        self.FALSE = self.FALSE()

        # they cross-reference each other
        self.TRUE.dual = self.FALSE
        self.FALSE.dual = self.TRUE

        # boolean operation types, defaulting to the standard types
        self.NOT = NOT_class or NOT
        self.AND = AND_class or AND
        self.OR = OR_class or OR

        # class used for Symbols
        self.Symbol = Symbol_class or Symbol

        tf_nao = {
            "TRUE": self.TRUE,
            "FALSE": self.FALSE,
            "NOT": self.NOT,
            "AND": self.AND,
            "OR": self.OR,
            "Symbol": self.Symbol,
        }

        # setup cross references such that all algebra types and
        # objects hold a named attribute for every other types and
        # objects, including themselves.
        for obj in tf_nao.values():
            for name, value in tf_nao.items():
                setattr(obj, name, value)

        # Set the set of characters allowed in tokens
        self.allowed_in_token = allowed_in_token

    def definition(self):
        """
        Return a tuple of this algebra defined elements and types as:
        (TRUE, FALSE, NOT, AND, OR, Symbol)
        """
        return self.TRUE, self.FALSE, self.NOT, self.AND, self.OR, self.Symbol

    def symbols(self, *args):
        """
        Return a tuple of symbols building a new Symbol from each argument.
        """
        return tuple(map(self.Symbol, args))

    def parse(self, expr, simplify=False):
        """
        Return a boolean expression parsed from `expr` either a unicode string
        or tokens iterable.

        Optionally simplify the expression if `simplify` is True.

        Raise ParseError on errors.

        If `expr` is a string, the standard `tokenizer` is used for tokenization
        and the algebra configured Symbol type is used to create Symbol
        instances from Symbol tokens.

        If `expr` is an iterable, it should contain 3-tuples of: (token_type,
        token_string, token_position). In this case, the `token_type` can be
        a Symbol instance or one of the TOKEN_* constant types.
        See the `tokenize()` method for detailed specification.
        """

        precedence = {self.NOT: 5, self.AND: 10, self.OR: 15, TOKEN_LPAR: 20}

        if isinstance(expr, str):
            tokenized = self.tokenize(expr)
        else:
            tokenized = iter(expr)

        if TRACE_PARSE:
            tokenized = list(tokenized)
            print("tokens:")
            for t in tokenized:
                print(t)
            tokenized = iter(tokenized)

        # the abstract syntax tree for this expression that will be build as we
        # process tokens
        # the first two items are None
        # symbol items are appended to this structure
        ast = [None, None]

        def is_sym(_t):
            return isinstance(_t, Symbol) or _t in (TOKEN_TRUE, TOKEN_FALSE, TOKEN_SYMBOL)

        def is_operator(_t):
            return _t in (TOKEN_AND, TOKEN_OR)

        prev_token = None
        for token_type, token_string, token_position in tokenized:
            if TRACE_PARSE:
                print(
                    "\nprocessing token_type:",
                    repr(token_type),
                    "token_string:",
                    repr(token_string),
                    "token_position:",
                    repr(token_position),
                )

            if prev_token:
                prev_token_type, _prev_token_string, _prev_token_position = prev_token
                if TRACE_PARSE:
                    print("  prev_token:", repr(prev_token))

                if is_sym(prev_token_type) and (
                    is_sym(token_type)
                ):  # or token_type == TOKEN_LPAR) :
                    raise ParseError(
                        token_type, token_string, token_position, PARSE_INVALID_SYMBOL_SEQUENCE
                    )

                if is_operator(prev_token_type) and (
                    is_operator(token_type) or token_type == TOKEN_RPAR
                ):
                    raise ParseError(
                        token_type, token_string, token_position, PARSE_INVALID_OPERATOR_SEQUENCE
                    )

            else:
                if is_operator(token_type):
                    raise ParseError(
                        token_type, token_string, token_position, PARSE_INVALID_OPERATOR_SEQUENCE
                    )

            if token_type == TOKEN_SYMBOL:
                ast.append(self.Symbol(token_string))
                if TRACE_PARSE:
                    print(" ast: token_type is TOKEN_SYMBOL: append new symbol", repr(ast))

            elif isinstance(token_type, Symbol):
                ast.append(token_type)
                if TRACE_PARSE:
                    print(" ast: token_type is Symbol): append existing symbol", repr(ast))

            elif token_type == TOKEN_TRUE:
                ast.append(self.TRUE)
                if TRACE_PARSE:
                    print(" ast: token_type is TOKEN_TRUE:", repr(ast))

            elif token_type == TOKEN_FALSE:
                ast.append(self.FALSE)
                if TRACE_PARSE:
                    print(" ast: token_type is TOKEN_FALSE:", repr(ast))

            elif token_type == TOKEN_NOT:
                ast = [ast, self.NOT]
                if TRACE_PARSE:
                    print(" ast: token_type is TOKEN_NOT:", repr(ast))

            elif token_type == TOKEN_AND:
                ast = self._start_operation(ast, self.AND, precedence)
                if TRACE_PARSE:
                    print("  ast:token_type is TOKEN_AND: start_operation", ast)

            elif token_type == TOKEN_OR:
                ast = self._start_operation(ast, self.OR, precedence)
                if TRACE_PARSE:
                    print("  ast:token_type is TOKEN_OR: start_operation", ast)

            elif token_type == TOKEN_LPAR:
                if prev_token:
                    # Check that an opening parens is preceded by a function
                    # or an opening parens
                    if prev_token_type not in (TOKEN_NOT, TOKEN_AND, TOKEN_OR, TOKEN_LPAR):
                        raise ParseError(
                            token_type, token_string, token_position, PARSE_INVALID_NESTING
                        )
                ast = [ast, TOKEN_LPAR]

            elif token_type == TOKEN_RPAR:
                while True:
                    if ast[0] is None:
                        raise ParseError(
                            token_type,
                            token_string,
                            token_position,
                            PARSE_UNBALANCED_CLOSING_PARENS,
                        )

                    if ast[1] is TOKEN_LPAR:
                        ast[0].append(ast[2])
                        if TRACE_PARSE:
                            print("ast9:", repr(ast))
                        ast = ast[0]
                        if TRACE_PARSE:
                            print("ast10:", repr(ast))
                        break

                    if isinstance(ast[1], int):
                        raise ParseError(
                            token_type,
                            token_string,
                            token_position,
                            PARSE_UNBALANCED_CLOSING_PARENS,
                        )

                    # the parens are properly nested
                    # the top ast node should be a function subclass
                    if not (inspect.isclass(ast[1]) and issubclass(ast[1], Function)):
                        raise ParseError(
                            token_type, token_string, token_position, PARSE_INVALID_NESTING
                        )

                    subex = ast[1](*ast[2:])
                    ast[0].append(subex)
                    if TRACE_PARSE:
                        print("ast11:", repr(ast))
                    ast = ast[0]
                    if TRACE_PARSE:
                        print("ast12:", repr(ast))
            else:
                raise ParseError(token_type, token_string, token_position, PARSE_UNKNOWN_TOKEN)

            prev_token = (token_type, token_string, token_position)

        try:
            while True:
                if ast[0] is None:
                    if TRACE_PARSE:
                        print("ast[0] is None:", repr(ast))
                    if ast[1] is None:
                        if TRACE_PARSE:
                            print("  ast[1] is None:", repr(ast))
                        if len(ast) != 3:
                            raise ParseError(error_code=PARSE_INVALID_EXPRESSION)
                        parsed = ast[2]
                        if TRACE_PARSE:
                            print("    parsed = ast[2]:", repr(parsed))

                    else:
                        # call the function in ast[1] with the rest of the ast as args
                        parsed = ast[1](*ast[2:])
                        if TRACE_PARSE:
                            print("  parsed = ast[1](*ast[2:]):", repr(parsed))
                    break
                else:
                    if TRACE_PARSE:
                        print("subex = ast[1](*ast[2:]):", repr(ast))
                    subex = ast[1](*ast[2:])
                    ast[0].append(subex)
                    if TRACE_PARSE:
                        print("  ast[0].append(subex):", repr(ast))
                    ast = ast[0]
                    if TRACE_PARSE:
                        print("    ast = ast[0]:", repr(ast))
        except TypeError:
            raise ParseError(error_code=PARSE_INVALID_EXPRESSION)

        if simplify:
            return parsed.simplify()

        if TRACE_PARSE:
            print("final parsed:", repr(parsed))
        return parsed

    def _start_operation(self, ast, operation, precedence):
        """
        Return an AST where all operations of lower precedence are finalized.
        """
        if TRACE_PARSE:
            print("   start_operation:", repr(operation), "AST:", ast)

        op_prec = precedence[operation]
        while True:
            if ast[1] is None:
                # [None, None, x]
                if TRACE_PARSE:
                    print("     start_op: ast[1] is None:", repr(ast))
                ast[1] = operation
                if TRACE_PARSE:
                    print("     --> start_op: ast[1] is None:", repr(ast))
                return ast

            prec = precedence[ast[1]]
            if prec > op_prec:  # op=&, [ast, |, x, y] -> [[ast, |, x], &, y]
                if TRACE_PARSE:
                    print("     start_op: prec > op_prec:", repr(ast))
                ast = [ast, operation, ast.pop(-1)]
                if TRACE_PARSE:
                    print("     --> start_op: prec > op_prec:", repr(ast))
                return ast

            if prec == op_prec:  # op=&, [ast, &, x] -> [ast, &, x]
                if TRACE_PARSE:
                    print("     start_op: prec == op_prec:", repr(ast))
                return ast

            if not (inspect.isclass(ast[1]) and issubclass(ast[1], Function)):
                # the top ast node should be a function subclass at this stage
                raise ParseError(error_code=PARSE_INVALID_NESTING)

            if ast[0] is None:  # op=|, [None, &, x, y] -> [None, |, x&y]
                if TRACE_PARSE:
                    print("     start_op: ast[0] is None:", repr(ast))
                subexp = ast[1](*ast[2:])
                new_ast = [ast[0], operation, subexp]
                if TRACE_PARSE:
                    print("     --> start_op: ast[0] is None:", repr(new_ast))
                return new_ast

            else:  # op=|, [[ast, &, x], ~, y] -> [ast, &, x, ~y]
                if TRACE_PARSE:
                    print("     start_op: else:", repr(ast))
                ast[0].append(ast[1](*ast[2:]))
                ast = ast[0]
                if TRACE_PARSE:
                    print("     --> start_op: else:", repr(ast))

    def tokenize(self, expr):
        """
        Return an iterable of 3-tuple describing each token given an expression
        unicode string.

        This 3-tuple contains (token, token string, position):

        - token: either a Symbol instance or one of TOKEN_* token types.
        - token string: the original token unicode string.
        - position: some simple object describing the starting position of the
          original token string in the `expr` string. It can be an int for a
          character offset, or a tuple of starting (row/line, column).

        The token position is used only for error reporting and can be None or
        empty.

        Raise ParseError on errors. The ParseError.args is a tuple of:
        (token_string, position, error message)

        You can use this tokenizer as a base to create specialized tokenizers
        for your custom algebra by subclassing BooleanAlgebra. See also the
        tests for other examples of alternative tokenizers.

        This tokenizer has these characteristics:

        - The `expr` string can span multiple lines,
        - Whitespace is not significant.
        - The returned position is the starting character offset of a token.
        - A TOKEN_SYMBOL is returned for valid identifiers which is a string
          without spaces.

            - These are valid identifiers:
                - Python identifiers.
                - a string even if starting with digits
                - digits (except for 0 and 1).
                - dotted names : foo.bar consist of one token.
                - names with colons: foo:bar consist of one token.
            
            - These are not identifiers:
                - quoted strings.
                - any punctuation which is not an operation

        - Recognized operators are (in any upper/lower case combinations):

            - for and:  '*', '&', 'and'
            - for or: '+', '|', 'or'
            - for not: '~', '!', 'not'

        - Recognized special symbols are (in any upper/lower case combinations):

            - True symbols: 1 and True
            - False symbols: 0, False and None
        """
        if not isinstance(expr, str):
            raise TypeError(f"expr must be string but it is {type(expr)}.")

        # mapping of lowercase token strings to a token type id for the standard
        # operators, parens and common true or false symbols, as used in the
        # default tokenizer implementation.
        TOKENS = {
            "*": TOKEN_AND,
            "&": TOKEN_AND,
            "and": TOKEN_AND,
            "+": TOKEN_OR,
            "|": TOKEN_OR,
            "or": TOKEN_OR,
            "~": TOKEN_NOT,
            "!": TOKEN_NOT,
            "not": TOKEN_NOT,
            "(": TOKEN_LPAR,
            ")": TOKEN_RPAR,
            "[": TOKEN_LPAR,
            "]": TOKEN_RPAR,
            "true": TOKEN_TRUE,
            "1": TOKEN_TRUE,
            "false": TOKEN_FALSE,
            "0": TOKEN_FALSE,
            "none": TOKEN_FALSE,
        }

        position = 0
        length = len(expr)

        while position < length:
            tok = expr[position]

            sym = tok.isalnum() or tok == "_"
            if sym:
                position += 1
                while position < length:
                    char = expr[position]
                    if char.isalnum() or char in self.allowed_in_token:
                        position += 1
                        tok += char
                    else:
                        break
                position -= 1

            try:
                yield TOKENS[tok.lower()], tok, position
            except KeyError:
                if sym:
                    yield TOKEN_SYMBOL, tok, position
                elif tok not in (" ", "\t", "\r", "\n"):
                    raise ParseError(
                        token_string=tok, position=position, error_code=PARSE_UNKNOWN_TOKEN
                    )

            position += 1

    def _recurse_distributive(self, expr, operation_inst):
        """
        Recursively flatten, simplify and apply the distributive laws to the
        `expr` expression. Distributivity is considered for the AND or OR
        `operation_inst` instance.
        """
        if expr.isliteral:
            return expr

        args = (self._recurse_distributive(arg, operation_inst) for arg in expr.args)
        args = tuple(arg.simplify() for arg in args)
        if len(args) == 1:
            return args[0]

        flattened_expr = expr.__class__(*args)

        dualoperation = operation_inst.dual
        if isinstance(flattened_expr, dualoperation):
            flattened_expr = flattened_expr.distributive()
        return flattened_expr

    def normalize(self, expr, operation):
        """
        Return a normalized expression transformed to its normal form in the
        given AND or OR operation.

        The new expression arguments will satisfy these conditions:
    
        - ``operation(*args) == expr`` (here mathematical equality is meant)
        - the operation does not occur in any of its arg.
        - NOT is only appearing in literals (aka. Negation normal form).

        The operation must be an AND or OR operation or a subclass.
        """
        # Ensure that the operation is not NOT
        assert operation in (
            self.AND,
            self.OR,
        )
        # Move NOT inwards.
        expr = expr.literalize()
        # Simplify first otherwise _recurse_distributive() may take forever.
        expr = expr.simplify()
        operation_example = operation(self.TRUE, self.FALSE)

        # For large dual operations build up from normalized subexpressions,
        # otherwise we can get exponential blowup midway through
        expr.args = tuple(self.normalize(a, operation) for a in expr.args)
        if len(expr.args) > 1 and (
            (operation == self.AND and isinstance(expr, self.OR))
            or (operation == self.OR and isinstance(expr, self.AND))
        ):
            args = expr.args
            expr_class = expr.__class__
            expr = args[0]
            for arg in args[1:]:
                expr = expr_class(expr, arg)
                expr = self._recurse_distributive(expr, operation_example)
                # Canonicalize
                expr = expr.simplify()

        else:
            expr = self._recurse_distributive(expr, operation_example)
            # Canonicalize
            expr = expr.simplify()

        return expr

    def cnf(self, expr):
        """
        Return a conjunctive normal form of the `expr` expression.
        """
        return self.normalize(expr, self.AND)

    conjunctive_normal_form = cnf

    def dnf(self, expr):
        """
        Return a disjunctive normal form of the `expr` expression.
        """
        return self.normalize(expr, self.OR)

    disjunctive_normal_form = dnf


class Expression(object):
    """
    Abstract base class for all boolean expressions, including functions and
    variable symbols.
    """

    # these class attributes are configured when a new BooleanAlgebra is created
    TRUE = None
    FALSE = None
    NOT = None
    AND = None
    OR = None
    Symbol = None

    def __init__(self):
        # Defines sort and comparison order between expressions arguments
        self.sort_order = None

        # Store arguments aka. subterms of this expressions.
        # subterms are either literals or expressions.
        self.args = tuple()

        # True is this is a literal expression such as a Symbol, TRUE or FALSE
        self.isliteral = False

        # True if this expression has been simplified to in canonical form.
        self.iscanonical = False

    @property
    def objects(self):
        """
        Return a set of all associated objects with this expression symbols.
        Include recursively subexpressions objects.
        """
        return set(s.obj for s in self.symbols)

    def get_literals(self):
        """
        Return a list of all the literals contained in this expression.
        Include recursively subexpressions symbols.
        This includes duplicates.
        """
        if self.isliteral:
            return [self]
        if not self.args:
            return []
        return list(itertools.chain.from_iterable(arg.get_literals() for arg in self.args))

    @property
    def literals(self):
        """
        Return a set of all literals contained in this expression.
        Include recursively subexpressions literals.
        """
        return set(self.get_literals())

    def literalize(self):
        """
        Return an expression where NOTs are only occurring as literals.
        Applied recursively to subexpressions.
        """
        if self.isliteral:
            return self
        args = tuple(arg.literalize() for arg in self.args)
        if all(arg is self.args[i] for i, arg in enumerate(args)):
            return self

        return self.__class__(*args)

    def get_symbols(self):
        """
        Return a list of all the symbols contained in this expression.
        Include subexpressions symbols recursively.
        This includes duplicates.
        """
        return [s if isinstance(s, Symbol) else s.args[0] for s in self.get_literals()]

    @property
    def symbols(
        self,
    ):
        """
        Return a list of all the symbols contained in this expression.
        Include subexpressions symbols recursively.
        This includes duplicates.
        """
        return set(self.get_symbols())

    def subs(self, substitutions, default=None, simplify=False):
        """
        Return an expression where all subterms of this expression are
        by the new expression using a `substitutions` mapping of:
        {expr: replacement}

        Return the provided `default` value if this expression has no elements,
        e.g. is empty.

        Simplify the results if `simplify` is True.

        Return this expression unmodified if nothing could be substituted. Note
        that a possible usage of this function is to check for expression
        containment as the expression will be returned unmodified if if does not
        contain any of the provided substitutions.
        """
        # shortcut: check if we have our whole expression as a possible
        # subsitution source
        for expr, substitution in substitutions.items():
            if expr == self:
                return substitution

        # otherwise, do a proper substitution of subexpressions
        expr = self._subs(substitutions, default, simplify)
        return self if expr is None else expr

    def _subs(self, substitutions, default, simplify):
        """
        Return an expression where all subterms are substituted by the new
        expression using a `substitutions` mapping of: {expr: replacement}
        """
        # track the new list of unchanged args or replaced args through
        # a substitution
        new_arguments = []
        changed_something = False

        # shortcut for basic logic True or False
        if self is self.TRUE or self is self.FALSE:
            return self

        # if the expression has no elements, e.g. is empty, do not apply
        # substitutions
        if not self.args:
            return default

        # iterate the subexpressions: either plain symbols or a subexpressions
        for arg in self.args:
            # collect substitutions for exact matches
            # break as soon as we have a match
            for expr, substitution in substitutions.items():
                if arg == expr:
                    new_arguments.append(substitution)
                    changed_something = True
                    break

            # this will execute only if we did not break out of the
            # loop, e.g. if we did not change anything and did not
            # collect any substitutions
            else:
                # recursively call _subs on each arg to see if we get a
                # substituted arg
                new_arg = arg._subs(substitutions, default, simplify)
                if new_arg is None:
                    # if we did not collect a substitution for this arg,
                    # keep the arg as-is, it is not replaced by anything
                    new_arguments.append(arg)
                else:
                    # otherwise, we add the substitution for this arg instead
                    new_arguments.append(new_arg)
                    changed_something = True

        if not changed_something:
            return

        # here we did some substitution: we return a new expression
        # built from the new_arguments
        newexpr = self.__class__(*new_arguments)
        return newexpr.simplify() if simplify else newexpr

    def simplify(self):
        """
        Return a new simplified expression in canonical form built from this
        expression. The simplified expression may be exactly the same as this
        expression.

        Subclasses override this method to compute actual simplification.
        """
        return self

    def __hash__(self):
        """
        Expressions are immutable and hashable. The hash of Functions is
        computed by respecting the structure of the whole expression by mixing
        the class name hash and the recursive hash of a frozenset of arguments.
        Hash of elements is based on their boolean equivalent. Hash of symbols
        is based on their object.
        """
        if not self.args:
            arghash = id(self)
        else:
            arghash = hash(frozenset(map(hash, self.args)))
        return hash(self.__class__.__name__) ^ arghash

    def __eq__(self, other):
        """
        Test if other element is structurally the same as itself.

        This method does not make any simplification or transformation, so it
        will return False although the expression terms may be mathematically
        equal. Use simplify() before testing equality to check the mathematical
        equality.

        For literals, plain equality is used.

        For functions, equality uses the facts that operations are:

        - commutative: order does not matter and different orders are equal.
        - idempotent: so args can appear more often in one term than in the other.
        """
        if self is other:
            return True

        if isinstance(other, self.__class__):
            return frozenset(self.args) == frozenset(other.args)

        return NotImplemented

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if self.sort_order is not None and other.sort_order is not None:
            if self.sort_order == other.sort_order:
                return NotImplemented
            return self.sort_order < other.sort_order
        return NotImplemented

    def __gt__(self, other):
        lt = other.__lt__(self)
        if lt is NotImplemented:
            return not self.__lt__(other)
        return lt

    def __and__(self, other):
        return self.AND(self, other)

    __mul__ = __and__

    def __invert__(self):
        return self.NOT(self)

    def __or__(self, other):
        return self.OR(self, other)

    __add__ = __or__

    def __bool__(self):
        raise TypeError("Cannot evaluate expression as a Python Boolean.")

    __nonzero__ = __bool__


class BaseElement(Expression):
    """
    Abstract base class for the base elements TRUE and FALSE of the boolean
    algebra.
    """

    def __init__(self):
        super(BaseElement, self).__init__()
        self.sort_order = 0
        self.iscanonical = True
        # The dual Base Element class for this element: TRUE.dual returns
        # _FALSE() and FALSE.dual returns _TRUE(). This is a cyclic reference
        # and therefore only assigned after creation of the singletons,
        self.dual = None

    def __lt__(self, other):
        if isinstance(other, BaseElement):
            return self == self.FALSE
        return NotImplemented

    __nonzero__ = __bool__ = lambda s: None

    def pretty(self, indent=0, debug=False):
        """
        Return a pretty formatted representation of self.
        """
        return (" " * indent) + repr(self)


class _TRUE(BaseElement):
    """
    Boolean base element TRUE.
    Not meant to be subclassed nor instantiated directly.
    """

    def __init__(self):
        super(_TRUE, self).__init__()
        # assigned at singleton creation: self.dual = FALSE

    def __hash__(self):
        return hash(True)

    def __eq__(self, other):
        return self is other or other is True or isinstance(other, _TRUE)

    def __str__(self):
        return "1"

    def __repr__(self):
        return "TRUE"

    def __call__(self):
        return self

    __nonzero__ = __bool__ = lambda s: True


class _FALSE(BaseElement):
    """
    Boolean base element FALSE.
    Not meant to be subclassed nor instantiated directly.
    """

    def __init__(self):
        super(_FALSE, self).__init__()
        # assigned at singleton creation: self.dual = TRUE

    def __hash__(self):
        return hash(False)

    def __eq__(self, other):
        return self is other or other is False or isinstance(other, _FALSE)

    def __str__(self):
        return "0"

    def __repr__(self):
        return "FALSE"

    def __call__(self):
        return self

    __nonzero__ = __bool__ = lambda s: False


class Symbol(Expression):
    """
    Boolean variable.

    A Symbol can hold an object used to determine equality between symbols.
    """

    def __init__(self, obj):
        super(Symbol, self).__init__()
        self.sort_order = 5
        # Store an associated object. This object determines equality
        self.obj = obj
        self.iscanonical = True
        self.isliteral = True

    def __call__(self, **kwargs):
        """
        Return the evaluated value for this symbol from kwargs
        """
        return kwargs[self.obj]

    def __hash__(self):
        if self.obj is None:  # Anonymous Symbol.
            return id(self)
        return hash(self.obj)

    def __eq__(self, other):
        if self is other:
            return True
        if isinstance(other, self.__class__):
            return self.obj == other.obj
        return NotImplemented

    def __lt__(self, other):
        comparator = Expression.__lt__(self, other)
        if comparator is not NotImplemented:
            return comparator
        if isinstance(other, Symbol):
            return self.obj < other.obj
        return NotImplemented

    def __str__(self):
        return str(self.obj)

    def __repr__(self):
        obj = f"'{self.obj}'" if isinstance(self.obj, str) else repr(self.obj)
        return f"{self.__class__.__name__}({obj})"

    def pretty(self, indent=0, debug=False):
        """
        Return a pretty formatted representation of self.
        """
        debug_details = ""
        if debug:
            debug_details += f"<isliteral={self.isliteral!r}, iscanonical={self.iscanonical!r}>"

        obj = f"'{self.obj}'" if isinstance(self.obj, str) else repr(self.obj)
        return (" " * indent) + f"{self.__class__.__name__}({debug_details}{obj})"


class Function(Expression):
    """
    Boolean function.

    A boolean function takes n (one or more) boolean expressions as arguments
    where n is called the order of the function and maps them to one of the base
    elements TRUE or FALSE. Implemented functions are AND, OR and NOT.
    """

    def __init__(self, *args):
        super(Function, self).__init__()

        # Specifies an infix notation of an operator for printing such as | or &.
        self.operator = None

        assert all(
            isinstance(arg, Expression) for arg in args
        ), f"Bad arguments: all arguments must be an Expression: {args!r}"
        self.args = tuple(args)

    def __str__(self):
        args = self.args
        if len(args) == 1:
            if self.isliteral:
                return f"{self.operator}{args[0]}"
            return f"{self.operator}({args[0]})"

        args_str = []
        for arg in args:
            if arg.isliteral:
                args_str.append(str(arg))
            else:
                args_str.append(f"({arg})")

        return self.operator.join(args_str)

    def __repr__(self):
        args = ", ".join(map(repr, self.args))
        return f"{self.__class__.__name__}({args})"

    def pretty(self, indent=0, debug=False):
        """
        Return a pretty formatted representation of self as an indented tree.

        If debug is True, also prints debug information for each expression arg.

        For example:

        >>> print(BooleanAlgebra().parse(
        ...    u'not a and not b and not (a and ba and c) and c or c').pretty())
        OR(
          AND(
            NOT(Symbol('a')),
            NOT(Symbol('b')),
            NOT(
              AND(
                Symbol('a'),
                Symbol('ba'),
                Symbol('c')
              )
            ),
            Symbol('c')
          ),
          Symbol('c')
        )
        """
        debug_details = ""
        if debug:
            debug_details += f"<isliteral={self.isliteral!r}, iscanonical={self.iscanonical!r}"
            identity = getattr(self, "identity", None)
            if identity is not None:
                debug_details += f", identity={identity!r}"

            annihilator = getattr(self, "annihilator", None)
            if annihilator is not None:
                debug_details += f", annihilator={annihilator!r}"

            dual = getattr(self, "dual", None)
            if dual is not None:
                debug_details += f", dual={dual!r}"
            debug_details += ">"
        cls = self.__class__.__name__
        args = [a.pretty(indent=indent + 2, debug=debug) for a in self.args]
        pfargs = ",\n".join(args)
        cur_indent = " " * indent
        new_line = "" if self.isliteral else "\n"
        return f"{cur_indent}{cls}({debug_details}{new_line}{pfargs}\n{cur_indent})"


class NOT(Function):
    """
    Boolean NOT operation.

    The NOT operation takes exactly one argument. If this argument is a Symbol
    the resulting expression is also called a literal.

    The operator "~" can be used as abbreviation for NOT, e.g. instead of NOT(x)
    one can write ~x (where x is some boolean expression). Also for printing "~"
    is used for better readability.

    You can subclass to define alternative string representation.

    For example:

    >>> class NOT2(NOT):
    ...     def __init__(self, *args):
    ...         super(NOT2, self).__init__(*args)
    ...         self.operator = '!'
    """

    def __init__(self, arg1):
        super(NOT, self).__init__(arg1)
        self.isliteral = isinstance(self.args[0], Symbol)
        self.operator = "~"

    def literalize(self):
        """
        Return an expression where NOTs are only occurring as literals.
        """
        expr = self.demorgan()
        if isinstance(expr, self.__class__):
            return expr
        return expr.literalize()

    def simplify(self):
        """
        Return a simplified expr in canonical form.

        This means double negations are canceled out and all contained boolean
        objects are in their canonical form.
        """
        if self.iscanonical:
            return self

        expr = self.cancel()
        if not isinstance(expr, self.__class__):
            return expr.simplify()

        if expr.args[0] in (
            self.TRUE,
            self.FALSE,
        ):
            return expr.args[0].dual

        expr = self.__class__(expr.args[0].simplify())
        expr.iscanonical = True
        return expr

    def cancel(self):
        """
        Cancel itself and following NOTs as far as possible.
        Returns the simplified expression.
        """
        expr = self
        while True:
            arg = expr.args[0]
            if not isinstance(arg, self.__class__):
                return expr
            expr = arg.args[0]
            if not isinstance(expr, self.__class__):
                return expr

    def demorgan(self):
        """
        Return a expr where the NOT function is moved inward.
        This is achieved by canceling double NOTs and using De Morgan laws.
        """
        expr = self.cancel()
        if expr.isliteral or not isinstance(expr, self.NOT):
            return expr
        op = expr.args[0]
        return op.dual(*(self.__class__(arg).cancel() for arg in op.args))

    def __call__(self, **kwargs):
        """
        Return the evaluated (negated) value for this function.
        """
        return not self.args[0](**kwargs)

    def __lt__(self, other):
        return self.args[0] < other

    def pretty(self, indent=1, debug=False):
        """
        Return a pretty formatted representation of self.
        Include additional debug details if `debug` is True.
        """
        debug_details = ""
        if debug:
            debug_details += f"<isliteral={self.isliteral!r}, iscanonical={self.iscanonical!r}>"
        if self.isliteral:
            pretty_literal = self.args[0].pretty(indent=0, debug=debug)
            return (" " * indent) + f"{self.__class__.__name__}({debug_details}{pretty_literal})"
        else:
            return super(NOT, self).pretty(indent=indent, debug=debug)


class DualBase(Function):
    """
    Base class for AND and OR function.

    This class uses the duality principle to combine similar methods of AND
    and OR. Both operations take two or more arguments and can be created using
    "|" for OR and "&" for AND.
    """

    _pyoperator = None

    def __init__(self, arg1, arg2, *args):
        super(DualBase, self).__init__(arg1, arg2, *args)

        # identity element for the specific operation.
        # This will be TRUE for the AND operation and FALSE for the OR operation.
        self.identity = None

        # annihilator element for this function.
        # This will be FALSE for the AND operation and TRUE for the OR operation.
        self.annihilator = None

        # dual class of this function.
        # This means OR.dual returns AND and AND.dual returns OR.
        self.dual = None

    def __contains__(self, expr):
        """
        Test if expr is a subterm of this expression.
        """
        if expr in self.args:
            return True

        if isinstance(expr, self.__class__):
            return all(arg in self.args for arg in expr.args)

    def simplify(self, sort=True):
        """
        Return a new simplified expression in canonical form from this
        expression.

        For simplification of AND and OR fthe ollowing rules are used
        recursively bottom up:

        - Associativity (output does not contain same operations nested)::

            (A & B) & C = A & (B & C) = A & B & C
            (A | B) | C = A | (B | C) = A | B | C
         
         
        - Annihilation::

            A & 0 = 0, A | 1 = 1

        - Idempotence (e.g. removing duplicates)::

            A & A = A, A | A = A

        - Identity::

            A & 1 = A, A | 0 = A

        - Complementation::

            A & ~A = 0, A | ~A = 1

        - Elimination::

            (A & B) | (A & ~B) = A, (A | B) & (A | ~B) = A

        - Absorption::

            A & (A | B) = A, A | (A & B) = A

        - Negative absorption::

            A & (~A | B) = A & B, A | (~A & B) = A | B

        - Commutativity (output is always sorted)::

            A & B = B & A, A | B = B | A

        Other boolean objects are also in their canonical form.
        """
        # TODO: Refactor DualBase.simplify into different "sub-evals".

        # If self is already canonical do nothing.
        if self.iscanonical:
            return self

        # Otherwise bring arguments into canonical form.
        args = [arg.simplify() for arg in self.args]

        # Create new instance of own class with canonical args.
        # TODO: Only create new class if some args changed.
        expr = self.__class__(*args)

        # Literalize before doing anything, this also applies De Morgan's Law
        expr = expr.literalize()

        # Associativity:
        #     (A & B) & C = A & (B & C) = A & B & C
        #     (A | B) | C = A | (B | C) = A | B | C
        expr = expr.flatten()

        # Annihilation: A & 0 = 0, A | 1 = 1
        if self.annihilator in expr.args:
            return self.annihilator

        # Idempotence: A & A = A, A | A = A
        # this boils down to removing duplicates
        args = []
        for arg in expr.args:
            if arg not in args:
                args.append(arg)
        if len(args) == 1:
            return args[0]

        # Identity: A & 1 = A, A | 0 = A
        if self.identity in args:
            args.remove(self.identity)
            if len(args) == 1:
                return args[0]

        # Complementation: A & ~A = 0, A | ~A = 1
        for arg in args:
            if self.NOT(arg) in args:
                return self.annihilator

        # Elimination: (A & B) | (A & ~B) = A, (A | B) & (A | ~B) = A
        i = 0
        while i < len(args) - 1:
            j = i + 1
            ai = args[i]
            if not isinstance(ai, self.dual):
                i += 1
                continue
            while j < len(args):
                aj = args[j]
                if not isinstance(aj, self.dual) or len(ai.args) != len(aj.args):
                    j += 1
                    continue

                # Find terms where only one arg is different.
                negated = None
                for arg in ai.args:
                    # FIXME: what does this pass Do?
                    if arg in aj.args:
                        pass
                    elif self.NOT(arg).cancel() in aj.args:
                        if negated is None:
                            negated = arg
                        else:
                            negated = None
                            break
                    else:
                        negated = None
                        break

                # If the different arg is a negation simplify the expr.
                if negated is not None:
                    # Cancel out one of the two terms.
                    del args[j]
                    aiargs = list(ai.args)
                    aiargs.remove(negated)
                    if len(aiargs) == 1:
                        args[i] = aiargs[0]
                    else:
                        args[i] = self.dual(*aiargs)

                    if len(args) == 1:
                        return args[0]
                    else:
                        # Now the other simplifications have to be redone.
                        return self.__class__(*args).simplify()
                j += 1
            i += 1

        # Absorption: A & (A | B) = A, A | (A & B) = A
        # Negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B
        args = self.absorb(args)
        if len(args) == 1:
            return args[0]

        # Commutativity: A & B = B & A, A | B = B | A
        if sort:
            args.sort()

        # Create new (now canonical) expression.
        expr = self.__class__(*args)
        expr.iscanonical = True
        return expr

    def flatten(self):
        """
        Return a new expression where nested terms of this expression are
        flattened as far as possible.

        E.g.::

            A & (B & C) becomes A & B & C.
        """
        args = list(self.args)
        i = 0
        for arg in self.args:
            if isinstance(arg, self.__class__):
                args[i : i + 1] = arg.args
                i += len(arg.args)
            else:
                i += 1

        return self.__class__(*args)

    def absorb(self, args):
        """
        Given an `args` sequence of expressions, return a new list of expression
        applying absorption and negative absorption.

        See https://en.wikipedia.org/wiki/Absorption_law

        Absorption::

            A & (A | B) = A, A | (A & B) = A

        Negative absorption::

            A & (~A | B) = A & B, A | (~A & B) = A | B
        """
        args = list(args)
        if not args:
            args = list(self.args)
        i = 0
        while i < len(args):
            absorber = args[i]
            j = 0
            while j < len(args):
                if j == i:
                    j += 1
                    continue
                target = args[j]
                if not isinstance(target, self.dual):
                    j += 1
                    continue

                # Absorption
                if absorber in target:
                    del args[j]
                    if j < i:
                        i -= 1
                    continue

                # Negative absorption
                neg_absorber = self.NOT(absorber).cancel()
                if neg_absorber in target:
                    b = target.subtract(neg_absorber, simplify=False)
                    if b is None:
                        del args[j]
                        if j < i:
                            i -= 1
                        continue
                    else:
                        args[j] = b
                        j += 1
                        continue

                if isinstance(absorber, self.dual):
                    remove = None
                    for arg in absorber.args:
                        narg = self.NOT(arg).cancel()
                        if arg in target.args:
                            pass
                        elif narg in target.args:
                            if remove is None:
                                remove = narg
                            else:
                                remove = None
                                break
                        else:
                            remove = None
                            break
                    if remove is not None:
                        args[j] = target.subtract(remove, simplify=True)
                j += 1
            i += 1

        return args

    def subtract(self, expr, simplify):
        """
        Return a new expression where the `expr` expression has been removed
        from this expression if it exists.
        """
        args = self.args
        if expr in self.args:
            args = list(self.args)
            args.remove(expr)
        elif isinstance(expr, self.__class__):
            if all(arg in self.args for arg in expr.args):
                args = tuple(arg for arg in self.args if arg not in expr)
        if len(args) == 0:
            return None
        if len(args) == 1:
            return args[0]

        newexpr = self.__class__(*args)
        if simplify:
            newexpr = newexpr.simplify()
        return newexpr

    def distributive(self):
        """
        Return a term where the leading AND or OR terms are switched.

        This is done by applying the distributive laws::

            A & (B|C) = (A&B) | (A&C)
            A | (B&C) = (A|B) & (A|C)
        """
        dual = self.dual
        args = list(self.args)
        for i, arg in enumerate(args):
            if isinstance(arg, dual):
                args[i] = arg.args
            else:
                args[i] = (arg,)

        prod = itertools.product(*args)
        args = tuple(self.__class__(*arg).simplify() for arg in prod)

        if len(args) == 1:
            return args[0]
        else:
            return dual(*args)

    def __lt__(self, other):
        comparator = Expression.__lt__(self, other)
        if comparator is not NotImplemented:
            return comparator

        if isinstance(other, self.__class__):
            lenself = len(self.args)
            lenother = len(other.args)
            for i in range(min(lenself, lenother)):
                if self.args[i] == other.args[i]:
                    continue

                comparator = self.args[i] < other.args[i]
                if comparator is not NotImplemented:
                    return comparator

            if lenself != lenother:
                return lenself < lenother
        return NotImplemented

    def __call__(self, **kwargs):
        """
        Return the evaluation of this expression by calling each of its arg as
        arg(**kwargs) and applying its corresponding Python operator (and or or)
        to the results.

        Reduce is used as in e.g. AND(a, b, c, d) == AND(a, AND(b, AND(c, d)))
        ore.g. OR(a, b, c, d) == OR(a, OR(b, OR(c, d)))
        """
        return reduce(self._pyoperator, (a(**kwargs) for a in self.args))


class AND(DualBase):
    """
    Boolean AND operation, taking two or more arguments.

    It can also be created by using "&" between two boolean expressions.

    You can subclass to define alternative string representation by overriding
    self.operator.
    
    For example:

    >>> class AND2(AND):
    ...     def __init__(self, *args):
    ...         super(AND2, self).__init__(*args)
    ...         self.operator = 'AND'
    """

    _pyoperator = and_operator

    def __init__(self, arg1, arg2, *args):
        super(AND, self).__init__(arg1, arg2, *args)
        self.sort_order = 10
        self.identity = self.TRUE
        self.annihilator = self.FALSE
        self.dual = self.OR
        self.operator = "&"


class OR(DualBase):
    """
    Boolean OR operation, taking two or more arguments

    It can also be created by using "|" between two boolean expressions.

    You can subclass to define alternative string representation by overriding
    self.operator.

    For example:

    >>> class OR2(OR):
    ...     def __init__(self, *args):
    ...         super(OR2, self).__init__(*args)
    ...         self.operator = 'OR'
    """

    _pyoperator = or_operator

    def __init__(self, arg1, arg2, *args):
        super(OR, self).__init__(arg1, arg2, *args)
        self.sort_order = 25
        self.identity = self.FALSE
        self.annihilator = self.TRUE
        self.dual = self.AND
        self.operator = "|"

if __name__ == "__main__":
    class MySymbol(Symbol):
        pass

    isT=True
    try:
        expr_str = """(a or ~ b +_c  ) and
                      d & ( ! e_
                      | (my * g OR 1 or 0) ) AND that """

        algebra = BooleanAlgebra(Symbol_class=MySymbol)
        algebra1 = BooleanAlgebra()
        algebra2 = BooleanAlgebra()
        a = algebra1.Symbol("a")
        b = algebra1.Symbol("b")
        expr = algebra.parse(expr_str)

        expected = algebra.AND(
            algebra.OR(
                algebra.Symbol("a"),
                algebra.NOT(algebra.Symbol("b")),
                algebra.Symbol("_c"),
            ),
            algebra.Symbol("d"),
            algebra.OR(
                algebra.NOT(algebra.Symbol("e_")),
                algebra.OR(
                    algebra.AND(
                        algebra.Symbol("my"),
                        algebra.Symbol("g"),
                    ),
                    algebra.TRUE,
                    algebra.FALSE,
                ),
            ),
            algebra.Symbol("that"),
        )

        if not expr.pretty() == expected.pretty() or \
        not expr == expected:
            isT=False

        sorted_expression = (b & b & a).simplify()
        unsorted_expression = (b & b & a).simplify(sort=False)
        if not unsorted_expression == sorted_expression or \
        not sorted_expression.pretty() != unsorted_expression.pretty():
            isT=False

        sorted_expression = (b | b | a).simplify()
        unsorted_expression = (b | b | a).simplify(sort=False)
        if not unsorted_expression == sorted_expression or\
        not sorted_expression.pretty() != unsorted_expression.pretty():
            isT=False

        expected = algebra1.parse("(~b&~d&a) | (~c&~d&b) | (a&c&d)", simplify=True)
        result = algebra1.parse(
            """(~a&b&~c&~d) | (a&~b&~c&~d) | (a&~b&c&~d) |
                          (a&~b&c&d) | (a&b&~c&~d) | (a&b&c&d)""",
            simplify=True,
        )
        if not result.pretty() == expected.pretty():
            isT=False

        expected = algebra1.parse("(~b&~d&a) | (~c&~d&b) | (a&c&d)", simplify=True)
        result = algebra2.parse(
            """(~a&b&~c&~d) | (a&~b&~c&~d) | (a&~b&c&~d) |
                          (a&~b&c&d) | (a&b&~c&~d) | (a&b&c&d)""",
            simplify=True,
        )
        if not result.pretty() == expected.pretty():
            isT=False


        expr_str = """(a or ~ b +_c  ) and
                      d & ( ! e_
                      | (my * g OR 1 or 0) ) AND that """

        algebra = BooleanAlgebra(Symbol_class=MySymbol)
        expr = algebra.parse(expr_str)

        expected = algebra.AND(
            algebra.OR(
                algebra.Symbol("a"),
                algebra.NOT(algebra.Symbol("b")),
                algebra.Symbol("_c"),
            ),
            algebra.Symbol("d"),
            algebra.OR(
                algebra.NOT(algebra.Symbol("e_")),
                algebra.OR(
                    algebra.AND(
                        algebra.Symbol("my"),
                        algebra.Symbol("g"),
                    ),
                    algebra.TRUE,
                    algebra.FALSE,
                ),
            ),
            algebra.Symbol("that"),
        )

        if not expr.pretty() == expected.pretty() or \
        not expr == expected:
            isT=False
    except:
        isT=False
    # import dill
    # import os
    #
    # isT = True
    # for l in os.listdir("/home/travis/builds/repos/bastikr---boolean/data_passk_platform/62b46740d2f69a53b466171a/"):
    #     f = open("/home/travis/builds/repos/bastikr---boolean/data_passk_platform/62b46740d2f69a53b466171a/" + l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = Symbol(1)
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.pretty(args1, args2)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/bastikr---boolean/boolean/boolean_absorb_passk_validte.py
"""
Boolean expressions algebra.

This module defines a Boolean algebra over the set {TRUE, FALSE} with boolean
variables called Symbols and the boolean functions AND, OR, NOT.

Some basic logic comparison is supported: two expressions can be
compared for equivalence or containment. Furthermore you can simplify
an expression and obtain its normal form.

You can create expressions in Python using familiar boolean operators
or parse expressions from strings. The parsing can be extended with
your own tokenizer.  You can also customize how expressions behave and
how they are presented.

For extensive documentation look either into the docs directory or view it
online, at https://booleanpy.readthedocs.org/en/latest/.

Copyright (c) Sebastian Kraemer, basti.kr@gmail.com and others

SPDX-License-Identifier: BSD-2-Clause
"""

import inspect
import itertools
from functools import reduce  # NOQA
from operator import and_ as and_operator
from operator import or_ as or_operator

# Set to True to enable tracing for parsing
TRACE_PARSE = False

# Token types for standard operators and parens
TOKEN_AND = 1
TOKEN_OR = 2
TOKEN_NOT = 3
TOKEN_LPAR = 4
TOKEN_RPAR = 5
TOKEN_TRUE = 6
TOKEN_FALSE = 7
TOKEN_SYMBOL = 8

TOKEN_TYPES = {
    TOKEN_AND: "AND",
    TOKEN_OR: "OR",
    TOKEN_NOT: "NOT",
    TOKEN_LPAR: "(",
    TOKEN_RPAR: ")",
    TOKEN_TRUE: "TRUE",
    TOKEN_FALSE: "FALSE",
    TOKEN_SYMBOL: "SYMBOL",
}

# parsing error code and messages
PARSE_UNKNOWN_TOKEN = 1
PARSE_UNBALANCED_CLOSING_PARENS = 2
PARSE_INVALID_EXPRESSION = 3
PARSE_INVALID_NESTING = 4
PARSE_INVALID_SYMBOL_SEQUENCE = 5
PARSE_INVALID_OPERATOR_SEQUENCE = 6

PARSE_ERRORS = {
    PARSE_UNKNOWN_TOKEN: "Unknown token",
    PARSE_UNBALANCED_CLOSING_PARENS: "Unbalanced parenthesis",
    PARSE_INVALID_EXPRESSION: "Invalid expression",
    PARSE_INVALID_NESTING: "Invalid expression nesting such as (AND xx)",
    PARSE_INVALID_SYMBOL_SEQUENCE: "Invalid symbols sequence such as (A B)",
    PARSE_INVALID_OPERATOR_SEQUENCE: "Invalid operator sequence without symbols such as AND OR or OR OR",
}


class ParseError(Exception):
    """
    Raised when the parser or tokenizer encounters a syntax error. Instances of
    this class have attributes token_type, token_string, position, error_code to
    access the details of the error. str() of the exception instance returns a
    formatted message.
    """

    def __init__(self, token_type=None, token_string="", position=-1, error_code=0):
        self.token_type = token_type
        self.token_string = token_string
        self.position = position
        self.error_code = error_code

    def __str__(self, *args, **kwargs):
        emsg = PARSE_ERRORS.get(self.error_code, "Unknown parsing error")

        tstr = ""
        if self.token_string:
            tstr = f' for token: "{self.token_string}"'

        pos = ""
        if self.position > 0:
            pos = f" at position: {self.position}"

        return f"{emsg}{tstr}{pos}"


class BooleanAlgebra(object):
    """
    An algebra is defined by:

    - the types of its operations and Symbol.
    - the tokenizer used when parsing expressions from strings.

    This class also serves as a base class for all boolean expressions,
    including base elements, functions and variable symbols.
    """

    def __init__(
        self,
        TRUE_class=None,
        FALSE_class=None,
        Symbol_class=None,
        NOT_class=None,
        AND_class=None,
        OR_class=None,
        allowed_in_token=(".", ":", "_"),
    ):
        """
        The types for TRUE, FALSE, NOT, AND, OR and Symbol define the boolean
        algebra elements, operations and Symbol variable. They default to the
        standard classes if not provided.

        You can customize an algebra by providing alternative subclasses of the
        standard types.
        """
        # TRUE and FALSE base elements are algebra-level "singleton" instances
        self.TRUE = TRUE_class or _TRUE
        self.TRUE = self.TRUE()

        self.FALSE = FALSE_class or _FALSE
        self.FALSE = self.FALSE()

        # they cross-reference each other
        self.TRUE.dual = self.FALSE
        self.FALSE.dual = self.TRUE

        # boolean operation types, defaulting to the standard types
        self.NOT = NOT_class or NOT
        self.AND = AND_class or AND
        self.OR = OR_class or OR

        # class used for Symbols
        self.Symbol = Symbol_class or Symbol

        tf_nao = {
            "TRUE": self.TRUE,
            "FALSE": self.FALSE,
            "NOT": self.NOT,
            "AND": self.AND,
            "OR": self.OR,
            "Symbol": self.Symbol,
        }

        # setup cross references such that all algebra types and
        # objects hold a named attribute for every other types and
        # objects, including themselves.
        for obj in tf_nao.values():
            for name, value in tf_nao.items():
                setattr(obj, name, value)

        # Set the set of characters allowed in tokens
        self.allowed_in_token = allowed_in_token

    def definition(self):
        """
        Return a tuple of this algebra defined elements and types as:
        (TRUE, FALSE, NOT, AND, OR, Symbol)
        """
        return self.TRUE, self.FALSE, self.NOT, self.AND, self.OR, self.Symbol

    def symbols(self, *args):
        """
        Return a tuple of symbols building a new Symbol from each argument.
        """
        return tuple(map(self.Symbol, args))

    def parse(self, expr, simplify=False):
        """
        Return a boolean expression parsed from `expr` either a unicode string
        or tokens iterable.

        Optionally simplify the expression if `simplify` is True.

        Raise ParseError on errors.

        If `expr` is a string, the standard `tokenizer` is used for tokenization
        and the algebra configured Symbol type is used to create Symbol
        instances from Symbol tokens.

        If `expr` is an iterable, it should contain 3-tuples of: (token_type,
        token_string, token_position). In this case, the `token_type` can be
        a Symbol instance or one of the TOKEN_* constant types.
        See the `tokenize()` method for detailed specification.
        """

        precedence = {self.NOT: 5, self.AND: 10, self.OR: 15, TOKEN_LPAR: 20}

        if isinstance(expr, str):
            tokenized = self.tokenize(expr)
        else:
            tokenized = iter(expr)

        if TRACE_PARSE:
            tokenized = list(tokenized)
            print("tokens:")
            for t in tokenized:
                print(t)
            tokenized = iter(tokenized)

        # the abstract syntax tree for this expression that will be build as we
        # process tokens
        # the first two items are None
        # symbol items are appended to this structure
        ast = [None, None]

        def is_sym(_t):
            return isinstance(_t, Symbol) or _t in (TOKEN_TRUE, TOKEN_FALSE, TOKEN_SYMBOL)

        def is_operator(_t):
            return _t in (TOKEN_AND, TOKEN_OR)

        prev_token = None
        for token_type, token_string, token_position in tokenized:
            if TRACE_PARSE:
                print(
                    "\nprocessing token_type:",
                    repr(token_type),
                    "token_string:",
                    repr(token_string),
                    "token_position:",
                    repr(token_position),
                )

            if prev_token:
                prev_token_type, _prev_token_string, _prev_token_position = prev_token
                if TRACE_PARSE:
                    print("  prev_token:", repr(prev_token))

                if is_sym(prev_token_type) and (
                    is_sym(token_type)
                ):  # or token_type == TOKEN_LPAR) :
                    raise ParseError(
                        token_type, token_string, token_position, PARSE_INVALID_SYMBOL_SEQUENCE
                    )

                if is_operator(prev_token_type) and (
                    is_operator(token_type) or token_type == TOKEN_RPAR
                ):
                    raise ParseError(
                        token_type, token_string, token_position, PARSE_INVALID_OPERATOR_SEQUENCE
                    )

            else:
                if is_operator(token_type):
                    raise ParseError(
                        token_type, token_string, token_position, PARSE_INVALID_OPERATOR_SEQUENCE
                    )

            if token_type == TOKEN_SYMBOL:
                ast.append(self.Symbol(token_string))
                if TRACE_PARSE:
                    print(" ast: token_type is TOKEN_SYMBOL: append new symbol", repr(ast))

            elif isinstance(token_type, Symbol):
                ast.append(token_type)
                if TRACE_PARSE:
                    print(" ast: token_type is Symbol): append existing symbol", repr(ast))

            elif token_type == TOKEN_TRUE:
                ast.append(self.TRUE)
                if TRACE_PARSE:
                    print(" ast: token_type is TOKEN_TRUE:", repr(ast))

            elif token_type == TOKEN_FALSE:
                ast.append(self.FALSE)
                if TRACE_PARSE:
                    print(" ast: token_type is TOKEN_FALSE:", repr(ast))

            elif token_type == TOKEN_NOT:
                ast = [ast, self.NOT]
                if TRACE_PARSE:
                    print(" ast: token_type is TOKEN_NOT:", repr(ast))

            elif token_type == TOKEN_AND:
                ast = self._start_operation(ast, self.AND, precedence)
                if TRACE_PARSE:
                    print("  ast:token_type is TOKEN_AND: start_operation", ast)

            elif token_type == TOKEN_OR:
                ast = self._start_operation(ast, self.OR, precedence)
                if TRACE_PARSE:
                    print("  ast:token_type is TOKEN_OR: start_operation", ast)

            elif token_type == TOKEN_LPAR:
                if prev_token:
                    # Check that an opening parens is preceded by a function
                    # or an opening parens
                    if prev_token_type not in (TOKEN_NOT, TOKEN_AND, TOKEN_OR, TOKEN_LPAR):
                        raise ParseError(
                            token_type, token_string, token_position, PARSE_INVALID_NESTING
                        )
                ast = [ast, TOKEN_LPAR]

            elif token_type == TOKEN_RPAR:
                while True:
                    if ast[0] is None:
                        raise ParseError(
                            token_type,
                            token_string,
                            token_position,
                            PARSE_UNBALANCED_CLOSING_PARENS,
                        )

                    if ast[1] is TOKEN_LPAR:
                        ast[0].append(ast[2])
                        if TRACE_PARSE:
                            print("ast9:", repr(ast))
                        ast = ast[0]
                        if TRACE_PARSE:
                            print("ast10:", repr(ast))
                        break

                    if isinstance(ast[1], int):
                        raise ParseError(
                            token_type,
                            token_string,
                            token_position,
                            PARSE_UNBALANCED_CLOSING_PARENS,
                        )

                    # the parens are properly nested
                    # the top ast node should be a function subclass
                    if not (inspect.isclass(ast[1]) and issubclass(ast[1], Function)):
                        raise ParseError(
                            token_type, token_string, token_position, PARSE_INVALID_NESTING
                        )

                    subex = ast[1](*ast[2:])
                    ast[0].append(subex)
                    if TRACE_PARSE:
                        print("ast11:", repr(ast))
                    ast = ast[0]
                    if TRACE_PARSE:
                        print("ast12:", repr(ast))
            else:
                raise ParseError(token_type, token_string, token_position, PARSE_UNKNOWN_TOKEN)

            prev_token = (token_type, token_string, token_position)

        try:
            while True:
                if ast[0] is None:
                    if TRACE_PARSE:
                        print("ast[0] is None:", repr(ast))
                    if ast[1] is None:
                        if TRACE_PARSE:
                            print("  ast[1] is None:", repr(ast))
                        if len(ast) != 3:
                            raise ParseError(error_code=PARSE_INVALID_EXPRESSION)
                        parsed = ast[2]
                        if TRACE_PARSE:
                            print("    parsed = ast[2]:", repr(parsed))

                    else:
                        # call the function in ast[1] with the rest of the ast as args
                        parsed = ast[1](*ast[2:])
                        if TRACE_PARSE:
                            print("  parsed = ast[1](*ast[2:]):", repr(parsed))
                    break
                else:
                    if TRACE_PARSE:
                        print("subex = ast[1](*ast[2:]):", repr(ast))
                    subex = ast[1](*ast[2:])
                    ast[0].append(subex)
                    if TRACE_PARSE:
                        print("  ast[0].append(subex):", repr(ast))
                    ast = ast[0]
                    if TRACE_PARSE:
                        print("    ast = ast[0]:", repr(ast))
        except TypeError:
            raise ParseError(error_code=PARSE_INVALID_EXPRESSION)

        if simplify:
            return parsed.simplify()

        if TRACE_PARSE:
            print("final parsed:", repr(parsed))
        return parsed

    def _start_operation(self, ast, operation, precedence):
        """
        Return an AST where all operations of lower precedence are finalized.
        """
        if TRACE_PARSE:
            print("   start_operation:", repr(operation), "AST:", ast)

        op_prec = precedence[operation]
        while True:
            if ast[1] is None:
                # [None, None, x]
                if TRACE_PARSE:
                    print("     start_op: ast[1] is None:", repr(ast))
                ast[1] = operation
                if TRACE_PARSE:
                    print("     --> start_op: ast[1] is None:", repr(ast))
                return ast

            prec = precedence[ast[1]]
            if prec > op_prec:  # op=&, [ast, |, x, y] -> [[ast, |, x], &, y]
                if TRACE_PARSE:
                    print("     start_op: prec > op_prec:", repr(ast))
                ast = [ast, operation, ast.pop(-1)]
                if TRACE_PARSE:
                    print("     --> start_op: prec > op_prec:", repr(ast))
                return ast

            if prec == op_prec:  # op=&, [ast, &, x] -> [ast, &, x]
                if TRACE_PARSE:
                    print("     start_op: prec == op_prec:", repr(ast))
                return ast

            if not (inspect.isclass(ast[1]) and issubclass(ast[1], Function)):
                # the top ast node should be a function subclass at this stage
                raise ParseError(error_code=PARSE_INVALID_NESTING)

            if ast[0] is None:  # op=|, [None, &, x, y] -> [None, |, x&y]
                if TRACE_PARSE:
                    print("     start_op: ast[0] is None:", repr(ast))
                subexp = ast[1](*ast[2:])
                new_ast = [ast[0], operation, subexp]
                if TRACE_PARSE:
                    print("     --> start_op: ast[0] is None:", repr(new_ast))
                return new_ast

            else:  # op=|, [[ast, &, x], ~, y] -> [ast, &, x, ~y]
                if TRACE_PARSE:
                    print("     start_op: else:", repr(ast))
                ast[0].append(ast[1](*ast[2:]))
                ast = ast[0]
                if TRACE_PARSE:
                    print("     --> start_op: else:", repr(ast))

    def tokenize(self, expr):
        """
        Return an iterable of 3-tuple describing each token given an expression
        unicode string.

        This 3-tuple contains (token, token string, position):

        - token: either a Symbol instance or one of TOKEN_* token types.
        - token string: the original token unicode string.
        - position: some simple object describing the starting position of the
          original token string in the `expr` string. It can be an int for a
          character offset, or a tuple of starting (row/line, column).

        The token position is used only for error reporting and can be None or
        empty.

        Raise ParseError on errors. The ParseError.args is a tuple of:
        (token_string, position, error message)

        You can use this tokenizer as a base to create specialized tokenizers
        for your custom algebra by subclassing BooleanAlgebra. See also the
        tests for other examples of alternative tokenizers.

        This tokenizer has these characteristics:

        - The `expr` string can span multiple lines,
        - Whitespace is not significant.
        - The returned position is the starting character offset of a token.
        - A TOKEN_SYMBOL is returned for valid identifiers which is a string
          without spaces.

            - These are valid identifiers:
                - Python identifiers.
                - a string even if starting with digits
                - digits (except for 0 and 1).
                - dotted names : foo.bar consist of one token.
                - names with colons: foo:bar consist of one token.
            
            - These are not identifiers:
                - quoted strings.
                - any punctuation which is not an operation

        - Recognized operators are (in any upper/lower case combinations):

            - for and:  '*', '&', 'and'
            - for or: '+', '|', 'or'
            - for not: '~', '!', 'not'

        - Recognized special symbols are (in any upper/lower case combinations):

            - True symbols: 1 and True
            - False symbols: 0, False and None
        """
        if not isinstance(expr, str):
            raise TypeError(f"expr must be string but it is {type(expr)}.")

        # mapping of lowercase token strings to a token type id for the standard
        # operators, parens and common true or false symbols, as used in the
        # default tokenizer implementation.
        TOKENS = {
            "*": TOKEN_AND,
            "&": TOKEN_AND,
            "and": TOKEN_AND,
            "+": TOKEN_OR,
            "|": TOKEN_OR,
            "or": TOKEN_OR,
            "~": TOKEN_NOT,
            "!": TOKEN_NOT,
            "not": TOKEN_NOT,
            "(": TOKEN_LPAR,
            ")": TOKEN_RPAR,
            "[": TOKEN_LPAR,
            "]": TOKEN_RPAR,
            "true": TOKEN_TRUE,
            "1": TOKEN_TRUE,
            "false": TOKEN_FALSE,
            "0": TOKEN_FALSE,
            "none": TOKEN_FALSE,
        }

        position = 0
        length = len(expr)

        while position < length:
            tok = expr[position]

            sym = tok.isalnum() or tok == "_"
            if sym:
                position += 1
                while position < length:
                    char = expr[position]
                    if char.isalnum() or char in self.allowed_in_token:
                        position += 1
                        tok += char
                    else:
                        break
                position -= 1

            try:
                yield TOKENS[tok.lower()], tok, position
            except KeyError:
                if sym:
                    yield TOKEN_SYMBOL, tok, position
                elif tok not in (" ", "\t", "\r", "\n"):
                    raise ParseError(
                        token_string=tok, position=position, error_code=PARSE_UNKNOWN_TOKEN
                    )

            position += 1

    def _recurse_distributive(self, expr, operation_inst):
        """
        Recursively flatten, simplify and apply the distributive laws to the
        `expr` expression. Distributivity is considered for the AND or OR
        `operation_inst` instance.
        """
        if expr.isliteral:
            return expr

        args = (self._recurse_distributive(arg, operation_inst) for arg in expr.args)
        args = tuple(arg.simplify() for arg in args)
        if len(args) == 1:
            return args[0]

        flattened_expr = expr.__class__(*args)

        dualoperation = operation_inst.dual
        if isinstance(flattened_expr, dualoperation):
            flattened_expr = flattened_expr.distributive()
        return flattened_expr

    def normalize(self, expr, operation):
        """
        Return a normalized expression transformed to its normal form in the
        given AND or OR operation.

        The new expression arguments will satisfy these conditions:
    
        - ``operation(*args) == expr`` (here mathematical equality is meant)
        - the operation does not occur in any of its arg.
        - NOT is only appearing in literals (aka. Negation normal form).

        The operation must be an AND or OR operation or a subclass.
        """
        # Ensure that the operation is not NOT
        assert operation in (
            self.AND,
            self.OR,
        )
        # Move NOT inwards.
        expr = expr.literalize()
        # Simplify first otherwise _recurse_distributive() may take forever.
        expr = expr.simplify()
        operation_example = operation(self.TRUE, self.FALSE)

        # For large dual operations build up from normalized subexpressions,
        # otherwise we can get exponential blowup midway through
        expr.args = tuple(self.normalize(a, operation) for a in expr.args)
        if len(expr.args) > 1 and (
            (operation == self.AND and isinstance(expr, self.OR))
            or (operation == self.OR and isinstance(expr, self.AND))
        ):
            args = expr.args
            expr_class = expr.__class__
            expr = args[0]
            for arg in args[1:]:
                expr = expr_class(expr, arg)
                expr = self._recurse_distributive(expr, operation_example)
                # Canonicalize
                expr = expr.simplify()

        else:
            expr = self._recurse_distributive(expr, operation_example)
            # Canonicalize
            expr = expr.simplify()

        return expr

    def cnf(self, expr):
        """
        Return a conjunctive normal form of the `expr` expression.
        """
        return self.normalize(expr, self.AND)

    conjunctive_normal_form = cnf

    def dnf(self, expr):
        """
        Return a disjunctive normal form of the `expr` expression.
        """
        return self.normalize(expr, self.OR)

    disjunctive_normal_form = dnf


class Expression(object):
    """
    Abstract base class for all boolean expressions, including functions and
    variable symbols.
    """

    # these class attributes are configured when a new BooleanAlgebra is created
    TRUE = None
    FALSE = None
    NOT = None
    AND = None
    OR = None
    Symbol = None

    def __init__(self):
        # Defines sort and comparison order between expressions arguments
        self.sort_order = None

        # Store arguments aka. subterms of this expressions.
        # subterms are either literals or expressions.
        self.args = tuple()

        # True is this is a literal expression such as a Symbol, TRUE or FALSE
        self.isliteral = False

        # True if this expression has been simplified to in canonical form.
        self.iscanonical = False

    @property
    def objects(self):
        """
        Return a set of all associated objects with this expression symbols.
        Include recursively subexpressions objects.
        """
        return set(s.obj for s in self.symbols)

    def get_literals(self):
        """
        Return a list of all the literals contained in this expression.
        Include recursively subexpressions symbols.
        This includes duplicates.
        """
        if self.isliteral:
            return [self]
        if not self.args:
            return []
        return list(itertools.chain.from_iterable(arg.get_literals() for arg in self.args))

    @property
    def literals(self):
        """
        Return a set of all literals contained in this expression.
        Include recursively subexpressions literals.
        """
        return set(self.get_literals())

    def literalize(self):
        """
        Return an expression where NOTs are only occurring as literals.
        Applied recursively to subexpressions.
        """
        if self.isliteral:
            return self
        args = tuple(arg.literalize() for arg in self.args)
        if all(arg is self.args[i] for i, arg in enumerate(args)):
            return self

        return self.__class__(*args)

    def get_symbols(self):
        """
        Return a list of all the symbols contained in this expression.
        Include subexpressions symbols recursively.
        This includes duplicates.
        """
        return [s if isinstance(s, Symbol) else s.args[0] for s in self.get_literals()]

    @property
    def symbols(
        self,
    ):
        """
        Return a list of all the symbols contained in this expression.
        Include subexpressions symbols recursively.
        This includes duplicates.
        """
        return set(self.get_symbols())

    def subs(self, substitutions, default=None, simplify=False):
        """
        Return an expression where all subterms of this expression are
        by the new expression using a `substitutions` mapping of:
        {expr: replacement}

        Return the provided `default` value if this expression has no elements,
        e.g. is empty.

        Simplify the results if `simplify` is True.

        Return this expression unmodified if nothing could be substituted. Note
        that a possible usage of this function is to check for expression
        containment as the expression will be returned unmodified if if does not
        contain any of the provided substitutions.
        """
        # shortcut: check if we have our whole expression as a possible
        # subsitution source
        for expr, substitution in substitutions.items():
            if expr == self:
                return substitution

        # otherwise, do a proper substitution of subexpressions
        expr = self._subs(substitutions, default, simplify)
        return self if expr is None else expr

    def _subs(self, substitutions, default, simplify):
        """
        Return an expression where all subterms are substituted by the new
        expression using a `substitutions` mapping of: {expr: replacement}
        """
        # track the new list of unchanged args or replaced args through
        # a substitution
        new_arguments = []
        changed_something = False

        # shortcut for basic logic True or False
        if self is self.TRUE or self is self.FALSE:
            return self

        # if the expression has no elements, e.g. is empty, do not apply
        # substitutions
        if not self.args:
            return default

        # iterate the subexpressions: either plain symbols or a subexpressions
        for arg in self.args:
            # collect substitutions for exact matches
            # break as soon as we have a match
            for expr, substitution in substitutions.items():
                if arg == expr:
                    new_arguments.append(substitution)
                    changed_something = True
                    break

            # this will execute only if we did not break out of the
            # loop, e.g. if we did not change anything and did not
            # collect any substitutions
            else:
                # recursively call _subs on each arg to see if we get a
                # substituted arg
                new_arg = arg._subs(substitutions, default, simplify)
                if new_arg is None:
                    # if we did not collect a substitution for this arg,
                    # keep the arg as-is, it is not replaced by anything
                    new_arguments.append(arg)
                else:
                    # otherwise, we add the substitution for this arg instead
                    new_arguments.append(new_arg)
                    changed_something = True

        if not changed_something:
            return

        # here we did some substitution: we return a new expression
        # built from the new_arguments
        newexpr = self.__class__(*new_arguments)
        return newexpr.simplify() if simplify else newexpr

    def simplify(self):
        """
        Return a new simplified expression in canonical form built from this
        expression. The simplified expression may be exactly the same as this
        expression.

        Subclasses override this method to compute actual simplification.
        """
        return self

    def __hash__(self):
        """
        Expressions are immutable and hashable. The hash of Functions is
        computed by respecting the structure of the whole expression by mixing
        the class name hash and the recursive hash of a frozenset of arguments.
        Hash of elements is based on their boolean equivalent. Hash of symbols
        is based on their object.
        """
        if not self.args:
            arghash = id(self)
        else:
            arghash = hash(frozenset(map(hash, self.args)))
        return hash(self.__class__.__name__) ^ arghash

    def __eq__(self, other):
        """
        Test if other element is structurally the same as itself.

        This method does not make any simplification or transformation, so it
        will return False although the expression terms may be mathematically
        equal. Use simplify() before testing equality to check the mathematical
        equality.

        For literals, plain equality is used.

        For functions, equality uses the facts that operations are:

        - commutative: order does not matter and different orders are equal.
        - idempotent: so args can appear more often in one term than in the other.
        """
        if self is other:
            return True

        if isinstance(other, self.__class__):
            return frozenset(self.args) == frozenset(other.args)

        return NotImplemented

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if self.sort_order is not None and other.sort_order is not None:
            if self.sort_order == other.sort_order:
                return NotImplemented
            return self.sort_order < other.sort_order
        return NotImplemented

    def __gt__(self, other):
        lt = other.__lt__(self)
        if lt is NotImplemented:
            return not self.__lt__(other)
        return lt

    def __and__(self, other):
        return self.AND(self, other)

    __mul__ = __and__

    def __invert__(self):
        return self.NOT(self)

    def __or__(self, other):
        return self.OR(self, other)

    __add__ = __or__

    def __bool__(self):
        raise TypeError("Cannot evaluate expression as a Python Boolean.")

    __nonzero__ = __bool__


class BaseElement(Expression):
    """
    Abstract base class for the base elements TRUE and FALSE of the boolean
    algebra.
    """

    def __init__(self):
        super(BaseElement, self).__init__()
        self.sort_order = 0
        self.iscanonical = True
        # The dual Base Element class for this element: TRUE.dual returns
        # _FALSE() and FALSE.dual returns _TRUE(). This is a cyclic reference
        # and therefore only assigned after creation of the singletons,
        self.dual = None

    def __lt__(self, other):
        if isinstance(other, BaseElement):
            return self == self.FALSE
        return NotImplemented

    __nonzero__ = __bool__ = lambda s: None

    def pretty(self, indent=0, debug=False):
        """
        Return a pretty formatted representation of self.
        """
        return (" " * indent) + repr(self)


class _TRUE(BaseElement):
    """
    Boolean base element TRUE.
    Not meant to be subclassed nor instantiated directly.
    """

    def __init__(self):
        super(_TRUE, self).__init__()
        # assigned at singleton creation: self.dual = FALSE

    def __hash__(self):
        return hash(True)

    def __eq__(self, other):
        return self is other or other is True or isinstance(other, _TRUE)

    def __str__(self):
        return "1"

    def __repr__(self):
        return "TRUE"

    def __call__(self):
        return self

    __nonzero__ = __bool__ = lambda s: True


class _FALSE(BaseElement):
    """
    Boolean base element FALSE.
    Not meant to be subclassed nor instantiated directly.
    """

    def __init__(self):
        super(_FALSE, self).__init__()
        # assigned at singleton creation: self.dual = TRUE

    def __hash__(self):
        return hash(False)

    def __eq__(self, other):
        return self is other or other is False or isinstance(other, _FALSE)

    def __str__(self):
        return "0"

    def __repr__(self):
        return "FALSE"

    def __call__(self):
        return self

    __nonzero__ = __bool__ = lambda s: False


class Symbol(Expression):
    """
    Boolean variable.

    A Symbol can hold an object used to determine equality between symbols.
    """

    def __init__(self, obj):
        super(Symbol, self).__init__()
        self.sort_order = 5
        # Store an associated object. This object determines equality
        self.obj = obj
        self.iscanonical = True
        self.isliteral = True

    def __call__(self, **kwargs):
        """
        Return the evaluated value for this symbol from kwargs
        """
        return kwargs[self.obj]

    def __hash__(self):
        if self.obj is None:  # Anonymous Symbol.
            return id(self)
        return hash(self.obj)

    def __eq__(self, other):
        if self is other:
            return True
        if isinstance(other, self.__class__):
            return self.obj == other.obj
        return NotImplemented

    def __lt__(self, other):
        comparator = Expression.__lt__(self, other)
        if comparator is not NotImplemented:
            return comparator
        if isinstance(other, Symbol):
            return self.obj < other.obj
        return NotImplemented

    def __str__(self):
        return str(self.obj)

    def __repr__(self):
        obj = f"'{self.obj}'" if isinstance(self.obj, str) else repr(self.obj)
        return f"{self.__class__.__name__}({obj})"

    def pretty(self, indent=0, debug=False):
        """
        Return a pretty formatted representation of self.
        """
        debug_details = ""
        if debug:
            debug_details += f"<isliteral={self.isliteral!r}, iscanonical={self.iscanonical!r}>"

        obj = f"'{self.obj}'" if isinstance(self.obj, str) else repr(self.obj)
        return (" " * indent) + f"{self.__class__.__name__}({debug_details}{obj})"


class Function(Expression):
    """
    Boolean function.

    A boolean function takes n (one or more) boolean expressions as arguments
    where n is called the order of the function and maps them to one of the base
    elements TRUE or FALSE. Implemented functions are AND, OR and NOT.
    """

    def __init__(self, *args):
        super(Function, self).__init__()

        # Specifies an infix notation of an operator for printing such as | or &.
        self.operator = None

        assert all(
            isinstance(arg, Expression) for arg in args
        ), f"Bad arguments: all arguments must be an Expression: {args!r}"
        self.args = tuple(args)

    def __str__(self):
        args = self.args
        if len(args) == 1:
            if self.isliteral:
                return f"{self.operator}{args[0]}"
            return f"{self.operator}({args[0]})"

        args_str = []
        for arg in args:
            if arg.isliteral:
                args_str.append(str(arg))
            else:
                args_str.append(f"({arg})")

        return self.operator.join(args_str)

    def __repr__(self):
        args = ", ".join(map(repr, self.args))
        return f"{self.__class__.__name__}({args})"

    def pretty(self, indent=0, debug=False):
        """
        Return a pretty formatted representation of self as an indented tree.

        If debug is True, also prints debug information for each expression arg.

        For example:

        >>> print(BooleanAlgebra().parse(
        ...    u'not a and not b and not (a and ba and c) and c or c').pretty())
        OR(
          AND(
            NOT(Symbol('a')),
            NOT(Symbol('b')),
            NOT(
              AND(
                Symbol('a'),
                Symbol('ba'),
                Symbol('c')
              )
            ),
            Symbol('c')
          ),
          Symbol('c')
        )
        """
        debug_details = ""
        if debug:
            debug_details += f"<isliteral={self.isliteral!r}, iscanonical={self.iscanonical!r}"
            identity = getattr(self, "identity", None)
            if identity is not None:
                debug_details += f", identity={identity!r}"

            annihilator = getattr(self, "annihilator", None)
            if annihilator is not None:
                debug_details += f", annihilator={annihilator!r}"

            dual = getattr(self, "dual", None)
            if dual is not None:
                debug_details += f", dual={dual!r}"
            debug_details += ">"
        cls = self.__class__.__name__
        args = [a.pretty(indent=indent + 2, debug=debug) for a in self.args]
        pfargs = ",\n".join(args)
        cur_indent = " " * indent
        new_line = "" if self.isliteral else "\n"
        return f"{cur_indent}{cls}({debug_details}{new_line}{pfargs}\n{cur_indent})"


class NOT(Function):
    """
    Boolean NOT operation.

    The NOT operation takes exactly one argument. If this argument is a Symbol
    the resulting expression is also called a literal.

    The operator "~" can be used as abbreviation for NOT, e.g. instead of NOT(x)
    one can write ~x (where x is some boolean expression). Also for printing "~"
    is used for better readability.

    You can subclass to define alternative string representation.

    For example:

    >>> class NOT2(NOT):
    ...     def __init__(self, *args):
    ...         super(NOT2, self).__init__(*args)
    ...         self.operator = '!'
    """

    def __init__(self, arg1):
        super(NOT, self).__init__(arg1)
        self.isliteral = isinstance(self.args[0], Symbol)
        self.operator = "~"

    def literalize(self):
        """
        Return an expression where NOTs are only occurring as literals.
        """
        expr = self.demorgan()
        if isinstance(expr, self.__class__):
            return expr
        return expr.literalize()

    def simplify(self):
        """
        Return a simplified expr in canonical form.

        This means double negations are canceled out and all contained boolean
        objects are in their canonical form.
        """
        if self.iscanonical:
            return self

        expr = self.cancel()
        if not isinstance(expr, self.__class__):
            return expr.simplify()

        if expr.args[0] in (
            self.TRUE,
            self.FALSE,
        ):
            return expr.args[0].dual

        expr = self.__class__(expr.args[0].simplify())
        expr.iscanonical = True
        return expr

    def cancel(self):
        """
        Cancel itself and following NOTs as far as possible.
        Returns the simplified expression.
        """
        expr = self
        while True:
            arg = expr.args[0]
            if not isinstance(arg, self.__class__):
                return expr
            expr = arg.args[0]
            if not isinstance(expr, self.__class__):
                return expr

    def demorgan(self):
        """
        Return a expr where the NOT function is moved inward.
        This is achieved by canceling double NOTs and using De Morgan laws.
        """
        expr = self.cancel()
        if expr.isliteral or not isinstance(expr, self.NOT):
            return expr
        op = expr.args[0]
        return op.dual(*(self.__class__(arg).cancel() for arg in op.args))

    def __call__(self, **kwargs):
        """
        Return the evaluated (negated) value for this function.
        """
        return not self.args[0](**kwargs)

    def __lt__(self, other):
        return self.args[0] < other

    def pretty(self, indent=1, debug=False):
        """
        Return a pretty formatted representation of self.
        Include additional debug details if `debug` is True.
        """
        debug_details = ""
        if debug:
            debug_details += f"<isliteral={self.isliteral!r}, iscanonical={self.iscanonical!r}>"
        if self.isliteral:
            pretty_literal = self.args[0].pretty(indent=0, debug=debug)
            return (" " * indent) + f"{self.__class__.__name__}({debug_details}{pretty_literal})"
        else:
            return super(NOT, self).pretty(indent=indent, debug=debug)


class DualBase(Function):
    """
    Base class for AND and OR function.

    This class uses the duality principle to combine similar methods of AND
    and OR. Both operations take two or more arguments and can be created using
    "|" for OR and "&" for AND.
    """

    _pyoperator = None

    def __init__(self, arg1, arg2, *args):
        super(DualBase, self).__init__(arg1, arg2, *args)

        # identity element for the specific operation.
        # This will be TRUE for the AND operation and FALSE for the OR operation.
        self.identity = None

        # annihilator element for this function.
        # This will be FALSE for the AND operation and TRUE for the OR operation.
        self.annihilator = None

        # dual class of this function.
        # This means OR.dual returns AND and AND.dual returns OR.
        self.dual = None

    def __contains__(self, expr):
        """
        Test if expr is a subterm of this expression.
        """
        if expr in self.args:
            return True

        if isinstance(expr, self.__class__):
            return all(arg in self.args for arg in expr.args)

    def simplify(self, sort=True):
        """
        Return a new simplified expression in canonical form from this
        expression.

        For simplification of AND and OR fthe ollowing rules are used
        recursively bottom up:

        - Associativity (output does not contain same operations nested)::

            (A & B) & C = A & (B & C) = A & B & C
            (A | B) | C = A | (B | C) = A | B | C
         
         
        - Annihilation::

            A & 0 = 0, A | 1 = 1

        - Idempotence (e.g. removing duplicates)::

            A & A = A, A | A = A

        - Identity::

            A & 1 = A, A | 0 = A

        - Complementation::

            A & ~A = 0, A | ~A = 1

        - Elimination::

            (A & B) | (A & ~B) = A, (A | B) & (A | ~B) = A

        - Absorption::

            A & (A | B) = A, A | (A & B) = A

        - Negative absorption::

            A & (~A | B) = A & B, A | (~A & B) = A | B

        - Commutativity (output is always sorted)::

            A & B = B & A, A | B = B | A

        Other boolean objects are also in their canonical form.
        """
        # TODO: Refactor DualBase.simplify into different "sub-evals".

        # If self is already canonical do nothing.
        if self.iscanonical:
            return self

        # Otherwise bring arguments into canonical form.
        args = [arg.simplify() for arg in self.args]

        # Create new instance of own class with canonical args.
        # TODO: Only create new class if some args changed.
        expr = self.__class__(*args)

        # Literalize before doing anything, this also applies De Morgan's Law
        expr = expr.literalize()

        # Associativity:
        #     (A & B) & C = A & (B & C) = A & B & C
        #     (A | B) | C = A | (B | C) = A | B | C
        expr = expr.flatten()

        # Annihilation: A & 0 = 0, A | 1 = 1
        if self.annihilator in expr.args:
            return self.annihilator

        # Idempotence: A & A = A, A | A = A
        # this boils down to removing duplicates
        args = []
        for arg in expr.args:
            if arg not in args:
                args.append(arg)
        if len(args) == 1:
            return args[0]

        # Identity: A & 1 = A, A | 0 = A
        if self.identity in args:
            args.remove(self.identity)
            if len(args) == 1:
                return args[0]

        # Complementation: A & ~A = 0, A | ~A = 1
        for arg in args:
            if self.NOT(arg) in args:
                return self.annihilator

        # Elimination: (A & B) | (A & ~B) = A, (A | B) & (A | ~B) = A
        i = 0
        while i < len(args) - 1:
            j = i + 1
            ai = args[i]
            if not isinstance(ai, self.dual):
                i += 1
                continue
            while j < len(args):
                aj = args[j]
                if not isinstance(aj, self.dual) or len(ai.args) != len(aj.args):
                    j += 1
                    continue

                # Find terms where only one arg is different.
                negated = None
                for arg in ai.args:
                    # FIXME: what does this pass Do?
                    if arg in aj.args:
                        pass
                    elif self.NOT(arg).cancel() in aj.args:
                        if negated is None:
                            negated = arg
                        else:
                            negated = None
                            break
                    else:
                        negated = None
                        break

                # If the different arg is a negation simplify the expr.
                if negated is not None:
                    # Cancel out one of the two terms.
                    del args[j]
                    aiargs = list(ai.args)
                    aiargs.remove(negated)
                    if len(aiargs) == 1:
                        args[i] = aiargs[0]
                    else:
                        args[i] = self.dual(*aiargs)

                    if len(args) == 1:
                        return args[0]
                    else:
                        # Now the other simplifications have to be redone.
                        return self.__class__(*args).simplify()
                j += 1
            i += 1

        # Absorption: A & (A | B) = A, A | (A & B) = A
        # Negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B
        args = self.absorb(args)
        if len(args) == 1:
            return args[0]

        # Commutativity: A & B = B & A, A | B = B | A
        if sort:
            args.sort()

        # Create new (now canonical) expression.
        expr = self.__class__(*args)
        expr.iscanonical = True
        return expr

    def flatten(self):
        """
        Return a new expression where nested terms of this expression are
        flattened as far as possible.

        E.g.::

            A & (B & C) becomes A & B & C.
        """
        args = list(self.args)
        i = 0
        for arg in self.args:
            if isinstance(arg, self.__class__):
                args[i : i + 1] = arg.args
                i += len(arg.args)
            else:
                i += 1

        return self.__class__(*args)

    def absorb(self, args):
        """
        Given an `args` sequence of expressions, return a new list of expression
        applying absorption and negative absorption.

        See https://en.wikipedia.org/wiki/Absorption_law

        Absorption::

            A & (A | B) = A, A | (A & B) = A

        Negative absorption::

            A & (~A | B) = A & B, A | (~A & B) = A | B
        """
        args = list(args)
        if not args:
            args = list(self.args)
        i = 0
        while i < len(args):
            absorber = args[i]
            j = 0
            while j < len(args):
                if j == i:
                    j += 1
                    continue
                target = args[j]
                if not isinstance(target, self.dual):
                    j += 1
                    continue

                # Absorption
                if absorber in target:
                    del args[j]
                    if j < i:
                        i -= 1
                    continue

                # Negative absorption
                neg_absorber = self.NOT(absorber).cancel()
                if neg_absorber in target:
                    b = target.subtract(neg_absorber, simplify=False)
                    if b is None:
                        del args[j]
                        if j < i:
                            i -= 1
                        continue
                    else:
                        args[j] = b
                        j += 1
                        continue

                if isinstance(absorber, self.dual):
                    remove = None
                    for arg in absorber.args:
                        narg = self.NOT(arg).cancel()
                        if arg in target.args:
                            pass
                        elif narg in target.args:
                            if remove is None:
                                remove = narg
                            else:
                                remove = None
                                break
                        else:
                            remove = None
                            break
                    if remove is not None:
                        args[j] = target.subtract(remove, simplify=True)
                j += 1
            i += 1

        return args

    def subtract(self, expr, simplify):
        """
        Return a new expression where the `expr` expression has been removed
        from this expression if it exists.
        """
        args = self.args
        if expr in self.args:
            args = list(self.args)
            args.remove(expr)
        elif isinstance(expr, self.__class__):
            if all(arg in self.args for arg in expr.args):
                args = tuple(arg for arg in self.args if arg not in expr)
        if len(args) == 0:
            return None
        if len(args) == 1:
            return args[0]

        newexpr = self.__class__(*args)
        if simplify:
            newexpr = newexpr.simplify()
        return newexpr

    def distributive(self):
        """
        Return a term where the leading AND or OR terms are switched.

        This is done by applying the distributive laws::

            A & (B|C) = (A&B) | (A&C)
            A | (B&C) = (A|B) & (A|C)
        """
        dual = self.dual
        args = list(self.args)
        for i, arg in enumerate(args):
            if isinstance(arg, dual):
                args[i] = arg.args
            else:
                args[i] = (arg,)

        prod = itertools.product(*args)
        args = tuple(self.__class__(*arg).simplify() for arg in prod)

        if len(args) == 1:
            return args[0]
        else:
            return dual(*args)

    def __lt__(self, other):
        comparator = Expression.__lt__(self, other)
        if comparator is not NotImplemented:
            return comparator

        if isinstance(other, self.__class__):
            lenself = len(self.args)
            lenother = len(other.args)
            for i in range(min(lenself, lenother)):
                if self.args[i] == other.args[i]:
                    continue

                comparator = self.args[i] < other.args[i]
                if comparator is not NotImplemented:
                    return comparator

            if lenself != lenother:
                return lenself < lenother
        return NotImplemented

    def __call__(self, **kwargs):
        """
        Return the evaluation of this expression by calling each of its arg as
        arg(**kwargs) and applying its corresponding Python operator (and or or)
        to the results.

        Reduce is used as in e.g. AND(a, b, c, d) == AND(a, AND(b, AND(c, d)))
        ore.g. OR(a, b, c, d) == OR(a, OR(b, OR(c, d)))
        """
        return reduce(self._pyoperator, (a(**kwargs) for a in self.args))


class AND(DualBase):
    """
    Boolean AND operation, taking two or more arguments.

    It can also be created by using "&" between two boolean expressions.

    You can subclass to define alternative string representation by overriding
    self.operator.
    
    For example:

    >>> class AND2(AND):
    ...     def __init__(self, *args):
    ...         super(AND2, self).__init__(*args)
    ...         self.operator = 'AND'
    """

    _pyoperator = and_operator

    def __init__(self, arg1, arg2, *args):
        super(AND, self).__init__(arg1, arg2, *args)
        self.sort_order = 10
        self.identity = self.TRUE
        self.annihilator = self.FALSE
        self.dual = self.OR
        self.operator = "&"


class OR(DualBase):
    """
    Boolean OR operation, taking two or more arguments

    It can also be created by using "|" between two boolean expressions.

    You can subclass to define alternative string representation by overriding
    self.operator.

    For example:

    >>> class OR2(OR):
    ...     def __init__(self, *args):
    ...         super(OR2, self).__init__(*args)
    ...         self.operator = 'OR'
    """

    _pyoperator = or_operator

    def __init__(self, arg1, arg2, *args):
        super(OR, self).__init__(arg1, arg2, *args)
        self.sort_order = 25
        self.identity = self.FALSE
        self.annihilator = self.TRUE
        self.dual = self.AND
        self.operator = "|"

if __name__ == "__main__":
    isT=True
    try:
        algebra1 = BooleanAlgebra()
        algebra2 = BooleanAlgebra()
        a = algebra1.Symbol("a")
        b = algebra1.Symbol("b")

        if not (a & (a | b)).simplify() == a or \
        not (a | (a & b)).simplify() == a or \
        not (a & (~a | b)).simplify() == a & b or \
        not (a | (~a & b)).simplify() == a | b:
            isT=False
    except:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("D:/fse/python_test/repos/bastikr---boolean/data_passk_platform1/62b46746d2f69a53b4661722/"):
    #     f = open("D:/fse/python_test/repos/bastikr---boolean/data_passk_platform1/62b46746d2f69a53b4661722/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][0]["bytes"], bytes):
    #         object_class = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         object_class = content["input"]["args"][0]["bytes"]
    #     print(object_class)
    #     temp_class=DualBase(Symbol("a"),Symbol("b"))
    #     temp_class.dual = OR
    #     print(args1)
    #     res0 = temp_class.absorb(args1)
    #     print(res0)
    #     print(content['output'][0])
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/rak-n-rok---Krake/krake/krake/controller/kubernetes/hooks_on_passk_validte.py
"""This module defines the Hook Dispatcher and listeners for registering and
executing hooks. Hook Dispatcher emits hooks based on :class:`Hook` attributes which
define when the hook will be executed.

"""
import asyncio
import logging
import random
from base64 import b64encode
from collections import defaultdict
from contextlib import suppress
from copy import deepcopy
from datetime import datetime
from functools import reduce
from operator import getitem
from enum import Enum, auto
from inspect import iscoroutinefunction
from OpenSSL import crypto
from typing import NamedTuple

import yarl
from aiohttp import ClientConnectorError
import sys
sys.path.append("/home/travis/builds/repos/rak-n-rok---Krake/krake")

from krake.controller import Observer
from krake.controller.kubernetes.client import KubernetesClient, InvalidManifestError
from krake.controller.kubernetes.tosca import ToscaParser, ToscaParserException
from krake.utils import camel_to_snake_case, get_kubernetes_resource_idx
from kubernetes_asyncio.client.rest import ApiException
from kubernetes_asyncio.client.api_client import ApiClient
from kubernetes_asyncio import client
from krake.data.kubernetes import (
    ClusterState,
    Application,
    ApplicationState,
    ContainerHealth,
    Cluster,
    ClusterNodeCondition,
    ClusterNode,
    ClusterNodeStatus,
    ClusterNodeMetadata,
)
from yarl import URL
from secrets import token_urlsafe

from kubernetes_asyncio.client import (
    Configuration,
    V1Secret,
    V1EnvVar,
    V1VolumeMount,
    V1Volume,
    V1SecretKeySelector,
    V1EnvVarSource,
)
from kubernetes_asyncio.config.kube_config import KubeConfigLoader

logger = logging.getLogger(__name__)


class HookType(Enum):
    ResourcePreCreate = auto()
    ResourcePostCreate = auto()
    ResourcePreUpdate = auto()
    ResourcePostUpdate = auto()
    ResourcePreDelete = auto()
    ResourcePostDelete = auto()
    ApplicationToscaTranslation = auto()
    ApplicationMangling = auto()
    ApplicationPreMigrate = auto()
    ApplicationPostMigrate = auto()
    ApplicationPreReconcile = auto()
    ApplicationPostReconcile = auto()
    ApplicationPreDelete = auto()
    ApplicationPostDelete = auto()
    ClusterCreation = auto()
    ClusterDeletion = auto()


class HookDispatcher(object):
    """Simple wrapper around a registry of handlers associated to :class:`Hook`
     attributes. Each :class:`Hook` attribute defines when the handler will be
     executed.

    Listeners for certain hooks can be registered via :meth:`on`. Registered
    listeners are executed via :meth:`hook`.

    Example:
        .. code:: python

        listen = HookDispatcher()

        @listen.on(HookType.PreApply)
        def to_perform_before_app_creation(app, cluster, resource, controller):
            # Do Stuff

        @listen.on(HookType.PostApply)
        def another_to_perform_after_app_creation(app, cluster, resource, resp):
            # Do Stuff

        @listen.on(HookType.PostDelete)
        def to_perform_after_app_deletion(app, cluster, resource, resp):
            # Do Stuff

    """

    def __init__(self):
        self.registry = defaultdict(list)

    def on(self, hook):
        """Decorator function to add a new handler to the registry.

        Args:
            hook (HookType): Hook attribute for which to register the handler.

        Returns:
            callable: Decorator for registering listeners for the specified
            hook.

        """

        def decorator(handler):
            self.registry[hook].append(handler)

            return handler

        return decorator

    async def hook(self, hook, **kwargs):
        """Execute the list of handlers associated to the provided :class:`Hook`
        attribute.

        Args:
            hook (HookType): The hook attribute for which to execute handlers.

        """
        try:
            handlers = self.registry[hook]
        except KeyError:
            pass
        else:
            for handler in handlers:
                if iscoroutinefunction(handler):
                    await handler(**kwargs)
                else:
                    handler(**kwargs)


listen = HookDispatcher()


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
async def register_service(app, cluster, resource, response):
    """Register endpoint of Kubernetes Service object on creation and update.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        cluster (krake.data.kubernetes.Cluster): The cluster on which the
            application is running
        resource (dict): Kubernetes object description as specified in the
            specification of the application.
        response (kubernetes_asyncio.client.V1Service): Response of the
            Kubernetes API

    """
    if resource["kind"] != "Service":
        return

    service_name = resource["metadata"]["name"]

    if response.spec and response.spec.type == "LoadBalancer":
        # For a "LoadBalancer" type of Service, an external IP is given in the cluster
        # by a load balancer controller to the service. In this case, the "port"
        # specified in the spec is reachable from the outside.
        if (
            not response.status.load_balancer
            or not response.status.load_balancer.ingress
        ):
            # When a "LoadBalancer" type of service is created, the IP is given by an
            # additional controller (e.g. a controller that requests a floating IP to an
            # OpenStack infrastructure). This process can take some time, but the
            # Service itself already exist before the IP is assigned. In the case of an
            # error with the controller, the IP is also not given. This "<pending>" IP
            # just expresses that the Service exists, but the IP is not ready yet.
            external_ip = "<pending>"
        else:
            external_ip = response.status.load_balancer.ingress[0].ip

        if not response.spec.ports:
            external_port = "<pending>"
        else:
            external_port = response.spec.ports[0].port
        app.status.services[service_name] = f"{external_ip}:{external_port}"
        return

    node_port = None
    # Ensure that ports are specified
    if response.spec and response.spec.ports:
        node_port = response.spec.ports[0].node_port

    # If the service does not have a node port, remove a potential reference
    # and return.
    if node_port is None:
        try:
            del app.status.services[service_name]
        except KeyError:
            pass
        return

    # Determine URL of Kubernetes cluster API
    loader = KubeConfigLoader(cluster.spec.kubeconfig)
    config = Configuration()
    await loader.load_and_set(config)
    cluster_url = yarl.URL(config.host)

    app.status.services[service_name] = f"{cluster_url.host}:{node_port}"


@listen.on(HookType.ResourcePostDelete)
async def unregister_service(app, resource, **kwargs):
    """Unregister endpoint of Kubernetes Service object on deletion.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        resource (dict): Kubernetes object description as specified in the
            specification of the application.

    """
    if resource["kind"] != "Service":
        return

    service_name = resource["metadata"]["name"]
    try:
        del app.status.services[service_name]
    except KeyError:
        pass


@listen.on(HookType.ResourcePostDelete)
async def remove_resource_from_last_observed_manifest(app, resource, **kwargs):
    """Remove a given resource from the last_observed_manifest after its deletion

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        resource (dict): Kubernetes object description as specified in the
            specification of the application.

    """
    try:
        idx = get_kubernetes_resource_idx(app.status.last_observed_manifest, resource)
    except IndexError:
        return

    app.status.last_observed_manifest.pop(idx)


def update_last_applied_manifest_dict_from_resp(
    last_applied_manifest, observer_schema, response
):
    """Together with :func:``update_last_applied_manifest_list_from_resp``, this
    function is called recursively to update a partial ``last_applied_manifest``
    from a partial Kubernetes response

    Args:
        last_applied_manifest (dict): partial ``last_applied_manifest`` being
            updated
        observer_schema (dict): partial ``observer_schema``
        response (dict): partial response from the Kubernetes API.

    Raises:
        KeyError: If the observed field is not present in the Kubernetes response

    This function go through all observed fields, and initialized their value in
    last_applied_manifest if they are not yet present

    """
    for key, value in observer_schema.items():

        # Keys in the response are in camelCase
        camel_key = camel_to_snake_case(key)

        if camel_key not in response:
            # An observed key should always be present in the k8s response
            raise KeyError(
                f"Observed key {camel_key} is not present in response {response}"
            )

        if isinstance(value, dict):
            if key not in last_applied_manifest:
                # The dictionary is observed, but not present in
                # last_applied_manifest
                last_applied_manifest[key] = {}

            update_last_applied_manifest_dict_from_resp(
                last_applied_manifest[key], observer_schema[key], response[camel_key]
            )

        elif isinstance(value, list):
            if key not in last_applied_manifest:
                # The list is observed, but not present in last_applied_manifest
                last_applied_manifest[key] = []

            update_last_applied_manifest_list_from_resp(
                last_applied_manifest[key], observer_schema[key], response[camel_key]
            )

        elif key not in last_applied_manifest:
            # If key not present in last_applied_manifest, and value is neither a
            # dict nor a list, simply add it.
            last_applied_manifest[key] = response[camel_key]


def update_last_applied_manifest_list_from_resp(
    last_applied_manifest, observer_schema, response
):
    """Together with :func:``update_last_applied_manifest_dict_from_resp``, this
    function is called recursively to update a partial ``last_applied_manifest``
    from a partial Kubernetes response

    Args:
        last_applied_manifest (list): partial ``last_applied_manifest`` being
            updated
        observer_schema (list): partial ``observer_schema``
        response (list): partial response from the Kubernetes API.

    This function go through all observed fields, and initialized their value in
    last_applied_manifest if they are not yet present

    """
    # Looping over the observed resource, except the last element which is the
    # special control dictionary
    for idx, val in enumerate(observer_schema[:-1]):

        if idx >= len(response):
            # Element is observed but not present in k8s response, so following
            # elements will also not exist.
            #
            # This doesn't raise an Exception as observing the element of a list
            # doesn't ensure its presence. The list length is controlled by the
            # special control dictionary
            return

        if isinstance(val, dict):
            if idx >= len(last_applied_manifest):
                # The dict is observed, but not present in last_applied_manifest
                last_applied_manifest.append({})

            update_last_applied_manifest_dict_from_resp(
                last_applied_manifest[idx], observer_schema[idx], response[idx]
            )

        elif isinstance(response[idx], list):
            if idx >= len(last_applied_manifest):
                # The list is observed, but not present in last_applied_manifest
                last_applied_manifest.append([])

            update_last_applied_manifest_list_from_resp(
                last_applied_manifest[idx], observer_schema[idx], response[idx]
            )

        elif idx >= len(last_applied_manifest):
            # Element is not yet present in last_applied_manifest. Adding it.
            last_applied_manifest.append(response[idx])


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
def update_last_applied_manifest_from_resp(app, response, **kwargs):
    """Hook run after the creation or update of an application in order to update the
    `status.last_applied_manifest` using the k8s response.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        response (kubernetes_asyncio.client.V1Status): Response of the Kubernetes API

    After a Kubernetes resource has been created/updated, the
    `status.last_applied_manifest` has to be updated. All fields already initialized
    (either from the mangling of `spec.manifest`, or by a previous call to this
    function) should be left untouched. Only observed fields which are not present in
    `status.last_applied_manifest` should be initialized.

    """

    if isinstance(response, dict):
        # The Kubernetes API couldn't deserialize the k8s response into an object
        resp = response
    else:
        # The Kubernetes API deserialized the k8s response into an object
        resp = response.to_dict()

    idx_applied = get_kubernetes_resource_idx(app.status.last_applied_manifest, resp)

    idx_observed = get_kubernetes_resource_idx(app.status.mangled_observer_schema, resp)

    update_last_applied_manifest_dict_from_resp(
        app.status.last_applied_manifest[idx_applied],
        app.status.mangled_observer_schema[idx_observed],
        resp,
    )


@listen.on(HookType.ResourcePostCreate)
@listen.on(HookType.ResourcePostUpdate)
def update_last_observed_manifest_from_resp(app, response, **kwargs):
    """Handler to run after the creation or update of a Kubernetes resource to update
    the last_observed_manifest from the response of the Kubernetes API.

    Args:
        app (krake.data.kubernetes.Application): Application the service belongs to
        response (kubernetes_asyncio.client.V1Service): Response of the
            Kubernetes API

    The target last_observed_manifest holds the value of all observed fields plus the
    special control dictionaries for the list length

    """
    if isinstance(response, dict):
        # The Kubernetes API couldn't deserialize the k8s response into an object
        resp = response
    else:
        # The Kubernetes API deserialized the k8s response into an object
        resp = response.to_dict()

    try:
        idx_observed = get_kubernetes_resource_idx(
            app.status.mangled_observer_schema,
            resp,
        )
    except IndexError:
        # All created resources should be observed
        raise

    try:
        idx_last_observed = get_kubernetes_resource_idx(
            app.status.last_observed_manifest,
            resp,
        )
    except IndexError:
        # If the resource is not yes present in last_observed_manifest, append it.
        idx_last_observed = len(app.status.last_observed_manifest)
        app.status.last_observed_manifest.append({})

    # Overwrite the last_observed_manifest for this resource
    app.status.last_observed_manifest[
        idx_last_observed
    ] = update_last_observed_manifest_dict(
        app.status.mangled_observer_schema[idx_observed], resp
    )


def update_last_observed_manifest_dict(observed_resource, response):
    """Together with :func:``update_last_observed_manifest_list``, recursively
    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.

    Args:
        observed_resource (dict): The schema to observe for the partial given resource
        response (dict): The partial Kubernetes response for this resource.

    Raises:
        KeyError: If an observed key is not present in the Kubernetes response

    Returns:
        dict: The dictionary of observed keys and their value

    Get the value of all observed fields from the Kubernetes response
    """
    res = {}
    for key, value in observed_resource.items():

        camel_key = camel_to_snake_case(key)
        if camel_key not in response:
            raise KeyError(
                f"Observed key {camel_key} is not present in response {response}"
            )

        if isinstance(value, dict):
            res[key] = update_last_observed_manifest_dict(value, response[camel_key])

        elif isinstance(value, list):
            res[key] = update_last_observed_manifest_list(value, response[camel_key])

        else:
            res[key] = response[camel_key]

    return res


def update_last_observed_manifest_list(observed_resource, response):
    """Together with :func:``update_last_observed_manifest_dict``, recursively
    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.

    Args:
        observed_resource (list): the schema to observe for the partial given resource
        response (list): the partial Kubernetes response for this resource.

    Returns:
        list: The list of observed elements, plus the special list length control
            dictionary

    Get the value of all observed elements from the Kubernetes response
    """

    if not response:
        return [{"observer_schema_list_current_length": 0}]

    res = []
    # Looping over the observed resource, except the last element which is the special
    # control dictionary
    for idx, val in enumerate(observed_resource[:-1]):

        if idx >= len(response):
            # Element is not present in the Kubernetes response, nothing more to do
            break

        if type(response[idx]) is dict:
            res.append(update_last_observed_manifest_dict(val, response[idx]))

        elif type(response[idx]) is list:
            res.append(update_last_observed_manifest_list(val, response[idx]))

        else:
            res.append(response[idx])

    # Append the special control dictionary to the list
    res.append({"observer_schema_list_current_length": len(response)})

    return res


def update_last_applied_manifest_dict_from_spec(
    resource_status_new, resource_status_old, resource_observed
):
    """Together with :func:``update_last_applied_manifest_list_from_spec``, this
    function is called recursively to update a partial ``last_applied_manifest``

    Args:
        resource_status_new (dict): partial ``last_applied_manifest`` being updated
        resource_status_old (dict): partial of the current ``last_applied_manifest``
        resource_observed (dict): partial observer_schema for the manifest file
            being updated

    """
    for key, value in resource_observed.items():

        if key not in resource_status_old:
            continue

        if key in resource_status_new:

            if isinstance(value, dict):
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            elif isinstance(value, list):
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

        else:
            # If the key is not present the spec.manifest, we first need to
            # initialize it

            if isinstance(value, dict):
                resource_status_new[key] = {}
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            elif isinstance(value, list):
                resource_status_new[key] = []
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[key],
                    resource_status_old[key],
                    resource_observed[key],
                )

            else:
                resource_status_new[key] = resource_status_old[key]


def update_last_applied_manifest_list_from_spec(
    resource_status_new, resource_status_old, resource_observed
):
    """Together with :func:``update_last_applied_manifest_dict_from_spec``, this
    function is called recursively to update a partial ``last_applied_manifest``

    Args:
        resource_status_new (list): partial ``last_applied_manifest`` being updated
        resource_status_old (list): partial of the current ``last_applied_manifest``
        resource_observed (list): partial observer_schema for the manifest file
            being updated

    """

    # Looping over the observed resource, except the last element which is the
    # special control dictionary
    for idx, val in enumerate(resource_observed[:-1]):

        if idx >= len(resource_status_old):
            # The element in not in the current last_applied_manifest, and neither
            # is the rest of the list
            break

        if idx < len(resource_status_new):
            # The element is present in spec.manifest and in the current
            # last_applied_manifest. Updating observed fields

            if isinstance(val, dict):
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            elif isinstance(val, list):
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

        else:
            # If the element is not present in the spec.manifest, we first have to
            # initialize it.

            if isinstance(val, dict):
                resource_status_new.append({})
                update_last_applied_manifest_dict_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            elif isinstance(val, list):
                resource_status_new.append([])
                update_last_applied_manifest_list_from_spec(
                    resource_status_new[idx],
                    resource_status_old[idx],
                    resource_observed[idx],
                )

            else:
                resource_status_new.append(resource_status_old[idx])


def update_last_applied_manifest_from_spec(app):
    """Update the status.last_applied_manifest of an application from spec.manifests

    Args:
        app (krake.data.kubernetes.Application): Application to update

    This function is called on application creation and updates. The
    last_applied_manifest of an application is initialized as a copy of spec.manifest,
    and is augmented by all known observed fields not yet initialized (i.e. all observed
    fields or resources which are present in the current last_applied_manifest but not
    in the spec.manifest)

    """

    # The new last_applied_manifest is initialized as a copy of the spec.manifest, and
    # augmented by all observed fields which are present in the current
    # last_applied_manifest but not in the original spec.manifest
    new_last_applied_manifest = deepcopy(app.spec.manifest)

    # Loop over observed resources and observed fields, and check if they should be
    # added to the new last_applied_manifest (i.e. present in the current
    # last_applied_manifest but not in spec.manifest)
    for resource_observed in app.status.mangled_observer_schema:

        # If the resource is not present in the current last_applied_manifest, there is
        # nothing to do. Whether the resource was initialized by spec.manifest doesn't
        # matter.
        try:
            idx_status_old = get_kubernetes_resource_idx(
                app.status.last_applied_manifest, resource_observed
            )
        except IndexError:
            continue

        # As the resource is present in the current last_applied_manifest, we need to go
        # through it to check if observed fields should be set to their current value
        # (i.e. fields are present in the current last_applied_manifest, but not in
        # spec.manifest)
        try:
            # Check if the observed resource is present in spec.manifest
            idx_status_new = get_kubernetes_resource_idx(
                new_last_applied_manifest, resource_observed
            )
        except IndexError:
            # The resource is observed but is not present in the spec.manifest.
            # Create an empty resource, which will be augmented in
            # update_last_applied_manifest_dict_from_spec with the observed and known
            # fields.
            new_last_applied_manifest.append({})
            idx_status_new = len(new_last_applied_manifest) - 1

        update_last_applied_manifest_dict_from_spec(
            new_last_applied_manifest[idx_status_new],
            app.status.last_applied_manifest[idx_status_old],
            resource_observed,
        )

    app.status.last_applied_manifest = new_last_applied_manifest


class KubernetesApplicationObserver(Observer):
    """Observer specific for Kubernetes Applications. One observer is created for each
    Application managed by the Controller, but not one per Kubernetes resource
    (Deployment, Service...). If several resources are defined by an Application, they
    are all monitored by the same observer.

    The observer gets the actual status of the resources on the cluster using the
    Kubernetes API, and compare it to the status stored in the API.

    The observer is:
     * started at initial Krake resource creation;

     * deleted when a resource needs to be updated, then started again when it is done;

     * simply deleted on resource deletion.

    Args:
        cluster (krake.data.kubernetes.Cluster): the cluster on which the observed
            Application is created.
        resource (krake.data.kubernetes.Application): the application that will be
            observed.
        on_res_update (coroutine): a coroutine called when a resource's actual status
            differs from the status sent by the database. Its signature is:
            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of
            the resource that is up-to-date with the API. The Observer internal instance
            of the resource to observe will be updated. If the API cannot be contacted,
            ``None`` can be returned. In this case the internal instance of the Observer
            will not be updated.
        time_step (int, optional): how frequently the Observer should watch the actual
            status of the resources.

    """

    def __init__(self, cluster, resource, on_res_update, time_step=2):
        super().__init__(resource, on_res_update, time_step)
        self.cluster = cluster
        self.kubernetes_api = None

    def _set_container_health(self, resource, status):
        if status.container_health is None:
            status.container_health = ContainerHealth()

        if hasattr(resource, 'status') and resource.status is not None:
            if resource.kind == "Pod":
                container_state = resource.status.container_statuses[0].state
                status.container_health.desired_pods = 1
                if container_state.terminated is not None:
                    if resource.spec.restart_policy == "Never":
                        status.state = ApplicationState.DEGRADED
                    else:
                        status.state = ApplicationState.RESTARTING
                    status.container_health.running_pods = 0
                elif container_state.waiting is not None:
                    if container_state.waiting.reason == "CrashLoopBackOff":
                        status.state = ApplicationState.DEGRADED
                        status.container_health.running_pods = 0
                else:
                    status.state = ApplicationState.RUNNING
                    status.container_health.running_pods = 1

            elif (
                resource.kind == "Deployment" or
                resource.kind == "StatefulSet" or
                resource.kind == "ReplicaSet" or
                resource.kind == "DaemonSet"
            ):

                if resource.kind == "DaemonSet":
                    status.container_health.desired_pods = \
                        resource.status.current_number_scheduled
                    if isinstance(resource.status.desired_number_scheduled, int):
                        status.container_health.running_pods = \
                            resource.status.desired_number_scheduled
                    else:
                        status.container_health.running_pods = 0
                else:
                    status.container_health.desired_pods = resource.status.replicas
                    if isinstance(resource.status.ready_replicas, int):
                        status.container_health.running_pods = \
                            resource.status.ready_replicas
                    else:
                        status.container_health.running_pods = 0

                if status.container_health.running_pods != \
                   status.container_health.desired_pods:
                    status.state = ApplicationState.DEGRADED
                else:
                    status.state = ApplicationState.RUNNING

            elif resource.kind == "Job":
                if isinstance(resource.spec.completions, int):
                    status.container_health.desired_pods = resource.spec.completions
                else:
                    status.container_health.desired_pods = 1

                if isinstance(resource.status.active, int):
                    status.container_health.running_pods = resource.status.active
                else:
                    status.container_health.running_pods = 0

                if isinstance(resource.status.succeeded, int):
                    status.container_health.completed_pods = resource.status.succeeded
                else:
                    status.container_health.completed_pods = 0

                if isinstance(resource.status.failed, int):
                    status.container_health.failed_pods = resource.status.failed
                else:
                    status.container_health.failed_pods = 0

    async def poll_resource(self):
        """Fetch the current status of the Application monitored by the Observer.

        Returns:
            krake.data.core.Status: the status object created using information from the
                real world Applications resource.

        """
        app = self.resource

        status = deepcopy(app.status)
        status.last_observed_manifest = []

        # For each observed kubernetes resource of the Application,
        # get its current status on the cluster.
        for desired_resource in app.status.last_applied_manifest:
            kube = KubernetesClient(self.cluster.spec.kubeconfig)
            idx_observed = get_kubernetes_resource_idx(
                app.status.mangled_observer_schema, desired_resource
            )
            observed_resource = app.status.mangled_observer_schema[idx_observed]
            async with kube:
                try:
                    group, version, kind, name, namespace = kube.get_immutables(
                        desired_resource
                    )
                    resource_api = await kube.get_resource_api(group, version, kind)
                    resp = await resource_api.read(kind, name, namespace)
                except (ClientConnectorError, ApiException) as err:
                    if hasattr(err, "status") and err.status == 404:
                        # Resource does not exist
                        continue
                    # Otherwise, log the unexpected error and return the
                    # last known application status
                    logger.debug(err)
                    return app.status

                resource = resp
                self._set_container_health(resource, status)

            observed_manifest = update_last_observed_manifest_dict(
                observed_resource, resp.to_dict()
            )
            status.last_observed_manifest.append(observed_manifest)

        return status


class KubernetesClusterObserver(Observer):
    """Observer specific for Kubernetes Clusters. One observer is created for each
    Cluster managed by the Controller.

    The observer gets the actual status of the cluster using the
    Kubernetes API, and compare it to the status stored in the API.

    The observer is:
     * started at initial Krake resource creation;

     * deleted when a resource needs to be updated, then started again when it is done;

     * simply deleted on resource deletion.

    Args:
        resource (krake.data.kubernetes.Cluster): the cluster which will be observed.
        on_res_update (coroutine): a coroutine called when a resource's actual status
            differs from the status sent by the database. Its signature is:
            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of
            the resource that is up-to-date with the API. The Observer internal instance
            of the resource to observe will be updated. If the API cannot be contacted,
            ``None`` can be returned. In this case the internal instance of the Observer
            will not be updated.
        time_step (int, optional): how frequently the Observer should watch the actual
            status of the resources.

    """

    def __init__(self, resource, on_res_update, time_step=2):
        super().__init__(resource, on_res_update, time_step)

    async def poll_resource(self):
        """Fetch the current status of the Cluster monitored by the Observer.

        Note regarding exceptions handling:
          The current cluster status is fetched by :func:`poll_resource` from its API.
          If the cluster API is shutting down the API server responds with a 503
          (service unavailable, apiserver is shutting down) HTTP response which
          leads to the kubernetes client ApiException. If the cluster's API has been
          successfully shut down and there is an attempt to fetch cluster status,
          the ClientConnectorError is raised instead.
          Therefore, both exceptions should be handled.

        Returns:
            krake.data.core.Status: the status object created using information from the
                real world Cluster.

        """
        cluster = self.resource
        status = deepcopy(cluster.status)
        status.nodes = []
        # For each observed kubernetes cluster registered in Krake,
        # get its current node status.
        loader = KubeConfigLoader(cluster.spec.kubeconfig)
        config = Configuration()
        await loader.load_and_set(config)
        kube = ApiClient(config)

        async with kube as api:
            v1 = client.CoreV1Api(api)
            try:
                response = await v1.list_node()
            except (ClientConnectorError, ApiException) as err:
                # Log the error and set cluster state to OFFLINE
                logger.debug(err)
                status.state = ClusterState.OFFLINE
                return status

            # Fetch nodes conditions
            nodes = []
            for node in response.items:
                conditions = []
                for condition in node.status.conditions:
                    conditions.append(
                        ClusterNodeCondition(
                            message=condition.message,
                            reason=condition.reason,
                            status=condition.status,
                            type=condition.type,
                        )
                    )

                nodes.append(
                    ClusterNode(
                        metadata=ClusterNodeMetadata(name=node.metadata.name),
                        status=ClusterNodeStatus(conditions=conditions),
                    )
                )
            status.nodes = nodes

            # The scheduler is unable to fetch cluster metrics, hence
            # the cluster state should wait for it and the cluster
            # status should not be changed by the observer.
            if status.state == ClusterState.FAILING_METRICS:
                return status

            # Set the cluster state to CONNECTING if the previous state
            # was OFFLINE. It is due to smooth transition from
            # the OFFLINE to ONLINE state.
            if status.state == ClusterState.OFFLINE:
                status.state = ClusterState.CONNECTING
                return status

            for node in status.nodes:
                for condition in node.status.conditions:
                    if (
                        condition.type.lower().endswith("pressure")
                        and condition.status == "True"
                    ):
                        status.state = ClusterState.UNHEALTHY
                        return status

                    if condition.type.lower() == "ready" and condition.status != "True":
                        status.state = ClusterState.NOTREADY
                        return status

            status.state = ClusterState.ONLINE
            return status


@listen.on(HookType.ApplicationPostReconcile)
@listen.on(HookType.ApplicationPostMigrate)
@listen.on(HookType.ClusterCreation)
async def register_observer(controller, resource, start=True, **kwargs):
    """Create an observer for the given Application or Cluster, and start it as a
    background task if wanted.

    If an observer already existed for this Application or Cluster, it is stopped
    and deleted.

    Args:
        controller (KubernetesController): the controller for which the observer will be
            added in the list of working observers.
        resource (krake.data.kubernetes.Application): the Application to observe or
        resource (krake.data.kubernetes.Cluster): the Cluster to observe.
        start (bool, optional): if False, does not start the observer as background
            task.

    """
    if resource.kind == Application.kind:
        cluster = await controller.kubernetes_api.read_cluster(
            namespace=resource.status.running_on.namespace,
            name=resource.status.running_on.name,
        )
        observer = KubernetesApplicationObserver(
            cluster,
            resource,
            controller.on_status_update,
            time_step=controller.observer_time_step,
        )

    elif resource.kind == Cluster.kind:
        observer = KubernetesClusterObserver(
            resource,
            controller.on_status_update,
            time_step=controller.observer_time_step,
        )
    else:
        logger.debug("Unknown resource kind. No observer was registered.", resource)
        return

    logger.debug(f"Start observer for {resource.kind} %r", resource.metadata.name)
    task = None
    if start:
        task = controller.loop.create_task(observer.run())

    controller.observers[resource.metadata.uid] = (observer, task)


@listen.on(HookType.ApplicationPreReconcile)
@listen.on(HookType.ApplicationPreMigrate)
@listen.on(HookType.ApplicationPreDelete)
@listen.on(HookType.ClusterDeletion)
async def unregister_observer(controller, resource, **kwargs):
    """Stop and delete the observer for the given Application or Cluster. If no observer
    is started, do nothing.

    Args:
        controller (KubernetesController): the controller for which the observer will be
            removed from the list of working observers.
        resource (krake.data.kubernetes.Application): the Application whose observer
        will be stopped or
        resource (krake.data.kubernetes.Cluster): the Cluster whose observer will be
        stopped.

    """
    if resource.metadata.uid not in controller.observers:
        return

    logger.debug(f"Stop observer for {resource.kind} {resource.metadata.name}")
    _, task = controller.observers.pop(resource.metadata.uid)
    task.cancel()

    with suppress(asyncio.CancelledError):
        await task


@listen.on(HookType.ApplicationToscaTranslation)
async def translate_tosca(controller, app, **kwargs):
    """Translate a TOSCA template or CSAR archive into a Kubernetes manifest.

    Args:
        controller (KubernetesController): the controller that handles the application
            resource.
        app (krake.data.kubernetes.Application): the Application that could be defined
            by a TOSCA template or a CSAR archive.

    Raises:
        ToscaParserException: If the given application does not contain
         at least one from the following:
         - Kubernetes manifest
         - TOSCA template
         - CSAR archive

    """
    if app.spec.manifest:
        return

    if not app.spec.tosca and not app.spec.csar:
        raise ToscaParserException(
            "Application should be defined by a Kubernetes manifest,"
            " a TOSCA template or a CSAR archive: %r",
            app,
        )
    app.status.state = ApplicationState.TRANSLATING
    await controller.kubernetes_api.update_application_status(
        namespace=app.metadata.namespace, name=app.metadata.name, body=app
    )

    if app.spec.tosca and isinstance(app.spec.tosca, dict):

        manifest = ToscaParser.from_dict(app.spec.tosca).translate_to_manifests()
    else:
        manifest = ToscaParser.from_url(
            app.spec.tosca or app.spec.csar
        ).translate_to_manifests()

    app.spec.manifest = manifest
    await controller.kubernetes_api.update_application(
        namespace=app.metadata.namespace, name=app.metadata.name, body=app
    )


def utc_difference():
    """Get the difference in seconds between the current time and the current UTC time.

    Returns:
        int: the time difference in seconds.

    """
    delta = datetime.now() - datetime.utcnow()
    return delta.seconds


def generate_certificate(config):
    """Create and sign a new certificate using the one defined in the complete hook
    configuration as intermediate certificate.

    Args:
        config (krake.data.config.CompleteHookConfiguration): the configuration of the
            complete hook.

    Returns:
        CertificatePair: the content of the certificate created and its corresponding
            key.

    """
    with open(config.intermediate_src, "rb") as f:
        intermediate_src = crypto.load_certificate(crypto.FILETYPE_PEM, f.read())
    with open(config.intermediate_key_src, "rb") as f:
        intermediate_key_src = crypto.load_privatekey(crypto.FILETYPE_PEM, f.read())

    client_cert = crypto.X509()

    # Set general information
    client_cert.set_version(3)
    client_cert.set_serial_number(random.randint(50000000000000, 100000000000000))
    # If not set before, TLS will not accept to use this certificate in UTC cases, as
    # the server time may be earlier.
    time_offset = utc_difference() * -1
    client_cert.gmtime_adj_notBefore(time_offset)
    client_cert.gmtime_adj_notAfter(1 * 365 * 24 * 60 * 60)

    # Set issuer and subject
    intermediate_subject = intermediate_src.get_subject()
    client_cert.set_issuer(intermediate_subject)
    client_subj = crypto.X509Name(intermediate_subject)
    client_subj.CN = config.hook_user
    client_cert.set_subject(client_subj)

    # Create and set the private key
    client_key = crypto.PKey()
    client_key.generate_key(crypto.TYPE_RSA, 2048)
    client_cert.set_pubkey(client_key)

    client_cert.sign(intermediate_key_src, "sha256")

    cert_dump = crypto.dump_certificate(crypto.FILETYPE_PEM, client_cert).decode()
    key_dump = crypto.dump_privatekey(crypto.FILETYPE_PEM, client_key).decode()
    return CertificatePair(cert=cert_dump, key=key_dump)


def generate_default_observer_schema(app):
    """Generate the default observer schema for each Kubernetes resource present in
    ``spec.manifest`` for which a custom observer schema hasn't been specified.

    Args:
        app (krake.data.kubernetes.Application): The application for which to generate a
            default observer schema
    """

    app.status.mangled_observer_schema = deepcopy(app.spec.observer_schema)

    for resource_manifest in app.spec.manifest:
        try:
            get_kubernetes_resource_idx(
                app.status.mangled_observer_schema, resource_manifest
            )

        except IndexError:
            # Only create a default observer schema, if a custom observer schema hasn't
            # been set by the user.
            app.status.mangled_observer_schema.append(
                generate_default_observer_schema_dict(
                    resource_manifest,
                    first_level=True,
                )
            )


def generate_default_observer_schema_dict(manifest_dict, first_level=False):
    """Together with :func:``generate_default_observer_schema_list``, this function is
    called recursively to generate part of a default ``observer_schema`` from part of a
    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.

    Args:
        manifest_dict (dict): Partial Kubernetes resources
        first_level (bool, optional): If True, indicates that the dictionary represents
            the whole observer schema of a Kubernetes resource

    Returns:
        dict: Generated partial observer_schema

    This function creates a new dictionary from ``manifest_dict`` and replaces all
    non-list and non-dict values by ``None``.

    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a
    resource), the values of the identifying fields are copied from the manifest file.

    """
    observer_schema_dict = {}

    for key, value in manifest_dict.items():

        if isinstance(value, dict):
            observer_schema_dict[key] = generate_default_observer_schema_dict(value)

        elif isinstance(value, list):
            observer_schema_dict[key] = generate_default_observer_schema_list(value)

        else:
            observer_schema_dict[key] = None

    if first_level:
        observer_schema_dict["apiVersion"] = manifest_dict["apiVersion"]
        observer_schema_dict["kind"] = manifest_dict["kind"]
        observer_schema_dict["metadata"]["name"] = manifest_dict["metadata"]["name"]

        if (
            "spec" in manifest_dict
            and "type" in manifest_dict["spec"]
            and manifest_dict["spec"]["type"] == "LoadBalancer"
        ):
            observer_schema_dict["status"] = {"load_balancer": {"ingress": None}}

    return observer_schema_dict


def generate_default_observer_schema_list(manifest_list):
    """Together with :func:``generate_default_observer_schema_dict``, this function is
    called recursively to generate part of a default ``observer_schema`` from part of a
    Kubernetes resource, defined respectively by ``manifest_list`` or ``manifest_dict``.

    Args:
        manifest_list (list): Partial Kubernetes resources

    Returns:
        list: Generated partial observer_schema

    This function creates a new list from ``manifest_list`` and replaces all non-list
    and non-dict elements by ``None``.

    Additionally, it generates the default list control dictionary, using the current
    length of the list as default minimum and maximum values.

    """
    observer_schema_list = []

    for value in manifest_list:

        if isinstance(value, dict):
            observer_schema_list.append(generate_default_observer_schema_dict(value))

        elif isinstance(value, list):
            observer_schema_list.append(generate_default_observer_schema_list(value))

        else:
            observer_schema_list.append(None)

    observer_schema_list.append(
        {
            "observer_schema_list_min_length": len(manifest_list),
            "observer_schema_list_max_length": len(manifest_list),
        }
    )

    return observer_schema_list


@listen.on(HookType.ApplicationMangling)
async def complete(app, api_endpoint, ssl_context, config):
    """Execute application complete hook defined by :class:`Complete`.
    Hook mangles given application and injects complete hooks variables.

    Application complete hook is disabled by default.
    User enables this hook by the --hook-complete argument in rok cli.

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        config (krake.data.config.HooksConfiguration): Complete hook
            configuration.

    """
    if "complete" not in app.spec.hooks:
        return

    # Use the endpoint of the API only if the external endpoint has not been set.
    if config.complete.external_endpoint:
        api_endpoint = config.complete.external_endpoint

    app.status.complete_token = (
        app.status.complete_token if app.status.complete_token else token_urlsafe()
    )

    # Generate only once the certificate and key for a specific Application
    generated_cert = CertificatePair(
        cert=app.status.complete_cert, key=app.status.complete_key
    )
    if ssl_context and generated_cert == (None, None):
        generated_cert = generate_certificate(config.complete)
        app.status.complete_cert = generated_cert.cert
        app.status.complete_key = generated_cert.key

    hook = Complete(
        api_endpoint,
        ssl_context,
        hook_user=config.complete.hook_user,
        cert_dest=config.complete.cert_dest,
        env_token=config.complete.env_token,
        env_url=config.complete.env_url,
    )
    hook.mangle_app(
        app.metadata.name,
        app.metadata.namespace,
        app.status.complete_token,
        app.status.last_applied_manifest,
        config.complete.intermediate_src,
        generated_cert,
        app.status.mangled_observer_schema,
        "complete",
    )


@listen.on(HookType.ApplicationMangling)
async def shutdown(app, api_endpoint, ssl_context, config):
    """Executes an application shutdown hook defined by :class:`Shutdown`.
    The hook mangles the given application and injects shutdown hooks variables.

    Application shutdown hook is disabled by default.
    User enables this hook by the --hook-shutdown argument in rok cli.

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        config (krake.data.config.HooksConfiguration): Shutdown hook
            configuration.

    """
    if "shutdown" not in app.spec.hooks:
        return

    # Use the endpoint of the API only if the external endpoint has not been set.
    if config.shutdown.external_endpoint:
        api_endpoint = config.shutdown.external_endpoint

    app.status.shutdown_token = (
        app.status.shutdown_token if app.status.shutdown_token else token_urlsafe()
    )

    # Generate only once the certificate and key for a specific Application
    generated_cert = CertificatePair(
        cert=app.status.shutdown_cert, key=app.status.shutdown_key
    )
    if ssl_context and generated_cert == (None, None):
        generated_cert = generate_certificate(config.shutdown)
        app.status.shutdown_cert = generated_cert.cert
        app.status.shutdown_key = generated_cert.key

    hook = Shutdown(
        api_endpoint,
        ssl_context,
        hook_user=config.shutdown.hook_user,
        cert_dest=config.shutdown.cert_dest,
        env_token=config.shutdown.env_token,
        env_url=config.shutdown.env_url,
    )
    hook.mangle_app(
        app.metadata.name,
        app.metadata.namespace,
        app.status.shutdown_token,
        app.status.last_applied_manifest,
        config.shutdown.intermediate_src,
        generated_cert,
        app.status.mangled_observer_schema,
        "shutdown",
    )


@listen.on(HookType.ResourcePreDelete)
async def pre_shutdown(controller, app, **kwargs):
    """

    Args:
        app (krake.data.kubernetes.Application): Application object processed
            when the hook is called
    """
    if "shutdown" not in app.spec.hooks:
        return

    return


class SubResource(NamedTuple):
    group: str
    name: str
    body: dict
    path: tuple


class CertificatePair(NamedTuple):
    """Tuple which contains a certificate and its corresponding key.

    Attributes:
        cert (str): content of a certificate.
        key (str): content of the key that corresponds to the certificate.

    """

    cert: str
    key: str


class Hook(object):
    hook_resources = ()

    ca_name = "ca-bundle.pem"
    cert_name = "cert.pem"
    key_name = "key.pem"

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        self.api_endpoint = api_endpoint
        self.ssl_context = ssl_context
        self.hook_user = hook_user
        self.cert_dest = cert_dest
        self.env_token = env_token
        self.env_url = env_url

    def mangle_app(
        self,
        name,
        namespace,
        token,
        last_applied_manifest,
        intermediate_src,
        generated_cert,
        mangled_observer_schema,
        hook_type="",
    ):
        """Mangle a given application and inject complete hook resources and
        sub-resources into the :attr:`last_applied_manifest` object by :meth:`mangle`.
        Also mangle the observer_schema as new resources and sub-resources should
        be observed.

        :attr:`last_applied_manifest` is created as a deep copy of the desired
        application resources, as defined by user. It can be updated by custom hook
        resources or modified by custom hook sub-resources. It is used as a desired
        state for the Krake deployment process.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            token (str): Complete hook authentication token
            last_applied_manifest (list): Application resources
            intermediate_src (str): content of the certificate that is used to sign new
                certificates for the complete hook.
            generated_cert (CertificatePair): tuple that contains the content of the
                new signed certificate for the Application, and the content of its
                corresponding key.
            mangled_observer_schema (list): Observed fields
            hook_type (str, optional): Name of the hook the app should be mangled for

        """

        secret_certs_name = "-".join([name, "krake", hook_type, "secret", "certs"])
        secret_token_name = "-".join([name, "krake", hook_type, "secret", "token"])
        volume_name = "-".join([name, "krake", hook_type, "volume"])
        ca_certs = (
            self.ssl_context.get_ca_certs(binary_form=True)
            if self.ssl_context
            else None
        )

        # Extract all different namespaces
        # FIXME: too many assumptions here: do we create one ConfigMap for each
        #  namespace?
        resource_namespaces = {
            resource["metadata"].get("namespace", "default")
            for resource in last_applied_manifest
        }

        hook_resources = []
        hook_sub_resources = []
        if ca_certs:
            hook_resources.extend(
                [
                    self.secret_certs(
                        secret_certs_name,
                        resource_namespace,
                        intermediate_src=intermediate_src,
                        generated_cert=generated_cert,
                        ca_certs=ca_certs,
                    )
                    for resource_namespace in resource_namespaces
                ]
            )
            hook_sub_resources.extend(
                [*self.volumes(secret_certs_name, volume_name, self.cert_dest)]
            )

        hook_resources.extend(
            [
                self.secret_token(
                    secret_token_name,
                    name,
                    namespace,
                    resource_namespace,
                    self.api_endpoint,
                    token,
                )
                for resource_namespace in resource_namespaces
            ]
        )
        hook_sub_resources.extend(
            [
                *self.env_vars(secret_token_name),
            ]
        )

        self.mangle(
            hook_resources,
            last_applied_manifest,
            mangled_observer_schema,
        )
        self.mangle(
            hook_sub_resources,
            last_applied_manifest,
            mangled_observer_schema,
            is_sub_resource=True,
        )

    def mangle(
        self,
        items,
        last_applied_manifest,
        mangled_observer_schema,
        is_sub_resource=False,
    ):
        """Mangle applications desired state with custom hook resources or
        sub-resources.

        Example:
            .. code:: python

            last_applied_manifest = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Pod',
                    'metadata': {'name': 'test', 'namespace': 'default'},
                    'spec': {'containers': [{'name': 'test'}]}
                }
            ]
            mangled_observer_schema = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Pod',
                    'metadata': {'name': 'test', 'namespace': 'default'},
                    'spec': {
                        'containers': [
                            {'name': None},
                            {
                                'observer_schema_list_max_length': 1,
                                'observer_schema_list_min_length': 1,
                            },
                        ]
                    },
                }
            ]
            hook_resources = [
                {
                    'apiVersion': 'v1',
                    'kind': 'Secret',
                    'metadata': {'name': 'sct', 'namespace': 'default'}
                }
            ]
            hook_sub_resources = [
                SubResource(
                    group='env', name='env', body={'name': 'test', 'value': 'test'},
                    path=(('spec', 'containers'),)
                )
            ]

            mangle(
                hook_resources,
                last_applied_manifest,
                mangled_observer_schema,
            )
            mangle(
                hook_sub_resources,
                last_applied_manifest,
                mangled_observer_schema,
                is_sub_resource=True
            )

            assert last_applied_manifest == [
                {
                    "apiVersion": "v1",
                    "kind": "Pod",
                    "metadata": {"name": "test", 'namespace': 'default'},
                    "spec": {
                        "containers": [
                            {
                                "name": "test",
                                "env": [{"name": "test", "value": "test"}]
                            }
                        ]
                    },
                },
                {"apiVersion": "v1", "kind": "Secret", "metadata": {"name": "sct"}},
            ]

            assert mangled_observer_schema == [
                {
                    "apiVersion": "v1",
                    "kind": "Pod",
                    "metadata": {"name": "test", "namespace": None},
                    "spec": {
                        "containers": [
                            {
                                "name": None,
                                "env": [
                                    {"name": None, "value": None},
                                    {
                                        "observer_schema_list_max_length": 1,
                                        "observer_schema_list_min_length": 1,
                                    },
                                ],
                            },
                            {
                                "observer_schema_list_max_length": 1,
                                "observer_schema_list_min_length": 1,
                            },
                        ]
                    },
                },
                {
                    "apiVersion": "v1",
                    "kind": "Secret",
                    "metadata": {"name": "sct", "namespace": None},
                },
            ]

        Args:
            items (list[SubResource]): Custom hook resources or sub-resources
            last_applied_manifest (list): Application resources
            mangled_observer_schema (list): Observed resources
            is_sub_resource (bool, optional): if False, the function only extend the
                list of Kubernetes resources defined in :attr:`last_applied_manifest`
                with new hook resources. Otherwise, the function injects each new hook
                sub-resource into the :attr:`last_applied_manifest` object
                sub-resources. Defaults to False.

        """

        if not items:
            return

        if not is_sub_resource:
            last_applied_manifest.extend(items)
            for sub_resource in items:
                # Generate the default observer schema for each resource
                mangled_observer_schema.append(
                    generate_default_observer_schema_dict(
                        sub_resource,
                        first_level=True,
                    )
                )
            return

        def inject(sub_resource, sub_resource_to_mangle, observed_resource_to_mangle):
            """Inject a hooks defined sub-resource into a Kubernetes sub-resource.

            Args:
                sub_resource (SubResource): Hook sub-resource that needs to be injected
                    into :attr:`last_applied_manifest`
                sub_resource_to_mangle (object): Kubernetes sub-resources from
                    :attr:`last_applied_manifest` which need to be processed
                observed_resource_to_mangle (dict): partial mangled_observer_schema
                    corresponding to the Kubernetes sub-resource.

            Raises:
                InvalidManifestError: if the sub-resource which will be mangled is not a
                    list or a dict.

            """

            # Create sub-resource group if not present in the Kubernetes sub-resource
            if sub_resource.group not in sub_resource_to_mangle:
                # FIXME: This assumes the subresource group contains a list
                sub_resource_to_mangle.update({sub_resource.group: []})

            # Create sub-resource group if not present in the observed fields
            if sub_resource.group not in observed_resource_to_mangle:
                observed_resource_to_mangle.update(
                    {
                        sub_resource.group: [
                            {
                                "observer_schema_list_min_length": 0,
                                "observer_schema_list_max_length": 0,
                            }
                        ]
                    }
                )

            # Inject sub-resource
            # If sub-resource name is already there update it, if not, append it
            if sub_resource.name in [
                g["name"] for g in sub_resource_to_mangle[sub_resource.group]
            ]:
                # FIXME: Assuming we are dealing with a list
                for idx, item in enumerate(sub_resource_to_mangle[sub_resource.group]):
                    if item["name"]:
                        if hasattr(item, "body"):
                            sub_resource_to_mangle[item.group][idx] = item["body"]
            else:
                sub_resource_to_mangle[sub_resource.group].append(sub_resource.body)

            # Make sure the value is observed
            if sub_resource.name not in [
                g["name"] for g in observed_resource_to_mangle[sub_resource.group][:-1]
            ]:
                observed_resource_to_mangle[sub_resource.group].insert(
                    -1, generate_default_observer_schema_dict(sub_resource.body)
                )
                observed_resource_to_mangle[sub_resource.group][-1][
                    "observer_schema_list_min_length"
                ] += 1
                observed_resource_to_mangle[sub_resource.group][-1][
                    "observer_schema_list_max_length"
                ] += 1

        for resource in last_applied_manifest:
            # Complete hook is applied only on defined Kubernetes resources
            if resource["kind"] not in self.hook_resources:
                continue

            for sub_resource in items:
                sub_resources_to_mangle = None
                idx_observed = get_kubernetes_resource_idx(
                    mangled_observer_schema, resource
                )
                for keys in sub_resource.path:
                    try:
                        sub_resources_to_mangle = reduce(getitem, keys, resource)
                    except KeyError:
                        continue

                    break

                # Create the path to the observed sub-resource, if it doesn't yet exist
                try:
                    observed_sub_resources = reduce(
                        getitem, keys, mangled_observer_schema[idx_observed]
                    )
                except KeyError:
                    Complete.create_path(
                        mangled_observer_schema[idx_observed], list(keys)
                    )
                    observed_sub_resources = reduce(
                        getitem, keys, mangled_observer_schema[idx_observed]
                    )

                if isinstance(sub_resources_to_mangle, list):
                    for idx, sub_resource_to_mangle in enumerate(
                        sub_resources_to_mangle
                    ):

                        # Ensure that each element of the list is observed.
                        idx_observed = idx
                        if idx >= len(observed_sub_resources[:-1]):
                            idx_observed = len(observed_sub_resources[:-1])
                            # FIXME: Assuming each element of the list contains a
                            # dictionary, therefore initializing new elements with an
                            # empty dict
                            observed_sub_resources.insert(-1, {})
                        observed_sub_resource = observed_sub_resources[idx_observed]

                        # FIXME: This is assuming a list always contains dict
                        inject(
                            sub_resource, sub_resource_to_mangle, observed_sub_resource
                        )

                elif isinstance(sub_resources_to_mangle, dict):
                    inject(
                        sub_resource, sub_resources_to_mangle, observed_sub_resources
                    )

                else:
                    message = (
                        f"The sub-resource to mangle {sub_resources_to_mangle!r} has an"
                        "invalid type, should be in '[dict, list]'"
                    )
                    raise InvalidManifestError(message)

    @staticmethod
    def attribute_map(obj):
        """Convert a Kubernetes object to dict based on its attribute mapping

        Example:
            .. code:: python

            from kubernetes_asyncio.client import V1VolumeMount

            d = attribute_map(
                    V1VolumeMount(name="name", mount_path="path")
            )
            assert d == {'mountPath': 'path', 'name': 'name'}

        Args:
            obj (object): Kubernetes object

        Returns:
            dict: Converted Kubernetes object

        """
        return {
            obj.attribute_map[attr]: getattr(obj, attr)
            for attr, _ in obj.to_dict().items()
            if getattr(obj, attr) is not None
        }

    @staticmethod
    def create_path(mangled_observer_schema, keys):
        """Create the path to the observed field in the observer schema.

        When a sub-resource is mangled, it should be observed. This function creates
        the path to the subresource to observe.

        Args:
            mangled_observer_schema (dict): Partial observer schema of a resource
            keys (list): list of keys forming the path to the sub-resource to
                observe

        FIXME: This assumes we are only adding keys to dict. We don't consider lists

        """

        # Unpack the first key first, as it contains the base directory
        key = keys.pop(0)

        # If the key is the last of the list, we reached the end of the path.
        if len(keys) == 0:
            mangled_observer_schema[key] = None
            return

        if key not in mangled_observer_schema:
            mangled_observer_schema[key] = {}
        Hook.create_path(mangled_observer_schema[key], keys)

    def secret_certs(
        self,
        secret_name,
        namespace,
        ca_certs=None,
        intermediate_src=None,
        generated_cert=None,
    ):
        """Create a complete hooks secret resource.

        Complete hook secret stores Krake CAs and client certificates to communicate
        with the Krake API.

        Args:
            secret_name (str): Secret name
            namespace (str): Kubernetes namespace where the Secret will be created.
            ca_certs (list): Krake CA list
            intermediate_src (str): content of the certificate that is used to sign new
                certificates for the complete hook.
            generated_cert (CertificatePair): tuple that contains the content of the
                new signed certificate for the Application, and the content of its
                corresponding key.

        Returns:
            dict: complete hook secret resource

        """
        ca_certs_pem = ""
        for ca_cert in ca_certs:
            x509 = crypto.load_certificate(crypto.FILETYPE_ASN1, ca_cert)
            ca_certs_pem += crypto.dump_certificate(crypto.FILETYPE_PEM, x509).decode()

        # Add the intermediate certificate into the chain
        with open(intermediate_src, "r") as f:
            intermediate_src_content = f.read()
        ca_certs_pem += intermediate_src_content

        data = {
            self.ca_name: self._encode_to_64(ca_certs_pem),
            self.cert_name: self._encode_to_64(generated_cert.cert),
            self.key_name: self._encode_to_64(generated_cert.key),
        }
        return self.secret(secret_name, data, namespace)

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create a hooks secret resource.

        The hook secret stores Krake authentication token
        and hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Complete hook authentication token

        Returns:
            dict: complete hook secret resource

        """
        pass

    def volumes(self, secret_name, volume_name, mount_path):
        """Create complete hooks volume and volume mount sub-resources

        Complete hook volume gives access to hook's secret, which stores
        Krake CAs and client certificates to communicate with the Krake API.
        Complete hook volume mount puts the volume into the application

        Args:
            secret_name (str): Secret name
            volume_name (str): Volume name
            mount_path (list): Volume mount path

        Returns:
            list: List of complete hook volume and volume mount sub-resources

        """
        volume = V1Volume(name=volume_name, secret={"secretName": secret_name})
        volume_mount = V1VolumeMount(name=volume_name, mount_path=mount_path)
        return [
            SubResource(
                group="volumes",
                name=volume.name,
                body=self.attribute_map(volume),
                path=(("spec", "template", "spec"), ("spec",)),
            ),
            SubResource(
                group="volumeMounts",
                name=volume_mount.name,
                body=self.attribute_map(volume_mount),
                path=(
                    ("spec", "template", "spec", "containers"),
                    ("spec", "containers"),  # kind: Pod
                ),
            ),
        ]

    @staticmethod
    def _encode_to_64(string):
        """Compute the base 64 encoding of a string.

        Args:
            string (str): the string to encode.

        Returns:
            str: the result of the encoding.

        """
        return b64encode(string.encode()).decode()

    def secret(self, secret_name, secret_data, namespace, _type="Opaque"):
        """Create a secret resource.

        Args:
            secret_name (str): Secret name
            secret_data (dict): Secret data
            namespace (str): Kubernetes namespace where the Secret will be created.
            _type (str, optional): Secret type. Defaults to Opaque.

        Returns:
            dict: secret resource

        """
        return self.attribute_map(
            V1Secret(
                api_version="v1",
                kind="Secret",
                data=secret_data,
                metadata={"name": secret_name, "namespace": namespace},
                type=_type,
            )
        )

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' hook URL.
        Function needs to be specified for each hook.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application shutdown url

        """
        pass

    def env_vars(self, secret_name):
        """Create the hooks' environment variables sub-resources.
        Function needs to be specified for each hook.

        Creates hook environment variables to store Krake authentication token
        and a hook URL for the given applications.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of shutdown hook environment variables sub-resources

        """
        pass


class Complete(Hook):
    """Mangle given application and inject complete hooks variables into it.

    Hook injects a Kubernetes secret, which stores Krake authentication token
    and the Krake complete hook URL for the given application. The variables
    from Kubernetes secret are imported as environment variables
    into the application resource definition. Only resources defined in
    :args:`hook_resources` can be modified.

    Names of environment variables are defined in the application controller
    configuration file.

    If TLS is enabled on the Krake API, the complete hook injects a Kubernetes secret,
    and it's corresponding volume and volume mount definitions for the Krake CA,
    the client certificate with the right CN, and its key. The directory where the
    secret is mounted is defined in the configuration.

    Args:
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        cert_dest (str, optional): Path of the directory where the CA, client
            certificate and key to the Krake API will be stored.
        env_token (str, optional): Name of the environment variable, which stores Krake
            authentication token.
        env_url (str, optional): Name of the environment variable,
            which stores Krake complete hook URL.

    """

    hook_resources = ("Pod", "Deployment", "ReplicationController")

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        super().__init__(
            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
        )
        self.env_url = env_url

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create complete hooks secret resource.

        Complete hook secret stores Krake authentication token
        and complete hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Complete hook authentication token

        Returns:
            dict: complete hook secret resource

        """
        complete_url = self.create_hook_url(name, namespace, api_endpoint)
        data = {
            self.env_token.lower(): self._encode_to_64(token),
            self.env_url.lower(): self._encode_to_64(complete_url),
        }
        return self.secret(secret_name, data, resource_namespace)

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' complete URL.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application complete url

        """
        api_url = URL(api_endpoint)
        return str(
            api_url.with_path(
                f"/kubernetes/namespaces/{namespace}/applications/{name}/complete"
            )
        )

    def env_vars(self, secret_name):
        """Create complete hooks environment variables sub-resources

        Create complete hook environment variables store Krake authentication token
        and complete hook URL for given application.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of complete hook environment variables sub-resources

        """
        sub_resources = []

        env_token = V1EnvVar(
            name=self.env_token,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(
                            name=secret_name, key=self.env_token.lower()
                        )
                    )
                )
            ),
        )
        env_url = V1EnvVar(
            name=self.env_url,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())
                    )
                )
            ),
        )

        for env in (env_token, env_url):
            sub_resources.append(
                SubResource(
                    group="env",
                    name=env.name,
                    body=self.attribute_map(env),
                    path=(
                        ("spec", "template", "spec", "containers"),
                        ("spec", "containers"),  # kind: Pod
                    ),
                )
            )
        return sub_resources


class Shutdown(Hook):
    """Mangle given application and inject shutdown hooks variables into it.

    Hook injects a Kubernetes secret, which stores Krake authentication token
    and the Krake complete hook URL for the given application. The variables
    from the Kubernetes secret are imported as environment variables
    into the application resource definition. Only resources defined in
    :args:`hook_resources` can be modified.

    Names of environment variables are defined in the application controller
    configuration file.

    If TLS is enabled on the Krake API, the shutdown hook injects a Kubernetes secret,
    and it's corresponding volume and volume mount definitions for the Krake CA,
    the client certificate with the right CN, and its key. The directory where the
    secret is mounted is defined in the configuration.

    Args:
        api_endpoint (str): the given API endpoint
        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint
        cert_dest (str, optional): Path of the directory where the CA, client
            certificate and key to the Krake API will be stored.
        env_token (str, optional): Name of the environment variable, which stores Krake
            authentication token.
        env_url (str, optional): Name of the environment variable,
            which stores Krake complete hook URL.

    """

    hook_resources = ("Pod", "Deployment", "ReplicationController")

    def __init__(
        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
    ):
        super().__init__(
            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url
        )
        self.env_url = env_url

    def secret_token(
        self, secret_name, name, namespace, resource_namespace, api_endpoint, token
    ):
        """Create shutdown hooks secret resource.

        Shutdown hook secret stores Krake authentication token
        and shutdown hook URL for given application.

        Args:
            secret_name (str): Secret name
            name (str): Application name
            namespace (str): Application namespace
            resource_namespace (str): Kubernetes namespace where the
                Secret will be created.
            api_endpoint (str): Krake API endpoint
            token (str): Shutdown hook authentication token

        Returns:
            dict: shutdown hook secret resource

        """
        shutdown_url = self.create_hook_url(name, namespace, api_endpoint)
        data = {
            self.env_token.lower(): self._encode_to_64(token),
            self.env_url.lower(): self._encode_to_64(shutdown_url),
        }
        return self.secret(secret_name, data, resource_namespace)

    @staticmethod
    def create_hook_url(name, namespace, api_endpoint):
        """Create an applications' shutdown URL.

        Args:
            name (str): Application name
            namespace (str): Application namespace
            api_endpoint (str): Krake API endpoint

        Returns:
            str: Application shutdown url

        """
        api_url = URL(api_endpoint)
        return str(
            api_url.with_path(
                f"/kubernetes/namespaces/{namespace}/applications/{name}/shutdown"
            )
        )

    def env_vars(self, secret_name):
        """Create shutdown hooks environment variables sub-resources.

        Creates shutdown hook environment variables to store Krake authentication token
        and a shutdown hook URL for given applications.

        Args:
            secret_name (str): Secret name

        Returns:
            list: List of shutdown hook environment variables sub-resources

        """
        sub_resources = []

        env_resources = []

        env_token = V1EnvVar(
            name=self.env_token,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(
                            name=secret_name, key=self.env_token.lower()
                        )
                    )
                )
            ),
        )
        env_resources.append(env_token)

        env_url = V1EnvVar(
            name=self.env_url,
            value_from=self.attribute_map(
                V1EnvVarSource(
                    secret_key_ref=self.attribute_map(
                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())
                    )
                )
            ),
        )
        env_resources.append(env_url)

        for env in env_resources:
            sub_resources.append(
                SubResource(
                    group="env",
                    name=env.name,
                    body=self.attribute_map(env),
                    path=(
                        ("spec", "template", "spec", "containers"),
                        ("spec", "containers"),  # kind: Pod
                    ),
                )
            )
        return sub_resources

if __name__ == "__main__":
    import dill
    import os
    isT=True
    @listen.on(HookType.ResourcePostUpdate)
    def on_test(dictt):

        return HookType.ResourcePostUpdate, "kkk" ,dictt
    from krake.data.kubernetes import Application
    # aa=Application()
    diccc={"zz":"aa"}
    out1, out2, out3=on_test(diccc)
    if str(out1)!="HookType.ResourcePostUpdate" or out2!="kkk" or out3["zz"]!="aa":
        raise Exception("Result not True!!!")



----------------------------
/home/travis/builds/repos/rak-n-rok---Krake/krake/tests/conftest_base_config_passk_validte.py
def base_config(user, etcd_host="localhost", etcd_port=2379):
    """Creates a configuration with some simple parameters, which have a default value
    that can be set.

    Args:
        user (str): the name of the user for the static authentication
        etcd_host (str): the host for the database.
        etcd_port (int): the port for the database.

    Returns:
        dict: the created configuration.

    """
    return {
        "tls": {
            "enabled": False,
            "cert": "cert_path",
            "key": "key_path",
            "client_ca": "client_ca_path",
        },
        "authentication": {
            "allow_anonymous": True,
            "strategy": {
                "keystone": {"enabled": False, "endpoint": "http://localhost"},
                "keycloak": {
                    "enabled": False,
                    "endpoint": "no_endpoint",
                    "realm": "krake",
                },
                "static": {"enabled": True, "name": user},
            },
            "cors_origin": "http://example.com",
        },
        "authorization": "always-allow",
        "etcd": {"host": etcd_host, "port": etcd_port, "retry_transactions": 0},
        "docs": {"problem_base_url": "http://example.com/problem"},
        "log": {},
    }



if __name__ == "__main__":
    isT=True
    ist1=base_config("yh")=={'tls': {'enabled': False, 'cert': 'cert_path', 'key': 'key_path', 'client_ca': 'client_ca_path'}, 'authentication': {'allow_anonymous': True, 'strategy': {'keystone': {'enabled': False, 'endpoint': 'http://localhost'}, 'keycloak': {'enabled': False, 'endpoint': 'no_endpoint', 'realm': 'krake'}, 'static': {'enabled': True, 'name': 'yh'}}, 'cors_origin': 'http://example.com'}, 'authorization': 'always-allow', 'etcd': {'host': 'localhost', 'port': 2379, 'retry_transactions': 0}, 'docs': {'problem_base_url': 'http://example.com/problem'}, 'log': {}}
    ist2=base_config("whw", "local", 8080)=={'tls': {'enabled': False, 'cert': 'cert_path', 'key': 'key_path', 'client_ca': 'client_ca_path'}, 'authentication': {'allow_anonymous': True, 'strategy': {'keystone': {'enabled': False, 'endpoint': 'http://localhost'}, 'keycloak': {'enabled': False, 'endpoint': 'no_endpoint', 'realm': 'krake'}, 'static': {'enabled': True, 'name': 'whw'}}, 'cors_origin': 'http://example.com'}, 'authorization': 'always-allow', 'etcd': {'host': 'local', 'port': 8080, 'retry_transactions': 0}, 'docs': {'problem_base_url': 'http://example.com/problem'}, 'log': {}}
    if not ist1 or not ist2:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")

----------------------------
/home/travis/builds/repos/pexip---os-python-dateutil/dateutil/tz/_common__fromutc_passk_validte.py
from six import PY2

from functools import wraps

from datetime import datetime, timedelta, tzinfo


ZERO = timedelta(0)

__all__ = ['tzname_in_python2', 'enfold']


def tzname_in_python2(namefunc):
    """Change unicode output into bytestrings in Python 2

    tzname() API changed in Python 3. It used to return bytes, but was changed
    to unicode strings
    """
    if PY2:
        @wraps(namefunc)
        def adjust_encoding(*args, **kwargs):
            name = namefunc(*args, **kwargs)
            if name is not None:
                name = name.encode()

            return name

        return adjust_encoding
    else:
        return namefunc


# The following is adapted from Alexander Belopolsky's tz library
# https://github.com/abalkin/tz
if hasattr(datetime, 'fold'):
    # This is the pre-python 3.6 fold situation
    def enfold(dt, fold=1):
        """
        Provides a unified interface for assigning the ``fold`` attribute to
        datetimes both before and after the implementation of PEP-495.

        :param fold:
            The value for the ``fold`` attribute in the returned datetime. This
            should be either 0 or 1.

        :return:
            Returns an object for which ``getattr(dt, 'fold', 0)`` returns
            ``fold`` for all versions of Python. In versions prior to
            Python 3.6, this is a ``_DatetimeWithFold`` object, which is a
            subclass of :py:class:`datetime.datetime` with the ``fold``
            attribute added, if ``fold`` is 1.

        .. versionadded:: 2.6.0
        """
        return dt.replace(fold=fold)

else:
    class _DatetimeWithFold(datetime):
        """
        This is a class designed to provide a PEP 495-compliant interface for
        Python versions before 3.6. It is used only for dates in a fold, so
        the ``fold`` attribute is fixed at ``1``.

        .. versionadded:: 2.6.0
        """
        __slots__ = ()

        def replace(self, *args, **kwargs):
            """
            Return a datetime with the same attributes, except for those
            attributes given new values by whichever keyword arguments are
            specified. Note that tzinfo=None can be specified to create a naive
            datetime from an aware datetime with no conversion of date and time
            data.

            This is reimplemented in ``_DatetimeWithFold`` because pypy3 will
            return a ``datetime.datetime`` even if ``fold`` is unchanged.
            """
            argnames = (
                'year', 'month', 'day', 'hour', 'minute', 'second',
                'microsecond', 'tzinfo'
            )

            for arg, argname in zip(args, argnames):
                if argname in kwargs:
                    raise TypeError('Duplicate argument: {}'.format(argname))

                kwargs[argname] = arg

            for argname in argnames:
                if argname not in kwargs:
                    kwargs[argname] = getattr(self, argname)

            dt_class = self.__class__ if kwargs.get('fold', 1) else datetime

            return dt_class(**kwargs)

        @property
        def fold(self):
            return 1

    def enfold(dt, fold=1):
        """
        Provides a unified interface for assigning the ``fold`` attribute to
        datetimes both before and after the implementation of PEP-495.

        :param fold:
            The value for the ``fold`` attribute in the returned datetime. This
            should be either 0 or 1.

        :return:
            Returns an object for which ``getattr(dt, 'fold', 0)`` returns
            ``fold`` for all versions of Python. In versions prior to
            Python 3.6, this is a ``_DatetimeWithFold`` object, which is a
            subclass of :py:class:`datetime.datetime` with the ``fold``
            attribute added, if ``fold`` is 1.

        .. versionadded:: 2.6.0
        """
        if getattr(dt, 'fold', 0) == fold:
            return dt

        args = dt.timetuple()[:6]
        args += (dt.microsecond, dt.tzinfo)

        if fold:
            return _DatetimeWithFold(*args)
        else:
            return datetime(*args)


def _validate_fromutc_inputs(f):
    """
    The CPython version of ``fromutc`` checks that the input is a ``datetime``
    object and that ``self`` is attached as its ``tzinfo``.
    """
    @wraps(f)
    def fromutc(self, dt):
        if not isinstance(dt, datetime):
            raise TypeError("fromutc() requires a datetime argument")
        if dt.tzinfo is not self:
            raise ValueError("dt.tzinfo is not self")

        return f(self, dt)

    return fromutc


class _tzinfo(tzinfo):
    """
    Base class for all ``dateutil`` ``tzinfo`` objects.
    """

    def is_ambiguous(self, dt):
        """
        Whether or not the "wall time" of a given datetime is ambiguous in this
        zone.

        :param dt:
            A :py:class:`datetime.datetime`, naive or time zone aware.


        :return:
            Returns ``True`` if ambiguous, ``False`` otherwise.

        .. versionadded:: 2.6.0
        """

        dt = dt.replace(tzinfo=self)

        wall_0 = enfold(dt, fold=0)
        wall_1 = enfold(dt, fold=1)

        same_offset = wall_0.utcoffset() == wall_1.utcoffset()
        same_dt = wall_0.replace(tzinfo=None) == wall_1.replace(tzinfo=None)

        return same_dt and not same_offset

    def _fold_status(self, dt_utc, dt_wall):
        """
        Determine the fold status of a "wall" datetime, given a representation
        of the same datetime as a (naive) UTC datetime. This is calculated based
        on the assumption that ``dt.utcoffset() - dt.dst()`` is constant for all
        datetimes, and that this offset is the actual number of hours separating
        ``dt_utc`` and ``dt_wall``.

        :param dt_utc:
            Representation of the datetime as UTC

        :param dt_wall:
            Representation of the datetime as "wall time". This parameter must
            either have a `fold` attribute or have a fold-naive
            :class:`datetime.tzinfo` attached, otherwise the calculation may
            fail.
        """
        if self.is_ambiguous(dt_wall):
            delta_wall = dt_wall - dt_utc
            _fold = int(delta_wall == (dt_utc.utcoffset() - dt_utc.dst()))
        else:
            _fold = 0

        return _fold

    def _fold(self, dt):
        return getattr(dt, 'fold', 0)

    def _fromutc(self, dt):
        """
        Given a timezone-aware datetime in a given timezone, calculates a
        timezone-aware datetime in a new timezone.

        Since this is the one time that we *know* we have an unambiguous
        datetime object, we take this opportunity to determine whether the
        datetime is ambiguous and in a "fold" state (e.g. if it's the first
        occurrence, chronologically, of the ambiguous datetime).

        :param dt:
            A timezone-aware :class:`datetime.datetime` object.
        """

        # Re-implement the algorithm from Python's datetime.py
        dtoff = dt.utcoffset()
        if dtoff is None:
            raise ValueError("fromutc() requires a non-None utcoffset() "
                             "result")

        # The original datetime.py code assumes that `dst()` defaults to
        # zero during ambiguous times. PEP 495 inverts this presumption, so
        # for pre-PEP 495 versions of python, we need to tweak the algorithm.
        dtdst = dt.dst()
        if dtdst is None:
            raise ValueError("fromutc() requires a non-None dst() result")
        delta = dtoff - dtdst

        dt += delta
        # Set fold=1 so we can default to being in the fold for
        # ambiguous dates.
        dtdst = enfold(dt, fold=1).dst()
        if dtdst is None:
            raise ValueError("fromutc(): dt.dst gave inconsistent "
                             "results; cannot convert")
        return dt + dtdst

    @_validate_fromutc_inputs
    def fromutc(self, dt):
        """
        Given a timezone-aware datetime in a given timezone, calculates a
        timezone-aware datetime in a new timezone.

        Since this is the one time that we *know* we have an unambiguous
        datetime object, we take this opportunity to determine whether the
        datetime is ambiguous and in a "fold" state (e.g. if it's the first
        occurrence, chronologically, of the ambiguous datetime).

        :param dt:
            A timezone-aware :class:`datetime.datetime` object.
        """
        dt_wall = self._fromutc(dt)

        # Calculate the fold status given the two datetimes.
        _fold = self._fold_status(dt, dt_wall)

        # Set the default fold value for ambiguous dates
        return enfold(dt_wall, fold=_fold)


class tzrangebase(_tzinfo):
    """
    This is an abstract base class for time zones represented by an annual
    transition into and out of DST. Child classes should implement the following
    methods:

        * ``__init__(self, *args, **kwargs)``
        * ``transitions(self, year)`` - this is expected to return a tuple of
          datetimes representing the DST on and off transitions in standard
          time.

    A fully initialized ``tzrangebase`` subclass should also provide the
    following attributes:
        * ``hasdst``: Boolean whether or not the zone uses DST.
        * ``_dst_offset`` / ``_std_offset``: :class:`datetime.timedelta` objects
          representing the respective UTC offsets.
        * ``_dst_abbr`` / ``_std_abbr``: Strings representing the timezone short
          abbreviations in DST and STD, respectively.
        * ``_hasdst``: Whether or not the zone has DST.

    .. versionadded:: 2.6.0
    """
    def __init__(self):
        raise NotImplementedError('tzrangebase is an abstract base class')

    def utcoffset(self, dt):
        isdst = self._isdst(dt)

        if isdst is None:
            return None
        elif isdst:
            return self._dst_offset
        else:
            return self._std_offset

    def dst(self, dt):
        isdst = self._isdst(dt)

        if isdst is None:
            return None
        elif isdst:
            return self._dst_base_offset
        else:
            return ZERO

    @tzname_in_python2
    def tzname(self, dt):
        if self._isdst(dt):
            return self._dst_abbr
        else:
            return self._std_abbr

    def fromutc(self, dt):
        """ Given a datetime in UTC, return local time """
        if not isinstance(dt, datetime):
            raise TypeError("fromutc() requires a datetime argument")

        if dt.tzinfo is not self:
            raise ValueError("dt.tzinfo is not self")

        # Get transitions - if there are none, fixed offset
        transitions = self.transitions(dt.year)
        if transitions is None:
            return dt + self.utcoffset(dt)

        # Get the transition times in UTC
        dston, dstoff = transitions

        dston -= self._std_offset
        dstoff -= self._std_offset

        utc_transitions = (dston, dstoff)
        dt_utc = dt.replace(tzinfo=None)

        isdst = self._naive_isdst(dt_utc, utc_transitions)

        if isdst:
            dt_wall = dt + self._dst_offset
        else:
            dt_wall = dt + self._std_offset

        _fold = int(not isdst and self.is_ambiguous(dt_wall))

        return enfold(dt_wall, fold=_fold)

    def is_ambiguous(self, dt):
        """
        Whether or not the "wall time" of a given datetime is ambiguous in this
        zone.

        :param dt:
            A :py:class:`datetime.datetime`, naive or time zone aware.


        :return:
            Returns ``True`` if ambiguous, ``False`` otherwise.

        .. versionadded:: 2.6.0
        """
        if not self.hasdst:
            return False

        start, end = self.transitions(dt.year)

        dt = dt.replace(tzinfo=None)
        return (end <= dt < end + self._dst_base_offset)

    def _isdst(self, dt):
        if not self.hasdst:
            return False
        elif dt is None:
            return None

        transitions = self.transitions(dt.year)

        if transitions is None:
            return False

        dt = dt.replace(tzinfo=None)

        isdst = self._naive_isdst(dt, transitions)

        # Handle ambiguous dates
        if not isdst and self.is_ambiguous(dt):
            return not self._fold(dt)
        else:
            return isdst

    def _naive_isdst(self, dt, transitions):
        dston, dstoff = transitions

        dt = dt.replace(tzinfo=None)

        if dston < dstoff:
            isdst = dston <= dt < dstoff
        else:
            isdst = not dstoff <= dt < dston

        return isdst

    @property
    def _dst_base_offset(self):
        return self._dst_offset - self._std_offset

    __hash__ = None

    def __ne__(self, other):
        return not (self == other)

    def __repr__(self):
        return "%s(...)" % self.__class__.__name__

    __reduce__ = object.__reduce__

if __name__ == "__main__":
    import time
    from tz import tzlocal
    
    isT=True
    args = [datetime(2015, 11, 1, 6, 30, tzinfo=tzlocal()),
            datetime(2013, 10, 27, 0, 30, tzinfo=tzlocal()),
            datetime(2012, 10, 6, 16, 30, tzinfo=tzlocal()),
            datetime(2013, 10, 27, 1, 30, tzinfo=tzlocal()),
            datetime(2011, 11, 6, 5, 30, tzinfo=tzlocal()),
            datetime(2011, 11, 6, 6, 30, tzinfo=tzlocal()),
            datetime(2012, 3, 31, 15, 30, tzinfo=tzlocal()),
            datetime(2012, 3, 31, 16, 30, tzinfo=tzlocal()),
            datetime(2011, 3, 13, 6, 30, tzinfo=tzlocal()),
            datetime(2011, 3, 13, 7, 30, tzinfo=tzlocal()),
            datetime(2012, 10, 6, 15, 30, tzinfo=tzlocal())]

    res = [1446359400.0,
           1382833800.0,
           1349541000.0,
           1382837400.0,
           1320557400.0,
           1320561000.0,
           1333207800.0,
           1333211400.0,
           1299997800.0,
           1300001400.0,
           1349537400.0]

    temp_class = _tzinfo()
    for args1, target in zip(args, res):
        tmp = temp_class._fromutc(args1)
        res0 = time.mktime(tmp.timetuple())
        if res0 != target:
            isT=False


    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-dateutil/dateutil/relativedelta_normalized_passk_validte.py
    # -*- coding: utf-8 -*-
import datetime
import calendar

import operator
from math import copysign

from six import integer_types
from warnings import warn

from dateutil._common import weekday

MO, TU, WE, TH, FR, SA, SU = weekdays = tuple(weekday(x) for x in range(7))

__all__ = ["relativedelta", "MO", "TU", "WE", "TH", "FR", "SA", "SU"]


class relativedelta(object):
    """
    The relativedelta type is designed to be applied to an existing datetime and
    can replace specific components of that datetime, or represents an interval
    of time.

    It is based on the specification of the excellent work done by M.-A. Lemburg
    in his
    `mx.DateTime <https://www.egenix.com/products/python/mxBase/mxDateTime/>`_ extension.
    However, notice that this type does *NOT* implement the same algorithm as
    his work. Do *NOT* expect it to behave like mx.DateTime's counterpart.

    There are two different ways to build a relativedelta instance. The
    first one is passing it two date/datetime classes::

        relativedelta(datetime1, datetime2)

    The second one is passing it any number of the following keyword arguments::

        relativedelta(arg1=x,arg2=y,arg3=z...)

        year, month, day, hour, minute, second, microsecond:
            Absolute information (argument is singular); adding or subtracting a
            relativedelta with absolute information does not perform an arithmetic
            operation, but rather REPLACES the corresponding value in the
            original datetime with the value(s) in relativedelta.

        years, months, weeks, days, hours, minutes, seconds, microseconds:
            Relative information, may be negative (argument is plural); adding
            or subtracting a relativedelta with relative information performs
            the corresponding arithmetic operation on the original datetime value
            with the information in the relativedelta.

        weekday:
            One of the weekday instances (MO, TU, etc) available in the
            relativedelta module. These instances may receive a parameter N,
            specifying the Nth weekday, which could be positive or negative
            (like MO(+1) or MO(-2)). Not specifying it is the same as specifying
            +1. You can also use an integer, where 0=MO. This argument is always
            relative e.g. if the calculated date is already Monday, using MO(1)
            or MO(-1) won't change the day. To effectively make it absolute, use
            it in combination with the day argument (e.g. day=1, MO(1) for first
            Monday of the month).

        leapdays:
            Will add given days to the date found, if year is a leap
            year, and the date found is post 28 of february.

        yearday, nlyearday:
            Set the yearday or the non-leap year day (jump leap days).
            These are converted to day/month/leapdays information.

    There are relative and absolute forms of the keyword
    arguments. The plural is relative, and the singular is
    absolute. For each argument in the order below, the absolute form
    is applied first (by setting each attribute to that value) and
    then the relative form (by adding the value to the attribute).

    The order of attributes considered when this relativedelta is
    added to a datetime is:

    1. Year
    2. Month
    3. Day
    4. Hours
    5. Minutes
    6. Seconds
    7. Microseconds

    Finally, weekday is applied, using the rule described above.

    For example

    >>> from datetime import datetime
    >>> from dateutil.relativedelta import relativedelta, MO
    >>> dt = datetime(2018, 4, 9, 13, 37, 0)
    >>> delta = relativedelta(hours=25, day=1, weekday=MO(1))
    >>> dt + delta
    datetime.datetime(2018, 4, 2, 14, 37)

    First, the day is set to 1 (the first of the month), then 25 hours
    are added, to get to the 2nd day and 14th hour, finally the
    weekday is applied, but since the 2nd is already a Monday there is
    no effect.

    """

    def __init__(self, dt1=None, dt2=None,
                 years=0, months=0, days=0, leapdays=0, weeks=0,
                 hours=0, minutes=0, seconds=0, microseconds=0,
                 year=None, month=None, day=None, weekday=None,
                 yearday=None, nlyearday=None,
                 hour=None, minute=None, second=None, microsecond=None):

        if dt1 and dt2:
            # datetime is a subclass of date. So both must be date
            if not (isinstance(dt1, datetime.date) and
                    isinstance(dt2, datetime.date)):
                raise TypeError("relativedelta only diffs datetime/date")

            # We allow two dates, or two datetimes, so we coerce them to be
            # of the same type
            if (isinstance(dt1, datetime.datetime) !=
                    isinstance(dt2, datetime.datetime)):
                if not isinstance(dt1, datetime.datetime):
                    dt1 = datetime.datetime.fromordinal(dt1.toordinal())
                elif not isinstance(dt2, datetime.datetime):
                    dt2 = datetime.datetime.fromordinal(dt2.toordinal())

            self.years = 0
            self.months = 0
            self.days = 0
            self.leapdays = 0
            self.hours = 0
            self.minutes = 0
            self.seconds = 0
            self.microseconds = 0
            self.year = None
            self.month = None
            self.day = None
            self.weekday = None
            self.hour = None
            self.minute = None
            self.second = None
            self.microsecond = None
            self._has_time = 0

            # Get year / month delta between the two
            months = (dt1.year - dt2.year) * 12 + (dt1.month - dt2.month)
            self._set_months(months)

            # Remove the year/month delta so the timedelta is just well-defined
            # time units (seconds, days and microseconds)
            dtm = self.__radd__(dt2)

            # If we've overshot our target, make an adjustment
            if dt1 < dt2:
                compare = operator.gt
                increment = 1
            else:
                compare = operator.lt
                increment = -1

            while compare(dt1, dtm):
                months += increment
                self._set_months(months)
                dtm = self.__radd__(dt2)

            # Get the timedelta between the "months-adjusted" date and dt1
            delta = dt1 - dtm
            self.seconds = delta.seconds + delta.days * 86400
            self.microseconds = delta.microseconds
        else:
            # Check for non-integer values in integer-only quantities
            if any(x is not None and x != int(x) for x in (years, months)):
                raise ValueError("Non-integer years and months are "
                                 "ambiguous and not currently supported.")

            # Relative information
            self.years = int(years)
            self.months = int(months)
            self.days = days + weeks * 7
            self.leapdays = leapdays
            self.hours = hours
            self.minutes = minutes
            self.seconds = seconds
            self.microseconds = microseconds

            # Absolute information
            self.year = year
            self.month = month
            self.day = day
            self.hour = hour
            self.minute = minute
            self.second = second
            self.microsecond = microsecond

            if any(x is not None and int(x) != x
                   for x in (year, month, day, hour,
                             minute, second, microsecond)):
                # For now we'll deprecate floats - later it'll be an error.
                warn("Non-integer value passed as absolute information. " +
                     "This is not a well-defined condition and will raise " +
                     "errors in future versions.", DeprecationWarning)

            if isinstance(weekday, integer_types):
                self.weekday = weekdays[weekday]
            else:
                self.weekday = weekday

            yday = 0
            if nlyearday:
                yday = nlyearday
            elif yearday:
                yday = yearday
                if yearday > 59:
                    self.leapdays = -1
            if yday:
                ydayidx = [31, 59, 90, 120, 151, 181, 212,
                           243, 273, 304, 334, 366]
                for idx, ydays in enumerate(ydayidx):
                    if yday <= ydays:
                        self.month = idx+1
                        if idx == 0:
                            self.day = yday
                        else:
                            self.day = yday-ydayidx[idx-1]
                        break
                else:
                    raise ValueError("invalid year day (%d)" % yday)

        self._fix()

    def _fix(self):
        if abs(self.microseconds) > 999999:
            s = _sign(self.microseconds)
            div, mod = divmod(self.microseconds * s, 1000000)
            self.microseconds = mod * s
            self.seconds += div * s
        if abs(self.seconds) > 59:
            s = _sign(self.seconds)
            div, mod = divmod(self.seconds * s, 60)
            self.seconds = mod * s
            self.minutes += div * s
        if abs(self.minutes) > 59:
            s = _sign(self.minutes)
            div, mod = divmod(self.minutes * s, 60)
            self.minutes = mod * s
            self.hours += div * s
        if abs(self.hours) > 23:
            s = _sign(self.hours)
            div, mod = divmod(self.hours * s, 24)
            self.hours = mod * s
            self.days += div * s
        if abs(self.months) > 11:
            s = _sign(self.months)
            div, mod = divmod(self.months * s, 12)
            self.months = mod * s
            self.years += div * s
        if (self.hours or self.minutes or self.seconds or self.microseconds
                or self.hour is not None or self.minute is not None or
                self.second is not None or self.microsecond is not None):
            self._has_time = 1
        else:
            self._has_time = 0

    @property
    def weeks(self):
        return int(self.days / 7.0)

    @weeks.setter
    def weeks(self, value):
        self.days = self.days - (self.weeks * 7) + value * 7

    def _set_months(self, months):
        self.months = months
        if abs(self.months) > 11:
            s = _sign(self.months)
            div, mod = divmod(self.months * s, 12)
            self.months = mod * s
            self.years = div * s
        else:
            self.years = 0

    def normalized(self):
        """
        Return a version of this object represented entirely using integer
        values for the relative attributes.

        >>> relativedelta(days=1.5, hours=2).normalized()
        relativedelta(days=+1, hours=+14)

        :return:
            Returns a :class:`dateutil.relativedelta.relativedelta` object.
        """
        # Cascade remainders down (rounding each to roughly nearest microsecond)
        days = int(self.days)

        hours_f = round(self.hours + 24 * (self.days - days), 11)
        hours = int(hours_f)

        minutes_f = round(self.minutes + 60 * (hours_f - hours), 10)
        minutes = int(minutes_f)

        seconds_f = round(self.seconds + 60 * (minutes_f - minutes), 8)
        seconds = int(seconds_f)

        microseconds = round(self.microseconds + 1e6 * (seconds_f - seconds))

        # Constructor carries overflow back up with call to _fix()
        return self.__class__(years=self.years, months=self.months,
                              days=days, hours=hours, minutes=minutes,
                              seconds=seconds, microseconds=microseconds,
                              leapdays=self.leapdays, year=self.year,
                              month=self.month, day=self.day,
                              weekday=self.weekday, hour=self.hour,
                              minute=self.minute, second=self.second,
                              microsecond=self.microsecond)

    def __add__(self, other):
        if isinstance(other, relativedelta):
            return self.__class__(years=other.years + self.years,
                                 months=other.months + self.months,
                                 days=other.days + self.days,
                                 hours=other.hours + self.hours,
                                 minutes=other.minutes + self.minutes,
                                 seconds=other.seconds + self.seconds,
                                 microseconds=(other.microseconds +
                                               self.microseconds),
                                 leapdays=other.leapdays or self.leapdays,
                                 year=(other.year if other.year is not None
                                       else self.year),
                                 month=(other.month if other.month is not None
                                        else self.month),
                                 day=(other.day if other.day is not None
                                      else self.day),
                                 weekday=(other.weekday if other.weekday is not None
                                          else self.weekday),
                                 hour=(other.hour if other.hour is not None
                                       else self.hour),
                                 minute=(other.minute if other.minute is not None
                                         else self.minute),
                                 second=(other.second if other.second is not None
                                         else self.second),
                                 microsecond=(other.microsecond if other.microsecond
                                              is not None else
                                              self.microsecond))
        if isinstance(other, datetime.timedelta):
            return self.__class__(years=self.years,
                                  months=self.months,
                                  days=self.days + other.days,
                                  hours=self.hours,
                                  minutes=self.minutes,
                                  seconds=self.seconds + other.seconds,
                                  microseconds=self.microseconds + other.microseconds,
                                  leapdays=self.leapdays,
                                  year=self.year,
                                  month=self.month,
                                  day=self.day,
                                  weekday=self.weekday,
                                  hour=self.hour,
                                  minute=self.minute,
                                  second=self.second,
                                  microsecond=self.microsecond)
        if not isinstance(other, datetime.date):
            return NotImplemented
        elif self._has_time and not isinstance(other, datetime.datetime):
            other = datetime.datetime.fromordinal(other.toordinal())
        year = (self.year or other.year)+self.years
        month = self.month or other.month
        if self.months:
            assert 1 <= abs(self.months) <= 12
            month += self.months
            if month > 12:
                year += 1
                month -= 12
            elif month < 1:
                year -= 1
                month += 12
        day = min(calendar.monthrange(year, month)[1],
                  self.day or other.day)
        repl = {"year": year, "month": month, "day": day}
        for attr in ["hour", "minute", "second", "microsecond"]:
            value = getattr(self, attr)
            if value is not None:
                repl[attr] = value
        days = self.days
        if self.leapdays and month > 2 and calendar.isleap(year):
            days += self.leapdays
        ret = (other.replace(**repl)
               + datetime.timedelta(days=days,
                                    hours=self.hours,
                                    minutes=self.minutes,
                                    seconds=self.seconds,
                                    microseconds=self.microseconds))
        if self.weekday:
            weekday, nth = self.weekday.weekday, self.weekday.n or 1
            jumpdays = (abs(nth) - 1) * 7
            if nth > 0:
                jumpdays += (7 - ret.weekday() + weekday) % 7
            else:
                jumpdays += (ret.weekday() - weekday) % 7
                jumpdays *= -1
            ret += datetime.timedelta(days=jumpdays)
        return ret

    def __radd__(self, other):
        return self.__add__(other)

    def __rsub__(self, other):
        return self.__neg__().__radd__(other)

    def __sub__(self, other):
        if not isinstance(other, relativedelta):
            return NotImplemented   # In case the other object defines __rsub__
        return self.__class__(years=self.years - other.years,
                             months=self.months - other.months,
                             days=self.days - other.days,
                             hours=self.hours - other.hours,
                             minutes=self.minutes - other.minutes,
                             seconds=self.seconds - other.seconds,
                             microseconds=self.microseconds - other.microseconds,
                             leapdays=self.leapdays or other.leapdays,
                             year=(self.year if self.year is not None
                                   else other.year),
                             month=(self.month if self.month is not None else
                                    other.month),
                             day=(self.day if self.day is not None else
                                  other.day),
                             weekday=(self.weekday if self.weekday is not None else
                                      other.weekday),
                             hour=(self.hour if self.hour is not None else
                                   other.hour),
                             minute=(self.minute if self.minute is not None else
                                     other.minute),
                             second=(self.second if self.second is not None else
                                     other.second),
                             microsecond=(self.microsecond if self.microsecond
                                          is not None else
                                          other.microsecond))

    def __abs__(self):
        return self.__class__(years=abs(self.years),
                              months=abs(self.months),
                              days=abs(self.days),
                              hours=abs(self.hours),
                              minutes=abs(self.minutes),
                              seconds=abs(self.seconds),
                              microseconds=abs(self.microseconds),
                              leapdays=self.leapdays,
                              year=self.year,
                              month=self.month,
                              day=self.day,
                              weekday=self.weekday,
                              hour=self.hour,
                              minute=self.minute,
                              second=self.second,
                              microsecond=self.microsecond)

    def __neg__(self):
        return self.__class__(years=-self.years,
                             months=-self.months,
                             days=-self.days,
                             hours=-self.hours,
                             minutes=-self.minutes,
                             seconds=-self.seconds,
                             microseconds=-self.microseconds,
                             leapdays=self.leapdays,
                             year=self.year,
                             month=self.month,
                             day=self.day,
                             weekday=self.weekday,
                             hour=self.hour,
                             minute=self.minute,
                             second=self.second,
                             microsecond=self.microsecond)

    def __bool__(self):
        return not (not self.years and
                    not self.months and
                    not self.days and
                    not self.hours and
                    not self.minutes and
                    not self.seconds and
                    not self.microseconds and
                    not self.leapdays and
                    self.year is None and
                    self.month is None and
                    self.day is None and
                    self.weekday is None and
                    self.hour is None and
                    self.minute is None and
                    self.second is None and
                    self.microsecond is None)
    # Compatibility with Python 2.x
    __nonzero__ = __bool__

    def __mul__(self, other):
        try:
            f = float(other)
        except TypeError:
            return NotImplemented

        return self.__class__(years=int(self.years * f),
                             months=int(self.months * f),
                             days=int(self.days * f),
                             hours=int(self.hours * f),
                             minutes=int(self.minutes * f),
                             seconds=int(self.seconds * f),
                             microseconds=int(self.microseconds * f),
                             leapdays=self.leapdays,
                             year=self.year,
                             month=self.month,
                             day=self.day,
                             weekday=self.weekday,
                             hour=self.hour,
                             minute=self.minute,
                             second=self.second,
                             microsecond=self.microsecond)

    __rmul__ = __mul__

    def __eq__(self, other):
        if not isinstance(other, relativedelta):
            return NotImplemented
        if self.weekday or other.weekday:
            if not self.weekday or not other.weekday:
                return False
            if self.weekday.weekday != other.weekday.weekday:
                return False
            n1, n2 = self.weekday.n, other.weekday.n
            if n1 != n2 and not ((not n1 or n1 == 1) and (not n2 or n2 == 1)):
                return False
        return (self.years == other.years and
                self.months == other.months and
                self.days == other.days and
                self.hours == other.hours and
                self.minutes == other.minutes and
                self.seconds == other.seconds and
                self.microseconds == other.microseconds and
                self.leapdays == other.leapdays and
                self.year == other.year and
                self.month == other.month and
                self.day == other.day and
                self.hour == other.hour and
                self.minute == other.minute and
                self.second == other.second and
                self.microsecond == other.microsecond)

    def __hash__(self):
        return hash((
            self.weekday,
            self.years,
            self.months,
            self.days,
            self.hours,
            self.minutes,
            self.seconds,
            self.microseconds,
            self.leapdays,
            self.year,
            self.month,
            self.day,
            self.hour,
            self.minute,
            self.second,
            self.microsecond,
        ))

    def __ne__(self, other):
        return not self.__eq__(other)

    def __div__(self, other):
        try:
            reciprocal = 1 / float(other)
        except TypeError:
            return NotImplemented

        return self.__mul__(reciprocal)

    __truediv__ = __div__

    def __repr__(self):
        l = []
        for attr in ["years", "months", "days", "leapdays",
                     "hours", "minutes", "seconds", "microseconds"]:
            value = getattr(self, attr)
            if value:
                l.append("{attr}={value:+g}".format(attr=attr, value=value))
        for attr in ["year", "month", "day", "weekday",
                     "hour", "minute", "second", "microsecond"]:
            value = getattr(self, attr)
            if value is not None:
                l.append("{attr}={value}".format(attr=attr, value=repr(value)))
        return "{classname}({attrs})".format(classname=self.__class__.__name__,
                                             attrs=", ".join(l))


def _sign(x):
    return int(copysign(1, x))

    # vim:ts=4:sw=4:et

if __name__ == "__main__":
    from dateutil.relativedelta import relativedelta
    isT=True



    # Equivalent to (days=2, hours=18)
    rd1 = relativedelta(days=2.75)

    if not(rd1.normalized()==relativedelta(days=2, hours=18)):
        isT=False

    # Equivalent to (days=1, hours=11, minutes=31, seconds=12)
    rd2 = relativedelta(days=1.48)

    if not(rd2.normalized()==relativedelta(days=1, hours=11, minutes=31, seconds=12)):
        isT=False



    # Equivalent to (hours=1, minutes=30)
    rd1 = relativedelta(hours=1.5)

    if not(rd1.normalized()==relativedelta(hours=1, minutes=30)):
        isT=False

    # Equivalent to (hours=3, minutes=17, seconds=5, microseconds=100)
    rd2 = relativedelta(hours=3.28472225)

    if not(rd2.normalized()==relativedelta(hours=3, minutes=17, seconds=5, microseconds=100)):
        isT=False



    # Equivalent to (minutes=15, seconds=36)
    rd1 = relativedelta(minutes=15.6)

    if not(rd1.normalized()==relativedelta(minutes=15, seconds=36)):
        isT=False

    # Equivalent to (minutes=25, seconds=20, microseconds=25000)
    rd2 = relativedelta(minutes=25.33375)

    if not(rd2.normalized()==relativedelta(minutes=25, seconds=20, microseconds=25000)):
        isT=False



    # Equivalent to (seconds=45, microseconds=25000)
    rd1 = relativedelta(seconds=45.025)
    if not(rd1.normalized()==relativedelta(seconds=45, microseconds=25000)):
        isT=False



    # Equivalent to (days=1, hours=14)
    rd1 = relativedelta(days=1.5, hours=2)
    if not(rd1.normalized()==relativedelta(days=1, hours=14)):
        isT=False

    # Equivalent to (days=1, hours=14, minutes=45)
    rd2 = relativedelta(days=1.5, hours=2.5, minutes=15)
    if not(rd2.normalized()==relativedelta(days=1, hours=14, minutes=45)):
        isT=False

    # Carry back up - equivalent to:
    # (days=2, hours=2, minutes=0, seconds=2, microseconds=3)
    rd3 = relativedelta(days=1.5, hours=13, minutes=59.50045,
                        seconds=31.473, microseconds=500003)
    if not(rd3.normalized()==relativedelta(days=2, hours=2, minutes=0,seconds=2, microseconds=3)):
        isT=False



    # Equivalent to (days=-1)
    rd1 = relativedelta(days=-0.5, hours=-12)
    if not(rd1.normalized()==relativedelta(days=-1)):
        isT=False

    # Equivalent to (days=-1)
    rd2 = relativedelta(days=-1.5, hours=12)
    if not(rd2.normalized()==relativedelta(days=-1)):
        isT=False

    # Equivalent to (days=-1, hours=-14, minutes=-45)
    rd3 = relativedelta(days=-1.5, hours=-2.5, minutes=-15)
    if not(rd3.normalized()==relativedelta(days=-1, hours=-14, minutes=-45)):
        isT=False

    # Equivalent to (days=-1, hours=-14, minutes=+15)
    rd4 = relativedelta(days=-1.5, hours=-2.5, minutes=45)
    if not(rd4.normalized()==relativedelta(days=-1, hours=-14, minutes=+15)):
        isT=False

    # Carry back up - equivalent to:
    # (days=-2, hours=-2, minutes=0, seconds=-2, microseconds=-3)
    rd3 = relativedelta(days=-1.5, hours=-13, minutes=-59.50045,
                        seconds=-31.473, microseconds=-500003)
    if not(rd3.normalized()==relativedelta(days=-2, hours=-2, minutes=0,seconds=-2, microseconds=-3)):
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-python-dateutil/data_passk_platform/62b8982f755ee91dce50a241/"):
    #     f = open("/home/travis/builds/repos/pexip---os-python-dateutil/data_passk_platform/62b8982f755ee91dce50a241/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     object_class=dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class=relativedelta()
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.normalized()
    #     # print(res0)
    #     # print(content["output"][0])
    #     if not ( dill.dumps(str(res0))== dill.dumps(str(content["output"][0]))):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-dateutil/dateutil/tz/_common_tzname_in_python2_passk_validte.py
from six import PY2

from functools import wraps

from datetime import datetime, timedelta, tzinfo


ZERO = timedelta(0)

__all__ = ['tzname_in_python2', 'enfold']


def tzname_in_python2(namefunc):
    """Change unicode output into bytestrings in Python 2

    tzname() API changed in Python 3. It used to return bytes, but was changed
    to unicode strings
    """
    if PY2:
        @wraps(namefunc)
        def adjust_encoding(*args, **kwargs):
            name = namefunc(*args, **kwargs)
            if name is not None:
                name = name.encode()

            return name

        return adjust_encoding
    else:
        return namefunc


# The following is adapted from Alexander Belopolsky's tz library
# https://github.com/abalkin/tz
if hasattr(datetime, 'fold'):
    # This is the pre-python 3.6 fold situation
    def enfold(dt, fold=1):
        """
        Provides a unified interface for assigning the ``fold`` attribute to
        datetimes both before and after the implementation of PEP-495.

        :param fold:
            The value for the ``fold`` attribute in the returned datetime. This
            should be either 0 or 1.

        :return:
            Returns an object for which ``getattr(dt, 'fold', 0)`` returns
            ``fold`` for all versions of Python. In versions prior to
            Python 3.6, this is a ``_DatetimeWithFold`` object, which is a
            subclass of :py:class:`datetime.datetime` with the ``fold``
            attribute added, if ``fold`` is 1.

        .. versionadded:: 2.6.0
        """
        return dt.replace(fold=fold)

else:
    class _DatetimeWithFold(datetime):
        """
        This is a class designed to provide a PEP 495-compliant interface for
        Python versions before 3.6. It is used only for dates in a fold, so
        the ``fold`` attribute is fixed at ``1``.

        .. versionadded:: 2.6.0
        """
        __slots__ = ()

        def replace(self, *args, **kwargs):
            """
            Return a datetime with the same attributes, except for those
            attributes given new values by whichever keyword arguments are
            specified. Note that tzinfo=None can be specified to create a naive
            datetime from an aware datetime with no conversion of date and time
            data.

            This is reimplemented in ``_DatetimeWithFold`` because pypy3 will
            return a ``datetime.datetime`` even if ``fold`` is unchanged.
            """
            argnames = (
                'year', 'month', 'day', 'hour', 'minute', 'second',
                'microsecond', 'tzinfo'
            )

            for arg, argname in zip(args, argnames):
                if argname in kwargs:
                    raise TypeError('Duplicate argument: {}'.format(argname))

                kwargs[argname] = arg

            for argname in argnames:
                if argname not in kwargs:
                    kwargs[argname] = getattr(self, argname)

            dt_class = self.__class__ if kwargs.get('fold', 1) else datetime

            return dt_class(**kwargs)

        @property
        def fold(self):
            return 1

    def enfold(dt, fold=1):
        """
        Provides a unified interface for assigning the ``fold`` attribute to
        datetimes both before and after the implementation of PEP-495.

        :param fold:
            The value for the ``fold`` attribute in the returned datetime. This
            should be either 0 or 1.

        :return:
            Returns an object for which ``getattr(dt, 'fold', 0)`` returns
            ``fold`` for all versions of Python. In versions prior to
            Python 3.6, this is a ``_DatetimeWithFold`` object, which is a
            subclass of :py:class:`datetime.datetime` with the ``fold``
            attribute added, if ``fold`` is 1.

        .. versionadded:: 2.6.0
        """
        if getattr(dt, 'fold', 0) == fold:
            return dt

        args = dt.timetuple()[:6]
        args += (dt.microsecond, dt.tzinfo)

        if fold:
            return _DatetimeWithFold(*args)
        else:
            return datetime(*args)


def _validate_fromutc_inputs(f):
    """
    The CPython version of ``fromutc`` checks that the input is a ``datetime``
    object and that ``self`` is attached as its ``tzinfo``.
    """
    @wraps(f)
    def fromutc(self, dt):
        if not isinstance(dt, datetime):
            raise TypeError("fromutc() requires a datetime argument")
        if dt.tzinfo is not self:
            raise ValueError("dt.tzinfo is not self")

        return f(self, dt)

    return fromutc


class _tzinfo(tzinfo):
    """
    Base class for all ``dateutil`` ``tzinfo`` objects.
    """

    def is_ambiguous(self, dt):
        """
        Whether or not the "wall time" of a given datetime is ambiguous in this
        zone.

        :param dt:
            A :py:class:`datetime.datetime`, naive or time zone aware.


        :return:
            Returns ``True`` if ambiguous, ``False`` otherwise.

        .. versionadded:: 2.6.0
        """

        dt = dt.replace(tzinfo=self)

        wall_0 = enfold(dt, fold=0)
        wall_1 = enfold(dt, fold=1)

        same_offset = wall_0.utcoffset() == wall_1.utcoffset()
        same_dt = wall_0.replace(tzinfo=None) == wall_1.replace(tzinfo=None)

        return same_dt and not same_offset

    def _fold_status(self, dt_utc, dt_wall):
        """
        Determine the fold status of a "wall" datetime, given a representation
        of the same datetime as a (naive) UTC datetime. This is calculated based
        on the assumption that ``dt.utcoffset() - dt.dst()`` is constant for all
        datetimes, and that this offset is the actual number of hours separating
        ``dt_utc`` and ``dt_wall``.

        :param dt_utc:
            Representation of the datetime as UTC

        :param dt_wall:
            Representation of the datetime as "wall time". This parameter must
            either have a `fold` attribute or have a fold-naive
            :class:`datetime.tzinfo` attached, otherwise the calculation may
            fail.
        """
        if self.is_ambiguous(dt_wall):
            delta_wall = dt_wall - dt_utc
            _fold = int(delta_wall == (dt_utc.utcoffset() - dt_utc.dst()))
        else:
            _fold = 0

        return _fold

    def _fold(self, dt):
        return getattr(dt, 'fold', 0)

    def _fromutc(self, dt):
        """
        Given a timezone-aware datetime in a given timezone, calculates a
        timezone-aware datetime in a new timezone.

        Since this is the one time that we *know* we have an unambiguous
        datetime object, we take this opportunity to determine whether the
        datetime is ambiguous and in a "fold" state (e.g. if it's the first
        occurrence, chronologically, of the ambiguous datetime).

        :param dt:
            A timezone-aware :class:`datetime.datetime` object.
        """

        # Re-implement the algorithm from Python's datetime.py
        dtoff = dt.utcoffset()
        if dtoff is None:
            raise ValueError("fromutc() requires a non-None utcoffset() "
                             "result")

        # The original datetime.py code assumes that `dst()` defaults to
        # zero during ambiguous times. PEP 495 inverts this presumption, so
        # for pre-PEP 495 versions of python, we need to tweak the algorithm.
        dtdst = dt.dst()
        if dtdst is None:
            raise ValueError("fromutc() requires a non-None dst() result")
        delta = dtoff - dtdst

        dt += delta
        # Set fold=1 so we can default to being in the fold for
        # ambiguous dates.
        dtdst = enfold(dt, fold=1).dst()
        if dtdst is None:
            raise ValueError("fromutc(): dt.dst gave inconsistent "
                             "results; cannot convert")
        return dt + dtdst

    @_validate_fromutc_inputs
    def fromutc(self, dt):
        """
        Given a timezone-aware datetime in a given timezone, calculates a
        timezone-aware datetime in a new timezone.

        Since this is the one time that we *know* we have an unambiguous
        datetime object, we take this opportunity to determine whether the
        datetime is ambiguous and in a "fold" state (e.g. if it's the first
        occurrence, chronologically, of the ambiguous datetime).

        :param dt:
            A timezone-aware :class:`datetime.datetime` object.
        """
        dt_wall = self._fromutc(dt)

        # Calculate the fold status given the two datetimes.
        _fold = self._fold_status(dt, dt_wall)

        # Set the default fold value for ambiguous dates
        return enfold(dt_wall, fold=_fold)


class tzrangebase(_tzinfo):
    """
    This is an abstract base class for time zones represented by an annual
    transition into and out of DST. Child classes should implement the following
    methods:

        * ``__init__(self, *args, **kwargs)``
        * ``transitions(self, year)`` - this is expected to return a tuple of
          datetimes representing the DST on and off transitions in standard
          time.

    A fully initialized ``tzrangebase`` subclass should also provide the
    following attributes:
        * ``hasdst``: Boolean whether or not the zone uses DST.
        * ``_dst_offset`` / ``_std_offset``: :class:`datetime.timedelta` objects
          representing the respective UTC offsets.
        * ``_dst_abbr`` / ``_std_abbr``: Strings representing the timezone short
          abbreviations in DST and STD, respectively.
        * ``_hasdst``: Whether or not the zone has DST.

    .. versionadded:: 2.6.0
    """
    def __init__(self):
        raise NotImplementedError('tzrangebase is an abstract base class')

    def utcoffset(self, dt):
        isdst = self._isdst(dt)

        if isdst is None:
            return None
        elif isdst:
            return self._dst_offset
        else:
            return self._std_offset

    def dst(self, dt):
        isdst = self._isdst(dt)

        if isdst is None:
            return None
        elif isdst:
            return self._dst_base_offset
        else:
            return ZERO

    @tzname_in_python2
    def tzname(self, dt):
        if self._isdst(dt):
            return self._dst_abbr
        else:
            return self._std_abbr

    def fromutc(self, dt):
        """ Given a datetime in UTC, return local time """
        if not isinstance(dt, datetime):
            raise TypeError("fromutc() requires a datetime argument")

        if dt.tzinfo is not self:
            raise ValueError("dt.tzinfo is not self")

        # Get transitions - if there are none, fixed offset
        transitions = self.transitions(dt.year)
        if transitions is None:
            return dt + self.utcoffset(dt)

        # Get the transition times in UTC
        dston, dstoff = transitions

        dston -= self._std_offset
        dstoff -= self._std_offset

        utc_transitions = (dston, dstoff)
        dt_utc = dt.replace(tzinfo=None)

        isdst = self._naive_isdst(dt_utc, utc_transitions)

        if isdst:
            dt_wall = dt + self._dst_offset
        else:
            dt_wall = dt + self._std_offset

        _fold = int(not isdst and self.is_ambiguous(dt_wall))

        return enfold(dt_wall, fold=_fold)

    def is_ambiguous(self, dt):
        """
        Whether or not the "wall time" of a given datetime is ambiguous in this
        zone.

        :param dt:
            A :py:class:`datetime.datetime`, naive or time zone aware.


        :return:
            Returns ``True`` if ambiguous, ``False`` otherwise.

        .. versionadded:: 2.6.0
        """
        if not self.hasdst:
            return False

        start, end = self.transitions(dt.year)

        dt = dt.replace(tzinfo=None)
        return (end <= dt < end + self._dst_base_offset)

    def _isdst(self, dt):
        if not self.hasdst:
            return False
        elif dt is None:
            return None

        transitions = self.transitions(dt.year)

        if transitions is None:
            return False

        dt = dt.replace(tzinfo=None)

        isdst = self._naive_isdst(dt, transitions)

        # Handle ambiguous dates
        if not isdst and self.is_ambiguous(dt):
            return not self._fold(dt)
        else:
            return isdst

    def _naive_isdst(self, dt, transitions):
        dston, dstoff = transitions

        dt = dt.replace(tzinfo=None)

        if dston < dstoff:
            isdst = dston <= dt < dstoff
        else:
            isdst = not dstoff <= dt < dston

        return isdst

    @property
    def _dst_base_offset(self):
        return self._dst_offset - self._std_offset

    __hash__ = None

    def __ne__(self, other):
        return not (self == other)

    def __repr__(self):
        return "%s(...)" % self.__class__.__name__

    __reduce__ = object.__reduce__

if __name__ == "__main__":
    from dateutil.tz.tz import tzrange
    tz = datetime(2002,10,1)
    range1 = tzrange("EST", -18000, "EDT")
    dtt = range1.tzname(tz)
    if dtt!="EDT":
        raise Exception("Result not True!!!")





----------------------------
/home/travis/builds/repos/eykd---prestoplot/src/prestoplot/_version_get_versions_passk_validte.py
# This file helps to compute a version number in source trees obtained from
# git-archive tarball (such as those provided by githubs download-from-tag
# feature). Distribution tarballs (built by setup.py sdist) and build
# directories (produced by setup.py build) will contain a much shorter file
# that just contains the computed version number.

# This file is released into the public domain. Generated by
# versioneer-0.22 (https://github.com/python-versioneer/python-versioneer)

"""Git implementation of _version.py."""

import errno
import functools
import os
import re
import subprocess
import sys
from typing import Callable, Dict


def get_keywords():
    """Get the keywords needed to look up the version information."""
    # these strings will be replaced by git during git-archive.
    # setup.py/versioneer.py will grep for the variable names, so they must
    # each be defined on a line of their own. _version.py will just call
    # get_keywords().
    git_refnames = " (HEAD -> master, tag: 0.5.2)"
    git_full = "61c94a4a354806aacdd280c61caed76df2b63205"
    git_date = "2023-02-17 16:50:17 -0800"
    keywords = {"refnames": git_refnames, "full": git_full, "date": git_date}
    return keywords


class VersioneerConfig:
    """Container for Versioneer configuration parameters."""


def get_config():
    """Create, populate and return the VersioneerConfig() object."""
    # these strings are filled in when 'setup.py versioneer' creates
    # _version.py
    cfg = VersioneerConfig()
    cfg.VCS = "git"
    cfg.style = "pep440"
    cfg.tag_prefix = ""
    cfg.parentdir_prefix = "None"
    cfg.versionfile_source = "src/prestoplot/_version.py"
    cfg.verbose = False
    return cfg


class NotThisMethod(Exception):
    """Exception raised if a method is not valid for the current scenario."""


LONG_VERSION_PY: Dict[str, str] = {}
HANDLERS: Dict[str, Dict[str, Callable]] = {}


def register_vcs_handler(vcs, method):  # decorator
    """Create decorator to mark a method as the handler of a VCS."""

    def decorate(f):
        """Store f in HANDLERS[vcs][method]."""
        if vcs not in HANDLERS:
            HANDLERS[vcs] = {}
        HANDLERS[vcs][method] = f
        return f

    return decorate


def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):
    """Call the given command(s)."""
    assert isinstance(commands, list)
    process = None

    popen_kwargs = {}
    if sys.platform == "win32":
        # This hides the console window if pythonw.exe is used
        startupinfo = subprocess.STARTUPINFO()
        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
        popen_kwargs["startupinfo"] = startupinfo

    for command in commands:
        try:
            dispcmd = str([command] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            process = subprocess.Popen(
                [command] + args,
                cwd=cwd,
                env=env,
                stdout=subprocess.PIPE,
                stderr=(subprocess.PIPE if hide_stderr else None),
                **popen_kwargs
            )
            break
        except OSError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print("unable to run %s" % dispcmd)
                print(e)
            return None, None
    else:
        if verbose:
            print("unable to find command, tried %s" % (commands,))
        return None, None
    stdout = process.communicate()[0].strip().decode()
    if process.returncode != 0:
        if verbose:
            print("unable to run %s (error)" % dispcmd)
            print("stdout was %s" % stdout)
        return None, process.returncode
    return stdout, process.returncode


def versions_from_parentdir(parentdir_prefix, root, verbose):
    """Try to determine the version from the parent directory name.

    Source tarballs conventionally unpack into a directory that includes both
    the project name and a version string. We will also support searching up
    two directory levels for an appropriately named parent directory
    """
    rootdirs = []

    for _ in range(3):
        dirname = os.path.basename(root)
        if dirname.startswith(parentdir_prefix):
            return {
                "version": dirname[len(parentdir_prefix) :],
                "full-revisionid": None,
                "dirty": False,
                "error": None,
                "date": None,
            }
        rootdirs.append(root)
        root = os.path.dirname(root)  # up a level

    if verbose:
        print(
            "Tried directories %s but none started with prefix %s"
            % (str(rootdirs), parentdir_prefix)
        )
    raise NotThisMethod("rootdir doesn't start with parentdir_prefix")


@register_vcs_handler("git", "get_keywords")
def git_get_keywords(versionfile_abs):
    """Extract version information from the given file."""
    # the code embedded in _version.py can just fetch the value of these
    # keywords. When used from setup.py, we don't want to import _version.py,
    # so we do it with a regexp instead. This function is not used from
    # _version.py.
    keywords = {}
    try:
        with open(versionfile_abs, "r") as fobj:
            for line in fobj:
                if line.strip().startswith("git_refnames ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["refnames"] = mo.group(1)
                if line.strip().startswith("git_full ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["full"] = mo.group(1)
                if line.strip().startswith("git_date ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["date"] = mo.group(1)
    except OSError:
        pass
    return keywords


@register_vcs_handler("git", "keywords")
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """Get version information from git keywords."""
    if "refnames" not in keywords:
        raise NotThisMethod("Short version file found")
    date = keywords.get("date")
    if date is not None:
        # Use only the last line.  Previous lines may contain GPG signature
        # information.
        date = date.splitlines()[-1]

        # git-2.2.0 added "%cI", which expands to an ISO-8601 -compliant
        # datestamp. However we prefer "%ci" (which expands to an "ISO-8601
        # -like" string, which we must then edit to make compliant), because
        # it's been around since git-1.5.3, and it's too difficult to
        # discover which version we're using, or to work around using an
        # older one.
        date = date.strip().replace(" ", "T", 1).replace(" ", "", 1)
    refnames = keywords["refnames"].strip()
    if refnames.startswith("$Format"):
        if verbose:
            print("keywords are unexpanded, not using")
        raise NotThisMethod("unexpanded keywords, not a git-archive tarball")
    refs = {r.strip() for r in refnames.strip("()").split(",")}
    # starting in git-1.8.3, tags are listed as "tag: foo-1.0" instead of
    # just "foo-1.0". If we see a "tag: " prefix, prefer those.
    TAG = "tag: "
    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}
    if not tags:
        # Either we're using git < 1.8.3, or there really are no tags. We use
        # a heuristic: assume all version tags have a digit. The old git %d
        # expansion behaves like git log --decorate=short and strips out the
        # refs/heads/ and refs/tags/ prefixes that would let us distinguish
        # between branches and tags. By ignoring refnames without digits, we
        # filter out many common branch names like "release" and
        # "stabilization", as well as "HEAD" and "master".
        tags = {r for r in refs if re.search(r"\d", r)}
        if verbose:
            print("discarding '%s', no digits" % ",".join(refs - tags))
    if verbose:
        print("likely tags: %s" % ",".join(sorted(tags)))
    for ref in sorted(tags):
        # sorting will prefer e.g. "2.0" over "2.0rc1"
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix) :]
            # Filter out refs that exactly match prefix or that don't start
            # with a number once the prefix is stripped (mostly a concern
            # when prefix is '')
            if not re.match(r"\d", r):
                continue
            if verbose:
                print("picking %s" % r)
            return {
                "version": r,
                "full-revisionid": keywords["full"].strip(),
                "dirty": False,
                "error": None,
                "date": date,
            }
    # no suitable tags, so version is "0+unknown", but full hex is still there
    if verbose:
        print("no suitable tags, using unknown + full revision id")
    return {
        "version": "0+unknown",
        "full-revisionid": keywords["full"].strip(),
        "dirty": False,
        "error": "no suitable tags",
        "date": None,
    }


@register_vcs_handler("git", "pieces_from_vcs")
def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):
    """Get version from 'git describe' in the root of the source tree.

    This only gets called if the git-archive 'subst' keywords were *not*
    expanded, and _version.py hasn't already been rewritten with a short
    version string, meaning we're inside a checked out source tree.
    """
    GITS = ["git"]
    if sys.platform == "win32":
        GITS = ["git.cmd", "git.exe"]

    # GIT_DIR can interfere with correct operation of Versioneer.
    # It may be intended to be passed to the Versioneer-versioned project,
    # but that should not change where we get our version from.
    env = os.environ.copy()
    env.pop("GIT_DIR", None)
    runner = functools.partial(runner, env=env)

    _, rc = runner(GITS, ["rev-parse", "--git-dir"], cwd=root, hide_stderr=True)
    if rc != 0:
        if verbose:
            print("Directory %s not under git control" % root)
        raise NotThisMethod("'git rev-parse --git-dir' returned error")

    MATCH_ARGS = ["--match", "%s*" % tag_prefix] if tag_prefix else []

    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]
    # if there isn't one, this yields HEX[-dirty] (no NUM)
    describe_out, rc = runner(
        GITS,
        ["describe", "--tags", "--dirty", "--always", "--long", *MATCH_ARGS],
        cwd=root,
    )
    # --long was added in git-1.5.5
    if describe_out is None:
        raise NotThisMethod("'git describe' failed")
    describe_out = describe_out.strip()
    full_out, rc = runner(GITS, ["rev-parse", "HEAD"], cwd=root)
    if full_out is None:
        raise NotThisMethod("'git rev-parse' failed")
    full_out = full_out.strip()

    pieces = {}
    pieces["long"] = full_out
    pieces["short"] = full_out[:7]  # maybe improved later
    pieces["error"] = None

    branch_name, rc = runner(GITS, ["rev-parse", "--abbrev-ref", "HEAD"], cwd=root)
    # --abbrev-ref was added in git-1.6.3
    if rc != 0 or branch_name is None:
        raise NotThisMethod("'git rev-parse --abbrev-ref' returned error")
    branch_name = branch_name.strip()

    if branch_name == "HEAD":
        # If we aren't exactly on a branch, pick a branch which represents
        # the current commit. If all else fails, we are on a branchless
        # commit.
        branches, rc = runner(GITS, ["branch", "--contains"], cwd=root)
        # --contains was added in git-1.5.4
        if rc != 0 or branches is None:
            raise NotThisMethod("'git branch --contains' returned error")
        branches = branches.split("\n")

        # Remove the first line if we're running detached
        if "(" in branches[0]:
            branches.pop(0)

        # Strip off the leading "* " from the list of branches.
        branches = [branch[2:] for branch in branches]
        if "master" in branches:
            branch_name = "master"
        elif not branches:
            branch_name = None
        else:
            # Pick the first branch that is returned. Good or bad.
            branch_name = branches[0]

    pieces["branch"] = branch_name

    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]
    # TAG might have hyphens.
    git_describe = describe_out

    # look for -dirty suffix
    dirty = git_describe.endswith("-dirty")
    pieces["dirty"] = dirty
    if dirty:
        git_describe = git_describe[: git_describe.rindex("-dirty")]

    # now we have TAG-NUM-gHEX or HEX

    if "-" in git_describe:
        # TAG-NUM-gHEX
        mo = re.search(r"^(.+)-(\d+)-g([0-9a-f]+)$", git_describe)
        if not mo:
            # unparsable. Maybe git-describe is misbehaving?
            pieces["error"] = "unable to parse git-describe output: '%s'" % describe_out
            return pieces

        # tag
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = "tag '%s' doesn't start with prefix '%s'"
                print(fmt % (full_tag, tag_prefix))
            pieces["error"] = "tag '%s' doesn't start with prefix '%s'" % (
                full_tag,
                tag_prefix,
            )
            return pieces
        pieces["closest-tag"] = full_tag[len(tag_prefix) :]

        # distance: number of commits since tag
        pieces["distance"] = int(mo.group(2))

        # commit: short hex revision ID
        pieces["short"] = mo.group(3)

    else:
        # HEX: no tags
        pieces["closest-tag"] = None
        count_out, rc = runner(GITS, ["rev-list", "HEAD", "--count"], cwd=root)
        pieces["distance"] = int(count_out)  # total number of commits

    # commit date: see ISO-8601 comment in git_versions_from_keywords()
    date = runner(GITS, ["show", "-s", "--format=%ci", "HEAD"], cwd=root)[0].strip()
    # Use only the last line.  Previous lines may contain GPG signature
    # information.
    date = date.splitlines()[-1]
    pieces["date"] = date.strip().replace(" ", "T", 1).replace(" ", "", 1)

    return pieces


def plus_or_dot(pieces):
    """Return a + if we don't already have one, else return a ."""
    if "+" in pieces.get("closest-tag", ""):
        return "."
    return "+"


def render_pep440(pieces):
    """Build up version string, with post-release "local version identifier".

    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you
    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty

    Exceptions:
    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def render_pep440_branch(pieces):
    """TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .

    The ".dev0" means not master branch. Note that .dev0 sorts backwards
    (a feature branch will appear "older" than the master branch).

    Exceptions:
    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            if pieces["branch"] != "master":
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0"
        if pieces["branch"] != "master":
            rendered += ".dev0"
        rendered += "+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def pep440_split_post(ver):
    """Split pep440 version string at the post-release segment.

    Returns the release segments before the post-release and the
    post-release version number (or -1 if no post-release segment is present).
    """
    vc = str.split(ver, ".post")
    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None


def render_pep440_pre(pieces):
    """TAG[.postN.devDISTANCE] -- No -dirty.

    Exceptions:
    1: no tags. 0.post0.devDISTANCE
    """
    if pieces["closest-tag"]:
        if pieces["distance"]:
            # update the post release segment
            tag_version, post_version = pep440_split_post(pieces["closest-tag"])
            rendered = tag_version
            if post_version is not None:
                rendered += ".post%d.dev%d" % (post_version + 1, pieces["distance"])
            else:
                rendered += ".post0.dev%d" % (pieces["distance"])
        else:
            # no commits, use the tag as the version
            rendered = pieces["closest-tag"]
    else:
        # exception #1
        rendered = "0.post0.dev%d" % pieces["distance"]
    return rendered


def render_pep440_post(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX] .

    The ".dev0" means dirty. Note that .dev0 sorts backwards
    (a dirty tree will appear "older" than the corresponding clean one),
    but you shouldn't be releasing software with -dirty anyways.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
    return rendered


def render_pep440_post_branch(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .

    The ".dev0" means not master branch.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["branch"] != "master":
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["branch"] != "master":
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def render_pep440_old(pieces):
    """TAG[.postDISTANCE[.dev0]] .

    The ".dev0" means dirty.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
    return rendered


def render_git_describe(pieces):
    """TAG[-DISTANCE-gHEX][-dirty].

    Like 'git describe --tags --dirty --always'.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"]:
            rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


def render_git_describe_long(pieces):
    """TAG-DISTANCE-gHEX[-dirty].

    Like 'git describe --tags --dirty --always -long'.
    The distance/hash is unconditional.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


def render(pieces, style):
    """Render the given version pieces into the requested style."""
    if pieces["error"]:
        return {
            "version": "unknown",
            "full-revisionid": pieces.get("long"),
            "dirty": None,
            "error": pieces["error"],
            "date": None,
        }

    if not style or style == "default":
        style = "pep440"  # the default

    if style == "pep440":
        rendered = render_pep440(pieces)
    elif style == "pep440-branch":
        rendered = render_pep440_branch(pieces)
    elif style == "pep440-pre":
        rendered = render_pep440_pre(pieces)
    elif style == "pep440-post":
        rendered = render_pep440_post(pieces)
    elif style == "pep440-post-branch":
        rendered = render_pep440_post_branch(pieces)
    elif style == "pep440-old":
        rendered = render_pep440_old(pieces)
    elif style == "git-describe":
        rendered = render_git_describe(pieces)
    elif style == "git-describe-long":
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError("unknown style '%s'" % style)

    return {
        "version": rendered,
        "full-revisionid": pieces["long"],
        "dirty": pieces["dirty"],
        "error": None,
        "date": pieces.get("date"),
    }


def get_versions():
    """Get version information or return default if unable to do so."""
    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have
    # __file__, we can work backwards from there to the root. Some
    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which
    # case we can only use expanded keywords.

    cfg = get_config()
    verbose = cfg.verbose

    try:
        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)
    except NotThisMethod:
        pass

    try:
        root = os.path.realpath(__file__)
        # versionfile_source is the relative path from the top of the source
        # tree (where the .git directory might live) to this file. Invert
        # this to find the root from __file__.
        for _ in cfg.versionfile_source.split("/"):
            root = os.path.dirname(root)
    except NameError:
        return {
            "version": "0+unknown",
            "full-revisionid": None,
            "dirty": None,
            "error": "unable to find root of source tree",
            "date": None,
        }

    try:
        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)
        return render(pieces, cfg.style)
    except NotThisMethod:
        pass

    try:
        if cfg.parentdir_prefix:
            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)
    except NotThisMethod:
        pass

    return {
        "version": "0+unknown",
        "full-revisionid": None,
        "dirty": None,
        "error": "unable to compute version",
        "date": None,
    }

if __name__ == "__main__":
    isT={'version': '0.5.2', 'full-revisionid': '61c94a4a354806aacdd280c61caed76df2b63205', 'dirty': False, 'error': None, 'date': '2023-02-17T16:50:17-0800'}==get_versions()
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/eykd---prestoplot/src/prestoplot/_version_render_passk_validte.py
# This file helps to compute a version number in source trees obtained from
# git-archive tarball (such as those provided by githubs download-from-tag
# feature). Distribution tarballs (built by setup.py sdist) and build
# directories (produced by setup.py build) will contain a much shorter file
# that just contains the computed version number.

# This file is released into the public domain. Generated by
# versioneer-0.22 (https://github.com/python-versioneer/python-versioneer)

"""Git implementation of _version.py."""

import errno
import functools
import os
import re
import subprocess
import sys
from typing import Callable, Dict


def get_keywords():
    """Get the keywords needed to look up the version information."""
    # these strings will be replaced by git during git-archive.
    # setup.py/versioneer.py will grep for the variable names, so they must
    # each be defined on a line of their own. _version.py will just call
    # get_keywords().
    git_refnames = " (HEAD -> master, tag: 0.5.2)"
    git_full = "61c94a4a354806aacdd280c61caed76df2b63205"
    git_date = "2023-02-17 16:50:17 -0800"
    keywords = {"refnames": git_refnames, "full": git_full, "date": git_date}
    return keywords


class VersioneerConfig:
    """Container for Versioneer configuration parameters."""


def get_config():
    """Create, populate and return the VersioneerConfig() object."""
    # these strings are filled in when 'setup.py versioneer' creates
    # _version.py
    cfg = VersioneerConfig()
    cfg.VCS = "git"
    cfg.style = "pep440"
    cfg.tag_prefix = ""
    cfg.parentdir_prefix = "None"
    cfg.versionfile_source = "src/prestoplot/_version.py"
    cfg.verbose = False
    return cfg


class NotThisMethod(Exception):
    """Exception raised if a method is not valid for the current scenario."""


LONG_VERSION_PY: Dict[str, str] = {}
HANDLERS: Dict[str, Dict[str, Callable]] = {}


def register_vcs_handler(vcs, method):  # decorator
    """Create decorator to mark a method as the handler of a VCS."""

    def decorate(f):
        """Store f in HANDLERS[vcs][method]."""
        if vcs not in HANDLERS:
            HANDLERS[vcs] = {}
        HANDLERS[vcs][method] = f
        return f

    return decorate


def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):
    """Call the given command(s)."""
    assert isinstance(commands, list)
    process = None

    popen_kwargs = {}
    if sys.platform == "win32":
        # This hides the console window if pythonw.exe is used
        startupinfo = subprocess.STARTUPINFO()
        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
        popen_kwargs["startupinfo"] = startupinfo

    for command in commands:
        try:
            dispcmd = str([command] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            process = subprocess.Popen(
                [command] + args,
                cwd=cwd,
                env=env,
                stdout=subprocess.PIPE,
                stderr=(subprocess.PIPE if hide_stderr else None),
                **popen_kwargs
            )
            break
        except OSError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print("unable to run %s" % dispcmd)
                print(e)
            return None, None
    else:
        if verbose:
            print("unable to find command, tried %s" % (commands,))
        return None, None
    stdout = process.communicate()[0].strip().decode()
    if process.returncode != 0:
        if verbose:
            print("unable to run %s (error)" % dispcmd)
            print("stdout was %s" % stdout)
        return None, process.returncode
    return stdout, process.returncode


def versions_from_parentdir(parentdir_prefix, root, verbose):
    """Try to determine the version from the parent directory name.

    Source tarballs conventionally unpack into a directory that includes both
    the project name and a version string. We will also support searching up
    two directory levels for an appropriately named parent directory
    """
    rootdirs = []

    for _ in range(3):
        dirname = os.path.basename(root)
        if dirname.startswith(parentdir_prefix):
            return {
                "version": dirname[len(parentdir_prefix) :],
                "full-revisionid": None,
                "dirty": False,
                "error": None,
                "date": None,
            }
        rootdirs.append(root)
        root = os.path.dirname(root)  # up a level

    if verbose:
        print(
            "Tried directories %s but none started with prefix %s"
            % (str(rootdirs), parentdir_prefix)
        )
    raise NotThisMethod("rootdir doesn't start with parentdir_prefix")


@register_vcs_handler("git", "get_keywords")
def git_get_keywords(versionfile_abs):
    """Extract version information from the given file."""
    # the code embedded in _version.py can just fetch the value of these
    # keywords. When used from setup.py, we don't want to import _version.py,
    # so we do it with a regexp instead. This function is not used from
    # _version.py.
    keywords = {}
    try:
        with open(versionfile_abs, "r") as fobj:
            for line in fobj:
                if line.strip().startswith("git_refnames ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["refnames"] = mo.group(1)
                if line.strip().startswith("git_full ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["full"] = mo.group(1)
                if line.strip().startswith("git_date ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["date"] = mo.group(1)
    except OSError:
        pass
    return keywords


@register_vcs_handler("git", "keywords")
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """Get version information from git keywords."""
    if "refnames" not in keywords:
        raise NotThisMethod("Short version file found")
    date = keywords.get("date")
    if date is not None:
        # Use only the last line.  Previous lines may contain GPG signature
        # information.
        date = date.splitlines()[-1]

        # git-2.2.0 added "%cI", which expands to an ISO-8601 -compliant
        # datestamp. However we prefer "%ci" (which expands to an "ISO-8601
        # -like" string, which we must then edit to make compliant), because
        # it's been around since git-1.5.3, and it's too difficult to
        # discover which version we're using, or to work around using an
        # older one.
        date = date.strip().replace(" ", "T", 1).replace(" ", "", 1)
    refnames = keywords["refnames"].strip()
    if refnames.startswith("$Format"):
        if verbose:
            print("keywords are unexpanded, not using")
        raise NotThisMethod("unexpanded keywords, not a git-archive tarball")
    refs = {r.strip() for r in refnames.strip("()").split(",")}
    # starting in git-1.8.3, tags are listed as "tag: foo-1.0" instead of
    # just "foo-1.0". If we see a "tag: " prefix, prefer those.
    TAG = "tag: "
    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}
    if not tags:
        # Either we're using git < 1.8.3, or there really are no tags. We use
        # a heuristic: assume all version tags have a digit. The old git %d
        # expansion behaves like git log --decorate=short and strips out the
        # refs/heads/ and refs/tags/ prefixes that would let us distinguish
        # between branches and tags. By ignoring refnames without digits, we
        # filter out many common branch names like "release" and
        # "stabilization", as well as "HEAD" and "master".
        tags = {r for r in refs if re.search(r"\d", r)}
        if verbose:
            print("discarding '%s', no digits" % ",".join(refs - tags))
    if verbose:
        print("likely tags: %s" % ",".join(sorted(tags)))
    for ref in sorted(tags):
        # sorting will prefer e.g. "2.0" over "2.0rc1"
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix) :]
            # Filter out refs that exactly match prefix or that don't start
            # with a number once the prefix is stripped (mostly a concern
            # when prefix is '')
            if not re.match(r"\d", r):
                continue
            if verbose:
                print("picking %s" % r)
            return {
                "version": r,
                "full-revisionid": keywords["full"].strip(),
                "dirty": False,
                "error": None,
                "date": date,
            }
    # no suitable tags, so version is "0+unknown", but full hex is still there
    if verbose:
        print("no suitable tags, using unknown + full revision id")
    return {
        "version": "0+unknown",
        "full-revisionid": keywords["full"].strip(),
        "dirty": False,
        "error": "no suitable tags",
        "date": None,
    }


@register_vcs_handler("git", "pieces_from_vcs")
def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):
    """Get version from 'git describe' in the root of the source tree.

    This only gets called if the git-archive 'subst' keywords were *not*
    expanded, and _version.py hasn't already been rewritten with a short
    version string, meaning we're inside a checked out source tree.
    """
    GITS = ["git"]
    if sys.platform == "win32":
        GITS = ["git.cmd", "git.exe"]

    # GIT_DIR can interfere with correct operation of Versioneer.
    # It may be intended to be passed to the Versioneer-versioned project,
    # but that should not change where we get our version from.
    env = os.environ.copy()
    env.pop("GIT_DIR", None)
    runner = functools.partial(runner, env=env)

    _, rc = runner(GITS, ["rev-parse", "--git-dir"], cwd=root, hide_stderr=True)
    if rc != 0:
        if verbose:
            print("Directory %s not under git control" % root)
        raise NotThisMethod("'git rev-parse --git-dir' returned error")

    MATCH_ARGS = ["--match", "%s*" % tag_prefix] if tag_prefix else []

    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]
    # if there isn't one, this yields HEX[-dirty] (no NUM)
    describe_out, rc = runner(
        GITS,
        ["describe", "--tags", "--dirty", "--always", "--long", *MATCH_ARGS],
        cwd=root,
    )
    # --long was added in git-1.5.5
    if describe_out is None:
        raise NotThisMethod("'git describe' failed")
    describe_out = describe_out.strip()
    full_out, rc = runner(GITS, ["rev-parse", "HEAD"], cwd=root)
    if full_out is None:
        raise NotThisMethod("'git rev-parse' failed")
    full_out = full_out.strip()

    pieces = {}
    pieces["long"] = full_out
    pieces["short"] = full_out[:7]  # maybe improved later
    pieces["error"] = None

    branch_name, rc = runner(GITS, ["rev-parse", "--abbrev-ref", "HEAD"], cwd=root)
    # --abbrev-ref was added in git-1.6.3
    if rc != 0 or branch_name is None:
        raise NotThisMethod("'git rev-parse --abbrev-ref' returned error")
    branch_name = branch_name.strip()

    if branch_name == "HEAD":
        # If we aren't exactly on a branch, pick a branch which represents
        # the current commit. If all else fails, we are on a branchless
        # commit.
        branches, rc = runner(GITS, ["branch", "--contains"], cwd=root)
        # --contains was added in git-1.5.4
        if rc != 0 or branches is None:
            raise NotThisMethod("'git branch --contains' returned error")
        branches = branches.split("\n")

        # Remove the first line if we're running detached
        if "(" in branches[0]:
            branches.pop(0)

        # Strip off the leading "* " from the list of branches.
        branches = [branch[2:] for branch in branches]
        if "master" in branches:
            branch_name = "master"
        elif not branches:
            branch_name = None
        else:
            # Pick the first branch that is returned. Good or bad.
            branch_name = branches[0]

    pieces["branch"] = branch_name

    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]
    # TAG might have hyphens.
    git_describe = describe_out

    # look for -dirty suffix
    dirty = git_describe.endswith("-dirty")
    pieces["dirty"] = dirty
    if dirty:
        git_describe = git_describe[: git_describe.rindex("-dirty")]

    # now we have TAG-NUM-gHEX or HEX

    if "-" in git_describe:
        # TAG-NUM-gHEX
        mo = re.search(r"^(.+)-(\d+)-g([0-9a-f]+)$", git_describe)
        if not mo:
            # unparsable. Maybe git-describe is misbehaving?
            pieces["error"] = "unable to parse git-describe output: '%s'" % describe_out
            return pieces

        # tag
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = "tag '%s' doesn't start with prefix '%s'"
                print(fmt % (full_tag, tag_prefix))
            pieces["error"] = "tag '%s' doesn't start with prefix '%s'" % (
                full_tag,
                tag_prefix,
            )
            return pieces
        pieces["closest-tag"] = full_tag[len(tag_prefix) :]

        # distance: number of commits since tag
        pieces["distance"] = int(mo.group(2))

        # commit: short hex revision ID
        pieces["short"] = mo.group(3)

    else:
        # HEX: no tags
        pieces["closest-tag"] = None
        count_out, rc = runner(GITS, ["rev-list", "HEAD", "--count"], cwd=root)
        pieces["distance"] = int(count_out)  # total number of commits

    # commit date: see ISO-8601 comment in git_versions_from_keywords()
    date = runner(GITS, ["show", "-s", "--format=%ci", "HEAD"], cwd=root)[0].strip()
    # Use only the last line.  Previous lines may contain GPG signature
    # information.
    date = date.splitlines()[-1]
    pieces["date"] = date.strip().replace(" ", "T", 1).replace(" ", "", 1)

    return pieces


def plus_or_dot(pieces):
    """Return a + if we don't already have one, else return a ."""
    if "+" in pieces.get("closest-tag", ""):
        return "."
    return "+"


def render_pep440(pieces):
    """Build up version string, with post-release "local version identifier".

    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you
    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty

    Exceptions:
    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def render_pep440_branch(pieces):
    """TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .

    The ".dev0" means not master branch. Note that .dev0 sorts backwards
    (a feature branch will appear "older" than the master branch).

    Exceptions:
    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            if pieces["branch"] != "master":
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0"
        if pieces["branch"] != "master":
            rendered += ".dev0"
        rendered += "+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def pep440_split_post(ver):
    """Split pep440 version string at the post-release segment.

    Returns the release segments before the post-release and the
    post-release version number (or -1 if no post-release segment is present).
    """
    vc = str.split(ver, ".post")
    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None


def render_pep440_pre(pieces):
    """TAG[.postN.devDISTANCE] -- No -dirty.

    Exceptions:
    1: no tags. 0.post0.devDISTANCE
    """
    if pieces["closest-tag"]:
        if pieces["distance"]:
            # update the post release segment
            tag_version, post_version = pep440_split_post(pieces["closest-tag"])
            rendered = tag_version
            if post_version is not None:
                rendered += ".post%d.dev%d" % (post_version + 1, pieces["distance"])
            else:
                rendered += ".post0.dev%d" % (pieces["distance"])
        else:
            # no commits, use the tag as the version
            rendered = pieces["closest-tag"]
    else:
        # exception #1
        rendered = "0.post0.dev%d" % pieces["distance"]
    return rendered


def render_pep440_post(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX] .

    The ".dev0" means dirty. Note that .dev0 sorts backwards
    (a dirty tree will appear "older" than the corresponding clean one),
    but you shouldn't be releasing software with -dirty anyways.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
    return rendered


def render_pep440_post_branch(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .

    The ".dev0" means not master branch.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["branch"] != "master":
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["branch"] != "master":
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def render_pep440_old(pieces):
    """TAG[.postDISTANCE[.dev0]] .

    The ".dev0" means dirty.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
    return rendered


def render_git_describe(pieces):
    """TAG[-DISTANCE-gHEX][-dirty].

    Like 'git describe --tags --dirty --always'.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"]:
            rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


def render_git_describe_long(pieces):
    """TAG-DISTANCE-gHEX[-dirty].

    Like 'git describe --tags --dirty --always -long'.
    The distance/hash is unconditional.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


def render(pieces, style):
    """Render the given version pieces into the requested style."""
    if pieces["error"]:
        return {
            "version": "unknown",
            "full-revisionid": pieces.get("long"),
            "dirty": None,
            "error": pieces["error"],
            "date": None,
        }

    if not style or style == "default":
        style = "pep440"  # the default

    if style == "pep440":
        rendered = render_pep440(pieces)
    elif style == "pep440-branch":
        rendered = render_pep440_branch(pieces)
    elif style == "pep440-pre":
        rendered = render_pep440_pre(pieces)
    elif style == "pep440-post":
        rendered = render_pep440_post(pieces)
    elif style == "pep440-post-branch":
        rendered = render_pep440_post_branch(pieces)
    elif style == "pep440-old":
        rendered = render_pep440_old(pieces)
    elif style == "git-describe":
        rendered = render_git_describe(pieces)
    elif style == "git-describe-long":
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError("unknown style '%s'" % style)

    return {
        "version": rendered,
        "full-revisionid": pieces["long"],
        "dirty": pieces["dirty"],
        "error": None,
        "date": pieces.get("date"),
    }


def get_versions():
    """Get version information or return default if unable to do so."""
    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have
    # __file__, we can work backwards from there to the root. Some
    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which
    # case we can only use expanded keywords.

    cfg = get_config()
    verbose = cfg.verbose

    try:
        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)
    except NotThisMethod:
        pass

    try:
        root = os.path.realpath(__file__)
        # versionfile_source is the relative path from the top of the source
        # tree (where the .git directory might live) to this file. Invert
        # this to find the root from __file__.
        for _ in cfg.versionfile_source.split("/"):
            root = os.path.dirname(root)
    except NameError:
        return {
            "version": "0+unknown",
            "full-revisionid": None,
            "dirty": None,
            "error": "unable to find root of source tree",
            "date": None,
        }

    try:
        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)
        return render(pieces, cfg.style)
    except NotThisMethod:
        pass

    try:
        if cfg.parentdir_prefix:
            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)
    except NotThisMethod:
        pass

    return {
        "version": "0+unknown",
        "full-revisionid": None,
        "dirty": None,
        "error": "unable to compute version",
        "date": None,
    }

if __name__ == "__main__":
    isT={'version': '0.4+16.g638dcc4.dirty', 'full-revisionid': '638dcc4259f785acc35f8237451c6b5c65468c29', 'dirty': True, 'error': None, 'date': '2022-05-10T11:51:19-0700'}==render({'long': '638dcc4259f785acc35f8237451c6b5c65468c29', 'short': '638dcc4', 'error': None, 'branch': 'master', 'dirty': True, 'closest-tag': '0.4', 'distance': 16, 'date': '2022-05-10T11:51:19-0700'},'pep440')


    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/eykd---prestoplot/src/prestoplot/_version_plus_or_dot_passk_validte.py
# This file helps to compute a version number in source trees obtained from
# git-archive tarball (such as those provided by githubs download-from-tag
# feature). Distribution tarballs (built by setup.py sdist) and build
# directories (produced by setup.py build) will contain a much shorter file
# that just contains the computed version number.

# This file is released into the public domain. Generated by
# versioneer-0.22 (https://github.com/python-versioneer/python-versioneer)

"""Git implementation of _version.py."""

import errno
import functools
import os
import re
import subprocess
import sys
from typing import Callable, Dict


def get_keywords():
    """Get the keywords needed to look up the version information."""
    # these strings will be replaced by git during git-archive.
    # setup.py/versioneer.py will grep for the variable names, so they must
    # each be defined on a line of their own. _version.py will just call
    # get_keywords().
    git_refnames = " (HEAD -> master, tag: 0.5.2)"
    git_full = "61c94a4a354806aacdd280c61caed76df2b63205"
    git_date = "2023-02-17 16:50:17 -0800"
    keywords = {"refnames": git_refnames, "full": git_full, "date": git_date}
    return keywords


class VersioneerConfig:
    """Container for Versioneer configuration parameters."""


def get_config():
    """Create, populate and return the VersioneerConfig() object."""
    # these strings are filled in when 'setup.py versioneer' creates
    # _version.py
    cfg = VersioneerConfig()
    cfg.VCS = "git"
    cfg.style = "pep440"
    cfg.tag_prefix = ""
    cfg.parentdir_prefix = "None"
    cfg.versionfile_source = "src/prestoplot/_version.py"
    cfg.verbose = False
    return cfg


class NotThisMethod(Exception):
    """Exception raised if a method is not valid for the current scenario."""


LONG_VERSION_PY: Dict[str, str] = {}
HANDLERS: Dict[str, Dict[str, Callable]] = {}


def register_vcs_handler(vcs, method):  # decorator
    """Create decorator to mark a method as the handler of a VCS."""

    def decorate(f):
        """Store f in HANDLERS[vcs][method]."""
        if vcs not in HANDLERS:
            HANDLERS[vcs] = {}
        HANDLERS[vcs][method] = f
        return f

    return decorate


def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):
    """Call the given command(s)."""
    assert isinstance(commands, list)
    process = None

    popen_kwargs = {}
    if sys.platform == "win32":
        # This hides the console window if pythonw.exe is used
        startupinfo = subprocess.STARTUPINFO()
        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
        popen_kwargs["startupinfo"] = startupinfo

    for command in commands:
        try:
            dispcmd = str([command] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            process = subprocess.Popen(
                [command] + args,
                cwd=cwd,
                env=env,
                stdout=subprocess.PIPE,
                stderr=(subprocess.PIPE if hide_stderr else None),
                **popen_kwargs
            )
            break
        except OSError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print("unable to run %s" % dispcmd)
                print(e)
            return None, None
    else:
        if verbose:
            print("unable to find command, tried %s" % (commands,))
        return None, None
    stdout = process.communicate()[0].strip().decode()
    if process.returncode != 0:
        if verbose:
            print("unable to run %s (error)" % dispcmd)
            print("stdout was %s" % stdout)
        return None, process.returncode
    return stdout, process.returncode


def versions_from_parentdir(parentdir_prefix, root, verbose):
    """Try to determine the version from the parent directory name.

    Source tarballs conventionally unpack into a directory that includes both
    the project name and a version string. We will also support searching up
    two directory levels for an appropriately named parent directory
    """
    rootdirs = []

    for _ in range(3):
        dirname = os.path.basename(root)
        if dirname.startswith(parentdir_prefix):
            return {
                "version": dirname[len(parentdir_prefix) :],
                "full-revisionid": None,
                "dirty": False,
                "error": None,
                "date": None,
            }
        rootdirs.append(root)
        root = os.path.dirname(root)  # up a level

    if verbose:
        print(
            "Tried directories %s but none started with prefix %s"
            % (str(rootdirs), parentdir_prefix)
        )
    raise NotThisMethod("rootdir doesn't start with parentdir_prefix")


@register_vcs_handler("git", "get_keywords")
def git_get_keywords(versionfile_abs):
    """Extract version information from the given file."""
    # the code embedded in _version.py can just fetch the value of these
    # keywords. When used from setup.py, we don't want to import _version.py,
    # so we do it with a regexp instead. This function is not used from
    # _version.py.
    keywords = {}
    try:
        with open(versionfile_abs, "r") as fobj:
            for line in fobj:
                if line.strip().startswith("git_refnames ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["refnames"] = mo.group(1)
                if line.strip().startswith("git_full ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["full"] = mo.group(1)
                if line.strip().startswith("git_date ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["date"] = mo.group(1)
    except OSError:
        pass
    return keywords


@register_vcs_handler("git", "keywords")
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """Get version information from git keywords."""
    if "refnames" not in keywords:
        raise NotThisMethod("Short version file found")
    date = keywords.get("date")
    if date is not None:
        # Use only the last line.  Previous lines may contain GPG signature
        # information.
        date = date.splitlines()[-1]

        # git-2.2.0 added "%cI", which expands to an ISO-8601 -compliant
        # datestamp. However we prefer "%ci" (which expands to an "ISO-8601
        # -like" string, which we must then edit to make compliant), because
        # it's been around since git-1.5.3, and it's too difficult to
        # discover which version we're using, or to work around using an
        # older one.
        date = date.strip().replace(" ", "T", 1).replace(" ", "", 1)
    refnames = keywords["refnames"].strip()
    if refnames.startswith("$Format"):
        if verbose:
            print("keywords are unexpanded, not using")
        raise NotThisMethod("unexpanded keywords, not a git-archive tarball")
    refs = {r.strip() for r in refnames.strip("()").split(",")}
    # starting in git-1.8.3, tags are listed as "tag: foo-1.0" instead of
    # just "foo-1.0". If we see a "tag: " prefix, prefer those.
    TAG = "tag: "
    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}
    if not tags:
        # Either we're using git < 1.8.3, or there really are no tags. We use
        # a heuristic: assume all version tags have a digit. The old git %d
        # expansion behaves like git log --decorate=short and strips out the
        # refs/heads/ and refs/tags/ prefixes that would let us distinguish
        # between branches and tags. By ignoring refnames without digits, we
        # filter out many common branch names like "release" and
        # "stabilization", as well as "HEAD" and "master".
        tags = {r for r in refs if re.search(r"\d", r)}
        if verbose:
            print("discarding '%s', no digits" % ",".join(refs - tags))
    if verbose:
        print("likely tags: %s" % ",".join(sorted(tags)))
    for ref in sorted(tags):
        # sorting will prefer e.g. "2.0" over "2.0rc1"
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix) :]
            # Filter out refs that exactly match prefix or that don't start
            # with a number once the prefix is stripped (mostly a concern
            # when prefix is '')
            if not re.match(r"\d", r):
                continue
            if verbose:
                print("picking %s" % r)
            return {
                "version": r,
                "full-revisionid": keywords["full"].strip(),
                "dirty": False,
                "error": None,
                "date": date,
            }
    # no suitable tags, so version is "0+unknown", but full hex is still there
    if verbose:
        print("no suitable tags, using unknown + full revision id")
    return {
        "version": "0+unknown",
        "full-revisionid": keywords["full"].strip(),
        "dirty": False,
        "error": "no suitable tags",
        "date": None,
    }


@register_vcs_handler("git", "pieces_from_vcs")
def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):
    """Get version from 'git describe' in the root of the source tree.

    This only gets called if the git-archive 'subst' keywords were *not*
    expanded, and _version.py hasn't already been rewritten with a short
    version string, meaning we're inside a checked out source tree.
    """
    GITS = ["git"]
    if sys.platform == "win32":
        GITS = ["git.cmd", "git.exe"]

    # GIT_DIR can interfere with correct operation of Versioneer.
    # It may be intended to be passed to the Versioneer-versioned project,
    # but that should not change where we get our version from.
    env = os.environ.copy()
    env.pop("GIT_DIR", None)
    runner = functools.partial(runner, env=env)

    _, rc = runner(GITS, ["rev-parse", "--git-dir"], cwd=root, hide_stderr=True)
    if rc != 0:
        if verbose:
            print("Directory %s not under git control" % root)
        raise NotThisMethod("'git rev-parse --git-dir' returned error")

    MATCH_ARGS = ["--match", "%s*" % tag_prefix] if tag_prefix else []

    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]
    # if there isn't one, this yields HEX[-dirty] (no NUM)
    describe_out, rc = runner(
        GITS,
        ["describe", "--tags", "--dirty", "--always", "--long", *MATCH_ARGS],
        cwd=root,
    )
    # --long was added in git-1.5.5
    if describe_out is None:
        raise NotThisMethod("'git describe' failed")
    describe_out = describe_out.strip()
    full_out, rc = runner(GITS, ["rev-parse", "HEAD"], cwd=root)
    if full_out is None:
        raise NotThisMethod("'git rev-parse' failed")
    full_out = full_out.strip()

    pieces = {}
    pieces["long"] = full_out
    pieces["short"] = full_out[:7]  # maybe improved later
    pieces["error"] = None

    branch_name, rc = runner(GITS, ["rev-parse", "--abbrev-ref", "HEAD"], cwd=root)
    # --abbrev-ref was added in git-1.6.3
    if rc != 0 or branch_name is None:
        raise NotThisMethod("'git rev-parse --abbrev-ref' returned error")
    branch_name = branch_name.strip()

    if branch_name == "HEAD":
        # If we aren't exactly on a branch, pick a branch which represents
        # the current commit. If all else fails, we are on a branchless
        # commit.
        branches, rc = runner(GITS, ["branch", "--contains"], cwd=root)
        # --contains was added in git-1.5.4
        if rc != 0 or branches is None:
            raise NotThisMethod("'git branch --contains' returned error")
        branches = branches.split("\n")

        # Remove the first line if we're running detached
        if "(" in branches[0]:
            branches.pop(0)

        # Strip off the leading "* " from the list of branches.
        branches = [branch[2:] for branch in branches]
        if "master" in branches:
            branch_name = "master"
        elif not branches:
            branch_name = None
        else:
            # Pick the first branch that is returned. Good or bad.
            branch_name = branches[0]

    pieces["branch"] = branch_name

    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]
    # TAG might have hyphens.
    git_describe = describe_out

    # look for -dirty suffix
    dirty = git_describe.endswith("-dirty")
    pieces["dirty"] = dirty
    if dirty:
        git_describe = git_describe[: git_describe.rindex("-dirty")]

    # now we have TAG-NUM-gHEX or HEX

    if "-" in git_describe:
        # TAG-NUM-gHEX
        mo = re.search(r"^(.+)-(\d+)-g([0-9a-f]+)$", git_describe)
        if not mo:
            # unparsable. Maybe git-describe is misbehaving?
            pieces["error"] = "unable to parse git-describe output: '%s'" % describe_out
            return pieces

        # tag
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = "tag '%s' doesn't start with prefix '%s'"
                print(fmt % (full_tag, tag_prefix))
            pieces["error"] = "tag '%s' doesn't start with prefix '%s'" % (
                full_tag,
                tag_prefix,
            )
            return pieces
        pieces["closest-tag"] = full_tag[len(tag_prefix) :]

        # distance: number of commits since tag
        pieces["distance"] = int(mo.group(2))

        # commit: short hex revision ID
        pieces["short"] = mo.group(3)

    else:
        # HEX: no tags
        pieces["closest-tag"] = None
        count_out, rc = runner(GITS, ["rev-list", "HEAD", "--count"], cwd=root)
        pieces["distance"] = int(count_out)  # total number of commits

    # commit date: see ISO-8601 comment in git_versions_from_keywords()
    date = runner(GITS, ["show", "-s", "--format=%ci", "HEAD"], cwd=root)[0].strip()
    # Use only the last line.  Previous lines may contain GPG signature
    # information.
    date = date.splitlines()[-1]
    pieces["date"] = date.strip().replace(" ", "T", 1).replace(" ", "", 1)

    return pieces


def plus_or_dot(pieces):
    """Return a + if we don't already have one, else return a ."""
    if "+" in pieces.get("closest-tag", ""):
        return "."
    return "+"


def render_pep440(pieces):
    """Build up version string, with post-release "local version identifier".

    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you
    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty

    Exceptions:
    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def render_pep440_branch(pieces):
    """TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .

    The ".dev0" means not master branch. Note that .dev0 sorts backwards
    (a feature branch will appear "older" than the master branch).

    Exceptions:
    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            if pieces["branch"] != "master":
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0"
        if pieces["branch"] != "master":
            rendered += ".dev0"
        rendered += "+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def pep440_split_post(ver):
    """Split pep440 version string at the post-release segment.

    Returns the release segments before the post-release and the
    post-release version number (or -1 if no post-release segment is present).
    """
    vc = str.split(ver, ".post")
    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None


def render_pep440_pre(pieces):
    """TAG[.postN.devDISTANCE] -- No -dirty.

    Exceptions:
    1: no tags. 0.post0.devDISTANCE
    """
    if pieces["closest-tag"]:
        if pieces["distance"]:
            # update the post release segment
            tag_version, post_version = pep440_split_post(pieces["closest-tag"])
            rendered = tag_version
            if post_version is not None:
                rendered += ".post%d.dev%d" % (post_version + 1, pieces["distance"])
            else:
                rendered += ".post0.dev%d" % (pieces["distance"])
        else:
            # no commits, use the tag as the version
            rendered = pieces["closest-tag"]
    else:
        # exception #1
        rendered = "0.post0.dev%d" % pieces["distance"]
    return rendered


def render_pep440_post(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX] .

    The ".dev0" means dirty. Note that .dev0 sorts backwards
    (a dirty tree will appear "older" than the corresponding clean one),
    but you shouldn't be releasing software with -dirty anyways.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
    return rendered


def render_pep440_post_branch(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .

    The ".dev0" means not master branch.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["branch"] != "master":
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["branch"] != "master":
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def render_pep440_old(pieces):
    """TAG[.postDISTANCE[.dev0]] .

    The ".dev0" means dirty.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
    return rendered


def render_git_describe(pieces):
    """TAG[-DISTANCE-gHEX][-dirty].

    Like 'git describe --tags --dirty --always'.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"]:
            rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


def render_git_describe_long(pieces):
    """TAG-DISTANCE-gHEX[-dirty].

    Like 'git describe --tags --dirty --always -long'.
    The distance/hash is unconditional.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


def render(pieces, style):
    """Render the given version pieces into the requested style."""
    if pieces["error"]:
        return {
            "version": "unknown",
            "full-revisionid": pieces.get("long"),
            "dirty": None,
            "error": pieces["error"],
            "date": None,
        }

    if not style or style == "default":
        style = "pep440"  # the default

    if style == "pep440":
        rendered = render_pep440(pieces)
    elif style == "pep440-branch":
        rendered = render_pep440_branch(pieces)
    elif style == "pep440-pre":
        rendered = render_pep440_pre(pieces)
    elif style == "pep440-post":
        rendered = render_pep440_post(pieces)
    elif style == "pep440-post-branch":
        rendered = render_pep440_post_branch(pieces)
    elif style == "pep440-old":
        rendered = render_pep440_old(pieces)
    elif style == "git-describe":
        rendered = render_git_describe(pieces)
    elif style == "git-describe-long":
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError("unknown style '%s'" % style)

    return {
        "version": rendered,
        "full-revisionid": pieces["long"],
        "dirty": pieces["dirty"],
        "error": None,
        "date": pieces.get("date"),
    }


def get_versions():
    """Get version information or return default if unable to do so."""
    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have
    # __file__, we can work backwards from there to the root. Some
    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which
    # case we can only use expanded keywords.

    cfg = get_config()
    verbose = cfg.verbose

    try:
        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)
    except NotThisMethod:
        pass

    try:
        root = os.path.realpath(__file__)
        # versionfile_source is the relative path from the top of the source
        # tree (where the .git directory might live) to this file. Invert
        # this to find the root from __file__.
        for _ in cfg.versionfile_source.split("/"):
            root = os.path.dirname(root)
    except NameError:
        return {
            "version": "0+unknown",
            "full-revisionid": None,
            "dirty": None,
            "error": "unable to find root of source tree",
            "date": None,
        }

    try:
        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)
        return render(pieces, cfg.style)
    except NotThisMethod:
        pass

    try:
        if cfg.parentdir_prefix:
            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)
    except NotThisMethod:
        pass

    return {
        "version": "0+unknown",
        "full-revisionid": None,
        "dirty": None,
        "error": "unable to compute version",
        "date": None,
    }

if __name__ == "__main__":
    isT=True
    input1={'long': '638dcc4259f785acc35f8237451c6b5c65468c29', 'short': '638dcc4', 'error': None, 'branch': 'master', 'dirty': True, 'closest-tag': '0.4', 'distance': 16, 'date': '2022-05-10T11:51:19-0700'}
    output1=plus_or_dot(input1)
    ist1=output1=="+"
    input2 = {'long': '638dcc4259f785acc35f8237451c6b5c65468c29', 'short': '638dcc4', 'error': None, 'branch': 'master',
              'dirty': True, 'closest-tag': ["+","-"], 'distance': 16, 'date': '2022-05-10T11:51:19-0700'}
    output2 = plus_or_dot(input2)
    ist2 = output2 == "."
    if not ist1 or not ist2:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/eykd---prestoplot/src/prestoplot/_version_run_command_passk_validte.py
# This file helps to compute a version number in source trees obtained from
# git-archive tarball (such as those provided by githubs download-from-tag
# feature). Distribution tarballs (built by setup.py sdist) and build
# directories (produced by setup.py build) will contain a much shorter file
# that just contains the computed version number.

# This file is released into the public domain. Generated by
# versioneer-0.22 (https://github.com/python-versioneer/python-versioneer)

"""Git implementation of _version.py."""

import errno
import functools
import os
import re
import subprocess
import sys
from typing import Callable, Dict


def get_keywords():
    """Get the keywords needed to look up the version information."""
    # these strings will be replaced by git during git-archive.
    # setup.py/versioneer.py will grep for the variable names, so they must
    # each be defined on a line of their own. _version.py will just call
    # get_keywords().
    git_refnames = " (HEAD -> master, tag: 0.5.2)"
    git_full = "61c94a4a354806aacdd280c61caed76df2b63205"
    git_date = "2023-02-17 16:50:17 -0800"
    keywords = {"refnames": git_refnames, "full": git_full, "date": git_date}
    return keywords


class VersioneerConfig:
    """Container for Versioneer configuration parameters."""


def get_config():
    """Create, populate and return the VersioneerConfig() object."""
    # these strings are filled in when 'setup.py versioneer' creates
    # _version.py
    cfg = VersioneerConfig()
    cfg.VCS = "git"
    cfg.style = "pep440"
    cfg.tag_prefix = ""
    cfg.parentdir_prefix = "None"
    cfg.versionfile_source = "src/prestoplot/_version.py"
    cfg.verbose = False
    return cfg


class NotThisMethod(Exception):
    """Exception raised if a method is not valid for the current scenario."""


LONG_VERSION_PY: Dict[str, str] = {}
HANDLERS: Dict[str, Dict[str, Callable]] = {}


def register_vcs_handler(vcs, method):  # decorator
    """Create decorator to mark a method as the handler of a VCS."""

    def decorate(f):
        """Store f in HANDLERS[vcs][method]."""
        if vcs not in HANDLERS:
            HANDLERS[vcs] = {}
        HANDLERS[vcs][method] = f
        return f

    return decorate


def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):
    """Call the given command(s)."""
    assert isinstance(commands, list)
    process = None

    popen_kwargs = {}
    if sys.platform == "win32":
        # This hides the console window if pythonw.exe is used
        startupinfo = subprocess.STARTUPINFO()
        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
        popen_kwargs["startupinfo"] = startupinfo

    for command in commands:
        try:
            dispcmd = str([command] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            process = subprocess.Popen(
                [command] + args,
                cwd=cwd,
                env=env,
                stdout=subprocess.PIPE,
                stderr=(subprocess.PIPE if hide_stderr else None),
                **popen_kwargs
            )
            break
        except OSError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print("unable to run %s" % dispcmd)
                print(e)
            return None, None
    else:
        if verbose:
            print("unable to find command, tried %s" % (commands,))
        return None, None
    stdout = process.communicate()[0].strip().decode()
    if process.returncode != 0:
        if verbose:
            print("unable to run %s (error)" % dispcmd)
            print("stdout was %s" % stdout)
        return None, process.returncode
    return stdout, process.returncode


def versions_from_parentdir(parentdir_prefix, root, verbose):
    """Try to determine the version from the parent directory name.

    Source tarballs conventionally unpack into a directory that includes both
    the project name and a version string. We will also support searching up
    two directory levels for an appropriately named parent directory
    """
    rootdirs = []

    for _ in range(3):
        dirname = os.path.basename(root)
        if dirname.startswith(parentdir_prefix):
            return {
                "version": dirname[len(parentdir_prefix) :],
                "full-revisionid": None,
                "dirty": False,
                "error": None,
                "date": None,
            }
        rootdirs.append(root)
        root = os.path.dirname(root)  # up a level

    if verbose:
        print(
            "Tried directories %s but none started with prefix %s"
            % (str(rootdirs), parentdir_prefix)
        )
    raise NotThisMethod("rootdir doesn't start with parentdir_prefix")


@register_vcs_handler("git", "get_keywords")
def git_get_keywords(versionfile_abs):
    """Extract version information from the given file."""
    # the code embedded in _version.py can just fetch the value of these
    # keywords. When used from setup.py, we don't want to import _version.py,
    # so we do it with a regexp instead. This function is not used from
    # _version.py.
    keywords = {}
    try:
        with open(versionfile_abs, "r") as fobj:
            for line in fobj:
                if line.strip().startswith("git_refnames ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["refnames"] = mo.group(1)
                if line.strip().startswith("git_full ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["full"] = mo.group(1)
                if line.strip().startswith("git_date ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["date"] = mo.group(1)
    except OSError:
        pass
    return keywords


@register_vcs_handler("git", "keywords")
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """Get version information from git keywords."""
    if "refnames" not in keywords:
        raise NotThisMethod("Short version file found")
    date = keywords.get("date")
    if date is not None:
        # Use only the last line.  Previous lines may contain GPG signature
        # information.
        date = date.splitlines()[-1]

        # git-2.2.0 added "%cI", which expands to an ISO-8601 -compliant
        # datestamp. However we prefer "%ci" (which expands to an "ISO-8601
        # -like" string, which we must then edit to make compliant), because
        # it's been around since git-1.5.3, and it's too difficult to
        # discover which version we're using, or to work around using an
        # older one.
        date = date.strip().replace(" ", "T", 1).replace(" ", "", 1)
    refnames = keywords["refnames"].strip()
    if refnames.startswith("$Format"):
        if verbose:
            print("keywords are unexpanded, not using")
        raise NotThisMethod("unexpanded keywords, not a git-archive tarball")
    refs = {r.strip() for r in refnames.strip("()").split(",")}
    # starting in git-1.8.3, tags are listed as "tag: foo-1.0" instead of
    # just "foo-1.0". If we see a "tag: " prefix, prefer those.
    TAG = "tag: "
    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}
    if not tags:
        # Either we're using git < 1.8.3, or there really are no tags. We use
        # a heuristic: assume all version tags have a digit. The old git %d
        # expansion behaves like git log --decorate=short and strips out the
        # refs/heads/ and refs/tags/ prefixes that would let us distinguish
        # between branches and tags. By ignoring refnames without digits, we
        # filter out many common branch names like "release" and
        # "stabilization", as well as "HEAD" and "master".
        tags = {r for r in refs if re.search(r"\d", r)}
        if verbose:
            print("discarding '%s', no digits" % ",".join(refs - tags))
    if verbose:
        print("likely tags: %s" % ",".join(sorted(tags)))
    for ref in sorted(tags):
        # sorting will prefer e.g. "2.0" over "2.0rc1"
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix) :]
            # Filter out refs that exactly match prefix or that don't start
            # with a number once the prefix is stripped (mostly a concern
            # when prefix is '')
            if not re.match(r"\d", r):
                continue
            if verbose:
                print("picking %s" % r)
            return {
                "version": r,
                "full-revisionid": keywords["full"].strip(),
                "dirty": False,
                "error": None,
                "date": date,
            }
    # no suitable tags, so version is "0+unknown", but full hex is still there
    if verbose:
        print("no suitable tags, using unknown + full revision id")
    return {
        "version": "0+unknown",
        "full-revisionid": keywords["full"].strip(),
        "dirty": False,
        "error": "no suitable tags",
        "date": None,
    }


@register_vcs_handler("git", "pieces_from_vcs")
def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):
    """Get version from 'git describe' in the root of the source tree.

    This only gets called if the git-archive 'subst' keywords were *not*
    expanded, and _version.py hasn't already been rewritten with a short
    version string, meaning we're inside a checked out source tree.
    """
    GITS = ["git"]
    if sys.platform == "win32":
        GITS = ["git.cmd", "git.exe"]

    # GIT_DIR can interfere with correct operation of Versioneer.
    # It may be intended to be passed to the Versioneer-versioned project,
    # but that should not change where we get our version from.
    env = os.environ.copy()
    env.pop("GIT_DIR", None)
    runner = functools.partial(runner, env=env)

    _, rc = runner(GITS, ["rev-parse", "--git-dir"], cwd=root, hide_stderr=True)
    if rc != 0:
        if verbose:
            print("Directory %s not under git control" % root)
        raise NotThisMethod("'git rev-parse --git-dir' returned error")

    MATCH_ARGS = ["--match", "%s*" % tag_prefix] if tag_prefix else []

    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]
    # if there isn't one, this yields HEX[-dirty] (no NUM)
    describe_out, rc = runner(
        GITS,
        ["describe", "--tags", "--dirty", "--always", "--long", *MATCH_ARGS],
        cwd=root,
    )
    # --long was added in git-1.5.5
    if describe_out is None:
        raise NotThisMethod("'git describe' failed")
    describe_out = describe_out.strip()
    full_out, rc = runner(GITS, ["rev-parse", "HEAD"], cwd=root)
    if full_out is None:
        raise NotThisMethod("'git rev-parse' failed")
    full_out = full_out.strip()

    pieces = {}
    pieces["long"] = full_out
    pieces["short"] = full_out[:7]  # maybe improved later
    pieces["error"] = None

    branch_name, rc = runner(GITS, ["rev-parse", "--abbrev-ref", "HEAD"], cwd=root)
    # --abbrev-ref was added in git-1.6.3
    if rc != 0 or branch_name is None:
        raise NotThisMethod("'git rev-parse --abbrev-ref' returned error")
    branch_name = branch_name.strip()

    if branch_name == "HEAD":
        # If we aren't exactly on a branch, pick a branch which represents
        # the current commit. If all else fails, we are on a branchless
        # commit.
        branches, rc = runner(GITS, ["branch", "--contains"], cwd=root)
        # --contains was added in git-1.5.4
        if rc != 0 or branches is None:
            raise NotThisMethod("'git branch --contains' returned error")
        branches = branches.split("\n")

        # Remove the first line if we're running detached
        if "(" in branches[0]:
            branches.pop(0)

        # Strip off the leading "* " from the list of branches.
        branches = [branch[2:] for branch in branches]
        if "master" in branches:
            branch_name = "master"
        elif not branches:
            branch_name = None
        else:
            # Pick the first branch that is returned. Good or bad.
            branch_name = branches[0]

    pieces["branch"] = branch_name

    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]
    # TAG might have hyphens.
    git_describe = describe_out

    # look for -dirty suffix
    dirty = git_describe.endswith("-dirty")
    pieces["dirty"] = dirty
    if dirty:
        git_describe = git_describe[: git_describe.rindex("-dirty")]

    # now we have TAG-NUM-gHEX or HEX

    if "-" in git_describe:
        # TAG-NUM-gHEX
        mo = re.search(r"^(.+)-(\d+)-g([0-9a-f]+)$", git_describe)
        if not mo:
            # unparsable. Maybe git-describe is misbehaving?
            pieces["error"] = "unable to parse git-describe output: '%s'" % describe_out
            return pieces

        # tag
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = "tag '%s' doesn't start with prefix '%s'"
                print(fmt % (full_tag, tag_prefix))
            pieces["error"] = "tag '%s' doesn't start with prefix '%s'" % (
                full_tag,
                tag_prefix,
            )
            return pieces
        pieces["closest-tag"] = full_tag[len(tag_prefix) :]

        # distance: number of commits since tag
        pieces["distance"] = int(mo.group(2))

        # commit: short hex revision ID
        pieces["short"] = mo.group(3)

    else:
        # HEX: no tags
        pieces["closest-tag"] = None
        count_out, rc = runner(GITS, ["rev-list", "HEAD", "--count"], cwd=root)
        pieces["distance"] = int(count_out)  # total number of commits

    # commit date: see ISO-8601 comment in git_versions_from_keywords()
    date = runner(GITS, ["show", "-s", "--format=%ci", "HEAD"], cwd=root)[0].strip()
    # Use only the last line.  Previous lines may contain GPG signature
    # information.
    date = date.splitlines()[-1]
    pieces["date"] = date.strip().replace(" ", "T", 1).replace(" ", "", 1)

    return pieces


def plus_or_dot(pieces):
    """Return a + if we don't already have one, else return a ."""
    if "+" in pieces.get("closest-tag", ""):
        return "."
    return "+"


def render_pep440(pieces):
    """Build up version string, with post-release "local version identifier".

    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you
    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty

    Exceptions:
    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def render_pep440_branch(pieces):
    """TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .

    The ".dev0" means not master branch. Note that .dev0 sorts backwards
    (a feature branch will appear "older" than the master branch).

    Exceptions:
    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            if pieces["branch"] != "master":
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0"
        if pieces["branch"] != "master":
            rendered += ".dev0"
        rendered += "+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def pep440_split_post(ver):
    """Split pep440 version string at the post-release segment.

    Returns the release segments before the post-release and the
    post-release version number (or -1 if no post-release segment is present).
    """
    vc = str.split(ver, ".post")
    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None


def render_pep440_pre(pieces):
    """TAG[.postN.devDISTANCE] -- No -dirty.

    Exceptions:
    1: no tags. 0.post0.devDISTANCE
    """
    if pieces["closest-tag"]:
        if pieces["distance"]:
            # update the post release segment
            tag_version, post_version = pep440_split_post(pieces["closest-tag"])
            rendered = tag_version
            if post_version is not None:
                rendered += ".post%d.dev%d" % (post_version + 1, pieces["distance"])
            else:
                rendered += ".post0.dev%d" % (pieces["distance"])
        else:
            # no commits, use the tag as the version
            rendered = pieces["closest-tag"]
    else:
        # exception #1
        rendered = "0.post0.dev%d" % pieces["distance"]
    return rendered


def render_pep440_post(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX] .

    The ".dev0" means dirty. Note that .dev0 sorts backwards
    (a dirty tree will appear "older" than the corresponding clean one),
    but you shouldn't be releasing software with -dirty anyways.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
    return rendered


def render_pep440_post_branch(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .

    The ".dev0" means not master branch.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["branch"] != "master":
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["branch"] != "master":
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def render_pep440_old(pieces):
    """TAG[.postDISTANCE[.dev0]] .

    The ".dev0" means dirty.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
    return rendered


def render_git_describe(pieces):
    """TAG[-DISTANCE-gHEX][-dirty].

    Like 'git describe --tags --dirty --always'.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"]:
            rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


def render_git_describe_long(pieces):
    """TAG-DISTANCE-gHEX[-dirty].

    Like 'git describe --tags --dirty --always -long'.
    The distance/hash is unconditional.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


def render(pieces, style):
    """Render the given version pieces into the requested style."""
    if pieces["error"]:
        return {
            "version": "unknown",
            "full-revisionid": pieces.get("long"),
            "dirty": None,
            "error": pieces["error"],
            "date": None,
        }

    if not style or style == "default":
        style = "pep440"  # the default

    if style == "pep440":
        rendered = render_pep440(pieces)
    elif style == "pep440-branch":
        rendered = render_pep440_branch(pieces)
    elif style == "pep440-pre":
        rendered = render_pep440_pre(pieces)
    elif style == "pep440-post":
        rendered = render_pep440_post(pieces)
    elif style == "pep440-post-branch":
        rendered = render_pep440_post_branch(pieces)
    elif style == "pep440-old":
        rendered = render_pep440_old(pieces)
    elif style == "git-describe":
        rendered = render_git_describe(pieces)
    elif style == "git-describe-long":
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError("unknown style '%s'" % style)

    return {
        "version": rendered,
        "full-revisionid": pieces["long"],
        "dirty": pieces["dirty"],
        "error": None,
        "date": pieces.get("date"),
    }


def get_versions():
    """Get version information or return default if unable to do so."""
    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have
    # __file__, we can work backwards from there to the root. Some
    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which
    # case we can only use expanded keywords.

    cfg = get_config()
    verbose = cfg.verbose

    try:
        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)
    except NotThisMethod:
        pass

    try:
        root = os.path.realpath(__file__)
        # versionfile_source is the relative path from the top of the source
        # tree (where the .git directory might live) to this file. Invert
        # this to find the root from __file__.
        for _ in cfg.versionfile_source.split("/"):
            root = os.path.dirname(root)
    except NameError:
        return {
            "version": "0+unknown",
            "full-revisionid": None,
            "dirty": None,
            "error": "unable to find root of source tree",
            "date": None,
        }

    try:
        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)
        return render(pieces, cfg.style)
    except NotThisMethod:
        pass

    try:
        if cfg.parentdir_prefix:
            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)
    except NotThisMethod:
        pass

    return {
        "version": "0+unknown",
        "full-revisionid": None,
        "dirty": None,
        "error": "unable to compute version",
        "date": None,
    }

if __name__ == "__main__":
    args1=['git']
    args2=['rev-parse', '--git-dir']
    args3='/home/travis/builds/repos/eykd---prestoplot'
    args4=False
    args5=True
    args6={'LD_LIBRARY_PATH': '/home/travis/openssl_1.0/lib', 'LANGUAGE': 'en_US.UTF-8',
     'https_proxy': 'http://s00646297:qhet26836x_6@proxysg.huawei.com:8080', 'LC_ALL': 'en_US.UTF-8',
     'TOX_WORK_DIR': '/home/travis/builds/repos/eykd---prestoplot/.tox', 'LANG': 'en_US.UTF-8',
     'PATH': '/home/travis/builds/repos/eykd---prestoplot/.tox/py37/bin:/home/travis/.pyenv/versions/3.7.0/bin:/home/travis/.pyenv/libexec:/home/travis/.pyenv/plugins/python-build/bin:/home/travis/.pyenv/shims:/home/travis/.pyenv/bin:/home/travis/openssl_1.0/bin:/home/travis/.phpenv/shims:/home/travis/gopath/bin:/home/travis/.local/bin:/opt/pyenv/shims:/home/travis/.phpenv/shims:/home/travis/perl5/perlbrew/bin:/home/travis/.pyenv/bin:/home/travis/openssl_1.0/bin:/home/travis/.phpenv/shims:/home/travis/gopath/bin:/home/travis/.gimme/versions/go1.11.1.linux.amd64/bin:/opt/python/3.7/bin:/usr/local/cmake-3.12.4/bin:/usr/local/clang-7.0.0/bin:/home/travis/.gimme/versions/go1.11.1.linux.amd64/bin:/opt/python/3.7/bin:/usr/local/cmake-3.12.4/bin:/usr/local/clang-7.0.0/bin:/home/travis/.local/bin:/home/travis/.phpenv/shims:/home/travis/.nvm/versions/node/v12.22.12/bin:/home/travis/.pyenv/bin:/home/travis/openssl_1.0/bin:/home/travis/.rvm/gems/ruby-2.7.0/bin:/home/travis/.rvm/gems/ruby-2.7.0@global/bin:/home/travis/.rvm/rubies/ruby-2.7.0/bin:/home/travis/.phpenv/shims:/home/travis/gopath/bin:/home/travis/.gimme/versions/go1.11.1.linux.amd64/bin:/opt/python/3.7/bin:/usr/local/cmake-3.12.4/bin:/usr/local/clang-7.0.0/bin:/home/travis/.gimme/versions/go1.11.1.linux.amd64/bin:/opt/python/3.7/bin:/usr/local/cmake-3.12.4/bin:/usr/local/clang-7.0.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/home/travis/.rvm/bin:/home/travis/.phpenv/bin:/opt/pyenv/bin:/home/travis/.phpenv/bin:/opt/pyenv/bin',
     'http_proxy': 'http://s00646297:qhet26836x_6@proxysg.huawei.com:8080', 'PYTHONHASHSEED': '1709895624',
     'TOX_ENV_NAME': 'py37', 'TOX_ENV_DIR': '/home/travis/builds/repos/eykd---prestoplot/.tox/py37',
     'VIRTUAL_ENV': '/home/travis/builds/repos/eykd---prestoplot/.tox/py37', 'COV_CORE_SOURCE': ':',
     'COV_CORE_CONFIG': ':', 'COV_CORE_DATAFILE': '/home/travis/builds/repos/eykd---prestoplot/.coverage'}

    res1,res2=run_command(args1,args2,args3,args4,args5,args6)
    isT=res1 is None and res2==128

    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/eykd---prestoplot/src/prestoplot/_version_get_config_passk_validte.py
# This file helps to compute a version number in source trees obtained from
# git-archive tarball (such as those provided by githubs download-from-tag
# feature). Distribution tarballs (built by setup.py sdist) and build
# directories (produced by setup.py build) will contain a much shorter file
# that just contains the computed version number.

# This file is released into the public domain. Generated by
# versioneer-0.22 (https://github.com/python-versioneer/python-versioneer)

"""Git implementation of _version.py."""

import errno
import functools
import os
import re
import subprocess
import sys
from typing import Callable, Dict


def get_keywords():
    """Get the keywords needed to look up the version information."""
    # these strings will be replaced by git during git-archive.
    # setup.py/versioneer.py will grep for the variable names, so they must
    # each be defined on a line of their own. _version.py will just call
    # get_keywords().
    git_refnames = " (HEAD -> master, tag: 0.5.2)"
    git_full = "61c94a4a354806aacdd280c61caed76df2b63205"
    git_date = "2023-02-17 16:50:17 -0800"
    keywords = {"refnames": git_refnames, "full": git_full, "date": git_date}
    return keywords


class VersioneerConfig:
    """Container for Versioneer configuration parameters."""


def get_config():
    """Create, populate and return the VersioneerConfig() object."""
    # these strings are filled in when 'setup.py versioneer' creates
    # _version.py
    cfg = VersioneerConfig()
    cfg.VCS = "git"
    cfg.style = "pep440"
    cfg.tag_prefix = ""
    cfg.parentdir_prefix = "None"
    cfg.versionfile_source = "src/prestoplot/_version.py"
    cfg.verbose = False
    return cfg


class NotThisMethod(Exception):
    """Exception raised if a method is not valid for the current scenario."""


LONG_VERSION_PY: Dict[str, str] = {}
HANDLERS: Dict[str, Dict[str, Callable]] = {}


def register_vcs_handler(vcs, method):  # decorator
    """Create decorator to mark a method as the handler of a VCS."""

    def decorate(f):
        """Store f in HANDLERS[vcs][method]."""
        if vcs not in HANDLERS:
            HANDLERS[vcs] = {}
        HANDLERS[vcs][method] = f
        return f

    return decorate


def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):
    """Call the given command(s)."""
    assert isinstance(commands, list)
    process = None

    popen_kwargs = {}
    if sys.platform == "win32":
        # This hides the console window if pythonw.exe is used
        startupinfo = subprocess.STARTUPINFO()
        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
        popen_kwargs["startupinfo"] = startupinfo

    for command in commands:
        try:
            dispcmd = str([command] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            process = subprocess.Popen(
                [command] + args,
                cwd=cwd,
                env=env,
                stdout=subprocess.PIPE,
                stderr=(subprocess.PIPE if hide_stderr else None),
                **popen_kwargs
            )
            break
        except OSError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print("unable to run %s" % dispcmd)
                print(e)
            return None, None
    else:
        if verbose:
            print("unable to find command, tried %s" % (commands,))
        return None, None
    stdout = process.communicate()[0].strip().decode()
    if process.returncode != 0:
        if verbose:
            print("unable to run %s (error)" % dispcmd)
            print("stdout was %s" % stdout)
        return None, process.returncode
    return stdout, process.returncode


def versions_from_parentdir(parentdir_prefix, root, verbose):
    """Try to determine the version from the parent directory name.

    Source tarballs conventionally unpack into a directory that includes both
    the project name and a version string. We will also support searching up
    two directory levels for an appropriately named parent directory
    """
    rootdirs = []

    for _ in range(3):
        dirname = os.path.basename(root)
        if dirname.startswith(parentdir_prefix):
            return {
                "version": dirname[len(parentdir_prefix) :],
                "full-revisionid": None,
                "dirty": False,
                "error": None,
                "date": None,
            }
        rootdirs.append(root)
        root = os.path.dirname(root)  # up a level

    if verbose:
        print(
            "Tried directories %s but none started with prefix %s"
            % (str(rootdirs), parentdir_prefix)
        )
    raise NotThisMethod("rootdir doesn't start with parentdir_prefix")


@register_vcs_handler("git", "get_keywords")
def git_get_keywords(versionfile_abs):
    """Extract version information from the given file."""
    # the code embedded in _version.py can just fetch the value of these
    # keywords. When used from setup.py, we don't want to import _version.py,
    # so we do it with a regexp instead. This function is not used from
    # _version.py.
    keywords = {}
    try:
        with open(versionfile_abs, "r") as fobj:
            for line in fobj:
                if line.strip().startswith("git_refnames ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["refnames"] = mo.group(1)
                if line.strip().startswith("git_full ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["full"] = mo.group(1)
                if line.strip().startswith("git_date ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["date"] = mo.group(1)
    except OSError:
        pass
    return keywords


@register_vcs_handler("git", "keywords")
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """Get version information from git keywords."""
    if "refnames" not in keywords:
        raise NotThisMethod("Short version file found")
    date = keywords.get("date")
    if date is not None:
        # Use only the last line.  Previous lines may contain GPG signature
        # information.
        date = date.splitlines()[-1]

        # git-2.2.0 added "%cI", which expands to an ISO-8601 -compliant
        # datestamp. However we prefer "%ci" (which expands to an "ISO-8601
        # -like" string, which we must then edit to make compliant), because
        # it's been around since git-1.5.3, and it's too difficult to
        # discover which version we're using, or to work around using an
        # older one.
        date = date.strip().replace(" ", "T", 1).replace(" ", "", 1)
    refnames = keywords["refnames"].strip()
    if refnames.startswith("$Format"):
        if verbose:
            print("keywords are unexpanded, not using")
        raise NotThisMethod("unexpanded keywords, not a git-archive tarball")
    refs = {r.strip() for r in refnames.strip("()").split(",")}
    # starting in git-1.8.3, tags are listed as "tag: foo-1.0" instead of
    # just "foo-1.0". If we see a "tag: " prefix, prefer those.
    TAG = "tag: "
    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}
    if not tags:
        # Either we're using git < 1.8.3, or there really are no tags. We use
        # a heuristic: assume all version tags have a digit. The old git %d
        # expansion behaves like git log --decorate=short and strips out the
        # refs/heads/ and refs/tags/ prefixes that would let us distinguish
        # between branches and tags. By ignoring refnames without digits, we
        # filter out many common branch names like "release" and
        # "stabilization", as well as "HEAD" and "master".
        tags = {r for r in refs if re.search(r"\d", r)}
        if verbose:
            print("discarding '%s', no digits" % ",".join(refs - tags))
    if verbose:
        print("likely tags: %s" % ",".join(sorted(tags)))
    for ref in sorted(tags):
        # sorting will prefer e.g. "2.0" over "2.0rc1"
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix) :]
            # Filter out refs that exactly match prefix or that don't start
            # with a number once the prefix is stripped (mostly a concern
            # when prefix is '')
            if not re.match(r"\d", r):
                continue
            if verbose:
                print("picking %s" % r)
            return {
                "version": r,
                "full-revisionid": keywords["full"].strip(),
                "dirty": False,
                "error": None,
                "date": date,
            }
    # no suitable tags, so version is "0+unknown", but full hex is still there
    if verbose:
        print("no suitable tags, using unknown + full revision id")
    return {
        "version": "0+unknown",
        "full-revisionid": keywords["full"].strip(),
        "dirty": False,
        "error": "no suitable tags",
        "date": None,
    }


@register_vcs_handler("git", "pieces_from_vcs")
def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):
    """Get version from 'git describe' in the root of the source tree.

    This only gets called if the git-archive 'subst' keywords were *not*
    expanded, and _version.py hasn't already been rewritten with a short
    version string, meaning we're inside a checked out source tree.
    """
    GITS = ["git"]
    if sys.platform == "win32":
        GITS = ["git.cmd", "git.exe"]

    # GIT_DIR can interfere with correct operation of Versioneer.
    # It may be intended to be passed to the Versioneer-versioned project,
    # but that should not change where we get our version from.
    env = os.environ.copy()
    env.pop("GIT_DIR", None)
    runner = functools.partial(runner, env=env)

    _, rc = runner(GITS, ["rev-parse", "--git-dir"], cwd=root, hide_stderr=True)
    if rc != 0:
        if verbose:
            print("Directory %s not under git control" % root)
        raise NotThisMethod("'git rev-parse --git-dir' returned error")

    MATCH_ARGS = ["--match", "%s*" % tag_prefix] if tag_prefix else []

    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]
    # if there isn't one, this yields HEX[-dirty] (no NUM)
    describe_out, rc = runner(
        GITS,
        ["describe", "--tags", "--dirty", "--always", "--long", *MATCH_ARGS],
        cwd=root,
    )
    # --long was added in git-1.5.5
    if describe_out is None:
        raise NotThisMethod("'git describe' failed")
    describe_out = describe_out.strip()
    full_out, rc = runner(GITS, ["rev-parse", "HEAD"], cwd=root)
    if full_out is None:
        raise NotThisMethod("'git rev-parse' failed")
    full_out = full_out.strip()

    pieces = {}
    pieces["long"] = full_out
    pieces["short"] = full_out[:7]  # maybe improved later
    pieces["error"] = None

    branch_name, rc = runner(GITS, ["rev-parse", "--abbrev-ref", "HEAD"], cwd=root)
    # --abbrev-ref was added in git-1.6.3
    if rc != 0 or branch_name is None:
        raise NotThisMethod("'git rev-parse --abbrev-ref' returned error")
    branch_name = branch_name.strip()

    if branch_name == "HEAD":
        # If we aren't exactly on a branch, pick a branch which represents
        # the current commit. If all else fails, we are on a branchless
        # commit.
        branches, rc = runner(GITS, ["branch", "--contains"], cwd=root)
        # --contains was added in git-1.5.4
        if rc != 0 or branches is None:
            raise NotThisMethod("'git branch --contains' returned error")
        branches = branches.split("\n")

        # Remove the first line if we're running detached
        if "(" in branches[0]:
            branches.pop(0)

        # Strip off the leading "* " from the list of branches.
        branches = [branch[2:] for branch in branches]
        if "master" in branches:
            branch_name = "master"
        elif not branches:
            branch_name = None
        else:
            # Pick the first branch that is returned. Good or bad.
            branch_name = branches[0]

    pieces["branch"] = branch_name

    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]
    # TAG might have hyphens.
    git_describe = describe_out

    # look for -dirty suffix
    dirty = git_describe.endswith("-dirty")
    pieces["dirty"] = dirty
    if dirty:
        git_describe = git_describe[: git_describe.rindex("-dirty")]

    # now we have TAG-NUM-gHEX or HEX

    if "-" in git_describe:
        # TAG-NUM-gHEX
        mo = re.search(r"^(.+)-(\d+)-g([0-9a-f]+)$", git_describe)
        if not mo:
            # unparsable. Maybe git-describe is misbehaving?
            pieces["error"] = "unable to parse git-describe output: '%s'" % describe_out
            return pieces

        # tag
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = "tag '%s' doesn't start with prefix '%s'"
                print(fmt % (full_tag, tag_prefix))
            pieces["error"] = "tag '%s' doesn't start with prefix '%s'" % (
                full_tag,
                tag_prefix,
            )
            return pieces
        pieces["closest-tag"] = full_tag[len(tag_prefix) :]

        # distance: number of commits since tag
        pieces["distance"] = int(mo.group(2))

        # commit: short hex revision ID
        pieces["short"] = mo.group(3)

    else:
        # HEX: no tags
        pieces["closest-tag"] = None
        count_out, rc = runner(GITS, ["rev-list", "HEAD", "--count"], cwd=root)
        pieces["distance"] = int(count_out)  # total number of commits

    # commit date: see ISO-8601 comment in git_versions_from_keywords()
    date = runner(GITS, ["show", "-s", "--format=%ci", "HEAD"], cwd=root)[0].strip()
    # Use only the last line.  Previous lines may contain GPG signature
    # information.
    date = date.splitlines()[-1]
    pieces["date"] = date.strip().replace(" ", "T", 1).replace(" ", "", 1)

    return pieces


def plus_or_dot(pieces):
    """Return a + if we don't already have one, else return a ."""
    if "+" in pieces.get("closest-tag", ""):
        return "."
    return "+"


def render_pep440(pieces):
    """Build up version string, with post-release "local version identifier".

    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you
    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty

    Exceptions:
    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def render_pep440_branch(pieces):
    """TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .

    The ".dev0" means not master branch. Note that .dev0 sorts backwards
    (a feature branch will appear "older" than the master branch).

    Exceptions:
    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            if pieces["branch"] != "master":
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0"
        if pieces["branch"] != "master":
            rendered += ".dev0"
        rendered += "+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def pep440_split_post(ver):
    """Split pep440 version string at the post-release segment.

    Returns the release segments before the post-release and the
    post-release version number (or -1 if no post-release segment is present).
    """
    vc = str.split(ver, ".post")
    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None


def render_pep440_pre(pieces):
    """TAG[.postN.devDISTANCE] -- No -dirty.

    Exceptions:
    1: no tags. 0.post0.devDISTANCE
    """
    if pieces["closest-tag"]:
        if pieces["distance"]:
            # update the post release segment
            tag_version, post_version = pep440_split_post(pieces["closest-tag"])
            rendered = tag_version
            if post_version is not None:
                rendered += ".post%d.dev%d" % (post_version + 1, pieces["distance"])
            else:
                rendered += ".post0.dev%d" % (pieces["distance"])
        else:
            # no commits, use the tag as the version
            rendered = pieces["closest-tag"]
    else:
        # exception #1
        rendered = "0.post0.dev%d" % pieces["distance"]
    return rendered


def render_pep440_post(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX] .

    The ".dev0" means dirty. Note that .dev0 sorts backwards
    (a dirty tree will appear "older" than the corresponding clean one),
    but you shouldn't be releasing software with -dirty anyways.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
    return rendered


def render_pep440_post_branch(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .

    The ".dev0" means not master branch.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["branch"] != "master":
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["branch"] != "master":
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def render_pep440_old(pieces):
    """TAG[.postDISTANCE[.dev0]] .

    The ".dev0" means dirty.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
    return rendered


def render_git_describe(pieces):
    """TAG[-DISTANCE-gHEX][-dirty].

    Like 'git describe --tags --dirty --always'.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"]:
            rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


def render_git_describe_long(pieces):
    """TAG-DISTANCE-gHEX[-dirty].

    Like 'git describe --tags --dirty --always -long'.
    The distance/hash is unconditional.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


def render(pieces, style):
    """Render the given version pieces into the requested style."""
    if pieces["error"]:
        return {
            "version": "unknown",
            "full-revisionid": pieces.get("long"),
            "dirty": None,
            "error": pieces["error"],
            "date": None,
        }

    if not style or style == "default":
        style = "pep440"  # the default

    if style == "pep440":
        rendered = render_pep440(pieces)
    elif style == "pep440-branch":
        rendered = render_pep440_branch(pieces)
    elif style == "pep440-pre":
        rendered = render_pep440_pre(pieces)
    elif style == "pep440-post":
        rendered = render_pep440_post(pieces)
    elif style == "pep440-post-branch":
        rendered = render_pep440_post_branch(pieces)
    elif style == "pep440-old":
        rendered = render_pep440_old(pieces)
    elif style == "git-describe":
        rendered = render_git_describe(pieces)
    elif style == "git-describe-long":
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError("unknown style '%s'" % style)

    return {
        "version": rendered,
        "full-revisionid": pieces["long"],
        "dirty": pieces["dirty"],
        "error": None,
        "date": pieces.get("date"),
    }


def get_versions():
    """Get version information or return default if unable to do so."""
    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have
    # __file__, we can work backwards from there to the root. Some
    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which
    # case we can only use expanded keywords.

    cfg = get_config()
    verbose = cfg.verbose

    try:
        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)
    except NotThisMethod:
        pass

    try:
        root = os.path.realpath(__file__)
        # versionfile_source is the relative path from the top of the source
        # tree (where the .git directory might live) to this file. Invert
        # this to find the root from __file__.
        for _ in cfg.versionfile_source.split("/"):
            root = os.path.dirname(root)
    except NameError:
        return {
            "version": "0+unknown",
            "full-revisionid": None,
            "dirty": None,
            "error": "unable to find root of source tree",
            "date": None,
        }

    try:
        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)
        return render(pieces, cfg.style)
    except NotThisMethod:
        pass

    try:
        if cfg.parentdir_prefix:
            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)
    except NotThisMethod:
        pass

    return {
        "version": "0+unknown",
        "full-revisionid": None,
        "dirty": None,
        "error": "unable to compute version",
        "date": None,
    }

if __name__ == "__main__":
    isT=True
    cfg=get_config()
    ist1=cfg.VCS == "git"
    ist2=cfg.style == "pep440"
    ist3=cfg.tag_prefix == ""
    ist4=cfg.parentdir_prefix == "None"
    ist5=cfg.versionfile_source == "src/prestoplot/_version.py"
    ist6=cfg.verbose == False
    if not ist1 or not ist2 or not ist3 or not ist4 or not ist5 or not ist6:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/eykd---prestoplot/src/prestoplot/_version_register_vcs_handler_passk_validte.py
# This file helps to compute a version number in source trees obtained from
# git-archive tarball (such as those provided by githubs download-from-tag
# feature). Distribution tarballs (built by setup.py sdist) and build
# directories (produced by setup.py build) will contain a much shorter file
# that just contains the computed version number.

# This file is released into the public domain. Generated by
# versioneer-0.22 (https://github.com/python-versioneer/python-versioneer)

"""Git implementation of _version.py."""

import errno
import functools
import os
import re
import subprocess
import sys
from typing import Callable, Dict


def get_keywords():
    """Get the keywords needed to look up the version information."""
    # these strings will be replaced by git during git-archive.
    # setup.py/versioneer.py will grep for the variable names, so they must
    # each be defined on a line of their own. _version.py will just call
    # get_keywords().
    git_refnames = " (HEAD -> master, tag: 0.5.2)"
    git_full = "61c94a4a354806aacdd280c61caed76df2b63205"
    git_date = "2023-02-17 16:50:17 -0800"
    keywords = {"refnames": git_refnames, "full": git_full, "date": git_date}
    return keywords


class VersioneerConfig:
    """Container for Versioneer configuration parameters."""


def get_config():
    """Create, populate and return the VersioneerConfig() object."""
    # these strings are filled in when 'setup.py versioneer' creates
    # _version.py
    cfg = VersioneerConfig()
    cfg.VCS = "git"
    cfg.style = "pep440"
    cfg.tag_prefix = ""
    cfg.parentdir_prefix = "None"
    cfg.versionfile_source = "src/prestoplot/_version.py"
    cfg.verbose = False
    return cfg


class NotThisMethod(Exception):
    """Exception raised if a method is not valid for the current scenario."""


LONG_VERSION_PY: Dict[str, str] = {}
HANDLERS: Dict[str, Dict[str, Callable]] = {}


def register_vcs_handler(vcs, method):  # decorator
    """Create decorator to mark a method as the handler of a VCS."""

    def decorate(f):
        """Store f in HANDLERS[vcs][method]."""
        if vcs not in HANDLERS:
            HANDLERS[vcs] = {}
        HANDLERS[vcs][method] = f
        return f

    return decorate


def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):
    """Call the given command(s)."""
    assert isinstance(commands, list)
    process = None

    popen_kwargs = {}
    if sys.platform == "win32":
        # This hides the console window if pythonw.exe is used
        startupinfo = subprocess.STARTUPINFO()
        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
        popen_kwargs["startupinfo"] = startupinfo

    for command in commands:
        try:
            dispcmd = str([command] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            process = subprocess.Popen(
                [command] + args,
                cwd=cwd,
                env=env,
                stdout=subprocess.PIPE,
                stderr=(subprocess.PIPE if hide_stderr else None),
                **popen_kwargs
            )
            break
        except OSError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print("unable to run %s" % dispcmd)
                print(e)
            return None, None
    else:
        if verbose:
            print("unable to find command, tried %s" % (commands,))
        return None, None
    stdout = process.communicate()[0].strip().decode()
    if process.returncode != 0:
        if verbose:
            print("unable to run %s (error)" % dispcmd)
            print("stdout was %s" % stdout)
        return None, process.returncode
    return stdout, process.returncode


def versions_from_parentdir(parentdir_prefix, root, verbose):
    """Try to determine the version from the parent directory name.

    Source tarballs conventionally unpack into a directory that includes both
    the project name and a version string. We will also support searching up
    two directory levels for an appropriately named parent directory
    """
    rootdirs = []

    for _ in range(3):
        dirname = os.path.basename(root)
        if dirname.startswith(parentdir_prefix):
            return {
                "version": dirname[len(parentdir_prefix) :],
                "full-revisionid": None,
                "dirty": False,
                "error": None,
                "date": None,
            }
        rootdirs.append(root)
        root = os.path.dirname(root)  # up a level

    if verbose:
        print(
            "Tried directories %s but none started with prefix %s"
            % (str(rootdirs), parentdir_prefix)
        )
    raise NotThisMethod("rootdir doesn't start with parentdir_prefix")


@register_vcs_handler("git", "get_keywords")
def git_get_keywords(versionfile_abs):
    """Extract version information from the given file."""
    # the code embedded in _version.py can just fetch the value of these
    # keywords. When used from setup.py, we don't want to import _version.py,
    # so we do it with a regexp instead. This function is not used from
    # _version.py.
    keywords = {}
    try:
        with open(versionfile_abs, "r") as fobj:
            for line in fobj:
                if line.strip().startswith("git_refnames ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["refnames"] = mo.group(1)
                if line.strip().startswith("git_full ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["full"] = mo.group(1)
                if line.strip().startswith("git_date ="):
                    mo = re.search(r'=\s*"(.*)"', line)
                    if mo:
                        keywords["date"] = mo.group(1)
    except OSError:
        pass
    return keywords


@register_vcs_handler("git", "keywords")
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """Get version information from git keywords."""
    if "refnames" not in keywords:
        raise NotThisMethod("Short version file found")
    date = keywords.get("date")
    if date is not None:
        # Use only the last line.  Previous lines may contain GPG signature
        # information.
        date = date.splitlines()[-1]

        # git-2.2.0 added "%cI", which expands to an ISO-8601 -compliant
        # datestamp. However we prefer "%ci" (which expands to an "ISO-8601
        # -like" string, which we must then edit to make compliant), because
        # it's been around since git-1.5.3, and it's too difficult to
        # discover which version we're using, or to work around using an
        # older one.
        date = date.strip().replace(" ", "T", 1).replace(" ", "", 1)
    refnames = keywords["refnames"].strip()
    if refnames.startswith("$Format"):
        if verbose:
            print("keywords are unexpanded, not using")
        raise NotThisMethod("unexpanded keywords, not a git-archive tarball")
    refs = {r.strip() for r in refnames.strip("()").split(",")}
    # starting in git-1.8.3, tags are listed as "tag: foo-1.0" instead of
    # just "foo-1.0". If we see a "tag: " prefix, prefer those.
    TAG = "tag: "
    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}
    if not tags:
        # Either we're using git < 1.8.3, or there really are no tags. We use
        # a heuristic: assume all version tags have a digit. The old git %d
        # expansion behaves like git log --decorate=short and strips out the
        # refs/heads/ and refs/tags/ prefixes that would let us distinguish
        # between branches and tags. By ignoring refnames without digits, we
        # filter out many common branch names like "release" and
        # "stabilization", as well as "HEAD" and "master".
        tags = {r for r in refs if re.search(r"\d", r)}
        if verbose:
            print("discarding '%s', no digits" % ",".join(refs - tags))
    if verbose:
        print("likely tags: %s" % ",".join(sorted(tags)))
    for ref in sorted(tags):
        # sorting will prefer e.g. "2.0" over "2.0rc1"
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix) :]
            # Filter out refs that exactly match prefix or that don't start
            # with a number once the prefix is stripped (mostly a concern
            # when prefix is '')
            if not re.match(r"\d", r):
                continue
            if verbose:
                print("picking %s" % r)
            return {
                "version": r,
                "full-revisionid": keywords["full"].strip(),
                "dirty": False,
                "error": None,
                "date": date,
            }
    # no suitable tags, so version is "0+unknown", but full hex is still there
    if verbose:
        print("no suitable tags, using unknown + full revision id")
    return {
        "version": "0+unknown",
        "full-revisionid": keywords["full"].strip(),
        "dirty": False,
        "error": "no suitable tags",
        "date": None,
    }


@register_vcs_handler("git", "pieces_from_vcs")
def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):
    """Get version from 'git describe' in the root of the source tree.

    This only gets called if the git-archive 'subst' keywords were *not*
    expanded, and _version.py hasn't already been rewritten with a short
    version string, meaning we're inside a checked out source tree.
    """
    GITS = ["git"]
    if sys.platform == "win32":
        GITS = ["git.cmd", "git.exe"]

    # GIT_DIR can interfere with correct operation of Versioneer.
    # It may be intended to be passed to the Versioneer-versioned project,
    # but that should not change where we get our version from.
    env = os.environ.copy()
    env.pop("GIT_DIR", None)
    runner = functools.partial(runner, env=env)

    _, rc = runner(GITS, ["rev-parse", "--git-dir"], cwd=root, hide_stderr=True)
    if rc != 0:
        if verbose:
            print("Directory %s not under git control" % root)
        raise NotThisMethod("'git rev-parse --git-dir' returned error")

    MATCH_ARGS = ["--match", "%s*" % tag_prefix] if tag_prefix else []

    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]
    # if there isn't one, this yields HEX[-dirty] (no NUM)
    describe_out, rc = runner(
        GITS,
        ["describe", "--tags", "--dirty", "--always", "--long", *MATCH_ARGS],
        cwd=root,
    )
    # --long was added in git-1.5.5
    if describe_out is None:
        raise NotThisMethod("'git describe' failed")
    describe_out = describe_out.strip()
    full_out, rc = runner(GITS, ["rev-parse", "HEAD"], cwd=root)
    if full_out is None:
        raise NotThisMethod("'git rev-parse' failed")
    full_out = full_out.strip()

    pieces = {}
    pieces["long"] = full_out
    pieces["short"] = full_out[:7]  # maybe improved later
    pieces["error"] = None

    branch_name, rc = runner(GITS, ["rev-parse", "--abbrev-ref", "HEAD"], cwd=root)
    # --abbrev-ref was added in git-1.6.3
    if rc != 0 or branch_name is None:
        raise NotThisMethod("'git rev-parse --abbrev-ref' returned error")
    branch_name = branch_name.strip()

    if branch_name == "HEAD":
        # If we aren't exactly on a branch, pick a branch which represents
        # the current commit. If all else fails, we are on a branchless
        # commit.
        branches, rc = runner(GITS, ["branch", "--contains"], cwd=root)
        # --contains was added in git-1.5.4
        if rc != 0 or branches is None:
            raise NotThisMethod("'git branch --contains' returned error")
        branches = branches.split("\n")

        # Remove the first line if we're running detached
        if "(" in branches[0]:
            branches.pop(0)

        # Strip off the leading "* " from the list of branches.
        branches = [branch[2:] for branch in branches]
        if "master" in branches:
            branch_name = "master"
        elif not branches:
            branch_name = None
        else:
            # Pick the first branch that is returned. Good or bad.
            branch_name = branches[0]

    pieces["branch"] = branch_name

    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]
    # TAG might have hyphens.
    git_describe = describe_out

    # look for -dirty suffix
    dirty = git_describe.endswith("-dirty")
    pieces["dirty"] = dirty
    if dirty:
        git_describe = git_describe[: git_describe.rindex("-dirty")]

    # now we have TAG-NUM-gHEX or HEX

    if "-" in git_describe:
        # TAG-NUM-gHEX
        mo = re.search(r"^(.+)-(\d+)-g([0-9a-f]+)$", git_describe)
        if not mo:
            # unparsable. Maybe git-describe is misbehaving?
            pieces["error"] = "unable to parse git-describe output: '%s'" % describe_out
            return pieces

        # tag
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = "tag '%s' doesn't start with prefix '%s'"
                print(fmt % (full_tag, tag_prefix))
            pieces["error"] = "tag '%s' doesn't start with prefix '%s'" % (
                full_tag,
                tag_prefix,
            )
            return pieces
        pieces["closest-tag"] = full_tag[len(tag_prefix) :]

        # distance: number of commits since tag
        pieces["distance"] = int(mo.group(2))

        # commit: short hex revision ID
        pieces["short"] = mo.group(3)

    else:
        # HEX: no tags
        pieces["closest-tag"] = None
        count_out, rc = runner(GITS, ["rev-list", "HEAD", "--count"], cwd=root)
        pieces["distance"] = int(count_out)  # total number of commits

    # commit date: see ISO-8601 comment in git_versions_from_keywords()
    date = runner(GITS, ["show", "-s", "--format=%ci", "HEAD"], cwd=root)[0].strip()
    # Use only the last line.  Previous lines may contain GPG signature
    # information.
    date = date.splitlines()[-1]
    pieces["date"] = date.strip().replace(" ", "T", 1).replace(" ", "", 1)

    return pieces


def plus_or_dot(pieces):
    """Return a + if we don't already have one, else return a ."""
    if "+" in pieces.get("closest-tag", ""):
        return "."
    return "+"


def render_pep440(pieces):
    """Build up version string, with post-release "local version identifier".

    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you
    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty

    Exceptions:
    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def render_pep440_branch(pieces):
    """TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .

    The ".dev0" means not master branch. Note that .dev0 sorts backwards
    (a feature branch will appear "older" than the master branch).

    Exceptions:
    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            if pieces["branch"] != "master":
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "%d.g%s" % (pieces["distance"], pieces["short"])
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0"
        if pieces["branch"] != "master":
            rendered += ".dev0"
        rendered += "+untagged.%d.g%s" % (pieces["distance"], pieces["short"])
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def pep440_split_post(ver):
    """Split pep440 version string at the post-release segment.

    Returns the release segments before the post-release and the
    post-release version number (or -1 if no post-release segment is present).
    """
    vc = str.split(ver, ".post")
    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None


def render_pep440_pre(pieces):
    """TAG[.postN.devDISTANCE] -- No -dirty.

    Exceptions:
    1: no tags. 0.post0.devDISTANCE
    """
    if pieces["closest-tag"]:
        if pieces["distance"]:
            # update the post release segment
            tag_version, post_version = pep440_split_post(pieces["closest-tag"])
            rendered = tag_version
            if post_version is not None:
                rendered += ".post%d.dev%d" % (post_version + 1, pieces["distance"])
            else:
                rendered += ".post0.dev%d" % (pieces["distance"])
        else:
            # no commits, use the tag as the version
            rendered = pieces["closest-tag"]
    else:
        # exception #1
        rendered = "0.post0.dev%d" % pieces["distance"]
    return rendered


def render_pep440_post(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX] .

    The ".dev0" means dirty. Note that .dev0 sorts backwards
    (a dirty tree will appear "older" than the corresponding clean one),
    but you shouldn't be releasing software with -dirty anyways.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
    return rendered


def render_pep440_post_branch(pieces):
    """TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .

    The ".dev0" means not master branch.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["branch"] != "master":
                rendered += ".dev0"
            rendered += plus_or_dot(pieces)
            rendered += "g%s" % pieces["short"]
            if pieces["dirty"]:
                rendered += ".dirty"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["branch"] != "master":
            rendered += ".dev0"
        rendered += "+g%s" % pieces["short"]
        if pieces["dirty"]:
            rendered += ".dirty"
    return rendered


def render_pep440_old(pieces):
    """TAG[.postDISTANCE[.dev0]] .

    The ".dev0" means dirty.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"] or pieces["dirty"]:
            rendered += ".post%d" % pieces["distance"]
            if pieces["dirty"]:
                rendered += ".dev0"
    else:
        # exception #1
        rendered = "0.post%d" % pieces["distance"]
        if pieces["dirty"]:
            rendered += ".dev0"
    return rendered


def render_git_describe(pieces):
    """TAG[-DISTANCE-gHEX][-dirty].

    Like 'git describe --tags --dirty --always'.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        if pieces["distance"]:
            rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


def render_git_describe_long(pieces):
    """TAG-DISTANCE-gHEX[-dirty].

    Like 'git describe --tags --dirty --always -long'.
    The distance/hash is unconditional.

    Exceptions:
    1: no tags. HEX[-dirty]  (note: no 'g' prefix)
    """
    if pieces["closest-tag"]:
        rendered = pieces["closest-tag"]
        rendered += "-%d-g%s" % (pieces["distance"], pieces["short"])
    else:
        # exception #1
        rendered = pieces["short"]
    if pieces["dirty"]:
        rendered += "-dirty"
    return rendered


def render(pieces, style):
    """Render the given version pieces into the requested style."""
    if pieces["error"]:
        return {
            "version": "unknown",
            "full-revisionid": pieces.get("long"),
            "dirty": None,
            "error": pieces["error"],
            "date": None,
        }

    if not style or style == "default":
        style = "pep440"  # the default

    if style == "pep440":
        rendered = render_pep440(pieces)
    elif style == "pep440-branch":
        rendered = render_pep440_branch(pieces)
    elif style == "pep440-pre":
        rendered = render_pep440_pre(pieces)
    elif style == "pep440-post":
        rendered = render_pep440_post(pieces)
    elif style == "pep440-post-branch":
        rendered = render_pep440_post_branch(pieces)
    elif style == "pep440-old":
        rendered = render_pep440_old(pieces)
    elif style == "git-describe":
        rendered = render_git_describe(pieces)
    elif style == "git-describe-long":
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError("unknown style '%s'" % style)

    return {
        "version": rendered,
        "full-revisionid": pieces["long"],
        "dirty": pieces["dirty"],
        "error": None,
        "date": pieces.get("date"),
    }


def get_versions():
    """Get version information or return default if unable to do so."""
    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have
    # __file__, we can work backwards from there to the root. Some
    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which
    # case we can only use expanded keywords.

    cfg = get_config()
    verbose = cfg.verbose

    try:
        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)
    except NotThisMethod:
        pass

    try:
        root = os.path.realpath(__file__)
        # versionfile_source is the relative path from the top of the source
        # tree (where the .git directory might live) to this file. Invert
        # this to find the root from __file__.
        for _ in cfg.versionfile_source.split("/"):
            root = os.path.dirname(root)
    except NameError:
        return {
            "version": "0+unknown",
            "full-revisionid": None,
            "dirty": None,
            "error": "unable to find root of source tree",
            "date": None,
        }

    try:
        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)
        return render(pieces, cfg.style)
    except NotThisMethod:
        pass

    try:
        if cfg.parentdir_prefix:
            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)
    except NotThisMethod:
        pass

    return {
        "version": "0+unknown",
        "full-revisionid": None,
        "dirty": None,
        "error": "unable to compute version",
        "date": None,
    }

if __name__ == "__main__":
    args0_1,args0_2='git', 'get_keywords'
    res0 = register_vcs_handler(args0_1,args0_2)

    def add_two(num1, num2):
        return num1+num2
    res0(add_two(4,5))
    isT=HANDLERS["git"]["get_keywords"]==9
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/store_validate_hierarchy_passk_validte.py
"""OCFL Storage Root library.

This code uses PyFilesystem (import fs) exclusively for access to files. This
should enable application beyond the operating system filesystem.
"""
import json
import logging
import re
import fs
from fs.copy import copy_dir

from disposition import get_dispositor
from namaste import find_namastes, Namaste
from object import Object
from pyfs import open_fs, ocfl_walk, ocfl_opendir
from validator import Validator
from validation_logger import ValidationLogger


class StoreException(Exception):
    """Exception class for OCFL Storage Root."""


class Store():
    """Class for handling OCFL Storage Root and include OCFL Objects."""

    def __init__(self, root=None, disposition=None, lax_digests=False):
        """Initialize OCFL Storage Root."""
        self.root = root
        self.disposition = disposition
        self.lax_digests = lax_digests
        self._dispositor = None
        #
        self.declaration_tvalue = 'ocfl_1.0'
        self.spec_file = 'ocfl_1.0.txt'
        self.layout_file = 'ocfl_layout.json'
        self.registered_extensions = [
            # '0002-flat-direct-storage-layout',  # not included because doesn't have config
            '0003-hash-and-id-n-tuple-storage-layout',
            '0004-hashed-n-tuple-storage-layout'
        ]
        #
        self.root_fs = None
        self.num_traversal_errors = 0
        self.extension = None
        self.description = None
        self.log = None
        self.num_objects = 0
        self.good_objects = 0

    def open_root_fs(self, create=False):
        """Open pyfs filesystem for this OCFL storage root."""
        try:
            self.root_fs = open_fs(self.root, create=create)
        except (fs.opener.errors.OpenerError, fs.errors.CreateFailed) as e:
            raise StoreException("Failed to open OCFL storage root filesystem '%s' (%s)" % (self.root, str(e)))

    @property
    def dispositor(self):
        """Instance of dispositor class.

        Lazily initialized.
        """
        if not self._dispositor:
            self._dispositor = get_dispositor(disposition=self.disposition)
        return self._dispositor

    def traversal_error(self, code, **kwargs):
        """Record error traversing OCFL storage root."""
        self.num_traversal_errors += 1
        if self.log is None:  # FIXME - What to do in non-validator context?
            args = ', '.join('{0}={1!r}'.format(k, v) for k, v in kwargs.items())
            logging.error("Traversal error %s - %s", code, args)
        else:
            self.log.error(code, **kwargs)

    def object_path(self, identifier):
        """Path to OCFL object with given identifier relative to the OCFL storage root."""
        return self.dispositor.identifier_to_path(identifier)

    def initialize(self):
        """Create and initialize a new OCFL storage root."""
        (parent, root_dir) = fs.path.split(self.root)
        parent_fs = open_fs(parent)
        if parent_fs.exists(root_dir):
            raise StoreException("OCFL storage root %s already exists, aborting!" % (self.root))
        self.root_fs = parent_fs.makedir(root_dir)
        logging.debug("Created OCFL storage root at %s", self.root)
        # Create root declaration
        Namaste(d=0, content=self.declaration_tvalue).write(pyfs=self.root_fs)
        # Create a layout declaration
        if self.disposition is not None:
            with self.root_fs.open(self.layout_file, 'w') as fh:
                layout = {'extension': self.disposition,
                          'description': "Non-standard layout from ocfl-py disposition -- FIXME"}
                json.dump(layout, fh, sort_keys=True, indent=2)
        logging.info("Created OCFL storage root %s", self.root)

    def check_root_structure(self):
        """Check the OCFL storage root structure.

        Assumed that self.root_fs filesystem is available. Raises
        StoreException if there is an error.
        """
        # Storage root declaration
        namastes = find_namastes(0, pyfs=self.root_fs)
        if len(namastes) == 0:
            raise StoreException("Storage root %s lacks required 0= declaration file" % (self.root))
        if len(namastes) > 1:
            raise StoreException("Storage root %s has more than one 0= style declaration file" % (self.root))
        if namastes[0].tvalue != self.declaration_tvalue:
            raise StoreException("Storage root %s declaration file not as expected, got %s" % (self.root, namastes[0].filename))
        if not namastes[0].content_ok(pyfs=self.root_fs):
            raise StoreException("Storage root %s required declaration file %s has invalid content" % (self.root, namastes[0].filename))
        # Specification file and layout file
        if self.root_fs.exists(self.spec_file) and not self.root_fs.isfile(self.spec_file):
            raise StoreException("Storage root %s includes a specification entry that isn't a file" % (self.root))
        self.extension, self.description = self.parse_layout_file()
        # Other files are allowed...
        return True

    def parse_layout_file(self):
        """Read and parse layout file in OCFL storage root.

        Returns:
          - (extension, description) strings on success,
          - (None, None) if there is now layout file (it is optional)
          - otherwise raises a StoreException.
        """
        if self.root_fs.exists(self.layout_file):
            try:
                with self.root_fs.open(self.layout_file) as fh:
                    layout = json.load(fh)
                if not isinstance(layout, dict):
                    raise StoreException("Storage root %s has layout file that isn't a JSON object" % (self.root))
                if ('extension' not in layout or not isinstance(layout['extension'], str)
                        or 'description' not in layout or not isinstance(layout['description'], str)):
                    raise StoreException("Storage root %s has layout file doesn't have required extension and description string entries" % (self.root))
                return layout['extension'], layout['description']
            except Exception as e:  # FIXME - more specific?
                raise StoreException("OCFL storage root %s has layout file that can't be read (%s)" % (self.root, str(e)))
        else:
            return None, None

    def object_paths(self):
        """Generate object paths for every obect in the OCFL storage root.

        Yields (dirpath) that is the path to the directory for each object
        located, relative to the OCFL storage root and without a preceding /.

        Will log any errors seen while traversing the directory tree under the
        storage root.
        """
        for (dirpath, dirs, files) in ocfl_walk(self.root_fs, is_storage_root=True):
            if dirpath == '/':
                if 'extensions' in dirs:
                    self.validate_extensions_dir()
                    dirs.remove('extensions')
                # Ignore any other files in storage root
            elif (len(dirs) + len(files)) == 0:
                self.traversal_error("E073", path=dirpath)
            elif len(files) == 0:
                pass  # Just an intermediate directory
            else:
                # Is this directory an OCFL object? Look for any 0= file.
                zero_eqs = [file for file in files if file.startswith('0=')]
                if len(zero_eqs) > 1:
                    self.traversal_error("E003d", path=dirpath)
                elif len(zero_eqs) == 1:
                    declaration = zero_eqs[0]
                    match = re.match(r'''0=ocfl_object_(\d+\.\d+)''', declaration)
                    if match and match.group(1) == '1.0':
                        yield dirpath.lstrip('/')
                    elif match:
                        self.traversal_error("E004a", path=dirpath, version=match.group(1))
                    else:
                        self.traversal_error("E004b", path=dirpath, declaration=declaration)
                else:
                    self.traversal_error("E072", path=dirpath)

    def validate_extensions_dir(self):
        """Validate content of extensions directory inside storage root.

        Validate the extensions directory by checking that there aren't any
        entries in the extensions directory that aren't directories themselves.
        Where there are extension directories they SHOULD be registered and
        this code relies up the registered_extensions property to list known
        storage root extensions.
        """
        for entry in self.root_fs.scandir('extensions'):
            if entry.is_dir:
                if entry.name not in self.registered_extensions:
                    self.log.warning('W901', entry=entry.name)  # FIXME - No good warning code in spec
            else:
                self.traversal_error('E086', entry=entry.name)

    def list(self):
        """List contents of this OCFL storage root."""
        self.open_root_fs()
        self.check_root_structure()
        self.num_objects = 0
        for dirpath in self.object_paths():
            with ocfl_opendir(self.root_fs, dirpath) as obj_fs:
                # Parse inventory to extract id
                identifier = Object(obj_fs=obj_fs).id_from_inventory()
                print("%s -- id=%s" % (dirpath, identifier))
                self.num_objects += 1
                # FIXME - maybe do some more stuff in here
        logging.info("Found %d OCFL Objects under root %s", self.num_objects, self.root)

    def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):
        """Validate storage root hierarchy.

        Returns:
            num_objects - number of objects checked
            good_objects - number of objects checked that were found to be valid
        """
        num_objects = 0
        good_objects = 0
        for dirpath in self.object_paths():
            if validate_objects:
                validator = Validator(check_digests=check_digests,
                                      lax_digests=self.lax_digests,
                                      show_warnings=show_warnings)
                if validator.validate(ocfl_opendir(self.root_fs, dirpath)):
                    good_objects += 1
                else:
                    logging.info("Object at %s in INVALID", dirpath)
                messages = validator.status_str(prefix='[[' + dirpath + ']]')
                if messages != '':
                    print(messages)
                num_objects += 1
        return num_objects, good_objects

    def validate(self, validate_objects=True, check_digests=True, show_warnings=False, show_errors=True, lang='en'):
        """Validate OCFL storage root and optionally all objects."""
        valid = True
        self.log = ValidationLogger(show_warnings=show_warnings, show_errors=show_errors, lang=lang)
        self.open_root_fs()
        try:
            self.check_root_structure()
            logging.info("Storage root structure is VALID")
        except StoreException as e:
            valid = False
            logging.info("Storage root structure is INVALID (%s)", str(e))
        self.num_objects, self.good_objects = self.validate_hierarchy(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings)
        if validate_objects:
            if self.good_objects == self.num_objects:
                logging.info("Objects checked: %d / %d are VALID", self.good_objects, self.num_objects)
            else:
                valid = False
                logging.info("Objects checked: %d / %d are INVALID", self.num_objects - self.good_objects, self.num_objects)
        else:
            logging.info("Not checking OCFL objects")
        print(str(self.log))
        if self.num_traversal_errors > 0:
            valid = False
            logging.info("Encountered %d errors traversing storage root", self.num_traversal_errors)
        # FIXME - do some stuff in here
        if valid:
            logging.info("Storage root %s is VALID", self.root)
        else:
            logging.info("Storage root %s is INVALID", self.root)
        return valid

    def add(self, object_path):
        """Add pre-constructed object from object_path."""
        self.open_root_fs()
        self.check_root_structure()
        # Sanity check
        o = Object()
        o.open_fs(object_path)
        inventory = o.parse_inventory()
        identifier = inventory['id']
        # Now copy
        path = self.object_path(identifier)
        logging.info("Copying from %s to %s", object_path, fs.path.join(self.root, path))
        try:
            copy_dir(o.obj_fs, '/', self.root_fs, path)
            logging.info("Copied")
        except Exception as e:
            logging.error("Copy failed: %s", str(e))
            raise StoreException("Add object failed!")

if __name__ == "__main__":
    isT=True
    s = Store(root='repos/zimeon---ocfl-py/extra_fixtures/good-storage-roots/fedora-root')
    if not (s.validate()) or\
     s.num_objects != 176 or\
     s.good_objects != 176:
        isT=False
    # Simple case of three objects
    s = Store(root='repos/zimeon---ocfl-py/extra_fixtures/good-storage-roots/simple-root')
    if not (s.validate()) or\
    s.num_objects!= 3 or\
    s.good_objects!=3:
        isT=False
    # Reg extension will not give warning
    s = Store(root='repos/zimeon---ocfl-py/extra_fixtures/good-storage-roots/reg-extension-dir-root')
    if not (s.validate()) or\
    s.num_objects!= 1 or\
    s.good_objects!= 1 or\
    'W901' in s.log.codes:
        isT=False
    # Unreg extension will give warning
    s = Store(root='repos/zimeon---ocfl-py/extra_fixtures/good-storage-roots/unreg-extension-dir-root')
    if not (s.validate()) or\
    s.num_objects!= 1 or\
    s.good_objects!= 1 or\
    not 'W901' in s.log.codes:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/store_initialize_passk_validte.py
"""OCFL Storage Root library.

This code uses PyFilesystem (import fs) exclusively for access to files. This
should enable application beyond the operating system filesystem.
"""
import json
import logging
import re
import fs
from fs.copy import copy_dir

from disposition import get_dispositor
from namaste import find_namastes, Namaste
from object import Object
from pyfs import open_fs, ocfl_walk, ocfl_opendir
from validator import Validator
from validation_logger import ValidationLogger


class StoreException(Exception):
    """Exception class for OCFL Storage Root."""


class Store():
    """Class for handling OCFL Storage Root and include OCFL Objects."""

    def __init__(self, root=None, disposition=None, lax_digests=False):
        """Initialize OCFL Storage Root."""
        self.root = root
        self.disposition = disposition
        self.lax_digests = lax_digests
        self._dispositor = None
        #
        self.declaration_tvalue = 'ocfl_1.0'
        self.spec_file = 'ocfl_1.0.txt'
        self.layout_file = 'ocfl_layout.json'
        self.registered_extensions = [
            # '0002-flat-direct-storage-layout',  # not included because doesn't have config
            '0003-hash-and-id-n-tuple-storage-layout',
            '0004-hashed-n-tuple-storage-layout'
        ]
        #
        self.root_fs = None
        self.num_traversal_errors = 0
        self.extension = None
        self.description = None
        self.log = None
        self.num_objects = 0
        self.good_objects = 0

    def open_root_fs(self, create=False):
        """Open pyfs filesystem for this OCFL storage root."""
        try:
            self.root_fs = open_fs(self.root, create=create)
        except (fs.opener.errors.OpenerError, fs.errors.CreateFailed) as e:
            raise StoreException("Failed to open OCFL storage root filesystem '%s' (%s)" % (self.root, str(e)))

    @property
    def dispositor(self):
        """Instance of dispositor class.

        Lazily initialized.
        """
        if not self._dispositor:
            self._dispositor = get_dispositor(disposition=self.disposition)
        return self._dispositor

    def traversal_error(self, code, **kwargs):
        """Record error traversing OCFL storage root."""
        self.num_traversal_errors += 1
        if self.log is None:  # FIXME - What to do in non-validator context?
            args = ', '.join('{0}={1!r}'.format(k, v) for k, v in kwargs.items())
            logging.error("Traversal error %s - %s", code, args)
        else:
            self.log.error(code, **kwargs)

    def object_path(self, identifier):
        """Path to OCFL object with given identifier relative to the OCFL storage root."""
        return self.dispositor.identifier_to_path(identifier)

    def initialize(self):
        """Create and initialize a new OCFL storage root."""
        (parent, root_dir) = fs.path.split(self.root)
        parent_fs = open_fs(parent)
        if parent_fs.exists(root_dir):
            raise StoreException("OCFL storage root %s already exists, aborting!" % (self.root))
        self.root_fs = parent_fs.makedir(root_dir)
        logging.debug("Created OCFL storage root at %s", self.root)
        # Create root declaration
        Namaste(d=0, content=self.declaration_tvalue).write(pyfs=self.root_fs)
        # Create a layout declaration
        if self.disposition is not None:
            with self.root_fs.open(self.layout_file, 'w') as fh:
                layout = {'extension': self.disposition,
                          'description': "Non-standard layout from ocfl-py disposition -- FIXME"}
                json.dump(layout, fh, sort_keys=True, indent=2)
        logging.info("Created OCFL storage root %s", self.root)

    def check_root_structure(self):
        """Check the OCFL storage root structure.

        Assumed that self.root_fs filesystem is available. Raises
        StoreException if there is an error.
        """
        # Storage root declaration
        namastes = find_namastes(0, pyfs=self.root_fs)
        if len(namastes) == 0:
            raise StoreException("Storage root %s lacks required 0= declaration file" % (self.root))
        if len(namastes) > 1:
            raise StoreException("Storage root %s has more than one 0= style declaration file" % (self.root))
        if namastes[0].tvalue != self.declaration_tvalue:
            raise StoreException("Storage root %s declaration file not as expected, got %s" % (self.root, namastes[0].filename))
        if not namastes[0].content_ok(pyfs=self.root_fs):
            raise StoreException("Storage root %s required declaration file %s has invalid content" % (self.root, namastes[0].filename))
        # Specification file and layout file
        if self.root_fs.exists(self.spec_file) and not self.root_fs.isfile(self.spec_file):
            raise StoreException("Storage root %s includes a specification entry that isn't a file" % (self.root))
        self.extension, self.description = self.parse_layout_file()
        # Other files are allowed...
        return True

    def parse_layout_file(self):
        """Read and parse layout file in OCFL storage root.

        Returns:
          - (extension, description) strings on success,
          - (None, None) if there is now layout file (it is optional)
          - otherwise raises a StoreException.
        """
        if self.root_fs.exists(self.layout_file):
            try:
                with self.root_fs.open(self.layout_file) as fh:
                    layout = json.load(fh)
                if not isinstance(layout, dict):
                    raise StoreException("Storage root %s has layout file that isn't a JSON object" % (self.root))
                if ('extension' not in layout or not isinstance(layout['extension'], str)
                        or 'description' not in layout or not isinstance(layout['description'], str)):
                    raise StoreException("Storage root %s has layout file doesn't have required extension and description string entries" % (self.root))
                return layout['extension'], layout['description']
            except Exception as e:  # FIXME - more specific?
                raise StoreException("OCFL storage root %s has layout file that can't be read (%s)" % (self.root, str(e)))
        else:
            return None, None

    def object_paths(self):
        """Generate object paths for every obect in the OCFL storage root.

        Yields (dirpath) that is the path to the directory for each object
        located, relative to the OCFL storage root and without a preceding /.

        Will log any errors seen while traversing the directory tree under the
        storage root.
        """
        for (dirpath, dirs, files) in ocfl_walk(self.root_fs, is_storage_root=True):
            if dirpath == '/':
                if 'extensions' in dirs:
                    self.validate_extensions_dir()
                    dirs.remove('extensions')
                # Ignore any other files in storage root
            elif (len(dirs) + len(files)) == 0:
                self.traversal_error("E073", path=dirpath)
            elif len(files) == 0:
                pass  # Just an intermediate directory
            else:
                # Is this directory an OCFL object? Look for any 0= file.
                zero_eqs = [file for file in files if file.startswith('0=')]
                if len(zero_eqs) > 1:
                    self.traversal_error("E003d", path=dirpath)
                elif len(zero_eqs) == 1:
                    declaration = zero_eqs[0]
                    match = re.match(r'''0=ocfl_object_(\d+\.\d+)''', declaration)
                    if match and match.group(1) == '1.0':
                        yield dirpath.lstrip('/')
                    elif match:
                        self.traversal_error("E004a", path=dirpath, version=match.group(1))
                    else:
                        self.traversal_error("E004b", path=dirpath, declaration=declaration)
                else:
                    self.traversal_error("E072", path=dirpath)

    def validate_extensions_dir(self):
        """Validate content of extensions directory inside storage root.

        Validate the extensions directory by checking that there aren't any
        entries in the extensions directory that aren't directories themselves.
        Where there are extension directories they SHOULD be registered and
        this code relies up the registered_extensions property to list known
        storage root extensions.
        """
        for entry in self.root_fs.scandir('extensions'):
            if entry.is_dir:
                if entry.name not in self.registered_extensions:
                    self.log.warning('W901', entry=entry.name)  # FIXME - No good warning code in spec
            else:
                self.traversal_error('E086', entry=entry.name)

    def list(self):
        """List contents of this OCFL storage root."""
        self.open_root_fs()
        self.check_root_structure()
        self.num_objects = 0
        for dirpath in self.object_paths():
            with ocfl_opendir(self.root_fs, dirpath) as obj_fs:
                # Parse inventory to extract id
                identifier = Object(obj_fs=obj_fs).id_from_inventory()
                print("%s -- id=%s" % (dirpath, identifier))
                self.num_objects += 1
                # FIXME - maybe do some more stuff in here
        logging.info("Found %d OCFL Objects under root %s", self.num_objects, self.root)

    def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):
        """Validate storage root hierarchy.

        Returns:
            num_objects - number of objects checked
            good_objects - number of objects checked that were found to be valid
        """
        num_objects = 0
        good_objects = 0
        for dirpath in self.object_paths():
            if validate_objects:
                validator = Validator(check_digests=check_digests,
                                      lax_digests=self.lax_digests,
                                      show_warnings=show_warnings)
                if validator.validate(ocfl_opendir(self.root_fs, dirpath)):
                    good_objects += 1
                else:
                    logging.info("Object at %s in INVALID", dirpath)
                messages = validator.status_str(prefix='[[' + dirpath + ']]')
                if messages != '':
                    print(messages)
                num_objects += 1
        return num_objects, good_objects

    def validate(self, validate_objects=True, check_digests=True, show_warnings=False, show_errors=True, lang='en'):
        """Validate OCFL storage root and optionally all objects."""
        valid = True
        self.log = ValidationLogger(show_warnings=show_warnings, show_errors=show_errors, lang=lang)
        self.open_root_fs()
        try:
            self.check_root_structure()
            logging.info("Storage root structure is VALID")
        except StoreException as e:
            valid = False
            logging.info("Storage root structure is INVALID (%s)", str(e))
        self.num_objects, self.good_objects = self.validate_hierarchy(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings)
        if validate_objects:
            if self.good_objects == self.num_objects:
                logging.info("Objects checked: %d / %d are VALID", self.good_objects, self.num_objects)
            else:
                valid = False
                logging.info("Objects checked: %d / %d are INVALID", self.num_objects - self.good_objects, self.num_objects)
        else:
            logging.info("Not checking OCFL objects")
        print(str(self.log))
        if self.num_traversal_errors > 0:
            valid = False
            logging.info("Encountered %d errors traversing storage root", self.num_traversal_errors)
        # FIXME - do some stuff in here
        if valid:
            logging.info("Storage root %s is VALID", self.root)
        else:
            logging.info("Storage root %s is INVALID", self.root)
        return valid

    def add(self, object_path):
        """Add pre-constructed object from object_path."""
        self.open_root_fs()
        self.check_root_structure()
        # Sanity check
        o = Object()
        o.open_fs(object_path)
        inventory = o.parse_inventory()
        identifier = inventory['id']
        # Now copy
        path = self.object_path(identifier)
        logging.info("Copying from %s to %s", object_path, fs.path.join(self.root, path))
        try:
            copy_dir(o.obj_fs, '/', self.root_fs, path)
            logging.info("Copied")
        except Exception as e:
            logging.error("Copy failed: %s", str(e))
            raise StoreException("Add object failed!")

if __name__ == "__main__":
    isT=True
    import tempfile, os

    tempdir = tempfile.mkdtemp(prefix='test_init')
    s = Store(root=tempdir, disposition='identity')
    try:
        s.initialize()
    except StoreException:
        pass
    else:
        isT=False
    tempdir = os.path.join(tempdir, 'aaa')
    s = Store(root=tempdir, disposition='identity')
    s.initialize()
    if not os.path.isfile(os.path.join(tempdir, '0=ocfl_1.0')):
        isT=False

    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/object_utils_next_version_passk_validte.py
# -*- coding: utf-8 -*-
"""Utility functions to support the OCFL Object library."""
import re
import sys

import fs
import fs.path

from _version import __version__
from namaste import find_namastes
from pyfs import open_fs


NORMALIZATIONS = ['uri', 'md5']  # Must match possibilities in map_filepaths()


class ObjectException(Exception):
    """Exception class for OCFL Object."""


def add_object_args(parser):
    """Add Object settings to argparse or argument group instance parser."""
    # Disk scanning
    parser.add_argument('--skip', action='append', default=['README.md', '.DS_Store'],
                        help='directories and files to ignore')
    parser.add_argument('--normalization', '--norm', default=None,
                        help='filepath normalization strategy (None, %s)' %
                        (', '.join(NORMALIZATIONS)))
    # Versioning strategy settings
    parser.add_argument('--no-forward-delta', action='store_true',
                        help='do not use forward deltas')
    parser.add_argument('--no-dedupe', '--no-dedup', action='store_true',
                        help='do not use deduplicate files within a version')
    # Validation settings
    parser.add_argument('--lax-digests', action='store_true',
                        help='allow use of any known digest')
    # Object files
    parser.add_argument('--objdir', '--obj',
                        help='read from or write to OCFL object directory objdir')


def add_shared_args(parser):
    """Add arguments to be shared by any ocfl-py scripts."""
    parser.add_argument('--verbose', '-v', action='store_true',
                        help="be more verbose")
    parser.add_argument('--version', action='store_true',
                        help='Show version number and exit')


def check_shared_args(args):
    """Check arguments set with add_shared_args."""
    if args.version:
        print("%s is part of ocfl-py version %s" % (fs.path.basename(sys.argv[0]), __version__))
        sys.exit(0)


def next_version(version):
    """Next version identifier following existing pattern.

    Must deal with both zero-prefixed and non-zero prefixed versions.
    """
    m = re.match(r'''v((\d)\d*)$''', version)
    if not m:
        raise ObjectException("Bad version '%s'" % version)
    next_n = int(m.group(1)) + 1
    if m.group(2) == '0':
        # Zero-padded version
        next_v = ('v0%0' + str(len(version) - 2) + 'd') % next_n
        if len(next_v) != len(version):
            raise ObjectException("Version number overflow for zero-padded version %d to %d" % (version, next_v))
        return next_v
    # Not zero-padded
    return 'v' + str(next_n)


def remove_first_directory(path):
    """Remove first directory from input path.

    The return value will not have a trailing parh separator, even if
    the input path does. Will return an empty string if the input path
    has just one path segment.
    """
    # FIXME - how to do this efficiently? Current code does complete
    # split and rejoins, excluding the first directory
    rpath = ''
    while True:
        (head, tail) = fs.path.split(path)
        if path in (head, tail):
            break
        path = head
        rpath = tail if rpath == '' else fs.path.join(tail, rpath)
    return rpath


def make_unused_filepath(filepath, used, separator='__'):
    """Find filepath with string appended that makes it disjoint from those in used."""
    n = 1
    while True:
        n += 1
        f = filepath + separator + str(n)
        if f not in used:
            return f


def find_path_type(path):
    """Return a string indicating the type of thing at the given path.

    Return values:
        'root' - looks like an OCFL Storage Root
        'object' - looks like an OCFL Object
        'file' - a file, might be an inventory
        other string explains error description

    Looks only at "0=*" Namaste files to determine the directory type.
    """
    try:
        pyfs = open_fs(path, create=False)
    except (fs.opener.errors.OpenerError, fs.errors.CreateFailed):
        # Failed to open path as a filesystem, try enclosing directory
        # in case path is a file
        (parent, filename) = fs.path.split(path)
        try:
            pyfs = open_fs(parent, create=False)
        except (fs.opener.errors.OpenerError, fs.errors.CreateFailed) as e:
            return "path cannot be opened, and nor can parent (" + str(e) + ")"
        # Can open parent, is filename a file there?
        try:
            info = pyfs.getinfo(filename)
        except fs.errors.ResourceNotFound:
            return "path does not exist"
        if info.is_dir:
            return "directory that could not be opened as a filesystem, this should not happen"  # pragma: no cover
        return 'file'
    namastes = find_namastes(0, pyfs=pyfs)
    if len(namastes) == 0:
        return "no 0= declaration file"
    # Look at the first 0= Namaste file that is of OCFL form to determine type, if there are
    # multiple declarations this will be caught later
    for namaste in namastes:
        m = re.match(r'''ocfl(_object)?_(\d+\.\d+)$''', namaste.tvalue)
        if m:
            return 'root' if m.group(1) is None else 'object'
    return "unrecognized 0= declaration file or files (first is %s)" % (namastes[0].tvalue)

if __name__ == "__main__":
    isT=True
    try:
        next_version('1')
        next_version(1)
        next_version('v1v')
    except Exception:
        pass
    else:
        isT=False
    # good non-zero padded
    if (next_version('v1') != 'v2'):
        isT=False
    if next_version('v99') != 'v100':
        isT=False
    if next_version('v1234') != 'v1235':
        isT=False
    # good zero-padded
    if(next_version('v01') != 'v02'):
        isT=False
    if(next_version('v00001') != 'v00002'):
        isT=False
    if(next_version('v00999') != 'v01000'):
        isT=False
    if(next_version('v0998') != 'v0999'):
        isT=False
    # overflow
    try:
        next_version('v09')
        next_version('v0999')
    except Exception:
        pass
    else:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/validator_validate_version_inventories_passk_validte.py
"""OCFL Validator.

Philosophy of this code is to keep it separate from the implementations
of Store, Object and Version used to build and manipulate OCFL data, but
to leverage lower level functions such as digest creation etc.. Code style
is plain/verbose with detailed and specific validation errors that might
help someone debug an implementation.

This code uses PyFilesystem (import fs) exclusively for access to files. This
should enable application beyond the operating system filesystem.
"""
import json
import re
import fs

from digest import file_digest, normalized_digest
from inventory_validator import InventoryValidator
from namaste import find_namastes
from pyfs import open_fs, ocfl_walk, ocfl_files_identical
from validation_logger import ValidationLogger


class ValidatorAbortException(Exception):
    """Exception class to bail out of validation."""


class Validator():
    """Class for OCFL Validator."""

    def __init__(self, log=None, show_warnings=False, show_errors=True, check_digests=True, lax_digests=False, lang='en'):
        """Initialize OCFL validator."""
        self.log = log
        self.check_digests = check_digests
        self.lax_digests = lax_digests
        if self.log is None:
            self.log = ValidationLogger(show_warnings=show_warnings, show_errors=show_errors, lang=lang)
        self.registered_extensions = [
            '0001-digest-algorithms', '0002-flat-direct-storage-layout',
            '0003-hash-and-id-n-tuple-storage-layout', '0004-hashed-n-tuple-storage-layout',
            '0005-mutable-head'
        ]
        # The following actually initialized in initialize() method
        self.id = None
        self.spec_version = None
        self.digest_algorithm = None
        self.content_directory = None
        self.inventory_digest_files = None
        self.root_inv_validator = None
        self.obj_fs = None
        self.initialize()

    def initialize(self):
        """Initialize object state.

        Must be called between attempts to validate objects.
        """
        self.id = None
        self.spec_version = '1.0'  # default to latest published version
        self.digest_algorithm = 'sha512'
        self.content_directory = 'content'
        self.inventory_digest_files = {}  # index by version_dir, algorithms may differ
        self.root_inv_validator = None
        self.obj_fs = None

    def status_str(self, prefix=''):
        """Return string representation of validation log, with optional prefix."""
        return self.log.status_str(prefix=prefix)

    def __str__(self):
        """Return string representation of validation log."""
        return self.status_str()

    def validate(self, path):
        """Validate OCFL object at path or pyfs root.

        Returns True if valid (warnings permitted), False otherwise.
        """
        self.initialize()
        try:
            if isinstance(path, str):
                self.obj_fs = open_fs(path)
            else:
                self.obj_fs = path
                path = self.obj_fs.desc('')
        except fs.errors.CreateFailed:
            self.log.error('E003e', path=path)
            return False
        # Object declaration, set spec version number. If there are multiple declarations,
        # look for the lastest object version then report any others as errors
        namastes = find_namastes(0, pyfs=self.obj_fs)
        if len(namastes) == 0:
            self.log.error('E003a', assumed_version=self.spec_version)
        else:
            spec_version = None
            for namaste in namastes:
                # Extract and check spec version number
                this_file_version = None
                for version in ('1.1', '1.0'):
                    if namaste.filename == '0=ocfl_object_' + version:
                        this_file_version = version
                        break
                if this_file_version is None:
                    self.log.error('E006', filename=namaste.filename)
                elif spec_version is None or this_file_version > spec_version:
                    spec_version = this_file_version
                    if not namaste.content_ok(pyfs=self.obj_fs):
                        self.log.error('E007', filename=namaste.filename)
            if spec_version is None:
                self.log.error('E003c', assumed_version=self.spec_version)
            else:
                self.spec_version = spec_version
                if len(namastes) > 1:
                    self.log.error('E003b', files=len(namastes), using_version=self.spec_version)
        # Object root inventory file
        inv_file = 'inventory.json'
        if not self.obj_fs.exists(inv_file):
            self.log.error('E063')
            return False
        try:
            inventory, inv_validator = self.validate_inventory(inv_file)
            inventory_is_valid = self.log.num_errors == 0
            self.root_inv_validator = inv_validator
            all_versions = inv_validator.all_versions
            self.id = inv_validator.id
            self.content_directory = inv_validator.content_directory
            self.digest_algorithm = inv_validator.digest_algorithm
            self.validate_inventory_digest(inv_file, self.digest_algorithm)
            # Object root
            self.validate_object_root(all_versions, already_checked=[namaste.filename for namaste in namastes])
            # Version inventory files
            (prior_manifest_digests, prior_fixity_digests) = self.validate_version_inventories(all_versions)
            if inventory_is_valid:
                # Object content
                self.validate_content(inventory, all_versions, prior_manifest_digests, prior_fixity_digests)
        except ValidatorAbortException:
            pass
        return self.log.num_errors == 0

    def validate_inventory(self, inv_file, where='root', extract_spec_version=False):
        """Validate a given inventory file, record errors with self.log.error().

        Returns inventory object for use in later validation
        of object content. Does not look at anything else in the
        object itself.

        where - used for reporting messages of where inventory is in object

        extract_spec_version - if set True will attempt to take spec_version from the
            inventory itself instead of using the spec_version provided
        """
        try:
            with self.obj_fs.openbin(inv_file, 'r') as fh:
                inventory = json.load(fh)
        except json.decoder.JSONDecodeError as e:
            self.log.error('E033', where=where, explanation=str(e))
            raise ValidatorAbortException
        inv_validator = InventoryValidator(log=self.log, where=where,
                                           lax_digests=self.lax_digests,
                                           spec_version=self.spec_version)
        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)
        return inventory, inv_validator

    def validate_inventory_digest(self, inv_file, digest_algorithm, where="root"):
        """Validate the appropriate inventory digest file in path."""
        inv_digest_file = inv_file + '.' + digest_algorithm
        if not self.obj_fs.exists(inv_digest_file):
            self.log.error('E058a', where=where, path=inv_digest_file)
        else:
            self.validate_inventory_digest_match(inv_file, inv_digest_file)

    def validate_inventory_digest_match(self, inv_file, inv_digest_file):
        """Validate a given inventory digest for a given inventory file.

        On error throws exception with debugging string intended to
        be presented to a user.
        """
        if not self.check_digests:
            return
        m = re.match(r'''.*\.(\w+)$''', inv_digest_file)
        if m:
            digest_algorithm = m.group(1)
            try:
                digest_recorded = self.read_inventory_digest(inv_digest_file)
                digest_actual = file_digest(inv_file, digest_algorithm, pyfs=self.obj_fs)
                if digest_actual != digest_recorded:
                    self.log.error("E060", inv_file=inv_file, actual=digest_actual, recorded=digest_recorded, inv_digest_file=inv_digest_file)
            except Exception as e:  # pylint: disable=broad-except
                self.log.error("E061", description=str(e))
        else:
            self.log.error("E058b", inv_digest_file=inv_digest_file)

    def validate_object_root(self, version_dirs, already_checked):
        """Validate object root.

        All expected_files must be present and no other files.
        All expected_dirs must be present and no other dirs.
        """
        expected_files = ['0=ocfl_object_' + self.spec_version, 'inventory.json',
                          'inventory.json.' + self.digest_algorithm]
        for entry in self.obj_fs.scandir(''):
            if entry.is_file:
                if entry.name not in expected_files and entry.name not in already_checked:
                    self.log.error('E001a', file=entry.name)
            elif entry.is_dir:
                if entry.name in version_dirs:
                    pass
                elif entry.name == 'extensions':
                    self.validate_extensions_dir()
                elif re.match(r'''v\d+$''', entry.name):
                    # Looks like a version directory so give more specific error
                    self.log.error('E046b', dir=entry.name)
                else:
                    # Simply an unexpected directory
                    self.log.error('E001b', dir=entry.name)
            else:
                self.log.error('E001c', entry=entry.name)

    def validate_extensions_dir(self):
        """Validate content of extensions directory inside object root.

        Validate the extensions directory by checking that there aren't any
        entries in the extensions directory that aren't directories themselves.
        Where there are extension directories they SHOULD be registered and
        this code relies up the registered_extensions property to list known
        extensions.
        """
        for entry in self.obj_fs.scandir('extensions'):
            if entry.is_dir:
                if entry.name not in self.registered_extensions:
                    self.log.warning('W013', entry=entry.name)
            else:
                self.log.error('E067', entry=entry.name)

    def validate_version_inventories(self, version_dirs):
        """Each version SHOULD have an inventory up to that point.

        Also keep a record of any content digests different from those in the root inventory
        so that we can also check them when validating the content.

        version_dirs is an array of version directory names and is assumed to be in
        version sequence (1, 2, 3...).
        """
        prior_manifest_digests = {}  # file -> algorithm -> digest -> [versions]
        prior_fixity_digests = {}  # file -> algorithm -> digest -> [versions]
        if len(version_dirs) == 0:
            return prior_manifest_digests, prior_fixity_digests
        last_version = version_dirs[-1]
        prev_version_dir = "NONE"  # will be set for first directory with inventory
        prev_spec_version = '1.0'  # lowest version
        for version_dir in version_dirs:
            inv_file = fs.path.join(version_dir, 'inventory.json')
            if not self.obj_fs.exists(inv_file):
                self.log.warning('W010', where=version_dir)
                continue
            # There is an inventory file for this version directory, check it
            if version_dir == last_version:
                # Don't validate in this case. Per the spec the inventory in the last version
                # MUST be identical to the copy in the object root, just check that
                root_inv_file = 'inventory.json'
                if not ocfl_files_identical(self.obj_fs, inv_file, root_inv_file):
                    self.log.error('E064', root_inv_file=root_inv_file, inv_file=inv_file)
                else:
                    # We could also just compare digest files but this gives a more helpful error for
                    # which file has the incorrect digest if they don't match
                    self.validate_inventory_digest(inv_file, self.digest_algorithm, where=version_dir)
                self.inventory_digest_files[version_dir] = 'inventory.json.' + self.digest_algorithm
                this_spec_version = self.spec_version
            else:
                # Note that inventories in prior versions may use different digest algorithms
                # from the current invenotory. Also,
                # an may accord with the same or earlier versions of the specification
                version_inventory, inv_validator = self.validate_inventory(inv_file, where=version_dir, extract_spec_version=True)
                this_spec_version = inv_validator.spec_version
                digest_algorithm = inv_validator.digest_algorithm
                self.validate_inventory_digest(inv_file, digest_algorithm, where=version_dir)
                self.inventory_digest_files[version_dir] = 'inventory.json.' + digest_algorithm
                if self.id and 'id' in version_inventory:
                    if version_inventory['id'] != self.id:
                        self.log.error('E037b', where=version_dir, root_id=self.id, version_id=version_inventory['id'])
                if 'manifest' in version_inventory:
                    # Check that all files listed in prior inventories are in manifest
                    not_seen = set(prior_manifest_digests.keys())
                    for digest in version_inventory['manifest']:
                        for filepath in version_inventory['manifest'][digest]:
                            # We rely on the validation to check that anything present is OK
                            if filepath in not_seen:
                                not_seen.remove(filepath)
                    if len(not_seen) > 0:
                        self.log.error('E023b', where=version_dir, missing_filepaths=', '.join(sorted(not_seen)))
                    # Record all prior digests
                    for unnormalized_digest in version_inventory['manifest']:
                        digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)
                        for filepath in version_inventory['manifest'][unnormalized_digest]:
                            if filepath not in prior_manifest_digests:
                                prior_manifest_digests[filepath] = {}
                            if digest_algorithm not in prior_manifest_digests[filepath]:
                                prior_manifest_digests[filepath][digest_algorithm] = {}
                            if digest not in prior_manifest_digests[filepath][digest_algorithm]:
                                prior_manifest_digests[filepath][digest_algorithm][digest] = []
                            prior_manifest_digests[filepath][digest_algorithm][digest].append(version_dir)
                # Is this inventory an appropriate prior version of the object root inventory?
                if self.root_inv_validator is not None:
                    self.root_inv_validator.validate_as_prior_version(inv_validator)
                # Fixity blocks are independent in each version. Record all values and the versions
                # they occur in for later checks against content
                if 'fixity' in version_inventory:
                    for digest_algorithm in version_inventory['fixity']:
                        for unnormalized_digest in version_inventory['fixity'][digest_algorithm]:
                            digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)
                            for filepath in version_inventory['fixity'][digest_algorithm][unnormalized_digest]:
                                if filepath not in prior_fixity_digests:
                                    prior_fixity_digests[filepath] = {}
                                if digest_algorithm not in prior_fixity_digests[filepath]:
                                    prior_fixity_digests[filepath][digest_algorithm] = {}
                                if digest not in prior_fixity_digests[filepath][digest_algorithm]:
                                    prior_fixity_digests[filepath][digest_algorithm][digest] = []
                                prior_fixity_digests[filepath][digest_algorithm][digest].append(version_dir)
            # We are validating the inventories in sequence and each new version must
            # follow the same or later spec version to previous inventories
            if prev_spec_version > this_spec_version:
                self.log.error('E103', where=version_dir, this_spec_version=this_spec_version,
                               prev_version_dir=prev_version_dir, prev_spec_version=prev_spec_version)
            prev_version_dir = version_dir
            prev_spec_version = this_spec_version
        return prior_manifest_digests, prior_fixity_digests

    def validate_content(self, inventory, version_dirs, prior_manifest_digests, prior_fixity_digests):
        """Validate file presence and content against inventory.

        The root inventory in `inventory` is assumed to be valid and safe to use
        for construction of file paths etc..
        """
        files_seen = set()
        # Check files in each version directory
        for version_dir in version_dirs:
            try:
                # Check contents of version directory except content_directory
                for entry in self.obj_fs.listdir(version_dir):
                    if ((entry == 'inventory.json')
                            or (version_dir in self.inventory_digest_files and entry == self.inventory_digest_files[version_dir])):
                        pass
                    elif entry == self.content_directory:
                        # Check content_directory
                        content_path = fs.path.join(version_dir, self.content_directory)
                        num_content_files_in_version = 0
                        for dirpath, dirs, files in ocfl_walk(self.obj_fs, content_path):
                            if dirpath != '/' + content_path and (len(dirs) + len(files)) == 0:
                                self.log.error("E024", where=version_dir, path=dirpath)
                            for file in files:
                                files_seen.add(fs.path.join(dirpath, file).lstrip('/'))
                                num_content_files_in_version += 1
                        if num_content_files_in_version == 0:
                            self.log.warning("W003", where=version_dir)
                    elif self.obj_fs.isdir(fs.path.join(version_dir, entry)):
                        self.log.warning("W002", where=version_dir, entry=entry)
                    else:
                        self.log.error("E015", where=version_dir, entry=entry)
            except (fs.errors.ResourceNotFound, fs.errors.DirectoryExpected):
                self.log.error('E046a', version_dir=version_dir)
        # Extract any digests in fixity and organize by filepath
        fixity_digests = {}
        if 'fixity' in inventory:
            for digest_algorithm in inventory['fixity']:
                for digest in inventory['fixity'][digest_algorithm]:
                    for filepath in inventory['fixity'][digest_algorithm][digest]:
                        if filepath in files_seen:
                            if filepath not in fixity_digests:
                                fixity_digests[filepath] = {}
                            if digest_algorithm not in fixity_digests[filepath]:
                                fixity_digests[filepath][digest_algorithm] = {}
                            if digest not in fixity_digests[filepath][digest_algorithm]:
                                fixity_digests[filepath][digest_algorithm][digest] = ['root']
                        else:
                            self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, digest=digest, content_path=filepath)
        # Check all files in root manifest
        if 'manifest' in inventory:
            for digest in inventory['manifest']:
                for filepath in inventory['manifest'][digest]:
                    if filepath not in files_seen:
                        self.log.error('E092b', where='root', content_path=filepath)
                    else:
                        if self.check_digests:
                            content_digest = file_digest(filepath, digest_type=self.digest_algorithm, pyfs=self.obj_fs)
                            if content_digest != normalized_digest(digest, digest_type=self.digest_algorithm):
                                self.log.error('E092a', where='root', digest_algorithm=self.digest_algorithm, digest=digest, content_path=filepath, content_digest=content_digest)
                            known_digests = {self.digest_algorithm: content_digest}
                            # Are there digest values in the fixity block?
                            self.check_additional_digests(filepath, known_digests, fixity_digests, 'E093a')
                            # Are there other digests for this same file from other inventories?
                            self.check_additional_digests(filepath, known_digests, prior_manifest_digests, 'E092a')
                            self.check_additional_digests(filepath, known_digests, prior_fixity_digests, 'E093a')
                        files_seen.discard(filepath)
        # Anything left in files_seen is not mentioned in the inventory
        if len(files_seen) > 0:
            self.log.error('E023a', where='root', extra_files=', '.join(sorted(files_seen)))

    def check_additional_digests(self, filepath, known_digests, additional_digests, error_code):
        """Check all the additional digests for filepath.

        This method is intended to be used both for manifest digests in prior versions and
        for fixity digests. The digests_seen dict is used to store any values calculated
        so that we don't recalculate digests that might appear multiple times. It is added to
        with any additional values calculated.

        Parameters:
            filepath - path of file in object (`v1/content/something` etc.)
            known_digests - dict of algorithm->digest that we have calculated
            additional_digests - dict: filepath -> algorithm -> digest -> [versions appears in]
            error_code - error code to log on mismatch (E092a for manifest, E093a for fixity)
        """
        if filepath in additional_digests:
            for digest_algorithm in additional_digests[filepath]:
                if digest_algorithm in known_digests:
                    # Don't recompute anything, just use it if we've seen it before
                    content_digest = known_digests[digest_algorithm]
                else:
                    content_digest = file_digest(filepath, digest_type=digest_algorithm, pyfs=self.obj_fs)
                    known_digests[digest_algorithm] = content_digest
                for digest in additional_digests[filepath][digest_algorithm]:
                    if content_digest != normalized_digest(digest, digest_type=digest_algorithm):
                        where = ','.join(additional_digests[filepath][digest_algorithm][digest])
                        self.log.error(error_code, where=where, digest_algorithm=digest_algorithm, digest=digest, content_path=filepath, content_digest=content_digest)

    def read_inventory_digest(self, inv_digest_file):
        """Read inventory digest from sidecar file.

        Raise exception if there is an error, else return digest.
        """
        with self.obj_fs.open(inv_digest_file, 'r') as fh:
            line = fh.readline()
            # we ignore any following lines, could raise exception
        m = re.match(r'''(\w+)\s+(\S+)\s*$''', line)
        if not m:
            raise Exception("Bad inventory digest file %s, wrong format" % (inv_digest_file))
        if m.group(2) != 'inventory.json':
            raise Exception("Bad inventory name in inventory digest file %s" % (inv_digest_file))
        return m.group(1)

if __name__ == "__main__":
    # import dill
    # import os
    isT=True
    # for l in os.listdir("D:/fse/python_test/repos/zimeon---ocfl-py/data_passk_platform1/62b45e23e0d4551b0392c90a/"):
    #     f = open("D:/fse/python_test/repos/zimeon---ocfl-py/data_passk_platform1/62b45e23e0d4551b0392c90a/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
        #object_class=dill.loads(content["input"]["args"][0]["bytes"])
    temp_class=Validator()
    filepath = ['repos/zimeon---ocfl-py/extra_fixtures/good-storage-roots/fedora-root/0b/1b/17/821f56ff-e476-484c-8da7-b30f416a1c2e_child-11',
                'repos/zimeon---ocfl-py/extra_fixtures/1.1/bad-objects/E003_two_declarations',
                'repos/zimeon---ocfl-py/extra_fixtures/1.1/good-objects/empty_fixity']
    for f in filepath:
        temp_class.validate(f)
    #temp_class.__dict__.update(object_class)
        args1= ['v1']
        res0 = temp_class.validate_version_inventories(args1)
        if res0 != ({},{}):
            isT=False
        # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
        #     isT=False
            #break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/zimeon---ocfl-py/ocfl/object_utils_find_path_type_passk_validte.py
# -*- coding: utf-8 -*-
"""Utility functions to support the OCFL Object library."""
import re
import sys

import fs
import fs.path

from _version import __version__
from namaste import find_namastes
from pyfs import open_fs


NORMALIZATIONS = ['uri', 'md5']  # Must match possibilities in map_filepaths()


class ObjectException(Exception):
    """Exception class for OCFL Object."""


def add_object_args(parser):
    """Add Object settings to argparse or argument group instance parser."""
    # Disk scanning
    parser.add_argument('--skip', action='append', default=['README.md', '.DS_Store'],
                        help='directories and files to ignore')
    parser.add_argument('--normalization', '--norm', default=None,
                        help='filepath normalization strategy (None, %s)' %
                        (', '.join(NORMALIZATIONS)))
    # Versioning strategy settings
    parser.add_argument('--no-forward-delta', action='store_true',
                        help='do not use forward deltas')
    parser.add_argument('--no-dedupe', '--no-dedup', action='store_true',
                        help='do not use deduplicate files within a version')
    # Validation settings
    parser.add_argument('--lax-digests', action='store_true',
                        help='allow use of any known digest')
    # Object files
    parser.add_argument('--objdir', '--obj',
                        help='read from or write to OCFL object directory objdir')


def add_shared_args(parser):
    """Add arguments to be shared by any ocfl-py scripts."""
    parser.add_argument('--verbose', '-v', action='store_true',
                        help="be more verbose")
    parser.add_argument('--version', action='store_true',
                        help='Show version number and exit')


def check_shared_args(args):
    """Check arguments set with add_shared_args."""
    if args.version:
        print("%s is part of ocfl-py version %s" % (fs.path.basename(sys.argv[0]), __version__))
        sys.exit(0)


def next_version(version):
    """Next version identifier following existing pattern.

    Must deal with both zero-prefixed and non-zero prefixed versions.
    """
    m = re.match(r'''v((\d)\d*)$''', version)
    if not m:
        raise ObjectException("Bad version '%s'" % version)
    next_n = int(m.group(1)) + 1
    if m.group(2) == '0':
        # Zero-padded version
        next_v = ('v0%0' + str(len(version) - 2) + 'd') % next_n
        if len(next_v) != len(version):
            raise ObjectException("Version number overflow for zero-padded version %d to %d" % (version, next_v))
        return next_v
    # Not zero-padded
    return 'v' + str(next_n)


def remove_first_directory(path):
    """Remove first directory from input path.

    The return value will not have a trailing parh separator, even if
    the input path does. Will return an empty string if the input path
    has just one path segment.
    """
    # FIXME - how to do this efficiently? Current code does complete
    # split and rejoins, excluding the first directory
    rpath = ''
    while True:
        (head, tail) = fs.path.split(path)
        if path in (head, tail):
            break
        path = head
        rpath = tail if rpath == '' else fs.path.join(tail, rpath)
    return rpath


def make_unused_filepath(filepath, used, separator='__'):
    """Find filepath with string appended that makes it disjoint from those in used."""
    n = 1
    while True:
        n += 1
        f = filepath + separator + str(n)
        if f not in used:
            return f


def find_path_type(path):
    """Return a string indicating the type of thing at the given path.

    Return values:
        'root' - looks like an OCFL Storage Root
        'object' - looks like an OCFL Object
        'file' - a file, might be an inventory
        other string explains error description

    Looks only at "0=*" Namaste files to determine the directory type.
    """
    try:
        pyfs = open_fs(path, create=False)
    except (fs.opener.errors.OpenerError, fs.errors.CreateFailed):
        # Failed to open path as a filesystem, try enclosing directory
        # in case path is a file
        (parent, filename) = fs.path.split(path)
        try:
            pyfs = open_fs(parent, create=False)
        except (fs.opener.errors.OpenerError, fs.errors.CreateFailed) as e:
            return "path cannot be opened, and nor can parent (" + str(e) + ")"
        # Can open parent, is filename a file there?
        try:
            info = pyfs.getinfo(filename)
        except fs.errors.ResourceNotFound:
            return "path does not exist"
        if info.is_dir:
            return "directory that could not be opened as a filesystem, this should not happen"  # pragma: no cover
        return 'file'
    namastes = find_namastes(0, pyfs=pyfs)
    if len(namastes) == 0:
        return "no 0= declaration file"
    # Look at the first 0= Namaste file that is of OCFL form to determine type, if there are
    # multiple declarations this will be caught later
    for namaste in namastes:
        m = re.match(r'''ocfl(_object)?_(\d+\.\d+)$''', namaste.tvalue)
        if m:
            return 'root' if m.group(1) is None else 'object'
    return "unrecognized 0= declaration file or files (first is %s)" % (namastes[0].tvalue)

if __name__ == "__main__":
    isT=True

    if not (find_path_type("repos/zimeon---ocfl-py/extra_fixtures/good-storage-roots/fedora-root")=="root"):
        isT=False
    if not (find_path_type("repos/zimeon---ocfl-py/README")=="file"):
        isT=False
    if not ("does not exist" in find_path_type("this_path_does_not_exist")):
        isT=False
    if not ("nor can parent" in find_path_type("still_nope/nope_doesnt_exist")):
        isT=False
    if not (find_path_type("repos/zimeon---ocfl-py/ocfl")== "no 0= declaration file"):
        isT=False
    if not (find_path_type("repos/zimeon---ocfl-py/extra_fixtures/misc/multiple_declarations") == 'root'):
        isT=False
    if not ("unrecognized" in find_path_type("repos/zimeon---ocfl-py/extra_fixtures/misc/unknown_declaration")):
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/mozilla---relman-auto-nag/auto_nag/bzcleaner_amend_bzparams_passk_validte.py
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this file,
# You can obtain one at http://mozilla.org/MPL/2.0/.

import argparse
import logging
import os
import sys
import time
from collections import defaultdict
from datetime import datetime
from typing import Dict, List
import sys
sys.path.append("/home/travis/builds/repos/mozilla---relman-auto-nag/")
from dateutil.relativedelta import relativedelta
from jinja2 import Environment, FileSystemLoader, Template
from bugbot import config
from bugbot import utils as lmdutils
from libmozdata.bugzilla import Bugzilla

from bugbot import db, logger, mail, utils
from bugbot.cache import Cache
from bugbot.nag_me import Nag


class TooManyChangesError(Exception):
    """Exception raised when the rule is trying to apply too many changes"""

    def __init__(self, bugs, changes, max_changes):
        message = f"The rule has been aborted because it was attempting to apply changes on {len(changes)} bugs. Max is {max_changes}."
        super().__init__(message)
        self.bugs = bugs
        self.changes = changes


class SilentBugzilla(Bugzilla):
    """Same as Bugzilla but using an account that does not trigger bugmail"""

    # TOKEN = config.get("Bugzilla", "nomail-token", "")


class BzCleaner(object):
    """
    Attributes:
        no_bugmail: If `True`, a token for an account that does not trigger
            bugmail will be used when performing `PUT` actions on Bugzilla.
        normal_changes_max: The maximum number of changes that could be made in
            a normal situation. If exceeded, the rule will fail.
    """

    no_bugmail: bool = False
    normal_changes_max: int = 50

    def __init__(self):
        super(BzCleaner, self).__init__()
        self._set_rule_name()
        self.apply_autofix = True
        self.has_autofix = False
        self.autofix_changes = {}
        self.quota_actions = defaultdict(list)
        self.no_manager = set()
        self.auto_needinfo = {}
        self.has_flags = False
        self.cache = Cache(self.name(), self.max_days_in_cache())
        self.test_mode = utils.get_config("common", "test", False)
        self.versions = None
        logger.info("Run rule {}".format(self.get_rule_path()))

    def _set_rule_name(self):
        module = sys.modules[self.__class__.__module__]
        base = os.path.dirname(__file__)
        rules = os.path.join(base, "rules")
        self.__rule_path__ = os.path.relpath(module.__file__, rules)
        name = os.path.basename(module.__file__)
        name = os.path.splitext(name)[0]
        self.__rule_name__ = name

    def init_versions(self):
        self.versions = utils.get_checked_versions()
        return bool(self.versions)

    def max_days_in_cache(self):
        """Get the max number of days the data must be kept in cache"""
        return self.get_config("max_days_in_cache", -1)

    def description(self):
        """Get the description for the help"""
        return ""

    def name(self):
        """Get the rule name"""
        return self.__rule_name__

    def get_rule_path(self):
        """Get the rule path"""
        return self.__rule_path__

    def needinfo_template_name(self):
        """Get the txt template filename"""
        return self.name() + "_needinfo.txt"

    def template(self):
        """Get the html template filename"""
        return self.name() + ".html"

    def subject(self):
        """Get the partial email subject"""
        return self.description()

    def get_email_subject(self, date):
        """Get the email subject with a date or not"""
        af = "[autofix]" if self.has_autofix else ""
        if date:
            return "[bugbot]{} {} for the {}".format(af, self.subject(), date)
        return "[bugbot]{} {}".format(af, self.subject())

    def ignore_date(self):
        """Should we ignore the date ?"""
        return False

    def must_run(self, date):
        """Check if the rule must run for this date"""
        days = self.get_config("must_run", None)
        if not days:
            return True
        weekday = date.weekday()
        week = utils.get_weekdays()
        for day in days:
            if week[day] == weekday:
                return True
        return False

    def has_enough_data(self):
        """Check if the rule has enough data to run"""
        if self.versions is None:
            # init_versions() has never been called
            return True
        return bool(self.versions)

    def filter_no_nag_keyword(self):
        """If True, then remove the bugs with [no-nag] in whiteboard from the bug list"""
        return True

    def add_no_manager(self, bugid):
        self.no_manager.add(str(bugid))

    def has_assignee(self):
        return False

    def has_needinfo(self):
        return False

    def get_mail_to_auto_ni(self, bug):
        return None

    def all_include_fields(self):
        return False

    def get_max_ni(self):
        return -1

    def get_max_actions(self):
        return -1

    def exclude_no_action_bugs(self):
        """
        If `True`, then remove bugs that have no actions from the email (e.g.,
        needinfo got ignored due to exceeding the limit). This is applied only
        when using the `add_prioritized_action()` method.

        Returning `False` could be useful if we want to list all actions the rule
        would do if it had no limits.
        """
        return True

    def ignore_meta(self):
        return False

    def columns(self):
        """The fields to get for the columns in email report"""
        return ["id", "summary"]

    def sort_columns(self):
        """Returns the key to sort columns"""
        return None

    def get_dates(self, date):
        """Get the dates for the bugzilla query (changedafter and changedbefore fields)"""
        date = lmdutils.get_date_ymd(date)
        lookup = self.get_config("days_lookup", 7)
        start_date = date - relativedelta(days=lookup)
        end_date = date + relativedelta(days=1)

        return start_date, end_date

    def get_extra_for_template(self):
        """Get extra data to put in the template"""
        return {}

    def get_extra_for_needinfo_template(self):
        """Get extra data to put in the needinfo template"""
        return {}

    def get_config(self, entry, default=None):
        return utils.get_config(self.name(), entry, default=default)

    def get_bz_params(self, date):
        """Get the Bugzilla parameters for the search query"""
        return {}

    def get_data(self):
        """Get the data structure to use in the bughandler"""
        return {}

    def get_summary(self, bug):
        return "..." if bug["groups"] else bug["summary"]

    def has_default_products(self):
        return True

    def has_product_component(self):
        return False

    def get_product_component(self):
        return self.prod_comp

    def has_access_to_sec_bugs(self):
        return self.get_config("sec", True)

    def handle_bug(self, bug, data):
        """Implement this function to get all the bugs from the query"""
        return bug

    def get_db_extra(self):
        """Get extra information required for db insertion"""
        return {
            bugid: ni_mail
            for ni_mail, v in self.auto_needinfo.items()
            for bugid in v["bugids"]
        }

    def get_auto_ni_skiplist(self):
        """Return a set of email addresses that should never be needinfoed"""
        return set(self.get_config("needinfo_skiplist", default=[]))

    def add_auto_ni(self, bugid, data):
        if not data:
            return False

        ni_mail = data["mail"]
        if ni_mail in self.get_auto_ni_skiplist() or utils.is_no_assignee(ni_mail):
            return False
        if ni_mail in self.auto_needinfo:
            max_ni = self.get_max_ni()
            info = self.auto_needinfo[ni_mail]
            if max_ni > 0 and len(info["bugids"]) >= max_ni:
                return False
            info["bugids"].append(str(bugid))
        else:
            self.auto_needinfo[ni_mail] = {
                "nickname": data["nickname"],
                "bugids": [str(bugid)],
            }
        return True

    def add_prioritized_action(self, bug, quota_name, needinfo=None, autofix=None):
        """
        - `quota_name` is the key used to apply the limits, e.g., triage owner, team, or component
        """
        assert needinfo or autofix

        # Avoid having more than one ni from our bot
        if needinfo and self.has_bot_set_ni(bug):
            needinfo = autofix = None

        action = {
            "bug": bug,
            "needinfo": needinfo,
            "autofix": autofix,
        }

        self.quota_actions[quota_name].append(action)

    def get_bug_sort_key(self, bug):
        return None

    def _populate_prioritized_actions(self, bugs):
        max_actions = self.get_max_actions()
        max_ni = self.get_max_ni()
        exclude_no_action_bugs = (
            len(self.quota_actions) > 0 and self.exclude_no_action_bugs()
        )
        bugs_with_action = set()

        for actions in self.quota_actions.values():
            if len(actions) > max_ni or len(actions) > max_actions:
                actions.sort(
                    key=lambda action: (
                        not action["needinfo"],
                        self.get_bug_sort_key(action["bug"]),
                    )
                )

            ni_count = 0
            actions_count = 0
            for action in actions:
                bugid = str(action["bug"]["id"])
                if max_actions > 0 and actions_count >= max_actions:
                    break

                if action["needinfo"]:
                    if max_ni > 0 and ni_count >= max_ni:
                        continue

                    ok = self.add_auto_ni(bugid, action["needinfo"])
                    if not ok:
                        # If we can't needinfo, we do not add the autofix
                        continue

                    if "extra" in action["needinfo"]:
                        self.extra_ni[bugid] = action["needinfo"]["extra"]

                    bugs_with_action.add(bugid)
                    ni_count += 1

                if action["autofix"]:
                    assert bugid not in self.autofix_changes
                    self.autofix_changes[bugid] = action["autofix"]
                    bugs_with_action.add(bugid)

                if action["autofix"] or action["needinfo"]:
                    actions_count += 1

        if exclude_no_action_bugs:
            bugs = {id: bug for id, bug in bugs.items() if id in bugs_with_action}

        return bugs

    def bughandler(self, bug, data):
        """bug handler for the Bugzilla query"""
        if bug["id"] in self.cache:
            return

        if self.handle_bug(bug, data) is None:
            return

        bugid = str(bug["id"])
        res = {"id": bugid}

        auto_ni = self.get_mail_to_auto_ni(bug)
        self.add_auto_ni(bugid, auto_ni)

        res["summary"] = self.get_summary(bug)

        if self.has_assignee():
            res["assignee"] = utils.get_name_from_user_detail(bug["assigned_to_detail"])

        if self.has_needinfo():
            s = set()
            for flag in utils.get_needinfo(bug):
                s.add(flag["requestee"])
            res["needinfos"] = sorted(s)

        if self.has_product_component():
            for k in ["product", "component"]:
                res[k] = bug[k]

        if isinstance(self, Nag):
            bug = self.set_people_to_nag(bug, res)
            if not bug:
                return

        if bugid in data:
            data[bugid].update(res)
        else:
            data[bugid] = res

    def get_products(self):
        return self.get_config("products") + self.get_config("additional_products", [])

    def amend_bzparams(self, params, bug_ids):
        """Amend the Bugzilla params"""
        if not self.all_include_fields():
            if "include_fields" in params:
                fields = params["include_fields"]
                if isinstance(fields, list):
                    if "id" not in fields:
                        fields.append("id")
                elif isinstance(fields, str):
                    if fields != "id":
                        params["include_fields"] = [fields, "id"]
                else:
                    params["include_fields"] = [fields, "id"]
            else:
                params["include_fields"] = ["id"]

            params["include_fields"] += ["summary", "groups"]

            if self.has_assignee() and "assigned_to" not in params["include_fields"]:
                params["include_fields"].append("assigned_to")

            if self.has_product_component():
                if "product" not in params["include_fields"]:
                    params["include_fields"].append("product")
                if "component" not in params["include_fields"]:
                    params["include_fields"].append("component")

            if self.has_needinfo() and "flags" not in params["include_fields"]:
                params["include_fields"].append("flags")

        if bug_ids:
            params["bug_id"] = bug_ids

        if self.filter_no_nag_keyword():
            n = utils.get_last_field_num(params)
            params.update(
                {
                    "f" + n: "status_whiteboard",
                    "o" + n: "notsubstring",
                    "v" + n: "[no-nag]",
                }
            )

        if self.ignore_meta():
            n = utils.get_last_field_num(params)
            params.update({"f" + n: "keywords", "o" + n: "nowords", "v" + n: "meta"})

        if self.has_default_products():
            params["product"] = self.get_products()

        if not self.has_access_to_sec_bugs():
            n = utils.get_last_field_num(params)
            params.update({"f" + n: "bug_group", "o" + n: "isempty"})

        self.has_flags = "flags" in params.get("include_fields", [])

    def get_bugs(self, date="today", bug_ids=[], chunk_size=None):
        """Get the bugs"""
        bugs = self.get_data()
        params = self.get_bz_params(date)
        self.amend_bzparams(params, bug_ids)
        self.query_url = utils.get_bz_search_url(params)

        if isinstance(self, Nag):
            self.query_params: dict = params

        old_CHUNK_SIZE = Bugzilla.BUGZILLA_CHUNK_SIZE
        try:
            if chunk_size:
                Bugzilla.BUGZILLA_CHUNK_SIZE = chunk_size

            Bugzilla(
                params,
                bughandler=self.bughandler,
                bugdata=bugs,
                timeout=self.get_config("bz_query_timeout"),
            ).get_data().wait()
        finally:
            Bugzilla.BUGZILLA_CHUNK_SIZE = old_CHUNK_SIZE

        self.get_comments(bugs)

        return bugs

    def commenthandler(self, bug, bugid, data):
        return

    def _commenthandler(self, bug, bugid, data):
        comments = bug["comments"]
        bugid = str(bugid)
        if self.has_last_comment_time():
            if comments:
                data[bugid]["last_comment"] = utils.get_human_lag(comments[-1]["time"])
            else:
                data[bugid]["last_comment"] = ""

        self.commenthandler(bug, bugid, data)

    def get_comments(self, bugs):
        """Get the bugs comments"""
        if self.has_last_comment_time():
            bugids = self.get_list_bugs(bugs)
            Bugzilla(
                bugids=bugids, commenthandler=self._commenthandler, commentdata=bugs
            ).get_data().wait()
        return bugs

    def has_last_comment_time(self):
        return False

    def get_list_bugs(self, bugs):
        return [x["id"] for x in bugs.values()]

    def get_documentation(self):
        return "For more information, please visit [BugBot documentation](https://wiki.mozilla.org/BugBot#{}).".format(
            self.get_rule_path().replace("/", ".2F")
        )

    def has_bot_set_ni(self, bug):
        if not self.has_flags:
            raise Exception
        return utils.has_bot_set_ni(bug)

    def set_needinfo(self):
        if not self.auto_needinfo:
            return {}

        template = self.get_needinfo_template()
        res = {}

        doc = self.get_documentation()

        for ni_mail, info in self.auto_needinfo.items():
            nick = info["nickname"]
            for bugid in info["bugids"]:
                data = {
                    "comment": {"body": ""},
                    "flags": [
                        {
                            "name": "needinfo",
                            "requestee": ni_mail,
                            "status": "?",
                            "new": "true",
                        }
                    ],
                }

                comment = None
                if nick:
                    comment = template.render(
                        nickname=nick,
                        extra=self.get_extra_for_needinfo_template(),
                        plural=utils.plural,
                        bugid=bugid,
                        documentation=doc,
                    )
                    comment = comment.strip() + "\n"
                    data["comment"]["body"] = comment

                if bugid not in res:
                    res[bugid] = data
                else:
                    res[bugid]["flags"] += data["flags"]
                    if comment:
                        res[bugid]["comment"]["body"] = comment

        return res

    def get_needinfo_template(self) -> Template:
        """Get a template to render needinfo comment body"""

        template_name = self.needinfo_template_name()
        assert bool(template_name)
        env = Environment(loader=FileSystemLoader("templates"))
        template = env.get_template(template_name)

        return template

    def has_individual_autofix(self, changes):
        # check if we have a dictionary with bug numbers as keys
        # return True if all the keys are bug number
        # (which means that each bug has its own autofix)
        return changes and all(
            isinstance(bugid, int) or bugid.isdigit() for bugid in changes
        )

    def get_autofix_change(self):
        """Get the change to do to autofix the bugs"""
        return self.autofix_changes

    def autofix(self, bugs):
        """Autofix the bugs according to what is returned by get_autofix_change"""
        ni_changes = self.set_needinfo()
        change = self.get_autofix_change()

        if not ni_changes and not change:
            return bugs

        self.has_autofix = True
        new_changes = {}
        if not self.has_individual_autofix(change):
            bugids = self.get_list_bugs(bugs)
            for bugid in bugids:
                mrg = utils.merge_bz_changes(change, ni_changes.get(bugid, {}))
                if mrg:
                    new_changes[bugid] = mrg
        else:
            change = {str(k): v for k, v in change.items()}
            bugids = set(change.keys()) | set(ni_changes.keys())
            for bugid in bugids:
                mrg = utils.merge_bz_changes(
                    change.get(bugid, {}), ni_changes.get(bugid, {})
                )
                if mrg:
                    new_changes[bugid] = mrg

        if not self.apply_autofix:
            self.autofix_changes = new_changes
            return bugs

        extra = self.get_db_extra()

        if self.is_limited and len(new_changes) > self.normal_changes_max:
            raise TooManyChangesError(bugs, new_changes, self.normal_changes_max)

        self.apply_changes_on_bugzilla(
            self.name(),
            new_changes,
            self.no_bugmail,
            self.dryrun or self.test_mode,
            extra,
        )

        return bugs

    @staticmethod
    def apply_changes_on_bugzilla(
        rule_name: str,
        new_changes: Dict[str, dict],
        no_bugmail: bool = False,
        is_dryrun: bool = True,
        db_extra: Dict[str, str] = None,
    ) -> None:
        """Apply changes on Bugzilla

        Args:
            rule_name: the name of the rule that is performing the changes.
            new_changes: the changes that will be performed. The dictionary key
                should be the bug ID.
            no_bugmail: If True, an account that doesn't trigger bugmail will be
                used to apply the changes.
            is_dryrun: If True, no changes will be applied. Instead, the
                proposed changes will be logged.
            db_extra: extra data to be passed to the DB. The dictionary key
                should be the bug ID.
        """
        if is_dryrun:
            for bugid, ch in new_changes.items():
                logger.info("The bugs: %s\n will be autofixed with:\n%s", bugid, ch)
            return None

        if db_extra is None:
            db_extra = {}

        max_retries = utils.get_config("common", "bugzilla_max_retries", 3)
        bugzilla_cls = SilentBugzilla if no_bugmail else Bugzilla

        for bugid, ch in new_changes.items():
            added = False
            for _ in range(max_retries):
                failures = bugzilla_cls([str(bugid)]).put(ch)
                if failures:
                    time.sleep(1)
                else:
                    added = True
                    db.BugChange.add(rule_name, bugid, extra=db_extra.get(bugid, ""))
                    break
            if not added:
                logger.error(
                    "%s: Cannot put data for bug %s (change => %s): %s",
                    rule_name,
                    bugid,
                    ch,
                    failures,
                )

    def terminate(self):
        """Called when everything is done"""
        return

    def organize(self, bugs):
        return utils.organize(bugs, self.columns(), key=self.sort_columns())

    def add_to_cache(self, bugs):
        """Add the bug keys to cache"""
        if isinstance(bugs, dict):
            self.cache.add(bugs.keys())
        else:
            self.cache.add(bugs)

    def get_email_data(self, date: str) -> List[dict]:
        bugs = self.get_bugs(date=date)
        bugs = self._populate_prioritized_actions(bugs)
        bugs = self.autofix(bugs)
        self.add_to_cache(bugs)
        if not bugs:
            return []

        return self.organize(bugs)

    def get_email(self, date: str, data: dict, preamble: str = ""):
        """Get title and body for the email"""
        assert data, "No data to send"

        extra = self.get_extra_for_template()
        env = Environment(loader=FileSystemLoader("templates"))
        template = env.get_template(self.template())
        message = template.render(
            date=date,
            data=data,
            extra=extra,
            str=str,
            enumerate=enumerate,
            plural=utils.plural,
            no_manager=self.no_manager,
            table_attrs=self.get_config("table_attrs"),
        )
        common = env.get_template("common.html")
        body = common.render(
            preamble=preamble,
            message=message,
            query_url=utils.shorten_long_bz_url(self.query_url),
        )
        return self.get_email_subject(date), body

    def _send_alert_about_too_many_changes(self, err: TooManyChangesError):
        """Send an alert email when there are too many changes to apply"""

        env = Environment(loader=FileSystemLoader("templates"))
        template = env.get_template("aborted_preamble.html")
        preamble = template.render(
            changes=err.changes.items(),
            changes_size=len(err.changes),
            normal_changes_max=self.normal_changes_max,
            rule_name=self.name(),
            https_proxy=os.environ.get("https_proxy"),
            enumerate=enumerate,
            table_attrs=self.get_config("table_attrs"),
        )

        login_info = utils.get_login_info()
        receivers = utils.get_config("common", "receivers")
        date = lmdutils.get_date("today")
        data = self.organize(err.bugs)
        title, body = self.get_email(date, data, preamble)
        title = f"Aborted: {title}"

        mail.send(
            login_info["ldap_username"],
            receivers,
            title,
            body,
            html=True,
            login=login_info,
            dryrun=self.dryrun,
        )

    def send_email(self, date="today"):
        """Send the email"""
        if date:
            date = lmdutils.get_date(date)
            d = lmdutils.get_date_ymd(date)
            if isinstance(self, Nag):
                self.nag_date: datetime = d

            if not self.must_run(d):
                return

        if not self.has_enough_data():
            logger.info("The rule {} hasn't enough data to run".format(self.name()))
            return

        login_info = utils.get_login_info()
        data = self.get_email_data(date)
        if data:
            title, body = self.get_email(date, data)
            receivers = utils.get_receivers(self.name())
            status = "Success"
            try:
                mail.send(
                    login_info["ldap_username"],
                    receivers,
                    title,
                    body,
                    html=True,
                    login=login_info,
                    dryrun=self.dryrun,
                )
            except Exception:
                logger.exception("Rule {}".format(self.name()))
                status = "Failure"

            db.Email.add(self.name(), receivers, "global", status)
            if isinstance(self, Nag):
                self.send_mails(title, dryrun=self.dryrun)
        else:
            name = self.name().upper()
            if date:
                logger.info("{}: No data for {}".format(name, date))
            else:
                logger.info("{}: No data".format(name))
            logger.info("Query: {}".format(self.query_url))

    def add_custom_arguments(self, parser):
        pass

    def parse_custom_arguments(self, args):
        pass

    def get_args_parser(self):
        """Get the arguments from the command line"""
        parser = argparse.ArgumentParser(description=self.description())
        parser.add_argument(
            "--production",
            dest="dryrun",
            action="store_false",
            help="If the flag is not passed, just do the query, and print emails to console without emailing anyone",
        )

        parser.add_argument(
            "--no-limit",
            dest="is_limited",
            action="store_false",
            default=True,
            help=f"If the flag is not passed, the rule will be limited to touch a maximum of {self.normal_changes_max} bugs",
        )

        if not self.ignore_date():
            parser.add_argument(
                "-D",
                "--date",
                dest="date",
                action="store",
                default="today",
                help="Date for the query",
            )

        self.add_custom_arguments(parser)

        return parser

    def run(self):
        """Run the rule"""
        args = self.get_args_parser().parse_args()
        self.parse_custom_arguments(args)
        date = "" if self.ignore_date() else args.date
        self.dryrun = args.dryrun
        self.is_limited = args.is_limited
        self.cache.set_dry_run(self.dryrun)

        if self.dryrun:
            logger.setLevel(logging.DEBUG)

        try:
            self.send_email(date=date)
            self.terminate()
            logger.info("Rule {} has finished.".format(self.get_rule_path()))
        except TooManyChangesError as err:
            self._send_alert_about_too_many_changes(err)
            logger.exception("Rule %s", self.name())
        except Exception:
            logger.exception("Rule {}".format(self.name()))

if __name__ == "__main__":
    import dill
    import os
    param={"include_fields":"files"}
    bug_id="3412342234"
    temp_class=BzCleaner()
    temp_class.amend_bzparams(param,bug_id)
    if param["bug_id"]!="3412342234" or param["include_fields"][0]!="files" or param["include_fields"][1]!="id" or param["include_fields"][2]!="summary" \
        or param["include_fields"][3] != "groups" or param["f1"]!="status_whiteboard":
        raise Exception("Result not True!!!")
----------------------------
/home/travis/builds/repos/witten---atticmatic/borgmatic/config/load_deep_merge_nodes_passk_validte.py
import logging
import os

import ruamel.yaml

logger = logging.getLogger(__name__)


class Yaml_with_loader_stream(ruamel.yaml.YAML):
    '''
    A derived class of ruamel.yaml.YAML that simply tacks the loaded stream (file object) onto the
    loader class so that it's available anywhere that's passed a loader (in this case,
    include_configuration() below).
    '''

    def get_constructor_parser(self, stream):
        constructor, parser = super(Yaml_with_loader_stream, self).get_constructor_parser(stream)
        constructor.loader.stream = stream
        return constructor, parser


def load_configuration(filename):
    '''
    Load the given configuration file and return its contents as a data structure of nested dicts
    and lists.

    Raise ruamel.yaml.error.YAMLError if something goes wrong parsing the YAML, or RecursionError
    if there are too many recursive includes.
    '''
    yaml = Yaml_with_loader_stream(typ='safe')
    yaml.Constructor = Include_constructor

    return yaml.load(open(filename))


def include_configuration(loader, filename_node):
    '''
    Load the given YAML filename (ignoring the given loader so we can use our own) and return its
    contents as a data structure of nested dicts and lists. If the filename is relative, probe for
    it within 1. the current working directory and 2. the directory containing the YAML file doing
    the including.

    Raise FileNotFoundError if an included file was not found.
    '''
    include_directories = [os.getcwd(), os.path.abspath(os.path.dirname(loader.stream.name))]
    include_filename = os.path.expanduser(filename_node.value)

    if not os.path.isabs(include_filename):
        candidate_filenames = [
            os.path.join(directory, include_filename) for directory in include_directories
        ]

        for candidate_filename in candidate_filenames:
            if os.path.exists(candidate_filename):
                include_filename = candidate_filename
                break
        else:
            raise FileNotFoundError(
                f'Could not find include {filename_node.value} at {" or ".join(candidate_filenames)}'
            )

    return load_configuration(include_filename)


DELETED_NODE = object()


def deep_merge_nodes(nodes):
    '''
    Given a nested borgmatic configuration data structure as a list of tuples in the form of:

        (
            ruamel.yaml.nodes.ScalarNode as a key,
            ruamel.yaml.nodes.MappingNode or other Node as a value,
        ),

    ... deep merge any node values corresponding to duplicate keys and return the result. If
    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last
    of the values wins.

    For instance, given node values of:

        [
            (
                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),
                MappingNode(tag='tag:yaml.org,2002:map', value=[
                    (
                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),
                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')
                    ),
                    (
                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),
                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')
                    ),
                ]),
            ),
            (
                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),
                MappingNode(tag='tag:yaml.org,2002:map', value=[
                    (
                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),
                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')
                    ),
                ]),
            ),
        ]

    ... the returned result would be:

        [
            (
                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),
                MappingNode(tag='tag:yaml.org,2002:map', value=[
                    (
                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),
                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')
                    ),
                    (
                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),
                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')
                    ),
                ]),
            ),
        ]

    The purpose of deep merging like this is to support, for instance, merging one borgmatic
    configuration file into another for reuse, such that a configuration section ("retention",
    etc.) does not completely replace the corresponding section in a merged file.
    '''
    # Map from original node key/value to the replacement merged node. DELETED_NODE as a replacement
    # node indications deletion.
    replaced_nodes = {}

    # To find nodes that require merging, compare each node with each other node.
    for a_key, a_value in nodes:
        for b_key, b_value in nodes:
            # If we've already considered one of the nodes for merging, skip it.
            if (a_key, a_value) in replaced_nodes or (b_key, b_value) in replaced_nodes:
                continue

            # If the keys match and the values are different, we need to merge these two A and B nodes.
            if a_key.tag == b_key.tag and a_key.value == b_key.value and a_value != b_value:
                # Since we're merging into the B node, consider the A node a duplicate and remove it.
                replaced_nodes[(a_key, a_value)] = DELETED_NODE

                # If we're dealing with MappingNodes, recurse and merge its values as well.
                if isinstance(b_value, ruamel.yaml.nodes.MappingNode):
                    replaced_nodes[(b_key, b_value)] = (
                        b_key,
                        ruamel.yaml.nodes.MappingNode(
                            tag=b_value.tag,
                            value=deep_merge_nodes(a_value.value + b_value.value),
                            start_mark=b_value.start_mark,
                            end_mark=b_value.end_mark,
                            flow_style=b_value.flow_style,
                            comment=b_value.comment,
                            anchor=b_value.anchor,
                        ),
                    )
                # If we're dealing with SequenceNodes, merge by appending one sequence to the other.
                elif isinstance(b_value, ruamel.yaml.nodes.SequenceNode):
                    replaced_nodes[(b_key, b_value)] = (
                        b_key,
                        ruamel.yaml.nodes.SequenceNode(
                            tag=b_value.tag,
                            value=a_value.value + b_value.value,
                            start_mark=b_value.start_mark,
                            end_mark=b_value.end_mark,
                            flow_style=b_value.flow_style,
                            comment=b_value.comment,
                            anchor=b_value.anchor,
                        ),
                    )

    return [
        replaced_nodes.get(node, node) for node in nodes if replaced_nodes.get(node) != DELETED_NODE
    ]


class Include_constructor(ruamel.yaml.SafeConstructor):
    '''
    A YAML "constructor" (a ruamel.yaml concept) that supports a custom "!include" tag for including
    separate YAML configuration files. Example syntax: `retention: !include common.yaml`
    '''

    def __init__(self, preserve_quotes=None, loader=None):
        super(Include_constructor, self).__init__(preserve_quotes, loader)
        self.add_constructor('!include', include_configuration)

    def flatten_mapping(self, node):
        '''
        Support the special case of deep merging included configuration into an existing mapping
        using the YAML '<<' merge key. Example syntax:

        ```
        retention:
            keep_daily: 1

        <<: !include common.yaml
        ```

        These includes are deep merged into the current configuration file. For instance, in this
        example, any "retention" options in common.yaml will get merged into the "retention" section
        in the example configuration file.
        '''
        representer = ruamel.yaml.representer.SafeRepresenter()

        for index, (key_node, value_node) in enumerate(node.value):
            if key_node.tag == u'tag:yaml.org,2002:merge' and value_node.tag == '!include':
                included_value = representer.represent_data(self.construct_object(value_node))
                node.value[index] = (key_node, included_value)

        super(Include_constructor, self).flatten_mapping(node)

        node.value = deep_merge_nodes(node.value)

if __name__ == "__main__":
    isT=True
    node_values = [
        (
            ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),
            ruamel.yaml.nodes.MappingNode(
                tag='tag:yaml.org,2002:map',
                value=[
                    (
                        ruamel.yaml.nodes.ScalarNode(
                            tag='tag:yaml.org,2002:str', value='keep_hourly'
                        ),
                        ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:int', value='24'),
                    ),
                    (
                        ruamel.yaml.nodes.ScalarNode(
                            tag='tag:yaml.org,2002:str', value='keep_daily'
                        ),
                        ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:int', value='7'),
                    ),
                ],
            ),
        ),
        (
            ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),
            ruamel.yaml.nodes.MappingNode(
                tag='tag:yaml.org,2002:map',
                value=[
                    (
                        ruamel.yaml.nodes.ScalarNode(
                            tag='tag:yaml.org,2002:str', value='keep_daily'
                        ),
                        ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:int', value='5'),
                    ),
                ],
            ),
        ),
    ]

    result = deep_merge_nodes(node_values)
    if not len(result) == 1:
        isT=False
    (section_key, section_value) = result[0]
    if not section_key.value == 'retention':
        isT=False
    options = section_value.value
    if not len(options) == 2 or \
    not options[0][0].value == 'keep_hourly' or \
    not options[0][1].value == '24' or \
    not options[1][0].value == 'keep_daily'or \
    not options[1][1].value == '5':
        isT=False

    node_values = [
        (
            ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),
            ruamel.yaml.nodes.MappingNode(
                tag='tag:yaml.org,2002:map',
                value=[
                    (
                        ruamel.yaml.nodes.ScalarNode(
                            tag='tag:yaml.org,2002:str', value='keep_hourly'
                        ),
                        ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:int', value='24'),
                    ),
                    (
                        ruamel.yaml.nodes.ScalarNode(
                            tag='tag:yaml.org,2002:str', value='keep_daily'
                        ),
                        ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:int', value='7'),
                    ),
                ],
            ),
        ),
        (
            ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),
            ruamel.yaml.nodes.MappingNode(
                tag='tag:yaml.org,2002:map',
                value=[
                    (
                        ruamel.yaml.nodes.ScalarNode(
                            tag='tag:yaml.org,2002:str', value='keep_minutely'
                        ),
                        ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:int', value='10'),
                    ),
                ],
            ),
        ),
    ]

    result = deep_merge_nodes(node_values)
    if not len(result) == 1:
        isT=False
    (section_key, section_value) = result[0]
    if not section_key.value == 'retention':
        isT=False
    options = section_value.value
    if not len(options) == 3 or \
    not options[0][0].value == 'keep_hourly' or \
    not options[0][1].value == '24' or \
    not options[1][0].value == 'keep_daily' or \
    not options[1][1].value == '7' or \
    not options[2][0].value == 'keep_minutely' or \
    not options[2][1].value == '10':
        isT=False

    node_values = [
        (
            ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:str', value='storage'),
            ruamel.yaml.nodes.MappingNode(
                tag='tag:yaml.org,2002:map',
                value=[
                    (
                        ruamel.yaml.nodes.ScalarNode(
                            tag='tag:yaml.org,2002:str', value='lock_wait'
                        ),
                        ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:int', value='5'),
                    ),
                    (
                        ruamel.yaml.nodes.ScalarNode(
                            tag='tag:yaml.org,2002:str', value='extra_borg_options'
                        ),
                        ruamel.yaml.nodes.MappingNode(
                            tag='tag:yaml.org,2002:map',
                            value=[
                                (
                                    ruamel.yaml.nodes.ScalarNode(
                                        tag='tag:yaml.org,2002:str', value='init'
                                    ),
                                    ruamel.yaml.nodes.ScalarNode(
                                        tag='tag:yaml.org,2002:str', value='--init-option'
                                    ),
                                ),
                            ],
                        ),
                    ),
                ],
            ),
        ),
        (
            ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:str', value='storage'),
            ruamel.yaml.nodes.MappingNode(
                tag='tag:yaml.org,2002:map',
                value=[
                    (
                        ruamel.yaml.nodes.ScalarNode(
                            tag='tag:yaml.org,2002:str', value='extra_borg_options'
                        ),
                        ruamel.yaml.nodes.MappingNode(
                            tag='tag:yaml.org,2002:map',
                            value=[
                                (
                                    ruamel.yaml.nodes.ScalarNode(
                                        tag='tag:yaml.org,2002:str', value='prune'
                                    ),
                                    ruamel.yaml.nodes.ScalarNode(
                                        tag='tag:yaml.org,2002:str', value='--prune-option'
                                    ),
                                ),
                            ],
                        ),
                    ),
                ],
            ),
        ),
    ]

    result = deep_merge_nodes(node_values)
    if not len(result) == 1:
        isT=False
    (section_key, section_value) = result[0]
    if not section_key.value == 'storage':
        isT=False
    options = section_value.value
    if not len(options) == 2 or \
    not options[0][0].value == 'lock_wait' or \
    not options[0][1].value == '5' or \
    not options[1][0].value == 'extra_borg_options':
        isT=False
    nested_options = options[1][1].value
    if not len(nested_options) == 2 or \
    not nested_options[0][0].value == 'init' or \
    not nested_options[0][1].value == '--init-option' or \
    not nested_options[1][0].value == 'prune' or \
    not nested_options[1][1].value == '--prune-option':
        isT=False

    node_values = [
        (
            ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:str', value='hooks'),
            ruamel.yaml.nodes.MappingNode(
                tag='tag:yaml.org,2002:map',
                value=[
                    (
                        ruamel.yaml.nodes.ScalarNode(
                            tag='tag:yaml.org,2002:str', value='before_backup'
                        ),
                        ruamel.yaml.nodes.SequenceNode(
                            tag='tag:yaml.org,2002:int', value=['echo 1', 'echo 2']
                        ),
                    ),
                ],
            ),
        ),
        (
            ruamel.yaml.nodes.ScalarNode(tag='tag:yaml.org,2002:str', value='hooks'),
            ruamel.yaml.nodes.MappingNode(
                tag='tag:yaml.org,2002:map',
                value=[
                    (
                        ruamel.yaml.nodes.ScalarNode(
                            tag='tag:yaml.org,2002:str', value='before_backup'
                        ),
                        ruamel.yaml.nodes.SequenceNode(
                            tag='tag:yaml.org,2002:int', value=['echo 3', 'echo 4']
                        ),
                    ),
                ],
            ),
        ),
    ]

    result = deep_merge_nodes(node_values)
    if not len(result) == 1:
        isT=False
    (section_key, section_value) = result[0]
    if not section_key.value == 'hooks':
        isT=False
    options = section_value.value
    if not len(options) == 1 or \
    not options[0][0].value == 'before_backup' or \
    not options[0][1].value == ['echo 1', 'echo 2', 'echo 3', 'echo 4']:
        isT=False

    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/witten---atticmatic/data_passk_platform/62b4567ed7d32e5b55cc83d9/"):
    #     f = open("/home/travis/builds/repos/witten---atticmatic/data_passk_platform/62b4567ed7d32e5b55cc83d9/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = deep_merge_nodes(args0)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/witten---atticmatic/borgmatic/commands/generate_config_parse_arguments_passk_validte.py
import sys
sys.path.append("/home/travis/builds/repos/witten---atticmatic/borgmatic")
sys.path.append("/home/travis/builds/repos/witten---atticmatic")
sys.path.append("/home/travis/builds/repos")

from argparse import ArgumentParser
from importlib import import_module

generate = import_module("witten---atticmatic.borgmatic.config.generate")
validate = import_module("witten---atticmatic.borgmatic.config.validate")

DEFAULT_DESTINATION_CONFIG_FILENAME = '/etc/borgmatic/config.yaml'


def parse_arguments(*arguments):
    '''
    Given command-line arguments with which this script was invoked, parse the arguments and return
    them as an ArgumentParser instance.
    '''
    parser = ArgumentParser(description='Generate a sample borgmatic YAML configuration file.')
    parser.add_argument(
        '-s',
        '--source',
        dest='source_filename',
        help='Optional YAML configuration file to merge into the generated configuration, useful for upgrading your configuration',
    )
    parser.add_argument(
        '-d',
        '--destination',
        dest='destination_filename',
        default=DEFAULT_DESTINATION_CONFIG_FILENAME,
        help='Destination YAML configuration file, default: {}'.format(
            DEFAULT_DESTINATION_CONFIG_FILENAME
        ),
    )
    parser.add_argument(
        '--overwrite',
        default=False,
        action='store_true',
        help='Whether to overwrite any existing destination file, defaults to false',
    )

    return parser.parse_args(arguments)


def main():  # pragma: no cover
    try:
        args = parse_arguments(*sys.argv[1:])

        generate.generate_sample_configuration(
            args.source_filename,
            args.destination_filename,
            validate.schema_filename(),
            overwrite=args.overwrite,
        )

        print('Generated a sample configuration file at {}.'.format(args.destination_filename))
        print()
        if args.source_filename:
            print(
                'Merged in the contents of configuration file at {}.'.format(args.source_filename)
            )
            print('To review the changes made, run:')
            print()
            print(
                '    diff --unified {} {}'.format(args.source_filename, args.destination_filename)
            )
            print()
        print('This includes all available configuration options with example values. The few')
        print('required options are indicated. Please edit the file to suit your needs.')
        print()
        print('If you ever need help: https://torsion.org/borgmatic/#issues')
    except (ValueError, OSError) as error:
        print(error, file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    isT=True
    parser = parse_arguments()

    if not parser.destination_filename == DEFAULT_DESTINATION_CONFIG_FILENAME:
        isT=False
    
    parser = parse_arguments('--destination', 'config.yaml')

    if not parser.destination_filename == 'config.yaml':
        isT=False
    
    parser = parse_arguments('--source', 'source.yaml', '--destination', 'config.yaml')

    if not parser.source_filename == 'source.yaml':
        isT=False
    
    parser = parse_arguments('--destination', 'config.yaml', '--overwrite')

    if not parser.overwrite:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/witten---atticmatic/data_passk_platform/62b4567ad7d32e5b55cc83af/"):
    #     f = open("/home/travis/builds/repos/witten---atticmatic/data_passk_platform/62b4567ad7d32e5b55cc83af/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     res0 = parse_arguments()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/witten---atticmatic/borgmatic/commands/completion_parser_flags_passk_validte.py
import sys
sys.path.append("..")
import arguments

UPGRADE_MESSAGE = '''
Your bash completions script is from a different version of borgmatic than is
currently installed. Please upgrade your script so your completions match the
command-line flags in your installed borgmatic! Try this to upgrade:

    sudo sh -c "borgmatic --bash-completion > $BASH_SOURCE"
    source $BASH_SOURCE
'''


def parser_flags(parser):
    '''
    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated
    string.
    '''
    return ' '.join(option for action in parser._actions for option in action.option_strings)


def bash_completion():
    '''
    Return a bash completion script for the borgmatic command. Produce this by introspecting
    borgmatic's command-line argument parsers.
    '''
    top_level_parser, subparsers = arguments.make_parsers()
    global_flags = parser_flags(top_level_parser)
    actions = ' '.join(subparsers.choices.keys())

    # Avert your eyes.
    return '\n'.join(
        (
            'check_version() {',
            '    local this_script="$(cat "$BASH_SOURCE" 2> /dev/null)"',
            '    local installed_script="$(borgmatic --bash-completion 2> /dev/null)"',
            '    if [ "$this_script" != "$installed_script" ] && [ "$installed_script" != "" ];'
            '        then cat << EOF\n%s\nEOF' % UPGRADE_MESSAGE,
            '    fi',
            '}',
            'complete_borgmatic() {',
        )
        + tuple(
            '''    if [[ " ${COMP_WORDS[*]} " =~ " %s " ]]; then
        COMPREPLY=($(compgen -W "%s %s %s" -- "${COMP_WORDS[COMP_CWORD]}"))
        return 0
    fi'''
            % (action, parser_flags(subparser), actions, global_flags)
            for action, subparser in subparsers.choices.items()
        )
        + (
            '    COMPREPLY=($(compgen -W "%s %s" -- "${COMP_WORDS[COMP_CWORD]}"))'
            % (actions, global_flags),
            '    (check_version &)',
            '}',
            '\ncomplete -o bashdefault -o default -F complete_borgmatic borgmatic',
        )
    )

if __name__ == "__main__":
    import dill
    import os
    import argparse
    from argparse import  ArgumentParser
    isT=True
    # for l in os.listdir("D:/fse/python_test/repos/witten---atticmatic/data_passk_platform1/62b45679d7d32e5b55cc83a9/"):
    #     f = open("D:/fse/python_test/repos/witten---atticmatic/data_passk_platform1/62b45679d7d32e5b55cc83a9/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     print(args0)
    args0, args1 = arguments.make_parsers()
        # args0 = ArgumentParser(prog='pytest',
        #                        usage=None,
        #                        description='\n            Simple, configuration-driven backup software for servers and workstations. If none of\n            the action options are given, then borgmatic defaults to: prune, compact, create, and\n            check.\n            ',
        #                        formatter_class= argparse.HelpFormatter,
        #                     conflict_handler='error',
        #                        add_help=True)

    target = '-h --help -c --config --excludes -n --dry-run -nc --no-color -v --verbosity --syslog-verbosity --log-file-verbosity --monitoring-verbosity --log-file --override --no-environment-interpolation --bash-completion --version'
    res0 = parser_flags(args0)
    if not res0 == target:
        isT=False

    subparser = args1.choices['init']
    res = parser_flags(subparser)
    target = '-e --encryption --source-repository --other-repo --copy-crypt-key --append-only --storage-quota --make-parent-dirs -h --help'
    if res != target:
        isT=False
        # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
        #     isT=False
        #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/witten---atticmatic/borgmatic/commands/arguments_parse_arguments_passk_validte.py
import collections
from argparse import Action, ArgumentParser
import sys
sys.path.append("/home/travis/builds/repos/witten---atticmatic/borgmatic")
sys.path.append("/home/travis/builds/repos/witten---atticmatic")
sys.path.append("/home/travis/builds/repos")

from importlib import import_module
collect = import_module("witten---atticmatic.borgmatic.config.collect")

SUBPARSER_ALIASES = {
    'rcreate': ['init', '-I'],
    'prune': ['-p'],
    'compact': [],
    'create': ['-C'],
    'check': ['-k'],
    'extract': ['-x'],
    'export-tar': [],
    'mount': ['-m'],
    'umount': ['-u'],
    'restore': ['-r'],
    'rlist': [],
    'list': ['-l'],
    'rinfo': [],
    'info': ['-i'],
    'transfer': [],
    'borg': [],
}


def parse_subparser_arguments(unparsed_arguments, subparsers):
    '''
    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser
    instance, give each requested action's subparser a shot at parsing all arguments. This allows
    common arguments like "--repository" to be shared across multiple subparsers.

    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of
    arguments, a list of remaining arguments not claimed by any subparser).
    '''
    arguments = collections.OrderedDict()
    remaining_arguments = list(unparsed_arguments)
    alias_to_subparser_name = {
        alias: subparser_name
        for subparser_name, aliases in SUBPARSER_ALIASES.items()
        for alias in aliases
    }

    # If the "borg" action is used, skip all other subparsers. This avoids confusion like
    # "borg list" triggering borgmatic's own list action.
    if 'borg' in unparsed_arguments:
        subparsers = {'borg': subparsers['borg']}

    for subparser_name, subparser in subparsers.items():
        if subparser_name not in remaining_arguments:
            continue

        canonical_name = alias_to_subparser_name.get(subparser_name, subparser_name)

        # If a parsed value happens to be the same as the name of a subparser, remove it from the
        # remaining arguments. This prevents, for instance, "check --only extract" from triggering
        # the "extract" subparser.
        parsed, unused_remaining = subparser.parse_known_args(unparsed_arguments)
        for value in vars(parsed).values():
            if isinstance(value, str):
                if value in subparsers:
                    remaining_arguments.remove(value)
            elif isinstance(value, list):
                for item in value:
                    if item in subparsers:
                        remaining_arguments.remove(item)

        arguments[canonical_name] = parsed

    # If no actions are explicitly requested, assume defaults: prune, compact, create, and check.
    if not arguments and '--help' not in unparsed_arguments and '-h' not in unparsed_arguments:
        for subparser_name in ('prune', 'compact', 'create', 'check'):
            subparser = subparsers[subparser_name]
            parsed, unused_remaining = subparser.parse_known_args(unparsed_arguments)
            arguments[subparser_name] = parsed

    remaining_arguments = list(unparsed_arguments)

    # Now ask each subparser, one by one, to greedily consume arguments.
    for subparser_name, subparser in subparsers.items():
        if subparser_name not in arguments.keys():
            continue

        subparser = subparsers[subparser_name]
        unused_parsed, remaining_arguments = subparser.parse_known_args(remaining_arguments)

    # Special case: If "borg" is present in the arguments, consume all arguments after (+1) the
    # "borg" action.
    if 'borg' in arguments:
        borg_options_index = remaining_arguments.index('borg') + 1
        arguments['borg'].options = remaining_arguments[borg_options_index:]
        remaining_arguments = remaining_arguments[:borg_options_index]

    # Remove the subparser names themselves.
    for subparser_name, subparser in subparsers.items():
        if subparser_name in remaining_arguments:
            remaining_arguments.remove(subparser_name)

    return (arguments, remaining_arguments)


class Extend_action(Action):
    '''
    An argparse action to support Python 3.8's "extend" action in older versions of Python.
    '''

    def __call__(self, parser, namespace, values, option_string=None):
        items = getattr(namespace, self.dest, None)

        if items:
            items.extend(values)
        else:
            setattr(namespace, self.dest, list(values))


def make_parsers():
    '''
    Build a top-level parser and its subparsers and return them as a tuple.
    '''
    config_paths = collect.get_default_config_paths(expand_home=True)
    unexpanded_config_paths = collect.get_default_config_paths(expand_home=False)

    global_parser = ArgumentParser(add_help=False)
    global_parser.register('action', 'extend', Extend_action)
    global_group = global_parser.add_argument_group('global arguments')

    global_group.add_argument(
        '-c',
        '--config',
        nargs='*',
        dest='config_paths',
        default=config_paths,
        help='Configuration filenames or directories, defaults to: {}'.format(
            ' '.join(unexpanded_config_paths)
        ),
    )
    global_group.add_argument(
        '--excludes',
        dest='excludes_filename',
        help='Deprecated in favor of exclude_patterns within configuration',
    )
    global_group.add_argument(
        '-n',
        '--dry-run',
        dest='dry_run',
        action='store_true',
        help='Go through the motions, but do not actually write to any repositories',
    )
    global_group.add_argument(
        '-nc', '--no-color', dest='no_color', action='store_true', help='Disable colored output'
    )
    global_group.add_argument(
        '-v',
        '--verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Display verbose progress to the console (from only errors to very verbose: -1, 0, 1, or 2)',
    )
    global_group.add_argument(
        '--syslog-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to syslog (from only errors to very verbose: -1, 0, 1, or 2). Ignored when console is interactive or --log-file is given',
    )
    global_group.add_argument(
        '--log-file-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to log file (from only errors to very verbose: -1, 0, 1, or 2). Only used when --log-file is given',
    )
    global_group.add_argument(
        '--monitoring-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to monitoring integrations that support logging (from only errors to very verbose: -1, 0, 1, or 2)',
    )
    global_group.add_argument(
        '--log-file',
        type=str,
        default=None,
        help='Write log messages to this file instead of syslog',
    )
    global_group.add_argument(
        '--override',
        metavar='SECTION.OPTION=VALUE',
        nargs='+',
        dest='overrides',
        action='extend',
        help='One or more configuration file options to override with specified values',
    )
    global_group.add_argument(
        '--no-environment-interpolation',
        dest='resolve_env',
        action='store_false',
        help='Do not resolve environment variables in configuration file',
    )
    global_group.add_argument(
        '--bash-completion',
        default=False,
        action='store_true',
        help='Show bash completion script and exit',
    )
    global_group.add_argument(
        '--version',
        dest='version',
        default=False,
        action='store_true',
        help='Display installed version number of borgmatic and exit',
    )

    top_level_parser = ArgumentParser(
        description='''
            Simple, configuration-driven backup software for servers and workstations. If none of
            the action options are given, then borgmatic defaults to: prune, compact, create, and
            check.
            ''',
        parents=[global_parser],
    )

    subparsers = top_level_parser.add_subparsers(
        title='actions',
        metavar='',
        help='Specify zero or more actions. Defaults to prune, compact, create, and check. Use --help with action for details:',
    )
    rcreate_parser = subparsers.add_parser(
        'rcreate',
        aliases=SUBPARSER_ALIASES['rcreate'],
        help='Create a new, empty Borg repository',
        description='Create a new, empty Borg repository',
        add_help=False,
    )
    rcreate_group = rcreate_parser.add_argument_group('rcreate arguments')
    rcreate_group.add_argument(
        '-e',
        '--encryption',
        dest='encryption_mode',
        help='Borg repository encryption mode',
        required=True,
    )
    rcreate_group.add_argument(
        '--source-repository',
        '--other-repo',
        metavar='KEY_REPOSITORY',
        help='Path to an existing Borg repository whose key material should be reused (Borg 2.x+ only)',
    )
    rcreate_group.add_argument(
        '--copy-crypt-key',
        action='store_true',
        help='Copy the crypt key used for authenticated encryption from the source repository, defaults to a new random key (Borg 2.x+ only)',
    )
    rcreate_group.add_argument(
        '--append-only', action='store_true', help='Create an append-only repository',
    )
    rcreate_group.add_argument(
        '--storage-quota', help='Create a repository with a fixed storage quota',
    )
    rcreate_group.add_argument(
        '--make-parent-dirs',
        action='store_true',
        help='Create any missing parent directories of the repository directory',
    )
    rcreate_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    transfer_parser = subparsers.add_parser(
        'transfer',
        aliases=SUBPARSER_ALIASES['transfer'],
        help='Transfer archives from one repository to another, optionally upgrading the transferred data (Borg 2.0+ only)',
        description='Transfer archives from one repository to another, optionally upgrading the transferred data (Borg 2.0+ only)',
        add_help=False,
    )
    transfer_group = transfer_parser.add_argument_group('transfer arguments')
    transfer_group.add_argument(
        '--repository',
        help='Path of existing destination repository to transfer archives to, defaults to the configured repository if there is only one',
    )
    transfer_group.add_argument(
        '--source-repository',
        help='Path of existing source repository to transfer archives from',
        required=True,
    )
    transfer_group.add_argument(
        '--archive',
        help='Name of single archive to transfer (or "latest"), defaults to transferring all archives',
    )
    transfer_group.add_argument(
        '--upgrader',
        help='Upgrader type used to convert the transfered data, e.g. "From12To20" to upgrade data from Borg 1.2 to 2.0 format, defaults to no conversion',
    )
    transfer_group.add_argument(
        '-a',
        '--glob-archives',
        metavar='GLOB',
        help='Only transfer archives with names matching this glob',
    )
    transfer_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    transfer_group.add_argument(
        '--first',
        metavar='N',
        help='Only transfer first N archives after other filters are applied',
    )
    transfer_group.add_argument(
        '--last', metavar='N', help='Only transfer last N archives after other filters are applied'
    )
    transfer_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    prune_parser = subparsers.add_parser(
        'prune',
        aliases=SUBPARSER_ALIASES['prune'],
        help='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
        description='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
        add_help=False,
    )
    prune_group = prune_parser.add_argument_group('prune arguments')
    prune_group.add_argument(
        '--stats',
        dest='stats',
        default=False,
        action='store_true',
        help='Display statistics of archive',
    )
    prune_group.add_argument(
        '--list', dest='list_archives', action='store_true', help='List archives kept/pruned'
    )
    prune_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    compact_parser = subparsers.add_parser(
        'compact',
        aliases=SUBPARSER_ALIASES['compact'],
        help='Compact segments to free space (Borg 1.2+ only)',
        description='Compact segments to free space (Borg 1.2+ only)',
        add_help=False,
    )
    compact_group = compact_parser.add_argument_group('compact arguments')
    compact_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress as each segment is compacted',
    )
    compact_group.add_argument(
        '--cleanup-commits',
        dest='cleanup_commits',
        default=False,
        action='store_true',
        help='Cleanup commit-only 17-byte segment files left behind by Borg 1.1 (flag in Borg 1.2 only)',
    )
    compact_group.add_argument(
        '--threshold',
        type=int,
        dest='threshold',
        help='Minimum saved space percentage threshold for compacting a segment, defaults to 10',
    )
    compact_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    create_parser = subparsers.add_parser(
        'create',
        aliases=SUBPARSER_ALIASES['create'],
        help='Create an archive (actually perform a backup)',
        description='Create an archive (actually perform a backup)',
        add_help=False,
    )
    create_group = create_parser.add_argument_group('create arguments')
    create_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is backed up',
    )
    create_group.add_argument(
        '--stats',
        dest='stats',
        default=False,
        action='store_true',
        help='Display statistics of archive',
    )
    create_group.add_argument(
        '--list', '--files', dest='list_files', action='store_true', help='Show per-file details'
    )
    create_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    create_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    check_parser = subparsers.add_parser(
        'check',
        aliases=SUBPARSER_ALIASES['check'],
        help='Check archives for consistency',
        description='Check archives for consistency',
        add_help=False,
    )
    check_group = check_parser.add_argument_group('check arguments')
    check_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is checked',
    )
    check_group.add_argument(
        '--repair',
        dest='repair',
        default=False,
        action='store_true',
        help='Attempt to repair any inconsistencies found (for interactive use)',
    )
    check_group.add_argument(
        '--only',
        metavar='CHECK',
        choices=('repository', 'archives', 'data', 'extract'),
        dest='only',
        action='append',
        help='Run a particular consistency check (repository, archives, data, or extract) instead of configured checks (subject to configured frequency, can specify flag multiple times)',
    )
    check_group.add_argument(
        '--force',
        default=False,
        action='store_true',
        help='Ignore configured check frequencies and run checks unconditionally',
    )
    check_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    extract_parser = subparsers.add_parser(
        'extract',
        aliases=SUBPARSER_ALIASES['extract'],
        help='Extract files from a named archive to the current directory',
        description='Extract a named archive to the current directory',
        add_help=False,
    )
    extract_group = extract_parser.add_argument_group('extract arguments')
    extract_group.add_argument(
        '--repository',
        help='Path of repository to extract, defaults to the configured repository if there is only one',
    )
    extract_group.add_argument(
        '--archive', help='Name of archive to extract (or "latest")', required=True
    )
    extract_group.add_argument(
        '--path',
        '--restore-path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to extract from archive, defaults to the entire archive',
    )
    extract_group.add_argument(
        '--destination',
        metavar='PATH',
        dest='destination',
        help='Directory to extract files into, defaults to the current directory',
    )
    extract_group.add_argument(
        '--strip-components',
        type=int,
        metavar='NUMBER',
        dest='strip_components',
        help='Number of leading path components to remove from each extracted path. Skip paths with fewer elements',
    )
    extract_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is extracted',
    )
    extract_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    export_tar_parser = subparsers.add_parser(
        'export-tar',
        aliases=SUBPARSER_ALIASES['export-tar'],
        help='Export an archive to a tar-formatted file or stream',
        description='Export an archive to a tar-formatted file or stream',
        add_help=False,
    )
    export_tar_group = export_tar_parser.add_argument_group('export-tar arguments')
    export_tar_group.add_argument(
        '--repository',
        help='Path of repository to export from, defaults to the configured repository if there is only one',
    )
    export_tar_group.add_argument(
        '--archive', help='Name of archive to export (or "latest")', required=True
    )
    export_tar_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to export from archive, defaults to the entire archive',
    )
    export_tar_group.add_argument(
        '--destination',
        metavar='PATH',
        dest='destination',
        help='Path to destination export tar file, or "-" for stdout (but be careful about dirtying output with --verbosity or --list)',
        required=True,
    )
    export_tar_group.add_argument(
        '--tar-filter', help='Name of filter program to pipe data through'
    )
    export_tar_group.add_argument(
        '--list', '--files', dest='list_files', action='store_true', help='Show per-file details'
    )
    export_tar_group.add_argument(
        '--strip-components',
        type=int,
        metavar='NUMBER',
        dest='strip_components',
        help='Number of leading path components to remove from each exported path. Skip paths with fewer elements',
    )
    export_tar_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    mount_parser = subparsers.add_parser(
        'mount',
        aliases=SUBPARSER_ALIASES['mount'],
        help='Mount files from a named archive as a FUSE filesystem',
        description='Mount a named archive as a FUSE filesystem',
        add_help=False,
    )
    mount_group = mount_parser.add_argument_group('mount arguments')
    mount_group.add_argument(
        '--repository',
        help='Path of repository to use, defaults to the configured repository if there is only one',
    )
    mount_group.add_argument('--archive', help='Name of archive to mount (or "latest")')
    mount_group.add_argument(
        '--mount-point',
        metavar='PATH',
        dest='mount_point',
        help='Path where filesystem is to be mounted',
        required=True,
    )
    mount_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to mount from archive, defaults to the entire archive',
    )
    mount_group.add_argument(
        '--foreground',
        dest='foreground',
        default=False,
        action='store_true',
        help='Stay in foreground until ctrl-C is pressed',
    )
    mount_group.add_argument('--options', dest='options', help='Extra Borg mount options')
    mount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    umount_parser = subparsers.add_parser(
        'umount',
        aliases=SUBPARSER_ALIASES['umount'],
        help='Unmount a FUSE filesystem that was mounted with "borgmatic mount"',
        description='Unmount a mounted FUSE filesystem',
        add_help=False,
    )
    umount_group = umount_parser.add_argument_group('umount arguments')
    umount_group.add_argument(
        '--mount-point',
        metavar='PATH',
        dest='mount_point',
        help='Path of filesystem to unmount',
        required=True,
    )
    umount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    restore_parser = subparsers.add_parser(
        'restore',
        aliases=SUBPARSER_ALIASES['restore'],
        help='Restore database dumps from a named archive',
        description='Restore database dumps from a named archive. (To extract files instead, use "borgmatic extract".)',
        add_help=False,
    )
    restore_group = restore_parser.add_argument_group('restore arguments')
    restore_group.add_argument(
        '--repository',
        help='Path of repository to restore from, defaults to the configured repository if there is only one',
    )
    restore_group.add_argument(
        '--archive', help='Name of archive to restore from (or "latest")', required=True
    )
    restore_group.add_argument(
        '--database',
        metavar='NAME',
        nargs='+',
        dest='databases',
        help='Names of databases to restore from archive, defaults to all databases. Note that any databases to restore must be defined in borgmatic\'s configuration',
    )
    restore_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    rlist_parser = subparsers.add_parser(
        'rlist',
        aliases=SUBPARSER_ALIASES['rlist'],
        help='List repository',
        description='List the archives in a repository',
        add_help=False,
    )
    rlist_group = rlist_parser.add_argument_group('rlist arguments')
    rlist_group.add_argument(
        '--repository', help='Path of repository to list, defaults to the configured repositories',
    )
    rlist_group.add_argument(
        '--short', default=False, action='store_true', help='Output only archive names'
    )
    rlist_group.add_argument('--format', help='Format for archive listing')
    rlist_group.add_argument(
        '--json', default=False, action='store_true', help='Output results as JSON'
    )
    rlist_group.add_argument(
        '-P', '--prefix', help='Only list archive names starting with this prefix'
    )
    rlist_group.add_argument(
        '-a', '--glob-archives', metavar='GLOB', help='Only list archive names matching this glob'
    )
    rlist_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    rlist_group.add_argument(
        '--first', metavar='N', help='List first N archives after other filters are applied'
    )
    rlist_group.add_argument(
        '--last', metavar='N', help='List last N archives after other filters are applied'
    )
    rlist_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    list_parser = subparsers.add_parser(
        'list',
        aliases=SUBPARSER_ALIASES['list'],
        help='List archive',
        description='List the files in an archive or search for a file across archives',
        add_help=False,
    )
    list_group = list_parser.add_argument_group('list arguments')
    list_group.add_argument(
        '--repository',
        help='Path of repository containing archive to list, defaults to the configured repositories',
    )
    list_group.add_argument('--archive', help='Name of the archive to list (or "latest")')
    list_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths or patterns to list from a single selected archive (via "--archive"), defaults to listing the entire archive',
    )
    list_group.add_argument(
        '--find',
        metavar='PATH',
        nargs='+',
        dest='find_paths',
        help='Partial paths or patterns to search for and list across multiple archives',
    )
    list_group.add_argument(
        '--short', default=False, action='store_true', help='Output only path names'
    )
    list_group.add_argument('--format', help='Format for file listing')
    list_group.add_argument(
        '--json', default=False, action='store_true', help='Output results as JSON'
    )
    list_group.add_argument(
        '-P', '--prefix', help='Only list archive names starting with this prefix'
    )
    list_group.add_argument(
        '-a', '--glob-archives', metavar='GLOB', help='Only list archive names matching this glob'
    )
    list_group.add_argument(
        '--successful',
        default=True,
        action='store_true',
        help='Deprecated; no effect. Newer versions of Borg shows successful (non-checkpoint) archives by default.',
    )
    list_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    list_group.add_argument(
        '--first', metavar='N', help='List first N archives after other filters are applied'
    )
    list_group.add_argument(
        '--last', metavar='N', help='List last N archives after other filters are applied'
    )
    list_group.add_argument(
        '-e', '--exclude', metavar='PATTERN', help='Exclude paths matching the pattern'
    )
    list_group.add_argument(
        '--exclude-from', metavar='FILENAME', help='Exclude paths from exclude file, one per line'
    )
    list_group.add_argument('--pattern', help='Include or exclude paths matching a pattern')
    list_group.add_argument(
        '--patterns-from',
        metavar='FILENAME',
        help='Include or exclude paths matching patterns from pattern file, one per line',
    )
    list_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    rinfo_parser = subparsers.add_parser(
        'rinfo',
        aliases=SUBPARSER_ALIASES['rinfo'],
        help='Show repository summary information such as disk space used',
        description='Show repository summary information such as disk space used',
        add_help=False,
    )
    rinfo_group = rinfo_parser.add_argument_group('rinfo arguments')
    rinfo_group.add_argument(
        '--repository',
        help='Path of repository to show info for, defaults to the configured repository if there is only one',
    )
    rinfo_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    rinfo_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    info_parser = subparsers.add_parser(
        'info',
        aliases=SUBPARSER_ALIASES['info'],
        help='Show archive summary information such as disk space used',
        description='Show archive summary information such as disk space used',
        add_help=False,
    )
    info_group = info_parser.add_argument_group('info arguments')
    info_group.add_argument(
        '--repository',
        help='Path of repository containing archive to show info for, defaults to the configured repository if there is only one',
    )
    info_group.add_argument('--archive', help='Name of archive to show info for (or "latest")')
    info_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    info_group.add_argument(
        '-P', '--prefix', help='Only show info for archive names starting with this prefix'
    )
    info_group.add_argument(
        '-a',
        '--glob-archives',
        metavar='GLOB',
        help='Only show info for archive names matching this glob',
    )
    info_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    info_group.add_argument(
        '--first',
        metavar='N',
        help='Show info for first N archives after other filters are applied',
    )
    info_group.add_argument(
        '--last', metavar='N', help='Show info for last N archives after other filters are applied'
    )
    info_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    borg_parser = subparsers.add_parser(
        'borg',
        aliases=SUBPARSER_ALIASES['borg'],
        help='Run an arbitrary Borg command',
        description='Run an arbitrary Borg command based on borgmatic\'s configuration',
        add_help=False,
    )
    borg_group = borg_parser.add_argument_group('borg arguments')
    borg_group.add_argument(
        '--repository',
        help='Path of repository to pass to Borg, defaults to the configured repositories',
    )
    borg_group.add_argument('--archive', help='Name of archive to pass to Borg (or "latest")')
    borg_group.add_argument(
        '--',
        metavar='OPTION',
        dest='options',
        nargs='+',
        help='Options to pass to Borg, command first ("create", "list", etc). "--" is optional. To specify the repository or the archive, you must use --repository or --archive instead of providing them here.',
    )
    borg_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    return top_level_parser, subparsers


def parse_arguments(*unparsed_arguments):
    '''
    Given command-line arguments with which this script was invoked, parse the arguments and return
    them as a dict mapping from subparser name (or "global") to an argparse.Namespace instance.
    '''
    top_level_parser, subparsers = make_parsers()

    arguments, remaining_arguments = parse_subparser_arguments(
        unparsed_arguments, subparsers.choices
    )
    arguments['global'] = top_level_parser.parse_args(remaining_arguments)

    if arguments['global'].excludes_filename:
        raise ValueError(
            'The --excludes flag has been replaced with exclude_patterns in configuration.'
        )

    if (
        ('list' in arguments and 'rinfo' in arguments and arguments['list'].json)
        or ('list' in arguments and 'info' in arguments and arguments['list'].json)
        or ('rinfo' in arguments and 'info' in arguments and arguments['rinfo'].json)
    ):
        raise ValueError('With the --json flag, multiple actions cannot be used together.')

    if (
        'transfer' in arguments
        and arguments['transfer'].archive
        and arguments['transfer'].glob_archives
    ):
        raise ValueError(
            'With the transfer action, only one of --archive and --glob-archives flags can be used.'
        )

    if 'info' in arguments and (
        (arguments['info'].archive and arguments['info'].prefix)
        or (arguments['info'].archive and arguments['info'].glob_archives)
        or (arguments['info'].prefix and arguments['info'].glob_archives)
    ):
        raise ValueError(
            'With the info action, only one of --archive, --prefix, or --glob-archives flags can be used.'
        )

    return arguments

if __name__ == "__main__":
    from flexmock import flexmock

    isT=True

    config_paths = ['default']
    flexmock(collect).should_receive('get_default_config_paths').and_return(config_paths)

    arguments = parse_arguments()

    global_arguments = arguments['global']
    if not global_arguments.config_paths == config_paths or \
    not global_arguments.excludes_filename is None or \
    not global_arguments.verbosity == 0 or \
    not global_arguments.syslog_verbosity == 0 or \
    not global_arguments.log_file_verbosity == 0:
        #print(1)
        isT=False



    flexmock(collect).should_receive('get_default_config_paths').and_return(['default'])

    arguments = parse_arguments('--config', 'myconfig', 'otherconfig')

    global_arguments = arguments['global']
    if not global_arguments.config_paths == ['myconfig', 'otherconfig'] or \
    not global_arguments.verbosity == 0 or \
    not global_arguments.syslog_verbosity == 0 or \
    not global_arguments.log_file_verbosity == 0:
        #print(2)
        isT=False



    config_paths = ['default']
    flexmock(collect).should_receive('get_default_config_paths').and_return(config_paths)

    arguments = parse_arguments('--verbosity', '1')

    global_arguments = arguments['global']
    if not global_arguments.config_paths == config_paths or \
    not global_arguments.excludes_filename is None or \
    not global_arguments.verbosity == 1 or \
    not global_arguments.syslog_verbosity == 0 or \
    not global_arguments.log_file_verbosity == 0:
        #print(3)
        isT=False



    config_paths = ['default']
    flexmock(collect).should_receive('get_default_config_paths').and_return(config_paths)

    arguments = parse_arguments('--syslog-verbosity', '2')

    global_arguments = arguments['global']
    if not global_arguments.config_paths == config_paths or \
    not global_arguments.excludes_filename is None or \
    not global_arguments.verbosity == 0 or \
    not global_arguments.syslog_verbosity == 2:
        #print(4)
        isT=False



    config_paths = ['default']
    flexmock(collect).should_receive('get_default_config_paths').and_return(config_paths)

    arguments = parse_arguments('--log-file-verbosity', '-1')

    global_arguments = arguments['global']
    if not global_arguments.config_paths == config_paths or \
    not global_arguments.excludes_filename is None or \
    not global_arguments.verbosity == 0 or \
    not global_arguments.syslog_verbosity == 0 or \
    not global_arguments.log_file_verbosity == -1:
        #print(5)
        isT=False



    flexmock(collect).should_receive('get_default_config_paths').and_return(['default'])

    arguments = parse_arguments('--override', 'foo.bar=baz')

    global_arguments = arguments['global']
    if not global_arguments.overrides == ['foo.bar=baz']:
        #print(6)
        isT=False



    flexmock(collect).should_receive('get_default_config_paths').and_return(['default'])

    arguments = parse_arguments('--override', 'foo.bar=baz', 'foo.quux=7')

    global_arguments = arguments['global']
    if not global_arguments.overrides == ['foo.bar=baz', 'foo.quux=7']:
        #print(7)
        isT=False



    flexmock(collect).should_receive('get_default_config_paths').and_return(['default'])

    arguments = parse_arguments(
        '--override', 'foo.bar=baz', '--override', 'foo.quux=7', 'this.that=8'
    )

    global_arguments = arguments['global']
    if not global_arguments.overrides == ['foo.bar=baz', 'foo.quux=7', 'this.that=8']:
        #print(8)
        isT=False



    arguments = parse_arguments('list', '--json')

    if not 'list' in arguments or \
    not arguments['list'].json is True:
        #print(9)
        isT=False



    flexmock(collect).should_receive('get_default_config_paths').and_return(['default'])

    arguments = parse_arguments()

    if not 'prune' in arguments or \
    not 'create' in arguments or \
    not 'check' in arguments:
        #print(10)
        isT=False



    flexmock(collect).should_receive('get_default_config_paths').and_return(['default'])

    arguments = parse_arguments('--stats', '--list')

    if not 'prune' in arguments or \
    not arguments['prune'].stats or \
    not arguments['prune'].list_archives or \
    not 'create' in arguments or \
    not arguments['create'].stats or \
    not arguments['create'].list_files or \
    not 'check' in arguments:
        #print(11)
        isT=False

    flexmock(collect).should_receive('get_default_config_paths').and_return(['default'])

    arguments = parse_arguments('prune', '--verbosity', '2')

    if not 'prune' in arguments or \
    not arguments['global'].verbosity == 2:
        #print(12)
        isT=False



    flexmock(collect).should_receive('get_default_config_paths').and_return(['default'])

    arguments = parse_arguments('--verbosity', '2', 'prune')

    if not 'prune' in arguments or \
    not arguments['global'].verbosity == 2:
        #print(13)
        isT=False

    flexmock(collect).should_receive('get_default_config_paths').and_return(['default'])

    arguments = parse_arguments('prune')

    if not 'prune' in arguments or \
    not 'create' not in arguments or \
    not 'check' not in arguments:
        #print(14)
        isT=False


    flexmock(collect).should_receive('get_default_config_paths').and_return(['default'])

    arguments = parse_arguments('create', 'check')

    if not 'prune' not in arguments or \
    not 'create' in arguments or \
    not 'check' in arguments:
        #print(15)
        isT=False


    


    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/witten---atticmatic/data_passk_platform/62b45665d7d32e5b55cc8365/"):
    #     f = open("/home/travis/builds/repos/witten---atticmatic/data_passk_platform/62b45665d7d32e5b55cc8365/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     res0 = parse_arguments()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/witten---atticmatic/borgmatic/commands/arguments_parse_subparser_arguments_passk_validte.py
import collections
from argparse import Action, ArgumentParser
import sys
sys.path.append("/home/travis/builds/repos/witten---atticmatic/borgmatic")
sys.path.append("/home/travis/builds/repos/witten---atticmatic")
sys.path.append("/home/travis/builds/repos")

from importlib import import_module
collect = import_module("witten---atticmatic.borgmatic.config.collect")

SUBPARSER_ALIASES = {
    'rcreate': ['init', '-I'],
    'prune': ['-p'],
    'compact': [],
    'create': ['-C'],
    'check': ['-k'],
    'extract': ['-x'],
    'export-tar': [],
    'mount': ['-m'],
    'umount': ['-u'],
    'restore': ['-r'],
    'rlist': [],
    'list': ['-l'],
    'rinfo': [],
    'info': ['-i'],
    'transfer': [],
    'borg': [],
}


def parse_subparser_arguments(unparsed_arguments, subparsers):
    '''
    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser
    instance, give each requested action's subparser a shot at parsing all arguments. This allows
    common arguments like "--repository" to be shared across multiple subparsers.

    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of
    arguments, a list of remaining arguments not claimed by any subparser).
    '''
    arguments = collections.OrderedDict()
    remaining_arguments = list(unparsed_arguments)
    alias_to_subparser_name = {
        alias: subparser_name
        for subparser_name, aliases in SUBPARSER_ALIASES.items()
        for alias in aliases
    }

    # If the "borg" action is used, skip all other subparsers. This avoids confusion like
    # "borg list" triggering borgmatic's own list action.
    if 'borg' in unparsed_arguments:
        subparsers = {'borg': subparsers['borg']}

    for subparser_name, subparser in subparsers.items():
        if subparser_name not in remaining_arguments:
            continue

        canonical_name = alias_to_subparser_name.get(subparser_name, subparser_name)

        # If a parsed value happens to be the same as the name of a subparser, remove it from the
        # remaining arguments. This prevents, for instance, "check --only extract" from triggering
        # the "extract" subparser.
        parsed, unused_remaining = subparser.parse_known_args(unparsed_arguments)
        for value in vars(parsed).values():
            if isinstance(value, str):
                if value in subparsers:
                    remaining_arguments.remove(value)
            elif isinstance(value, list):
                for item in value:
                    if item in subparsers:
                        remaining_arguments.remove(item)

        arguments[canonical_name] = parsed

    # If no actions are explicitly requested, assume defaults: prune, compact, create, and check.
    if not arguments and '--help' not in unparsed_arguments and '-h' not in unparsed_arguments:
        for subparser_name in ('prune', 'compact', 'create', 'check'):
            subparser = subparsers[subparser_name]
            parsed, unused_remaining = subparser.parse_known_args(unparsed_arguments)
            arguments[subparser_name] = parsed

    remaining_arguments = list(unparsed_arguments)

    # Now ask each subparser, one by one, to greedily consume arguments.
    for subparser_name, subparser in subparsers.items():
        if subparser_name not in arguments.keys():
            continue

        subparser = subparsers[subparser_name]
        unused_parsed, remaining_arguments = subparser.parse_known_args(remaining_arguments)

    # Special case: If "borg" is present in the arguments, consume all arguments after (+1) the
    # "borg" action.
    if 'borg' in arguments:
        borg_options_index = remaining_arguments.index('borg') + 1
        arguments['borg'].options = remaining_arguments[borg_options_index:]
        remaining_arguments = remaining_arguments[:borg_options_index]

    # Remove the subparser names themselves.
    for subparser_name, subparser in subparsers.items():
        if subparser_name in remaining_arguments:
            remaining_arguments.remove(subparser_name)

    return (arguments, remaining_arguments)


class Extend_action(Action):
    '''
    An argparse action to support Python 3.8's "extend" action in older versions of Python.
    '''

    def __call__(self, parser, namespace, values, option_string=None):
        items = getattr(namespace, self.dest, None)

        if items:
            items.extend(values)
        else:
            setattr(namespace, self.dest, list(values))


def make_parsers():
    '''
    Build a top-level parser and its subparsers and return them as a tuple.
    '''
    config_paths = collect.get_default_config_paths(expand_home=True)
    unexpanded_config_paths = collect.get_default_config_paths(expand_home=False)

    global_parser = ArgumentParser(add_help=False)
    global_parser.register('action', 'extend', Extend_action)
    global_group = global_parser.add_argument_group('global arguments')

    global_group.add_argument(
        '-c',
        '--config',
        nargs='*',
        dest='config_paths',
        default=config_paths,
        help='Configuration filenames or directories, defaults to: {}'.format(
            ' '.join(unexpanded_config_paths)
        ),
    )
    global_group.add_argument(
        '--excludes',
        dest='excludes_filename',
        help='Deprecated in favor of exclude_patterns within configuration',
    )
    global_group.add_argument(
        '-n',
        '--dry-run',
        dest='dry_run',
        action='store_true',
        help='Go through the motions, but do not actually write to any repositories',
    )
    global_group.add_argument(
        '-nc', '--no-color', dest='no_color', action='store_true', help='Disable colored output'
    )
    global_group.add_argument(
        '-v',
        '--verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Display verbose progress to the console (from only errors to very verbose: -1, 0, 1, or 2)',
    )
    global_group.add_argument(
        '--syslog-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to syslog (from only errors to very verbose: -1, 0, 1, or 2). Ignored when console is interactive or --log-file is given',
    )
    global_group.add_argument(
        '--log-file-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to log file (from only errors to very verbose: -1, 0, 1, or 2). Only used when --log-file is given',
    )
    global_group.add_argument(
        '--monitoring-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to monitoring integrations that support logging (from only errors to very verbose: -1, 0, 1, or 2)',
    )
    global_group.add_argument(
        '--log-file',
        type=str,
        default=None,
        help='Write log messages to this file instead of syslog',
    )
    global_group.add_argument(
        '--override',
        metavar='SECTION.OPTION=VALUE',
        nargs='+',
        dest='overrides',
        action='extend',
        help='One or more configuration file options to override with specified values',
    )
    global_group.add_argument(
        '--no-environment-interpolation',
        dest='resolve_env',
        action='store_false',
        help='Do not resolve environment variables in configuration file',
    )
    global_group.add_argument(
        '--bash-completion',
        default=False,
        action='store_true',
        help='Show bash completion script and exit',
    )
    global_group.add_argument(
        '--version',
        dest='version',
        default=False,
        action='store_true',
        help='Display installed version number of borgmatic and exit',
    )

    top_level_parser = ArgumentParser(
        description='''
            Simple, configuration-driven backup software for servers and workstations. If none of
            the action options are given, then borgmatic defaults to: prune, compact, create, and
            check.
            ''',
        parents=[global_parser],
    )

    subparsers = top_level_parser.add_subparsers(
        title='actions',
        metavar='',
        help='Specify zero or more actions. Defaults to prune, compact, create, and check. Use --help with action for details:',
    )
    rcreate_parser = subparsers.add_parser(
        'rcreate',
        aliases=SUBPARSER_ALIASES['rcreate'],
        help='Create a new, empty Borg repository',
        description='Create a new, empty Borg repository',
        add_help=False,
    )
    rcreate_group = rcreate_parser.add_argument_group('rcreate arguments')
    rcreate_group.add_argument(
        '-e',
        '--encryption',
        dest='encryption_mode',
        help='Borg repository encryption mode',
        required=True,
    )
    rcreate_group.add_argument(
        '--source-repository',
        '--other-repo',
        metavar='KEY_REPOSITORY',
        help='Path to an existing Borg repository whose key material should be reused (Borg 2.x+ only)',
    )
    rcreate_group.add_argument(
        '--copy-crypt-key',
        action='store_true',
        help='Copy the crypt key used for authenticated encryption from the source repository, defaults to a new random key (Borg 2.x+ only)',
    )
    rcreate_group.add_argument(
        '--append-only', action='store_true', help='Create an append-only repository',
    )
    rcreate_group.add_argument(
        '--storage-quota', help='Create a repository with a fixed storage quota',
    )
    rcreate_group.add_argument(
        '--make-parent-dirs',
        action='store_true',
        help='Create any missing parent directories of the repository directory',
    )
    rcreate_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    transfer_parser = subparsers.add_parser(
        'transfer',
        aliases=SUBPARSER_ALIASES['transfer'],
        help='Transfer archives from one repository to another, optionally upgrading the transferred data (Borg 2.0+ only)',
        description='Transfer archives from one repository to another, optionally upgrading the transferred data (Borg 2.0+ only)',
        add_help=False,
    )
    transfer_group = transfer_parser.add_argument_group('transfer arguments')
    transfer_group.add_argument(
        '--repository',
        help='Path of existing destination repository to transfer archives to, defaults to the configured repository if there is only one',
    )
    transfer_group.add_argument(
        '--source-repository',
        help='Path of existing source repository to transfer archives from',
        required=True,
    )
    transfer_group.add_argument(
        '--archive',
        help='Name of single archive to transfer (or "latest"), defaults to transferring all archives',
    )
    transfer_group.add_argument(
        '--upgrader',
        help='Upgrader type used to convert the transfered data, e.g. "From12To20" to upgrade data from Borg 1.2 to 2.0 format, defaults to no conversion',
    )
    transfer_group.add_argument(
        '-a',
        '--glob-archives',
        metavar='GLOB',
        help='Only transfer archives with names matching this glob',
    )
    transfer_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    transfer_group.add_argument(
        '--first',
        metavar='N',
        help='Only transfer first N archives after other filters are applied',
    )
    transfer_group.add_argument(
        '--last', metavar='N', help='Only transfer last N archives after other filters are applied'
    )
    transfer_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    prune_parser = subparsers.add_parser(
        'prune',
        aliases=SUBPARSER_ALIASES['prune'],
        help='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
        description='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
        add_help=False,
    )
    prune_group = prune_parser.add_argument_group('prune arguments')
    prune_group.add_argument(
        '--stats',
        dest='stats',
        default=False,
        action='store_true',
        help='Display statistics of archive',
    )
    prune_group.add_argument(
        '--list', dest='list_archives', action='store_true', help='List archives kept/pruned'
    )
    prune_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    compact_parser = subparsers.add_parser(
        'compact',
        aliases=SUBPARSER_ALIASES['compact'],
        help='Compact segments to free space (Borg 1.2+ only)',
        description='Compact segments to free space (Borg 1.2+ only)',
        add_help=False,
    )
    compact_group = compact_parser.add_argument_group('compact arguments')
    compact_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress as each segment is compacted',
    )
    compact_group.add_argument(
        '--cleanup-commits',
        dest='cleanup_commits',
        default=False,
        action='store_true',
        help='Cleanup commit-only 17-byte segment files left behind by Borg 1.1 (flag in Borg 1.2 only)',
    )
    compact_group.add_argument(
        '--threshold',
        type=int,
        dest='threshold',
        help='Minimum saved space percentage threshold for compacting a segment, defaults to 10',
    )
    compact_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    create_parser = subparsers.add_parser(
        'create',
        aliases=SUBPARSER_ALIASES['create'],
        help='Create an archive (actually perform a backup)',
        description='Create an archive (actually perform a backup)',
        add_help=False,
    )
    create_group = create_parser.add_argument_group('create arguments')
    create_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is backed up',
    )
    create_group.add_argument(
        '--stats',
        dest='stats',
        default=False,
        action='store_true',
        help='Display statistics of archive',
    )
    create_group.add_argument(
        '--list', '--files', dest='list_files', action='store_true', help='Show per-file details'
    )
    create_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    create_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    check_parser = subparsers.add_parser(
        'check',
        aliases=SUBPARSER_ALIASES['check'],
        help='Check archives for consistency',
        description='Check archives for consistency',
        add_help=False,
    )
    check_group = check_parser.add_argument_group('check arguments')
    check_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is checked',
    )
    check_group.add_argument(
        '--repair',
        dest='repair',
        default=False,
        action='store_true',
        help='Attempt to repair any inconsistencies found (for interactive use)',
    )
    check_group.add_argument(
        '--only',
        metavar='CHECK',
        choices=('repository', 'archives', 'data', 'extract'),
        dest='only',
        action='append',
        help='Run a particular consistency check (repository, archives, data, or extract) instead of configured checks (subject to configured frequency, can specify flag multiple times)',
    )
    check_group.add_argument(
        '--force',
        default=False,
        action='store_true',
        help='Ignore configured check frequencies and run checks unconditionally',
    )
    check_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    extract_parser = subparsers.add_parser(
        'extract',
        aliases=SUBPARSER_ALIASES['extract'],
        help='Extract files from a named archive to the current directory',
        description='Extract a named archive to the current directory',
        add_help=False,
    )
    extract_group = extract_parser.add_argument_group('extract arguments')
    extract_group.add_argument(
        '--repository',
        help='Path of repository to extract, defaults to the configured repository if there is only one',
    )
    extract_group.add_argument(
        '--archive', help='Name of archive to extract (or "latest")', required=True
    )
    extract_group.add_argument(
        '--path',
        '--restore-path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to extract from archive, defaults to the entire archive',
    )
    extract_group.add_argument(
        '--destination',
        metavar='PATH',
        dest='destination',
        help='Directory to extract files into, defaults to the current directory',
    )
    extract_group.add_argument(
        '--strip-components',
        type=int,
        metavar='NUMBER',
        dest='strip_components',
        help='Number of leading path components to remove from each extracted path. Skip paths with fewer elements',
    )
    extract_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is extracted',
    )
    extract_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    export_tar_parser = subparsers.add_parser(
        'export-tar',
        aliases=SUBPARSER_ALIASES['export-tar'],
        help='Export an archive to a tar-formatted file or stream',
        description='Export an archive to a tar-formatted file or stream',
        add_help=False,
    )
    export_tar_group = export_tar_parser.add_argument_group('export-tar arguments')
    export_tar_group.add_argument(
        '--repository',
        help='Path of repository to export from, defaults to the configured repository if there is only one',
    )
    export_tar_group.add_argument(
        '--archive', help='Name of archive to export (or "latest")', required=True
    )
    export_tar_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to export from archive, defaults to the entire archive',
    )
    export_tar_group.add_argument(
        '--destination',
        metavar='PATH',
        dest='destination',
        help='Path to destination export tar file, or "-" for stdout (but be careful about dirtying output with --verbosity or --list)',
        required=True,
    )
    export_tar_group.add_argument(
        '--tar-filter', help='Name of filter program to pipe data through'
    )
    export_tar_group.add_argument(
        '--list', '--files', dest='list_files', action='store_true', help='Show per-file details'
    )
    export_tar_group.add_argument(
        '--strip-components',
        type=int,
        metavar='NUMBER',
        dest='strip_components',
        help='Number of leading path components to remove from each exported path. Skip paths with fewer elements',
    )
    export_tar_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    mount_parser = subparsers.add_parser(
        'mount',
        aliases=SUBPARSER_ALIASES['mount'],
        help='Mount files from a named archive as a FUSE filesystem',
        description='Mount a named archive as a FUSE filesystem',
        add_help=False,
    )
    mount_group = mount_parser.add_argument_group('mount arguments')
    mount_group.add_argument(
        '--repository',
        help='Path of repository to use, defaults to the configured repository if there is only one',
    )
    mount_group.add_argument('--archive', help='Name of archive to mount (or "latest")')
    mount_group.add_argument(
        '--mount-point',
        metavar='PATH',
        dest='mount_point',
        help='Path where filesystem is to be mounted',
        required=True,
    )
    mount_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to mount from archive, defaults to the entire archive',
    )
    mount_group.add_argument(
        '--foreground',
        dest='foreground',
        default=False,
        action='store_true',
        help='Stay in foreground until ctrl-C is pressed',
    )
    mount_group.add_argument('--options', dest='options', help='Extra Borg mount options')
    mount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    umount_parser = subparsers.add_parser(
        'umount',
        aliases=SUBPARSER_ALIASES['umount'],
        help='Unmount a FUSE filesystem that was mounted with "borgmatic mount"',
        description='Unmount a mounted FUSE filesystem',
        add_help=False,
    )
    umount_group = umount_parser.add_argument_group('umount arguments')
    umount_group.add_argument(
        '--mount-point',
        metavar='PATH',
        dest='mount_point',
        help='Path of filesystem to unmount',
        required=True,
    )
    umount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    restore_parser = subparsers.add_parser(
        'restore',
        aliases=SUBPARSER_ALIASES['restore'],
        help='Restore database dumps from a named archive',
        description='Restore database dumps from a named archive. (To extract files instead, use "borgmatic extract".)',
        add_help=False,
    )
    restore_group = restore_parser.add_argument_group('restore arguments')
    restore_group.add_argument(
        '--repository',
        help='Path of repository to restore from, defaults to the configured repository if there is only one',
    )
    restore_group.add_argument(
        '--archive', help='Name of archive to restore from (or "latest")', required=True
    )
    restore_group.add_argument(
        '--database',
        metavar='NAME',
        nargs='+',
        dest='databases',
        help='Names of databases to restore from archive, defaults to all databases. Note that any databases to restore must be defined in borgmatic\'s configuration',
    )
    restore_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    rlist_parser = subparsers.add_parser(
        'rlist',
        aliases=SUBPARSER_ALIASES['rlist'],
        help='List repository',
        description='List the archives in a repository',
        add_help=False,
    )
    rlist_group = rlist_parser.add_argument_group('rlist arguments')
    rlist_group.add_argument(
        '--repository', help='Path of repository to list, defaults to the configured repositories',
    )
    rlist_group.add_argument(
        '--short', default=False, action='store_true', help='Output only archive names'
    )
    rlist_group.add_argument('--format', help='Format for archive listing')
    rlist_group.add_argument(
        '--json', default=False, action='store_true', help='Output results as JSON'
    )
    rlist_group.add_argument(
        '-P', '--prefix', help='Only list archive names starting with this prefix'
    )
    rlist_group.add_argument(
        '-a', '--glob-archives', metavar='GLOB', help='Only list archive names matching this glob'
    )
    rlist_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    rlist_group.add_argument(
        '--first', metavar='N', help='List first N archives after other filters are applied'
    )
    rlist_group.add_argument(
        '--last', metavar='N', help='List last N archives after other filters are applied'
    )
    rlist_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    list_parser = subparsers.add_parser(
        'list',
        aliases=SUBPARSER_ALIASES['list'],
        help='List archive',
        description='List the files in an archive or search for a file across archives',
        add_help=False,
    )
    list_group = list_parser.add_argument_group('list arguments')
    list_group.add_argument(
        '--repository',
        help='Path of repository containing archive to list, defaults to the configured repositories',
    )
    list_group.add_argument('--archive', help='Name of the archive to list (or "latest")')
    list_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths or patterns to list from a single selected archive (via "--archive"), defaults to listing the entire archive',
    )
    list_group.add_argument(
        '--find',
        metavar='PATH',
        nargs='+',
        dest='find_paths',
        help='Partial paths or patterns to search for and list across multiple archives',
    )
    list_group.add_argument(
        '--short', default=False, action='store_true', help='Output only path names'
    )
    list_group.add_argument('--format', help='Format for file listing')
    list_group.add_argument(
        '--json', default=False, action='store_true', help='Output results as JSON'
    )
    list_group.add_argument(
        '-P', '--prefix', help='Only list archive names starting with this prefix'
    )
    list_group.add_argument(
        '-a', '--glob-archives', metavar='GLOB', help='Only list archive names matching this glob'
    )
    list_group.add_argument(
        '--successful',
        default=True,
        action='store_true',
        help='Deprecated; no effect. Newer versions of Borg shows successful (non-checkpoint) archives by default.',
    )
    list_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    list_group.add_argument(
        '--first', metavar='N', help='List first N archives after other filters are applied'
    )
    list_group.add_argument(
        '--last', metavar='N', help='List last N archives after other filters are applied'
    )
    list_group.add_argument(
        '-e', '--exclude', metavar='PATTERN', help='Exclude paths matching the pattern'
    )
    list_group.add_argument(
        '--exclude-from', metavar='FILENAME', help='Exclude paths from exclude file, one per line'
    )
    list_group.add_argument('--pattern', help='Include or exclude paths matching a pattern')
    list_group.add_argument(
        '--patterns-from',
        metavar='FILENAME',
        help='Include or exclude paths matching patterns from pattern file, one per line',
    )
    list_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    rinfo_parser = subparsers.add_parser(
        'rinfo',
        aliases=SUBPARSER_ALIASES['rinfo'],
        help='Show repository summary information such as disk space used',
        description='Show repository summary information such as disk space used',
        add_help=False,
    )
    rinfo_group = rinfo_parser.add_argument_group('rinfo arguments')
    rinfo_group.add_argument(
        '--repository',
        help='Path of repository to show info for, defaults to the configured repository if there is only one',
    )
    rinfo_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    rinfo_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    info_parser = subparsers.add_parser(
        'info',
        aliases=SUBPARSER_ALIASES['info'],
        help='Show archive summary information such as disk space used',
        description='Show archive summary information such as disk space used',
        add_help=False,
    )
    info_group = info_parser.add_argument_group('info arguments')
    info_group.add_argument(
        '--repository',
        help='Path of repository containing archive to show info for, defaults to the configured repository if there is only one',
    )
    info_group.add_argument('--archive', help='Name of archive to show info for (or "latest")')
    info_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    info_group.add_argument(
        '-P', '--prefix', help='Only show info for archive names starting with this prefix'
    )
    info_group.add_argument(
        '-a',
        '--glob-archives',
        metavar='GLOB',
        help='Only show info for archive names matching this glob',
    )
    info_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    info_group.add_argument(
        '--first',
        metavar='N',
        help='Show info for first N archives after other filters are applied',
    )
    info_group.add_argument(
        '--last', metavar='N', help='Show info for last N archives after other filters are applied'
    )
    info_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    borg_parser = subparsers.add_parser(
        'borg',
        aliases=SUBPARSER_ALIASES['borg'],
        help='Run an arbitrary Borg command',
        description='Run an arbitrary Borg command based on borgmatic\'s configuration',
        add_help=False,
    )
    borg_group = borg_parser.add_argument_group('borg arguments')
    borg_group.add_argument(
        '--repository',
        help='Path of repository to pass to Borg, defaults to the configured repositories',
    )
    borg_group.add_argument('--archive', help='Name of archive to pass to Borg (or "latest")')
    borg_group.add_argument(
        '--',
        metavar='OPTION',
        dest='options',
        nargs='+',
        help='Options to pass to Borg, command first ("create", "list", etc). "--" is optional. To specify the repository or the archive, you must use --repository or --archive instead of providing them here.',
    )
    borg_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    return top_level_parser, subparsers


def parse_arguments(*unparsed_arguments):
    '''
    Given command-line arguments with which this script was invoked, parse the arguments and return
    them as a dict mapping from subparser name (or "global") to an argparse.Namespace instance.
    '''
    top_level_parser, subparsers = make_parsers()

    arguments, remaining_arguments = parse_subparser_arguments(
        unparsed_arguments, subparsers.choices
    )
    arguments['global'] = top_level_parser.parse_args(remaining_arguments)

    if arguments['global'].excludes_filename:
        raise ValueError(
            'The --excludes flag has been replaced with exclude_patterns in configuration.'
        )

    if (
        ('list' in arguments and 'rinfo' in arguments and arguments['list'].json)
        or ('list' in arguments and 'info' in arguments and arguments['list'].json)
        or ('rinfo' in arguments and 'info' in arguments and arguments['rinfo'].json)
    ):
        raise ValueError('With the --json flag, multiple actions cannot be used together.')

    if (
        'transfer' in arguments
        and arguments['transfer'].archive
        and arguments['transfer'].glob_archives
    ):
        raise ValueError(
            'With the transfer action, only one of --archive and --glob-archives flags can be used.'
        )

    if 'info' in arguments and (
        (arguments['info'].archive and arguments['info'].prefix)
        or (arguments['info'].archive and arguments['info'].glob_archives)
        or (arguments['info'].prefix and arguments['info'].glob_archives)
    ):
        raise ValueError(
            'With the info action, only one of --archive, --prefix, or --glob-archives flags can be used.'
        )

    return arguments

if __name__ == "__main__":
    from flexmock import flexmock

    isT=True

    
    action_namespace = flexmock(foo=True)
    subparsers = {
        'action': flexmock(parse_known_args=lambda arguments: (action_namespace, ['action'])),
        'other': flexmock(),
    }

    arguments, remaining_arguments = parse_subparser_arguments(
        ('--foo', 'true', 'action'), subparsers
    )

    if not arguments == {'action': action_namespace} or\
    not remaining_arguments == []:
        isT=False



    action_namespace = flexmock(foo=True)
    subparsers = {
        'action': flexmock(parse_known_args=lambda arguments: (action_namespace, ['action'])),
        'other': flexmock(),
    }

    arguments, remaining_arguments = parse_subparser_arguments(
        ('action', '--foo', 'true'), subparsers
    )

    if not arguments == {'action': action_namespace} or\
    not remaining_arguments == []:
        isT=False




    action_namespace = flexmock(foo=True)
    other_namespace = flexmock(bar=3)
    subparsers = {
        'action': flexmock(
            parse_known_args=lambda arguments: (action_namespace, ['action', '--bar', '3'])
        ),
        'other': flexmock(parse_known_args=lambda arguments: (other_namespace, [])),
    }

    arguments, remaining_arguments = parse_subparser_arguments(
        ('action', '--foo', 'true', 'other', '--bar', '3'), subparsers
    )

    if not arguments == {'action': action_namespace, 'other': other_namespace} or\
    not remaining_arguments == []:
        isT=False



    prune_namespace = flexmock()
    compact_namespace = flexmock()
    create_namespace = flexmock(progress=True)
    check_namespace = flexmock()
    subparsers = {
        'prune': flexmock(
            parse_known_args=lambda arguments: (prune_namespace, ['prune', '--progress'])
        ),
        'compact': flexmock(parse_known_args=lambda arguments: (compact_namespace, [])),
        'create': flexmock(parse_known_args=lambda arguments: (create_namespace, [])),
        'check': flexmock(parse_known_args=lambda arguments: (check_namespace, [])),
        'other': flexmock(),
    }

    arguments, remaining_arguments = parse_subparser_arguments(('--progress'), subparsers)

    if not arguments == {
        'prune': prune_namespace,
        'compact': compact_namespace,
        'create': create_namespace,
        'check': check_namespace,
    } or\
    not remaining_arguments == []:
        isT=False



    action_namespace = flexmock()
    subparsers = {
        'action': flexmock(
            parse_known_args=lambda arguments: (action_namespace, ['action', '--verbosity', 'lots'])
        ),
        'other': flexmock(),
    }

    arguments, remaining_arguments = parse_subparser_arguments(
        ('--verbosity', 'lots', 'action'), subparsers
    )

    if not arguments == {'action': action_namespace} or\
    not remaining_arguments == ['--verbosity', 'lots']:
        isT=False



    action_namespace = flexmock()
    subparsers = {
        'action': flexmock(
            parse_known_args=lambda arguments: (action_namespace, ['action', '--verbosity', 'lots'])
        ),
        'other': flexmock(),
    }

    arguments, remaining_arguments = parse_subparser_arguments(
        ('action', '--verbosity', 'lots'), subparsers
    )

    if not arguments == {'action': action_namespace} or\
    not remaining_arguments == ['--verbosity', 'lots']:
        isT=False



    action_namespace = flexmock(options=[])
    subparsers = {
        'borg': flexmock(parse_known_args=lambda arguments: (action_namespace, ['borg', 'list'])),
        'list': flexmock(),
    }

    arguments, remaining_arguments = parse_subparser_arguments(('borg', 'list'), subparsers)

    if not arguments == {'borg': action_namespace} or\
    not arguments['borg'].options == ['list'] or\
    not remaining_arguments == []:
        isT=False


    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/witten---atticmatic/data_passk_platform/62b45665d7d32e5b55cc8364/"):
    #     f = open("/home/travis/builds/repos/witten---atticmatic/data_passk_platform/62b45665d7d32e5b55cc8364/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     res0 = parse_subparser_arguments(args0,args1)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/witten---atticmatic/borgmatic/commands/arguments_make_parsers_passk_validte.py
import collections
from argparse import Action, ArgumentParser
import sys
sys.path.append("/home/travis/builds/repos/witten---atticmatic/borgmatic")
sys.path.append("/home/travis/builds/repos/witten---atticmatic")
sys.path.append("/home/travis/builds/repos")

from importlib import import_module
collect = import_module("witten---atticmatic.borgmatic.config.collect")

SUBPARSER_ALIASES = {
    'rcreate': ['init', '-I'],
    'prune': ['-p'],
    'compact': [],
    'create': ['-C'],
    'check': ['-k'],
    'extract': ['-x'],
    'export-tar': [],
    'mount': ['-m'],
    'umount': ['-u'],
    'restore': ['-r'],
    'rlist': [],
    'list': ['-l'],
    'rinfo': [],
    'info': ['-i'],
    'transfer': [],
    'borg': [],
}


def parse_subparser_arguments(unparsed_arguments, subparsers):
    '''
    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser
    instance, give each requested action's subparser a shot at parsing all arguments. This allows
    common arguments like "--repository" to be shared across multiple subparsers.

    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of
    arguments, a list of remaining arguments not claimed by any subparser).
    '''
    arguments = collections.OrderedDict()
    remaining_arguments = list(unparsed_arguments)
    alias_to_subparser_name = {
        alias: subparser_name
        for subparser_name, aliases in SUBPARSER_ALIASES.items()
        for alias in aliases
    }

    # If the "borg" action is used, skip all other subparsers. This avoids confusion like
    # "borg list" triggering borgmatic's own list action.
    if 'borg' in unparsed_arguments:
        subparsers = {'borg': subparsers['borg']}

    for subparser_name, subparser in subparsers.items():
        if subparser_name not in remaining_arguments:
            continue

        canonical_name = alias_to_subparser_name.get(subparser_name, subparser_name)

        # If a parsed value happens to be the same as the name of a subparser, remove it from the
        # remaining arguments. This prevents, for instance, "check --only extract" from triggering
        # the "extract" subparser.
        parsed, unused_remaining = subparser.parse_known_args(unparsed_arguments)
        for value in vars(parsed).values():
            if isinstance(value, str):
                if value in subparsers:
                    remaining_arguments.remove(value)
            elif isinstance(value, list):
                for item in value:
                    if item in subparsers:
                        remaining_arguments.remove(item)

        arguments[canonical_name] = parsed

    # If no actions are explicitly requested, assume defaults: prune, compact, create, and check.
    if not arguments and '--help' not in unparsed_arguments and '-h' not in unparsed_arguments:
        for subparser_name in ('prune', 'compact', 'create', 'check'):
            subparser = subparsers[subparser_name]
            parsed, unused_remaining = subparser.parse_known_args(unparsed_arguments)
            arguments[subparser_name] = parsed

    remaining_arguments = list(unparsed_arguments)

    # Now ask each subparser, one by one, to greedily consume arguments.
    for subparser_name, subparser in subparsers.items():
        if subparser_name not in arguments.keys():
            continue

        subparser = subparsers[subparser_name]
        unused_parsed, remaining_arguments = subparser.parse_known_args(remaining_arguments)

    # Special case: If "borg" is present in the arguments, consume all arguments after (+1) the
    # "borg" action.
    if 'borg' in arguments:
        borg_options_index = remaining_arguments.index('borg') + 1
        arguments['borg'].options = remaining_arguments[borg_options_index:]
        remaining_arguments = remaining_arguments[:borg_options_index]

    # Remove the subparser names themselves.
    for subparser_name, subparser in subparsers.items():
        if subparser_name in remaining_arguments:
            remaining_arguments.remove(subparser_name)

    return (arguments, remaining_arguments)


class Extend_action(Action):
    '''
    An argparse action to support Python 3.8's "extend" action in older versions of Python.
    '''

    def __call__(self, parser, namespace, values, option_string=None):
        items = getattr(namespace, self.dest, None)

        if items:
            items.extend(values)
        else:
            setattr(namespace, self.dest, list(values))


def make_parsers():
    '''
    Build a top-level parser and its subparsers and return them as a tuple.
    '''
    config_paths = collect.get_default_config_paths(expand_home=True)
    unexpanded_config_paths = collect.get_default_config_paths(expand_home=False)

    global_parser = ArgumentParser(add_help=False)
    global_parser.register('action', 'extend', Extend_action)
    global_group = global_parser.add_argument_group('global arguments')

    global_group.add_argument(
        '-c',
        '--config',
        nargs='*',
        dest='config_paths',
        default=config_paths,
        help='Configuration filenames or directories, defaults to: {}'.format(
            ' '.join(unexpanded_config_paths)
        ),
    )
    global_group.add_argument(
        '--excludes',
        dest='excludes_filename',
        help='Deprecated in favor of exclude_patterns within configuration',
    )
    global_group.add_argument(
        '-n',
        '--dry-run',
        dest='dry_run',
        action='store_true',
        help='Go through the motions, but do not actually write to any repositories',
    )
    global_group.add_argument(
        '-nc', '--no-color', dest='no_color', action='store_true', help='Disable colored output'
    )
    global_group.add_argument(
        '-v',
        '--verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Display verbose progress to the console (from only errors to very verbose: -1, 0, 1, or 2)',
    )
    global_group.add_argument(
        '--syslog-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to syslog (from only errors to very verbose: -1, 0, 1, or 2). Ignored when console is interactive or --log-file is given',
    )
    global_group.add_argument(
        '--log-file-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to log file (from only errors to very verbose: -1, 0, 1, or 2). Only used when --log-file is given',
    )
    global_group.add_argument(
        '--monitoring-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to monitoring integrations that support logging (from only errors to very verbose: -1, 0, 1, or 2)',
    )
    global_group.add_argument(
        '--log-file',
        type=str,
        default=None,
        help='Write log messages to this file instead of syslog',
    )
    global_group.add_argument(
        '--override',
        metavar='SECTION.OPTION=VALUE',
        nargs='+',
        dest='overrides',
        action='extend',
        help='One or more configuration file options to override with specified values',
    )
    global_group.add_argument(
        '--no-environment-interpolation',
        dest='resolve_env',
        action='store_false',
        help='Do not resolve environment variables in configuration file',
    )
    global_group.add_argument(
        '--bash-completion',
        default=False,
        action='store_true',
        help='Show bash completion script and exit',
    )
    global_group.add_argument(
        '--version',
        dest='version',
        default=False,
        action='store_true',
        help='Display installed version number of borgmatic and exit',
    )

    top_level_parser = ArgumentParser(
        description='''
            Simple, configuration-driven backup software for servers and workstations. If none of
            the action options are given, then borgmatic defaults to: prune, compact, create, and
            check.
            ''',
        parents=[global_parser],
    )

    subparsers = top_level_parser.add_subparsers(
        title='actions',
        metavar='',
        help='Specify zero or more actions. Defaults to prune, compact, create, and check. Use --help with action for details:',
    )
    rcreate_parser = subparsers.add_parser(
        'rcreate',
        aliases=SUBPARSER_ALIASES['rcreate'],
        help='Create a new, empty Borg repository',
        description='Create a new, empty Borg repository',
        add_help=False,
    )
    rcreate_group = rcreate_parser.add_argument_group('rcreate arguments')
    rcreate_group.add_argument(
        '-e',
        '--encryption',
        dest='encryption_mode',
        help='Borg repository encryption mode',
        required=True,
    )
    rcreate_group.add_argument(
        '--source-repository',
        '--other-repo',
        metavar='KEY_REPOSITORY',
        help='Path to an existing Borg repository whose key material should be reused (Borg 2.x+ only)',
    )
    rcreate_group.add_argument(
        '--copy-crypt-key',
        action='store_true',
        help='Copy the crypt key used for authenticated encryption from the source repository, defaults to a new random key (Borg 2.x+ only)',
    )
    rcreate_group.add_argument(
        '--append-only', action='store_true', help='Create an append-only repository',
    )
    rcreate_group.add_argument(
        '--storage-quota', help='Create a repository with a fixed storage quota',
    )
    rcreate_group.add_argument(
        '--make-parent-dirs',
        action='store_true',
        help='Create any missing parent directories of the repository directory',
    )
    rcreate_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    transfer_parser = subparsers.add_parser(
        'transfer',
        aliases=SUBPARSER_ALIASES['transfer'],
        help='Transfer archives from one repository to another, optionally upgrading the transferred data (Borg 2.0+ only)',
        description='Transfer archives from one repository to another, optionally upgrading the transferred data (Borg 2.0+ only)',
        add_help=False,
    )
    transfer_group = transfer_parser.add_argument_group('transfer arguments')
    transfer_group.add_argument(
        '--repository',
        help='Path of existing destination repository to transfer archives to, defaults to the configured repository if there is only one',
    )
    transfer_group.add_argument(
        '--source-repository',
        help='Path of existing source repository to transfer archives from',
        required=True,
    )
    transfer_group.add_argument(
        '--archive',
        help='Name of single archive to transfer (or "latest"), defaults to transferring all archives',
    )
    transfer_group.add_argument(
        '--upgrader',
        help='Upgrader type used to convert the transfered data, e.g. "From12To20" to upgrade data from Borg 1.2 to 2.0 format, defaults to no conversion',
    )
    transfer_group.add_argument(
        '-a',
        '--glob-archives',
        metavar='GLOB',
        help='Only transfer archives with names matching this glob',
    )
    transfer_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    transfer_group.add_argument(
        '--first',
        metavar='N',
        help='Only transfer first N archives after other filters are applied',
    )
    transfer_group.add_argument(
        '--last', metavar='N', help='Only transfer last N archives after other filters are applied'
    )
    transfer_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    prune_parser = subparsers.add_parser(
        'prune',
        aliases=SUBPARSER_ALIASES['prune'],
        help='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
        description='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
        add_help=False,
    )
    prune_group = prune_parser.add_argument_group('prune arguments')
    prune_group.add_argument(
        '--stats',
        dest='stats',
        default=False,
        action='store_true',
        help='Display statistics of archive',
    )
    prune_group.add_argument(
        '--list', dest='list_archives', action='store_true', help='List archives kept/pruned'
    )
    prune_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    compact_parser = subparsers.add_parser(
        'compact',
        aliases=SUBPARSER_ALIASES['compact'],
        help='Compact segments to free space (Borg 1.2+ only)',
        description='Compact segments to free space (Borg 1.2+ only)',
        add_help=False,
    )
    compact_group = compact_parser.add_argument_group('compact arguments')
    compact_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress as each segment is compacted',
    )
    compact_group.add_argument(
        '--cleanup-commits',
        dest='cleanup_commits',
        default=False,
        action='store_true',
        help='Cleanup commit-only 17-byte segment files left behind by Borg 1.1 (flag in Borg 1.2 only)',
    )
    compact_group.add_argument(
        '--threshold',
        type=int,
        dest='threshold',
        help='Minimum saved space percentage threshold for compacting a segment, defaults to 10',
    )
    compact_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    create_parser = subparsers.add_parser(
        'create',
        aliases=SUBPARSER_ALIASES['create'],
        help='Create an archive (actually perform a backup)',
        description='Create an archive (actually perform a backup)',
        add_help=False,
    )
    create_group = create_parser.add_argument_group('create arguments')
    create_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is backed up',
    )
    create_group.add_argument(
        '--stats',
        dest='stats',
        default=False,
        action='store_true',
        help='Display statistics of archive',
    )
    create_group.add_argument(
        '--list', '--files', dest='list_files', action='store_true', help='Show per-file details'
    )
    create_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    create_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    check_parser = subparsers.add_parser(
        'check',
        aliases=SUBPARSER_ALIASES['check'],
        help='Check archives for consistency',
        description='Check archives for consistency',
        add_help=False,
    )
    check_group = check_parser.add_argument_group('check arguments')
    check_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is checked',
    )
    check_group.add_argument(
        '--repair',
        dest='repair',
        default=False,
        action='store_true',
        help='Attempt to repair any inconsistencies found (for interactive use)',
    )
    check_group.add_argument(
        '--only',
        metavar='CHECK',
        choices=('repository', 'archives', 'data', 'extract'),
        dest='only',
        action='append',
        help='Run a particular consistency check (repository, archives, data, or extract) instead of configured checks (subject to configured frequency, can specify flag multiple times)',
    )
    check_group.add_argument(
        '--force',
        default=False,
        action='store_true',
        help='Ignore configured check frequencies and run checks unconditionally',
    )
    check_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    extract_parser = subparsers.add_parser(
        'extract',
        aliases=SUBPARSER_ALIASES['extract'],
        help='Extract files from a named archive to the current directory',
        description='Extract a named archive to the current directory',
        add_help=False,
    )
    extract_group = extract_parser.add_argument_group('extract arguments')
    extract_group.add_argument(
        '--repository',
        help='Path of repository to extract, defaults to the configured repository if there is only one',
    )
    extract_group.add_argument(
        '--archive', help='Name of archive to extract (or "latest")', required=True
    )
    extract_group.add_argument(
        '--path',
        '--restore-path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to extract from archive, defaults to the entire archive',
    )
    extract_group.add_argument(
        '--destination',
        metavar='PATH',
        dest='destination',
        help='Directory to extract files into, defaults to the current directory',
    )
    extract_group.add_argument(
        '--strip-components',
        type=int,
        metavar='NUMBER',
        dest='strip_components',
        help='Number of leading path components to remove from each extracted path. Skip paths with fewer elements',
    )
    extract_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is extracted',
    )
    extract_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    export_tar_parser = subparsers.add_parser(
        'export-tar',
        aliases=SUBPARSER_ALIASES['export-tar'],
        help='Export an archive to a tar-formatted file or stream',
        description='Export an archive to a tar-formatted file or stream',
        add_help=False,
    )
    export_tar_group = export_tar_parser.add_argument_group('export-tar arguments')
    export_tar_group.add_argument(
        '--repository',
        help='Path of repository to export from, defaults to the configured repository if there is only one',
    )
    export_tar_group.add_argument(
        '--archive', help='Name of archive to export (or "latest")', required=True
    )
    export_tar_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to export from archive, defaults to the entire archive',
    )
    export_tar_group.add_argument(
        '--destination',
        metavar='PATH',
        dest='destination',
        help='Path to destination export tar file, or "-" for stdout (but be careful about dirtying output with --verbosity or --list)',
        required=True,
    )
    export_tar_group.add_argument(
        '--tar-filter', help='Name of filter program to pipe data through'
    )
    export_tar_group.add_argument(
        '--list', '--files', dest='list_files', action='store_true', help='Show per-file details'
    )
    export_tar_group.add_argument(
        '--strip-components',
        type=int,
        metavar='NUMBER',
        dest='strip_components',
        help='Number of leading path components to remove from each exported path. Skip paths with fewer elements',
    )
    export_tar_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    mount_parser = subparsers.add_parser(
        'mount',
        aliases=SUBPARSER_ALIASES['mount'],
        help='Mount files from a named archive as a FUSE filesystem',
        description='Mount a named archive as a FUSE filesystem',
        add_help=False,
    )
    mount_group = mount_parser.add_argument_group('mount arguments')
    mount_group.add_argument(
        '--repository',
        help='Path of repository to use, defaults to the configured repository if there is only one',
    )
    mount_group.add_argument('--archive', help='Name of archive to mount (or "latest")')
    mount_group.add_argument(
        '--mount-point',
        metavar='PATH',
        dest='mount_point',
        help='Path where filesystem is to be mounted',
        required=True,
    )
    mount_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to mount from archive, defaults to the entire archive',
    )
    mount_group.add_argument(
        '--foreground',
        dest='foreground',
        default=False,
        action='store_true',
        help='Stay in foreground until ctrl-C is pressed',
    )
    mount_group.add_argument('--options', dest='options', help='Extra Borg mount options')
    mount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    umount_parser = subparsers.add_parser(
        'umount',
        aliases=SUBPARSER_ALIASES['umount'],
        help='Unmount a FUSE filesystem that was mounted with "borgmatic mount"',
        description='Unmount a mounted FUSE filesystem',
        add_help=False,
    )
    umount_group = umount_parser.add_argument_group('umount arguments')
    umount_group.add_argument(
        '--mount-point',
        metavar='PATH',
        dest='mount_point',
        help='Path of filesystem to unmount',
        required=True,
    )
    umount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    restore_parser = subparsers.add_parser(
        'restore',
        aliases=SUBPARSER_ALIASES['restore'],
        help='Restore database dumps from a named archive',
        description='Restore database dumps from a named archive. (To extract files instead, use "borgmatic extract".)',
        add_help=False,
    )
    restore_group = restore_parser.add_argument_group('restore arguments')
    restore_group.add_argument(
        '--repository',
        help='Path of repository to restore from, defaults to the configured repository if there is only one',
    )
    restore_group.add_argument(
        '--archive', help='Name of archive to restore from (or "latest")', required=True
    )
    restore_group.add_argument(
        '--database',
        metavar='NAME',
        nargs='+',
        dest='databases',
        help='Names of databases to restore from archive, defaults to all databases. Note that any databases to restore must be defined in borgmatic\'s configuration',
    )
    restore_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    rlist_parser = subparsers.add_parser(
        'rlist',
        aliases=SUBPARSER_ALIASES['rlist'],
        help='List repository',
        description='List the archives in a repository',
        add_help=False,
    )
    rlist_group = rlist_parser.add_argument_group('rlist arguments')
    rlist_group.add_argument(
        '--repository', help='Path of repository to list, defaults to the configured repositories',
    )
    rlist_group.add_argument(
        '--short', default=False, action='store_true', help='Output only archive names'
    )
    rlist_group.add_argument('--format', help='Format for archive listing')
    rlist_group.add_argument(
        '--json', default=False, action='store_true', help='Output results as JSON'
    )
    rlist_group.add_argument(
        '-P', '--prefix', help='Only list archive names starting with this prefix'
    )
    rlist_group.add_argument(
        '-a', '--glob-archives', metavar='GLOB', help='Only list archive names matching this glob'
    )
    rlist_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    rlist_group.add_argument(
        '--first', metavar='N', help='List first N archives after other filters are applied'
    )
    rlist_group.add_argument(
        '--last', metavar='N', help='List last N archives after other filters are applied'
    )
    rlist_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    list_parser = subparsers.add_parser(
        'list',
        aliases=SUBPARSER_ALIASES['list'],
        help='List archive',
        description='List the files in an archive or search for a file across archives',
        add_help=False,
    )
    list_group = list_parser.add_argument_group('list arguments')
    list_group.add_argument(
        '--repository',
        help='Path of repository containing archive to list, defaults to the configured repositories',
    )
    list_group.add_argument('--archive', help='Name of the archive to list (or "latest")')
    list_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths or patterns to list from a single selected archive (via "--archive"), defaults to listing the entire archive',
    )
    list_group.add_argument(
        '--find',
        metavar='PATH',
        nargs='+',
        dest='find_paths',
        help='Partial paths or patterns to search for and list across multiple archives',
    )
    list_group.add_argument(
        '--short', default=False, action='store_true', help='Output only path names'
    )
    list_group.add_argument('--format', help='Format for file listing')
    list_group.add_argument(
        '--json', default=False, action='store_true', help='Output results as JSON'
    )
    list_group.add_argument(
        '-P', '--prefix', help='Only list archive names starting with this prefix'
    )
    list_group.add_argument(
        '-a', '--glob-archives', metavar='GLOB', help='Only list archive names matching this glob'
    )
    list_group.add_argument(
        '--successful',
        default=True,
        action='store_true',
        help='Deprecated; no effect. Newer versions of Borg shows successful (non-checkpoint) archives by default.',
    )
    list_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    list_group.add_argument(
        '--first', metavar='N', help='List first N archives after other filters are applied'
    )
    list_group.add_argument(
        '--last', metavar='N', help='List last N archives after other filters are applied'
    )
    list_group.add_argument(
        '-e', '--exclude', metavar='PATTERN', help='Exclude paths matching the pattern'
    )
    list_group.add_argument(
        '--exclude-from', metavar='FILENAME', help='Exclude paths from exclude file, one per line'
    )
    list_group.add_argument('--pattern', help='Include or exclude paths matching a pattern')
    list_group.add_argument(
        '--patterns-from',
        metavar='FILENAME',
        help='Include or exclude paths matching patterns from pattern file, one per line',
    )
    list_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    rinfo_parser = subparsers.add_parser(
        'rinfo',
        aliases=SUBPARSER_ALIASES['rinfo'],
        help='Show repository summary information such as disk space used',
        description='Show repository summary information such as disk space used',
        add_help=False,
    )
    rinfo_group = rinfo_parser.add_argument_group('rinfo arguments')
    rinfo_group.add_argument(
        '--repository',
        help='Path of repository to show info for, defaults to the configured repository if there is only one',
    )
    rinfo_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    rinfo_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    info_parser = subparsers.add_parser(
        'info',
        aliases=SUBPARSER_ALIASES['info'],
        help='Show archive summary information such as disk space used',
        description='Show archive summary information such as disk space used',
        add_help=False,
    )
    info_group = info_parser.add_argument_group('info arguments')
    info_group.add_argument(
        '--repository',
        help='Path of repository containing archive to show info for, defaults to the configured repository if there is only one',
    )
    info_group.add_argument('--archive', help='Name of archive to show info for (or "latest")')
    info_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    info_group.add_argument(
        '-P', '--prefix', help='Only show info for archive names starting with this prefix'
    )
    info_group.add_argument(
        '-a',
        '--glob-archives',
        metavar='GLOB',
        help='Only show info for archive names matching this glob',
    )
    info_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    info_group.add_argument(
        '--first',
        metavar='N',
        help='Show info for first N archives after other filters are applied',
    )
    info_group.add_argument(
        '--last', metavar='N', help='Show info for last N archives after other filters are applied'
    )
    info_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    borg_parser = subparsers.add_parser(
        'borg',
        aliases=SUBPARSER_ALIASES['borg'],
        help='Run an arbitrary Borg command',
        description='Run an arbitrary Borg command based on borgmatic\'s configuration',
        add_help=False,
    )
    borg_group = borg_parser.add_argument_group('borg arguments')
    borg_group.add_argument(
        '--repository',
        help='Path of repository to pass to Borg, defaults to the configured repositories',
    )
    borg_group.add_argument('--archive', help='Name of archive to pass to Borg (or "latest")')
    borg_group.add_argument(
        '--',
        metavar='OPTION',
        dest='options',
        nargs='+',
        help='Options to pass to Borg, command first ("create", "list", etc). "--" is optional. To specify the repository or the archive, you must use --repository or --archive instead of providing them here.',
    )
    borg_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    return top_level_parser, subparsers


def parse_arguments(*unparsed_arguments):
    '''
    Given command-line arguments with which this script was invoked, parse the arguments and return
    them as a dict mapping from subparser name (or "global") to an argparse.Namespace instance.
    '''
    top_level_parser, subparsers = make_parsers()

    arguments, remaining_arguments = parse_subparser_arguments(
        unparsed_arguments, subparsers.choices
    )
    arguments['global'] = top_level_parser.parse_args(remaining_arguments)

    if arguments['global'].excludes_filename:
        raise ValueError(
            'The --excludes flag has been replaced with exclude_patterns in configuration.'
        )

    if (
        ('list' in arguments and 'rinfo' in arguments and arguments['list'].json)
        or ('list' in arguments and 'info' in arguments and arguments['list'].json)
        or ('rinfo' in arguments and 'info' in arguments and arguments['rinfo'].json)
    ):
        raise ValueError('With the --json flag, multiple actions cannot be used together.')

    if (
        'transfer' in arguments
        and arguments['transfer'].archive
        and arguments['transfer'].glob_archives
    ):
        raise ValueError(
            'With the transfer action, only one of --archive and --glob-archives flags can be used.'
        )

    if 'info' in arguments and (
        (arguments['info'].archive and arguments['info'].prefix)
        or (arguments['info'].archive and arguments['info'].glob_archives)
        or (arguments['info'].prefix and arguments['info'].glob_archives)
    ):
        raise ValueError(
            'With the info action, only one of --archive, --prefix, or --glob-archives flags can be used.'
        )

    return arguments

if __name__ == "__main__":
    # import dill
    # import os
    isT=True
    # for l in os.listdir("/home/travis/builds/repos/witten---atticmatic/data_passk_platform/62b45665d7d32e5b55cc8363/"):
    #     f = open("/home/travis/builds/repos/witten---atticmatic/data_passk_platform/62b45665d7d32e5b55cc8363/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    res0,res1 = make_parsers()
    target0 = '''
            Simple, configuration-driven backup software for servers and workstations. If none of
            the action options are given, then borgmatic defaults to: prune, compact, create, and
            check.
            '''
    target1 = 'Create a new, empty Borg repository'
    if res0.description != target0:
        isT=False
    if res1.choices.get("init").description!=target1:
        raise Exception("Result not True!!!")





----------------------------
/home/travis/builds/repos/witten---borgmatic/borgmatic/config/load_deep_merge_nodes_passk_validte.py
import functools
import itertools
import json
import logging
import operator
import os

import ruamel.yaml
from yaml import ScalarNode, MappingNode

logger = logging.getLogger(__name__)


def probe_and_include_file(filename, include_directories):
    '''
    Given a filename to include and a list of include directories to search for matching files,
    probe for the file, load it, and return the loaded configuration as a data structure of nested
    dicts, lists, etc.

    Raise FileNotFoundError if the included file was not found.
    '''
    expanded_filename = os.path.expanduser(filename)

    if os.path.isabs(expanded_filename):
        return load_configuration(expanded_filename)

    candidate_filenames = {
        os.path.join(directory, expanded_filename) for directory in include_directories
    }

    for candidate_filename in candidate_filenames:
        if os.path.exists(candidate_filename):
            return load_configuration(candidate_filename)

    raise FileNotFoundError(
        f'Could not find include {filename} at {" or ".join(candidate_filenames)}'
    )


def include_configuration(loader, filename_node, include_directory):
    '''
    Given a ruamel.yaml.loader.Loader, a ruamel.yaml.nodes.ScalarNode containing the included
    filename (or a list containing multiple such filenames), and an include directory path to search
    for matching files, load the given YAML filenames (ignoring the given loader so we can use our
    own) and return their contents as data structure of nested dicts, lists, etc. If the given
    filename node's value is a scalar string, then the return value will be a single value. But if
    the given node value is a list, then the return value will be a list of values, one per loaded
    configuration file.

    If a filename is relative, probe for it within 1. the current working directory and 2. the given
    include directory.

    Raise FileNotFoundError if an included file was not found.
    '''
    include_directories = [os.getcwd(), os.path.abspath(include_directory)]

    if isinstance(filename_node.value, str):
        return probe_and_include_file(filename_node.value, include_directories)

    if (
        isinstance(filename_node.value, list)
        and len(filename_node.value)
        and isinstance(filename_node.value[0], ruamel.yaml.nodes.ScalarNode)
    ):
        # Reversing the values ensures the correct ordering if these includes are subsequently
        # merged together.
        return [
            probe_and_include_file(node.value, include_directories)
            for node in reversed(filename_node.value)
        ]

    raise ValueError(
        '!include value is not supported; use a single filename or a list of filenames'
    )


def raise_retain_node_error(loader, node):
    '''
    Given a ruamel.yaml.loader.Loader and a YAML node, raise an error about "!retain" usage.

    Raise ValueError if a mapping or sequence node is given, as that indicates that "!retain" was
    used in a configuration file without a merge. In configuration files with a merge, mapping and
    sequence nodes with "!retain" tags are handled by deep_merge_nodes() below.

    Also raise ValueError if a scalar node is given, as "!retain" is not supported on scalar nodes.
    '''
    if isinstance(node, (ruamel.yaml.nodes.MappingNode, ruamel.yaml.nodes.SequenceNode)):
        raise ValueError(
            'The !retain tag may only be used within a configuration file containing a merged !include tag.'
        )

    raise ValueError('The !retain tag may only be used on a mapping or list.')


def raise_omit_node_error(loader, node):
    '''
    Given a ruamel.yaml.loader.Loader and a YAML node, raise an error about "!omit" usage.

    Raise ValueError unconditionally, as an "!omit" node here indicates it was used in a
    configuration file without a merge. In configuration files with a merge, nodes with "!omit"
    tags are handled by deep_merge_nodes() below.
    '''
    raise ValueError(
        'The !omit tag may only be used on a scalar (e.g., string) or list element within a configuration file containing a merged !include tag.'
    )


class Include_constructor(ruamel.yaml.SafeConstructor):
    '''
    A YAML "constructor" (a ruamel.yaml concept) that supports a custom "!include" tag for including
    separate YAML configuration files. Example syntax: `option: !include common.yaml`
    '''

    def __init__(self, preserve_quotes=None, loader=None, include_directory=None):
        super(Include_constructor, self).__init__(preserve_quotes, loader)
        self.add_constructor(
            '!include',
            functools.partial(include_configuration, include_directory=include_directory),
        )

        # These are catch-all error handlers for tags that don't get applied and removed by
        # deep_merge_nodes() below.
        self.add_constructor('!retain', raise_retain_node_error)
        self.add_constructor('!omit', raise_omit_node_error)

    def flatten_mapping(self, node):
        '''
        Support the special case of deep merging included configuration into an existing mapping
        using the YAML '<<' merge key. Example syntax:

        ```
        option:
            sub_option: 1

        <<: !include common.yaml
        ```

        These includes are deep merged into the current configuration file. For instance, in this
        example, any "option" with sub-options in common.yaml will get merged into the corresponding
        "option" with sub-options in the example configuration file.
        '''
        representer = ruamel.yaml.representer.SafeRepresenter()

        for index, (key_node, value_node) in enumerate(node.value):
            if key_node.tag == u'tag:yaml.org,2002:merge' and value_node.tag == '!include':
                # Replace the merge include with a sequence of included configuration nodes ready
                # for merging. The construct_object() call here triggers include_configuration()
                # among other constructors.
                node.value[index] = (
                    key_node,
                    representer.represent_data(self.construct_object(value_node)),
                )

        # This super().flatten_mapping() call actually performs "<<" merges.
        super(Include_constructor, self).flatten_mapping(node)

        node.value = deep_merge_nodes(node.value)


def load_configuration(filename):
    '''
    Load the given configuration file and return its contents as a data structure of nested dicts
    and lists. Also, replace any "{constant}" strings with the value of the "constant" key in the
    "constants" option of the configuration file.

    Raise ruamel.yaml.error.YAMLError if something goes wrong parsing the YAML, or RecursionError
    if there are too many recursive includes.
    '''

    # Use an embedded derived class for the include constructor so as to capture the filename
    # value. (functools.partial doesn't work for this use case because yaml.Constructor has to be
    # an actual class.)
    class Include_constructor_with_include_directory(Include_constructor):
        def __init__(self, preserve_quotes=None, loader=None):
            super(Include_constructor_with_include_directory, self).__init__(
                preserve_quotes, loader, include_directory=os.path.dirname(filename)
            )

    yaml = ruamel.yaml.YAML(typ='safe')
    yaml.Constructor = Include_constructor_with_include_directory

    with open(filename) as file:
        file_contents = file.read()
        config = yaml.load(file_contents)

        try:
            has_constants = bool(config and 'constants' in config)
        except TypeError:
            has_constants = False

        if has_constants:
            for key, value in config['constants'].items():
                value = json.dumps(value)
                file_contents = file_contents.replace(f'{{{key}}}', value.strip('"'))

            config = yaml.load(file_contents)
            del config['constants']

        return config


def filter_omitted_nodes(nodes, values):
    '''
    Given a nested borgmatic configuration data structure as a list of tuples in the form of:

    [
        (
            ruamel.yaml.nodes.ScalarNode as a key,
            ruamel.yaml.nodes.MappingNode or other Node as a value,
        ),
        ...
    ]

    ... and a combined list of all values for those nodes, return a filtered list of the values,
    omitting any that have an "!omit" tag (or with a value matching such nodes).

    But if only a single node is given, bail and return the given values unfiltered, as "!omit" only
    applies when there are merge includes (and therefore multiple nodes).
    '''
    if len(nodes) <= 1:
        return values

    omitted_values = tuple(node.value for node in values if node.tag == '!omit')

    return [node for node in values if node.value not in omitted_values]


def merge_values(nodes):
    '''
    Given a nested borgmatic configuration data structure as a list of tuples in the form of:

    [
        (
            ruamel.yaml.nodes.ScalarNode as a key,
            ruamel.yaml.nodes.MappingNode or other Node as a value,
        ),
        ...
    ]

    ... merge its sequence or mapping node values and return the result. For sequence nodes, this
    means appending together its contained lists. For mapping nodes, it means merging its contained
    dicts.
    '''
    return functools.reduce(operator.add, (value.value for key, value in nodes))


def deep_merge_nodes(nodes):
    '''
    Given a nested borgmatic configuration data structure as a list of tuples in the form of:

    [
        (
            ruamel.yaml.nodes.ScalarNode as a key,
            ruamel.yaml.nodes.MappingNode or other Node as a value,
        ),
        ...
    ]

    ... deep merge any node values corresponding to duplicate keys and return the result. The
    purpose of merging like this is to support, for instance, merging one borgmatic configuration
    file into another for reuse, such that a configuration option with sub-options does not
    completely replace the corresponding option in a merged file.

    If there are colliding keys with scalar values (e.g., integers or strings), the last of the
    values wins.

    For instance, given node values of:

        [
            (
                ScalarNode(tag='tag:yaml.org,2002:str', value='option'),
                MappingNode(tag='tag:yaml.org,2002:map', value=[
                    (
                        ScalarNode(tag='tag:yaml.org,2002:str', value='sub_option1'),
                        ScalarNode(tag='tag:yaml.org,2002:int', value='1')
                    ),
                    (
                        ScalarNode(tag='tag:yaml.org,2002:str', value='sub_option2'),
                        ScalarNode(tag='tag:yaml.org,2002:int', value='2')
                    ),
                ]),
            ),
            (
                ScalarNode(tag='tag:yaml.org,2002:str', value='option'),
                MappingNode(tag='tag:yaml.org,2002:map', value=[
                    (
                        ScalarNode(tag='tag:yaml.org,2002:str', value='sub_option2'),
                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')
                    ),
                ]),
            ),
        ]

    ... the returned result would be:

        [
            (
                ScalarNode(tag='tag:yaml.org,2002:str', value='option'),
                MappingNode(tag='tag:yaml.org,2002:map', value=[
                    (
                        ScalarNode(tag='tag:yaml.org,2002:str', value='sub_option1'),
                        ScalarNode(tag='tag:yaml.org,2002:int', value='1')
                    ),
                    (
                        ScalarNode(tag='tag:yaml.org,2002:str', value='sub_option2'),
                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')
                    ),
                ]),
            ),
        ]

    This function supports multi-way merging, meaning that if the same option name exists three or
    more times (at the same scope level), all of those instances get merged together.

    If a mapping or sequence node has a YAML "!retain" tag, then that node is not merged.

    Raise ValueError if a merge is implied using multiple incompatible types.
    '''
    merged_nodes = []

    def get_node_key_name(node):
        return node[0].value

    # Bucket the nodes by their keys. Then merge all of the values sharing the same key.
    for key_name, grouped_nodes in itertools.groupby(
        sorted(nodes, key=get_node_key_name), get_node_key_name
    ):
        grouped_nodes = list(grouped_nodes)

        # The merged node inherits its attributes from the final node in the group.
        (last_node_key, last_node_value) = grouped_nodes[-1]
        value_types = set(type(value) for (_, value) in grouped_nodes)

        if len(value_types) > 1:
            raise ValueError(
                f'Incompatible types found when trying to merge "{key_name}:" values across configuration files: {", ".join(value_type.id for value_type in value_types)}'
            )

        # If we're dealing with MappingNodes, recurse and merge its values as well.
        if ruamel.yaml.nodes.MappingNode in value_types:
            # A "!retain" tag says to skip deep merging for this node. Replace the tag so
            # downstream schema validation doesn't break on our application-specific tag.
            if last_node_value.tag == '!retain' and len(grouped_nodes) > 1:
                last_node_value.tag = 'tag:yaml.org,2002:map'
                merged_nodes.append((last_node_key, last_node_value))
            else:
                merged_nodes.append(
                    (
                        last_node_key,
                        ruamel.yaml.nodes.MappingNode(
                            tag=last_node_value.tag,
                            value=deep_merge_nodes(merge_values(grouped_nodes)),
                            start_mark=last_node_value.start_mark,
                            end_mark=last_node_value.end_mark,
                            flow_style=last_node_value.flow_style,
                            comment=last_node_value.comment,
                            anchor=last_node_value.anchor,
                        ),
                    )
                )

            continue

        # If we're dealing with SequenceNodes, merge by appending sequences together.
        if ruamel.yaml.nodes.SequenceNode in value_types:
            if last_node_value.tag == '!retain' and len(grouped_nodes) > 1:
                last_node_value.tag = 'tag:yaml.org,2002:seq'
                merged_nodes.append((last_node_key, last_node_value))
            else:
                merged_nodes.append(
                    (
                        last_node_key,
                        ruamel.yaml.nodes.SequenceNode(
                            tag=last_node_value.tag,
                            value=filter_omitted_nodes(grouped_nodes, merge_values(grouped_nodes)),
                            start_mark=last_node_value.start_mark,
                            end_mark=last_node_value.end_mark,
                            flow_style=last_node_value.flow_style,
                            comment=last_node_value.comment,
                            anchor=last_node_value.anchor,
                        ),
                    )
                )

            continue

        merged_nodes.append((last_node_key, last_node_value))

    return merged_nodes

if __name__ == "__main__":
    input=[
            (
                ScalarNode(tag='tag:yaml.org,2002:str', value='option'),
                MappingNode(tag='tag:yaml.org,2002:map', value=[
                    (
                        ScalarNode(tag='tag:yaml.org,2002:str', value='sub_option1'),
                        ScalarNode(tag='tag:yaml.org,2002:int', value='1')
                    ),
                    (
                        ScalarNode(tag='tag:yaml.org,2002:str', value='sub_option2'),
                        ScalarNode(tag='tag:yaml.org,2002:int', value='2')
                    ),
                ]),
            ),
            (
                ScalarNode(tag='tag:yaml.org,2002:str', value='option'),
                MappingNode(tag='tag:yaml.org,2002:map', value=[
                    (
                        ScalarNode(tag='tag:yaml.org,2002:str', value='sub_option2'),
                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')
                    ),
                ]),
            ),
        ]
    output=[(ScalarNode(tag='tag:yaml.org,2002:str', value='option'), MappingNode(tag='tag:yaml.org,2002:map', value=[(ScalarNode(tag='tag:yaml.org,2002:str', value='sub_option2'), ScalarNode(tag='tag:yaml.org,2002:int', value='5'))]))]

    isT=str(deep_merge_nodes(input))==str(output)
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/witten---borgmatic/borgmatic/commands/generate_config_parse_arguments_passk_validte.py
import sys
from argparse import ArgumentParser, Namespace

sys.path.remove('/home/travis/builds/repos/witten---borgmatic/borgmatic/commands')
sys.path.append("/home/travis/builds/repos/witten---borgmatic")

from borgmatic.config import generate, validate

DEFAULT_DESTINATION_CONFIG_FILENAME = '/etc/borgmatic/config.yaml'


def parse_arguments(*arguments):
    '''
    Given command-line arguments with which this script was invoked, parse the arguments and return
    them as an ArgumentParser instance.
    '''
    parser = ArgumentParser(description='Generate a sample borgmatic YAML configuration file.')
    parser.add_argument(
        '-s',
        '--source',
        dest='source_filename',
        help='Optional YAML configuration file to merge into the generated configuration, useful for upgrading your configuration',
    )
    parser.add_argument(
        '-d',
        '--destination',
        dest='destination_filename',
        default=DEFAULT_DESTINATION_CONFIG_FILENAME,
        help='Destination YAML configuration file, default: {}'.format(
            DEFAULT_DESTINATION_CONFIG_FILENAME
        ),
    )
    parser.add_argument(
        '--overwrite',
        default=False,
        action='store_true',
        help='Whether to overwrite any existing destination file, defaults to false',
    )

    return parser.parse_args(arguments)


def main():  # pragma: no cover
    try:
        args = parse_arguments(*sys.argv[1:])

        generate.generate_sample_configuration(
            args.source_filename,
            args.destination_filename,
            validate.schema_filename(),
            overwrite=args.overwrite,
        )

        print('Generated a sample configuration file at {}.'.format(args.destination_filename))
        print()
        if args.source_filename:
            print(
                'Merged in the contents of configuration file at {}.'.format(args.source_filename)
            )
            print('To review the changes made, run:')
            print()
            print(
                '    diff --unified {} {}'.format(args.source_filename, args.destination_filename)
            )
            print()
        print('Please edit the file to suit your needs. The values are representative.')
        print('All fields are optional except where indicated.')
        print()
        print('If you ever need help: https://torsion.org/borgmatic/#issues')
    except (ValueError, OSError) as error:
        print(error, file=sys.stderr)
        sys.exit(1)





if __name__ == "__main__":
    out=Namespace(source_filename=None, destination_filename='/etc/borgmatic/config.yaml', overwrite=False)
    isT=parse_arguments()==out
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/witten---borgmatic/borgmatic/commands/completion_parser_flags_passk_validte.py
import argparse
import sys
from argparse import ArgumentParser

sys.path.remove('/home/travis/builds/repos/witten---borgmatic/borgmatic/commands')
sys.path.append("/home/travis/builds/repos/witten---borgmatic")
from borgmatic.commands import arguments

UPGRADE_MESSAGE = '''
Your bash completions script is from a different version of borgmatic than is
currently installed. Please upgrade your script so your completions match the
command-line flags in your installed borgmatic! Try this to upgrade:

    sudo sh -c "borgmatic --bash-completion > $BASH_SOURCE"
    source $BASH_SOURCE
'''


def parser_flags(parser):
    '''
    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated
    string.
    '''
    return ' '.join(option for action in parser._actions for option in action.option_strings)


def bash_completion():
    '''
    Return a bash completion script for the borgmatic command. Produce this by introspecting
    borgmatic's command-line argument parsers.
    '''
    top_level_parser, subparsers = arguments.make_parsers()
    global_flags = parser_flags(top_level_parser)
    actions = ' '.join(subparsers.choices.keys())

    # Avert your eyes.
    return '\n'.join(
        (
            'check_version() {',
            '    local this_script="$(cat "$BASH_SOURCE" 2> /dev/null)"',
            '    local installed_script="$(borgmatic --bash-completion 2> /dev/null)"',
            '    if [ "$this_script" != "$installed_script" ] && [ "$installed_script" != "" ];'
            '        then cat << EOF\n%s\nEOF' % UPGRADE_MESSAGE,
            '    fi',
            '}',
            'complete_borgmatic() {',
        )
        + tuple(
            '''    if [[ " ${COMP_WORDS[*]} " =~ " %s " ]]; then
        COMPREPLY=($(compgen -W "%s %s %s" -- "${COMP_WORDS[COMP_CWORD]}"))
        return 0
    fi'''
            % (action, parser_flags(subparser), actions, global_flags)
            for action, subparser in subparsers.choices.items()
        )
        + (
            '    COMPREPLY=($(compgen -W "%s %s" -- "${COMP_WORDS[COMP_CWORD]}"))'
            % (actions, global_flags),
            '    (check_version &)',
            '}',
            '\ncomplete -o bashdefault -o default -F complete_borgmatic borgmatic',
        )
    )

if __name__ == "__main__":
    isT=True
    arg1=ArgumentParser(prog='pytest', usage=None, description='\n            Simple, configuration-driven backup software for servers and workstations. If none of\n            the action options are given, then borgmatic defaults to: prune, compact, create, and\n            check.\n            ', conflict_handler='error', add_help=True)

    arg2=ArgumentParser(prog='pytest init', usage=None, description='Initialize an empty Borg repository',  conflict_handler='error', add_help=False)
    ist1=parser_flags(arg1)=="-h --help"
    ist2=parser_flags(arg2)==""
    if not ist1 or not ist2:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/witten---borgmatic/borgmatic/commands/completion_bash_completion_passk_validte.py
import sys
# sys.path.append("C:\\Users\yh199\Downloads\witten---borgmatic\witten---borgmatic\\borgmatic.egg-info")
sys.path.remove('/home/travis/builds/repos/witten---borgmatic/borgmatic/commands')
sys.path.append("/home/travis/builds/repos/witten---borgmatic")
from borgmatic.commands import arguments

UPGRADE_MESSAGE = '''
Your bash completions script is from a different version of borgmatic than is
currently installed. Please upgrade your script so your completions match the
command-line flags in your installed borgmatic! Try this to upgrade:

    sudo sh -c "borgmatic --bash-completion > $BASH_SOURCE"
    source $BASH_SOURCE
'''


def parser_flags(parser):
    '''
    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated
    string.
    '''
    return ' '.join(option for action in parser._actions for option in action.option_strings)


def bash_completion():
    '''
    Return a bash completion script for the borgmatic command. Produce this by introspecting
    borgmatic's command-line argument parsers.
    '''
    top_level_parser, subparsers,_ = arguments.make_parsers()
    global_flags = parser_flags(top_level_parser)
    actions = ' '.join(subparsers.choices.keys())

    # Avert your eyes.
    return '\n'.join(
        (
            'check_version() {',
            '    local this_script="$(cat "$BASH_SOURCE" 2> /dev/null)"',
            '    local installed_script="$(borgmatic --bash-completion 2> /dev/null)"',
            '    if [ "$this_script" != "$installed_script" ] && [ "$installed_script" != "" ];'
            '        then cat << EOF\n%s\nEOF' % UPGRADE_MESSAGE,
            '    fi',
            '}',
            'complete_borgmatic() {',
        )
        + tuple(
            '''    if [[ " ${COMP_WORDS[*]} " =~ " %s " ]]; then
        COMPREPLY=($(compgen -W "%s %s %s" -- "${COMP_WORDS[COMP_CWORD]}"))
        return 0
    fi'''
            % (action, parser_flags(subparser), actions, global_flags)
            for action, subparser in subparsers.choices.items()
        )
        + (
            '    COMPREPLY=($(compgen -W "%s %s" -- "${COMP_WORDS[COMP_CWORD]}"))'
            % (actions, global_flags),
            '    (check_version &)',
            '}',
            '\ncomplete -o bashdefault -o default -F complete_borgmatic borgmatic',
        )
    )

if __name__ == "__main__":
    res=bash_completion()
    # f=open("/home/travis/builds/repos/witten---borgmatic/borgmatic/commands/res.txt",'w',encoding="utf-8")
    # content=f.write(res)
    # f.close()
    f = open("/home/travis/builds/repos/witten---borgmatic/borgmatic/commands/res.txt", 'r', encoding="utf-8")
    content_out = f.read()
    f.close()
    isT=content_out==res
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/witten---borgmatic/borgmatic/commands/arguments_parse_arguments_passk_validte.py
import collections
from argparse import Action, ArgumentParser, Namespace
import sys
sys.path.remove('/home/travis/builds/repos/witten---borgmatic/borgmatic/commands')
sys.path.append("/home/travis/builds/repos/witten---borgmatic")

from borgmatic.config import collect

SUBPARSER_ALIASES = {
    'init': ['--init', '-I'],
    'prune': ['--prune', '-p'],
    'compact': [],
    'create': ['--create', '-C'],
    'check': ['--check', '-k'],
    'extract': ['--extract', '-x'],
    'export-tar': ['--export-tar'],
    'mount': ['--mount', '-m'],
    'umount': ['--umount', '-u'],
    'restore': ['--restore', '-r'],
    'list': ['--list', '-l'],
    'info': ['--info', '-i'],
    'borg': [],
}


def parse_subparser_arguments(unparsed_arguments, subparsers):
    '''
    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser
    instance, give each requested action's subparser a shot at parsing all arguments. This allows
    common arguments like "--repository" to be shared across multiple subparsers.

    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of
    arguments, a list of remaining arguments not claimed by any subparser).
    '''
    arguments = collections.OrderedDict()
    remaining_arguments = list(unparsed_arguments)
    alias_to_subparser_name = {
        alias: subparser_name
        for subparser_name, aliases in SUBPARSER_ALIASES.items()
        for alias in aliases
    }

    # If the "borg" action is used, skip all other subparsers. This avoids confusion like
    # "borg list" triggering borgmatic's own list action.
    if 'borg' in unparsed_arguments:
        subparsers = {'borg': subparsers['borg']}

    for subparser_name, subparser in subparsers.items():
        if subparser_name not in remaining_arguments:
            continue

        canonical_name = alias_to_subparser_name.get(subparser_name, subparser_name)

        # If a parsed value happens to be the same as the name of a subparser, remove it from the
        # remaining arguments. This prevents, for instance, "check --only extract" from triggering
        # the "extract" subparser.
        parsed, unused_remaining = subparser.parse_known_args(unparsed_arguments)
        for value in vars(parsed).values():
            if isinstance(value, str):
                if value in subparsers:
                    remaining_arguments.remove(value)
            elif isinstance(value, list):
                for item in value:
                    if item in subparsers:
                        remaining_arguments.remove(item)

        arguments[canonical_name] = parsed

    # If no actions are explicitly requested, assume defaults: prune, compact, create, and check.
    if not arguments and '--help' not in unparsed_arguments and '-h' not in unparsed_arguments:
        for subparser_name in ('prune', 'compact', 'create', 'check'):
            subparser = subparsers[subparser_name]
            parsed, unused_remaining = subparser.parse_known_args(unparsed_arguments)
            arguments[subparser_name] = parsed

    remaining_arguments = list(unparsed_arguments)

    # Now ask each subparser, one by one, to greedily consume arguments.
    for subparser_name, subparser in subparsers.items():
        if subparser_name not in arguments.keys():
            continue

        subparser = subparsers[subparser_name]
        unused_parsed, remaining_arguments = subparser.parse_known_args(remaining_arguments)

    # Special case: If "borg" is present in the arguments, consume all arguments after (+1) the
    # "borg" action.
    if 'borg' in arguments:
        borg_options_index = remaining_arguments.index('borg') + 1
        arguments['borg'].options = remaining_arguments[borg_options_index:]
        remaining_arguments = remaining_arguments[:borg_options_index]

    # Remove the subparser names themselves.
    for subparser_name, subparser in subparsers.items():
        if subparser_name in remaining_arguments:
            remaining_arguments.remove(subparser_name)

    return (arguments, remaining_arguments)


class Extend_action(Action):
    '''
    An argparse action to support Python 3.8's "extend" action in older versions of Python.
    '''

    def __call__(self, parser, namespace, values, option_string=None):
        items = getattr(namespace, self.dest, None)

        if items:
            items.extend(values)
        else:
            setattr(namespace, self.dest, list(values))


def make_parsers():
    '''
    Build a top-level parser and its subparsers and return them as a tuple.
    '''
    config_paths = collect.get_default_config_paths(expand_home=True)
    unexpanded_config_paths = collect.get_default_config_paths(expand_home=False)

    global_parser = ArgumentParser(add_help=False)
    global_parser.register('action', 'extend', Extend_action)
    global_group = global_parser.add_argument_group('global arguments')

    global_group.add_argument(
        '-c',
        '--config',
        nargs='*',
        dest='config_paths',
        default=config_paths,
        help='Configuration filenames or directories, defaults to: {}'.format(
            ' '.join(unexpanded_config_paths)
        ),
    )
    global_group.add_argument(
        '--excludes',
        dest='excludes_filename',
        help='Deprecated in favor of exclude_patterns within configuration',
    )
    global_group.add_argument(
        '-n',
        '--dry-run',
        dest='dry_run',
        action='store_true',
        help='Go through the motions, but do not actually write to any repositories',
    )
    global_group.add_argument(
        '-nc', '--no-color', dest='no_color', action='store_true', help='Disable colored output'
    )
    global_group.add_argument(
        '-v',
        '--verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Display verbose progress to the console (from only errors to very verbose: -1, 0, 1, or 2)',
    )
    global_group.add_argument(
        '--syslog-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to syslog (from only errors to very verbose: -1, 0, 1, or 2). Ignored when console is interactive or --log-file is given',
    )
    global_group.add_argument(
        '--log-file-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to log file (from only errors to very verbose: -1, 0, 1, or 2). Only used when --log-file is given',
    )
    global_group.add_argument(
        '--monitoring-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to monitoring integrations that support logging (from only errors to very verbose: -1, 0, 1, or 2)',
    )
    global_group.add_argument(
        '--log-file',
        type=str,
        default=None,
        help='Write log messages to this file instead of syslog',
    )
    global_group.add_argument(
        '--override',
        metavar='SECTION.OPTION=VALUE',
        nargs='+',
        dest='overrides',
        action='extend',
        help='One or more configuration file options to override with specified values',
    )
    global_group.add_argument(
        '--no-environment-interpolation',
        dest='resolve_env',
        action='store_false',
        help='Do not resolve environment variables in configuration file',
    )
    global_group.add_argument(
        '--bash-completion',
        default=False,
        action='store_true',
        help='Show bash completion script and exit',
    )
    global_group.add_argument(
        '--version',
        dest='version',
        default=False,
        action='store_true',
        help='Display installed version number of borgmatic and exit',
    )

    top_level_parser = ArgumentParser(
        description='''
            Simple, configuration-driven backup software for servers and workstations. If none of
            the action options are given, then borgmatic defaults to: prune, compact, create, and
            check.
            ''',
        parents=[global_parser],
    )

    subparsers = top_level_parser.add_subparsers(
        title='actions',
        metavar='',
        help='Specify zero or more actions. Defaults to prune, compact, create, and check. Use --help with action for details:',
    )
    init_parser = subparsers.add_parser(
        'init',
        aliases=SUBPARSER_ALIASES['init'],
        help='Initialize an empty Borg repository',
        description='Initialize an empty Borg repository',
        add_help=False,
    )
    init_group = init_parser.add_argument_group('init arguments')
    init_group.add_argument(
        '-e',
        '--encryption',
        dest='encryption_mode',
        help='Borg repository encryption mode',
        required=True,
    )
    init_group.add_argument(
        '--append-only',
        dest='append_only',
        action='store_true',
        help='Create an append-only repository',
    )
    init_group.add_argument(
        '--storage-quota',
        dest='storage_quota',
        help='Create a repository with a fixed storage quota',
    )
    init_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    prune_parser = subparsers.add_parser(
        'prune',
        aliases=SUBPARSER_ALIASES['prune'],
        help='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
        description='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
        add_help=False,
    )
    prune_group = prune_parser.add_argument_group('prune arguments')
    prune_group.add_argument(
        '--stats',
        dest='stats',
        default=False,
        action='store_true',
        help='Display statistics of archive',
    )
    prune_group.add_argument(
        '--files', dest='files', default=False, action='store_true', help='Show per-file details'
    )
    prune_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    compact_parser = subparsers.add_parser(
        'compact',
        aliases=SUBPARSER_ALIASES['compact'],
        help='Compact segments to free space (Borg 1.2+ only)',
        description='Compact segments to free space (Borg 1.2+ only)',
        add_help=False,
    )
    compact_group = compact_parser.add_argument_group('compact arguments')
    compact_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress as each segment is compacted',
    )
    compact_group.add_argument(
        '--cleanup-commits',
        dest='cleanup_commits',
        default=False,
        action='store_true',
        help='Cleanup commit-only 17-byte segment files left behind by Borg 1.1',
    )
    compact_group.add_argument(
        '--threshold',
        type=int,
        dest='threshold',
        help='Minimum saved space percentage threshold for compacting a segment, defaults to 10',
    )
    compact_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    create_parser = subparsers.add_parser(
        'create',
        aliases=SUBPARSER_ALIASES['create'],
        help='Create archives (actually perform backups)',
        description='Create archives (actually perform backups)',
        add_help=False,
    )
    create_group = create_parser.add_argument_group('create arguments')
    create_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is backed up',
    )
    create_group.add_argument(
        '--stats',
        dest='stats',
        default=False,
        action='store_true',
        help='Display statistics of archive',
    )
    create_group.add_argument(
        '--files', dest='files', default=False, action='store_true', help='Show per-file details'
    )
    create_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    create_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    check_parser = subparsers.add_parser(
        'check',
        aliases=SUBPARSER_ALIASES['check'],
        help='Check archives for consistency',
        description='Check archives for consistency',
        add_help=False,
    )
    check_group = check_parser.add_argument_group('check arguments')
    check_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is checked',
    )
    check_group.add_argument(
        '--repair',
        dest='repair',
        default=False,
        action='store_true',
        help='Attempt to repair any inconsistencies found (for interactive use)',
    )
    check_group.add_argument(
        '--only',
        metavar='CHECK',
        choices=('repository', 'archives', 'data', 'extract'),
        dest='only',
        action='append',
        help='Run a particular consistency check (repository, archives, data, or extract) instead of configured checks (subject to configured frequency, can specify flag multiple times)',
    )
    check_group.add_argument(
        '--force',
        default=False,
        action='store_true',
        help='Ignore configured check frequencies and run checks unconditionally',
    )
    check_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    extract_parser = subparsers.add_parser(
        'extract',
        aliases=SUBPARSER_ALIASES['extract'],
        help='Extract files from a named archive to the current directory',
        description='Extract a named archive to the current directory',
        add_help=False,
    )
    extract_group = extract_parser.add_argument_group('extract arguments')
    extract_group.add_argument(
        '--repository',
        help='Path of repository to extract, defaults to the configured repository if there is only one',
    )
    extract_group.add_argument(
        '--archive', help='Name of archive to extract (or "latest")', required=True
    )
    extract_group.add_argument(
        '--path',
        '--restore-path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to extract from archive, defaults to the entire archive',
    )
    extract_group.add_argument(
        '--destination',
        metavar='PATH',
        dest='destination',
        help='Directory to extract files into, defaults to the current directory',
    )
    extract_group.add_argument(
        '--strip-components',
        type=int,
        metavar='NUMBER',
        dest='strip_components',
        help='Number of leading path components to remove from each extracted path. Skip paths with fewer elements',
    )
    extract_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is extracted',
    )
    extract_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    export_tar_parser = subparsers.add_parser(
        'export-tar',
        aliases=SUBPARSER_ALIASES['export-tar'],
        help='Export an archive to a tar-formatted file or stream',
        description='Export an archive to a tar-formatted file or stream',
        add_help=False,
    )
    export_tar_group = export_tar_parser.add_argument_group('export-tar arguments')
    export_tar_group.add_argument(
        '--repository',
        help='Path of repository to export from, defaults to the configured repository if there is only one',
    )
    export_tar_group.add_argument(
        '--archive', help='Name of archive to export (or "latest")', required=True
    )
    export_tar_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to export from archive, defaults to the entire archive',
    )
    export_tar_group.add_argument(
        '--destination',
        metavar='PATH',
        dest='destination',
        help='Path to destination export tar file, or "-" for stdout (but be careful about dirtying output with --verbosity or --files)',
        required=True,
    )
    export_tar_group.add_argument(
        '--tar-filter', help='Name of filter program to pipe data through'
    )
    export_tar_group.add_argument(
        '--files', default=False, action='store_true', help='Show per-file details'
    )
    export_tar_group.add_argument(
        '--strip-components',
        type=int,
        metavar='NUMBER',
        dest='strip_components',
        help='Number of leading path components to remove from each exported path. Skip paths with fewer elements',
    )
    export_tar_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    mount_parser = subparsers.add_parser(
        'mount',
        aliases=SUBPARSER_ALIASES['mount'],
        help='Mount files from a named archive as a FUSE filesystem',
        description='Mount a named archive as a FUSE filesystem',
        add_help=False,
    )
    mount_group = mount_parser.add_argument_group('mount arguments')
    mount_group.add_argument(
        '--repository',
        help='Path of repository to use, defaults to the configured repository if there is only one',
    )
    mount_group.add_argument('--archive', help='Name of archive to mount (or "latest")')
    mount_group.add_argument(
        '--mount-point',
        metavar='PATH',
        dest='mount_point',
        help='Path where filesystem is to be mounted',
        required=True,
    )
    mount_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to mount from archive, defaults to the entire archive',
    )
    mount_group.add_argument(
        '--foreground',
        dest='foreground',
        default=False,
        action='store_true',
        help='Stay in foreground until ctrl-C is pressed',
    )
    mount_group.add_argument('--options', dest='options', help='Extra Borg mount options')
    mount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    umount_parser = subparsers.add_parser(
        'umount',
        aliases=SUBPARSER_ALIASES['umount'],
        help='Unmount a FUSE filesystem that was mounted with "borgmatic mount"',
        description='Unmount a mounted FUSE filesystem',
        add_help=False,
    )
    umount_group = umount_parser.add_argument_group('umount arguments')
    umount_group.add_argument(
        '--mount-point',
        metavar='PATH',
        dest='mount_point',
        help='Path of filesystem to unmount',
        required=True,
    )
    umount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    restore_parser = subparsers.add_parser(
        'restore',
        aliases=SUBPARSER_ALIASES['restore'],
        help='Restore database dumps from a named archive',
        description='Restore database dumps from a named archive. (To extract files instead, use "borgmatic extract".)',
        add_help=False,
    )
    restore_group = restore_parser.add_argument_group('restore arguments')
    restore_group.add_argument(
        '--repository',
        help='Path of repository to restore from, defaults to the configured repository if there is only one',
    )
    restore_group.add_argument(
        '--archive', help='Name of archive to restore from (or "latest")', required=True
    )
    restore_group.add_argument(
        '--database',
        metavar='NAME',
        nargs='+',
        dest='databases',
        help='Names of databases to restore from archive, defaults to all databases. Note that any databases to restore must be defined in borgmatic\'s configuration',
    )
    restore_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    list_parser = subparsers.add_parser(
        'list',
        aliases=SUBPARSER_ALIASES['list'],
        help='List archives',
        description='List archives or the contents of an archive',
        add_help=False,
    )
    list_group = list_parser.add_argument_group('list arguments')
    list_group.add_argument(
        '--repository', help='Path of repository to list, defaults to the configured repositories',
    )
    list_group.add_argument('--archive', help='Name of archive to list (or "latest")')
    list_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths or patterns to list from a single selected archive (via "--archive"), defaults to listing the entire archive',
    )
    list_group.add_argument(
        '--find',
        metavar='PATH',
        nargs='+',
        dest='find_paths',
        help='Partial paths or patterns to search for and list across multiple archives',
    )
    list_group.add_argument(
        '--short', default=False, action='store_true', help='Output only archive or path names'
    )
    list_group.add_argument('--format', help='Format for file listing')
    list_group.add_argument(
        '--json', default=False, action='store_true', help='Output results as JSON'
    )
    list_group.add_argument(
        '-P', '--prefix', help='Only list archive names starting with this prefix'
    )
    list_group.add_argument(
        '-a', '--glob-archives', metavar='GLOB', help='Only list archive names matching this glob'
    )
    list_group.add_argument(
        '--successful',
        default=True,
        action='store_true',
        help='Deprecated in favor of listing successful (non-checkpoint) backups by default in newer versions of Borg',
    )
    list_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    list_group.add_argument(
        '--first', metavar='N', help='List first N archives after other filters are applied'
    )
    list_group.add_argument(
        '--last', metavar='N', help='List last N archives after other filters are applied'
    )
    list_group.add_argument(
        '-e', '--exclude', metavar='PATTERN', help='Exclude paths matching the pattern'
    )
    list_group.add_argument(
        '--exclude-from', metavar='FILENAME', help='Exclude paths from exclude file, one per line'
    )
    list_group.add_argument('--pattern', help='Include or exclude paths matching a pattern')
    list_group.add_argument(
        '--patterns-from',
        metavar='FILENAME',
        help='Include or exclude paths matching patterns from pattern file, one per line',
    )
    list_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    info_parser = subparsers.add_parser(
        'info',
        aliases=SUBPARSER_ALIASES['info'],
        help='Display summary information on archives',
        description='Display summary information on archives',
        add_help=False,
    )
    info_group = info_parser.add_argument_group('info arguments')
    info_group.add_argument(
        '--repository',
        help='Path of repository to show info for, defaults to the configured repository if there is only one',
    )
    info_group.add_argument('--archive', help='Name of archive to show info for (or "latest")')
    info_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    info_group.add_argument(
        '-P', '--prefix', help='Only show info for archive names starting with this prefix'
    )
    info_group.add_argument(
        '-a',
        '--glob-archives',
        metavar='GLOB',
        help='Only show info for archive names matching this glob',
    )
    info_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    info_group.add_argument(
        '--first',
        metavar='N',
        help='Show info for first N archives after other filters are applied',
    )
    info_group.add_argument(
        '--last', metavar='N', help='Show info for last N archives after other filters are applied'
    )
    info_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    borg_parser = subparsers.add_parser(
        'borg',
        aliases=SUBPARSER_ALIASES['borg'],
        help='Run an arbitrary Borg command',
        description='Run an arbitrary Borg command based on borgmatic\'s configuration',
        add_help=False,
    )
    borg_group = borg_parser.add_argument_group('borg arguments')
    borg_group.add_argument(
        '--repository',
        help='Path of repository to pass to Borg, defaults to the configured repositories',
    )
    borg_group.add_argument('--archive', help='Name of archive to pass to Borg (or "latest")')
    borg_group.add_argument(
        '--',
        metavar='OPTION',
        dest='options',
        nargs='+',
        help='Options to pass to Borg, command first ("create", "list", etc). "--" is optional. To specify the repository or the archive, you must use --repository or --archive instead of providing them here.',
    )
    borg_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    return top_level_parser, subparsers


def parse_arguments(*unparsed_arguments):
    '''
    Given command-line arguments with which this script was invoked, parse the arguments and return
    them as a dict mapping from subparser name (or "global") to an argparse.Namespace instance.
    '''
    top_level_parser, subparsers = make_parsers()

    arguments, remaining_arguments = parse_subparser_arguments(
        unparsed_arguments, subparsers.choices
    )
    arguments['global'] = top_level_parser.parse_args(remaining_arguments)

    if arguments['global'].excludes_filename:
        raise ValueError(
            'The --excludes option has been replaced with exclude_patterns in configuration'
        )

    if 'init' in arguments and arguments['global'].dry_run:
        raise ValueError('The init action cannot be used with the --dry-run option')

    if (
        'list' in arguments
        and 'info' in arguments
        and arguments['list'].json
        and arguments['info'].json
    ):
        raise ValueError('With the --json option, list and info actions cannot be used together')

    return arguments



if __name__ == "__main__":
    out = "OrderedDict([('create', Namespace(repository=None, progress=False, stats=False, list_files=False, json=False)), ('prune', Namespace(repository=None, stats=False, list_archives=False, oldest=None, newest=None, older=None, newer=None)), ('compact', Namespace(repository=None, progress=False, cleanup_commits=False, threshold=None)), ('check', Namespace(repository=None, progress=False, repair=False, only=None, force=False)), ('global', Namespace(config_paths=['/etc/borgmatic/config.yaml', '/etc/borgmatic.d', '$HOME\\.config\\borgmatic/config.yaml', '$HOME\\.config\\borgmatic.d'], dry_run=False, no_color=False, verbosity=0, syslog_verbosity=0, log_file_verbosity=0, monitoring_verbosity=0, log_file=None, log_file_format=None, log_json=False, overrides=None, resolve_env=True, bash_completion=False, fish_completion=False, version=False))])"
    ddd = collections.OrderedDict([('prune', Namespace(stats=False, files=False)), ('compact', Namespace(progress=False, cleanup_commits=False, threshold=None)), ('create', Namespace(progress=False, stats=False, files=False, json=False)), ('check', Namespace(progress=False, repair=False, only=None, force=False)), ('global', Namespace(config_paths=['/etc/borgmatic/config.yaml', '/etc/borgmatic.d', '/root/.config/borgmatic/config.yaml', '/root/.config/borgmatic.d'], excludes_filename=None, dry_run=False, no_color=False, verbosity=0, syslog_verbosity=0, log_file_verbosity=0, monitoring_verbosity=0, log_file=None, overrides=None, resolve_env=True, bash_completion=False, version=False))])
    isT=str(parse_arguments()) == str(ddd)
    if not isT:
        raise Exception("Result not True!!!")
----------------------------
/home/travis/builds/repos/witten---borgmatic/borgmatic/commands/arguments_parse_subparser_arguments_passk_validte.py
import collections
from argparse import Action, ArgumentParser, Namespace
import sys
sys.path.remove('/home/travis/builds/repos/witten---borgmatic/borgmatic/commands')
sys.path.append("/home/travis/builds/repos/witten---borgmatic")
from borgmatic.config import collect

SUBPARSER_ALIASES = {
    'init': ['--init', '-I'],
    'prune': ['--prune', '-p'],
    'compact': [],
    'create': ['--create', '-C'],
    'check': ['--check', '-k'],
    'extract': ['--extract', '-x'],
    'export-tar': ['--export-tar'],
    'mount': ['--mount', '-m'],
    'umount': ['--umount', '-u'],
    'restore': ['--restore', '-r'],
    'list': ['--list', '-l'],
    'info': ['--info', '-i'],
    'borg': [],
}


def parse_subparser_arguments(unparsed_arguments, subparsers):
    '''
    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser
    instance, give each requested action's subparser a shot at parsing all arguments. This allows
    common arguments like "--repository" to be shared across multiple subparsers.

    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of
    arguments, a list of remaining arguments not claimed by any subparser).
    '''
    arguments = collections.OrderedDict()
    remaining_arguments = list(unparsed_arguments)
    alias_to_subparser_name = {
        alias: subparser_name
        for subparser_name, aliases in SUBPARSER_ALIASES.items()
        for alias in aliases
    }

    # If the "borg" action is used, skip all other subparsers. This avoids confusion like
    # "borg list" triggering borgmatic's own list action.
    if 'borg' in unparsed_arguments:
        subparsers = {'borg': subparsers['borg']}

    for subparser_name, subparser in subparsers.items():
        if subparser_name not in remaining_arguments:
            continue

        canonical_name = alias_to_subparser_name.get(subparser_name, subparser_name)

        # If a parsed value happens to be the same as the name of a subparser, remove it from the
        # remaining arguments. This prevents, for instance, "check --only extract" from triggering
        # the "extract" subparser.
        parsed, unused_remaining = subparser.parse_known_args(unparsed_arguments)
        for value in vars(parsed).values():
            if isinstance(value, str):
                if value in subparsers:
                    remaining_arguments.remove(value)
            elif isinstance(value, list):
                for item in value:
                    if item in subparsers:
                        remaining_arguments.remove(item)

        arguments[canonical_name] = parsed

    # If no actions are explicitly requested, assume defaults: prune, compact, create, and check.
    if not arguments and '--help' not in unparsed_arguments and '-h' not in unparsed_arguments:
        for subparser_name in ('prune', 'compact', 'create', 'check'):
            subparser = subparsers[subparser_name]
            parsed, unused_remaining = subparser.parse_known_args(unparsed_arguments)
            arguments[subparser_name] = parsed

    remaining_arguments = list(unparsed_arguments)

    # Now ask each subparser, one by one, to greedily consume arguments.
    for subparser_name, subparser in subparsers.items():
        if subparser_name not in arguments.keys():
            continue

        subparser = subparsers[subparser_name]
        unused_parsed, remaining_arguments = subparser.parse_known_args(remaining_arguments)

    # Special case: If "borg" is present in the arguments, consume all arguments after (+1) the
    # "borg" action.
    if 'borg' in arguments:
        borg_options_index = remaining_arguments.index('borg') + 1
        arguments['borg'].options = remaining_arguments[borg_options_index:]
        remaining_arguments = remaining_arguments[:borg_options_index]

    # Remove the subparser names themselves.
    for subparser_name, subparser in subparsers.items():
        if subparser_name in remaining_arguments:
            remaining_arguments.remove(subparser_name)

    return (arguments, remaining_arguments)


class Extend_action(Action):
    '''
    An argparse action to support Python 3.8's "extend" action in older versions of Python.
    '''

    def __call__(self, parser, namespace, values, option_string=None):
        items = getattr(namespace, self.dest, None)

        if items:
            items.extend(values)
        else:
            setattr(namespace, self.dest, list(values))


def make_parsers():
    '''
    Build a top-level parser and its subparsers and return them as a tuple.
    '''
    config_paths = collect.get_default_config_paths(expand_home=True)
    unexpanded_config_paths = collect.get_default_config_paths(expand_home=False)

    global_parser = ArgumentParser(add_help=False)
    global_parser.register('action', 'extend', Extend_action)
    global_group = global_parser.add_argument_group('global arguments')

    global_group.add_argument(
        '-c',
        '--config',
        nargs='*',
        dest='config_paths',
        default=config_paths,
        help='Configuration filenames or directories, defaults to: {}'.format(
            ' '.join(unexpanded_config_paths)
        ),
    )
    global_group.add_argument(
        '--excludes',
        dest='excludes_filename',
        help='Deprecated in favor of exclude_patterns within configuration',
    )
    global_group.add_argument(
        '-n',
        '--dry-run',
        dest='dry_run',
        action='store_true',
        help='Go through the motions, but do not actually write to any repositories',
    )
    global_group.add_argument(
        '-nc', '--no-color', dest='no_color', action='store_true', help='Disable colored output'
    )
    global_group.add_argument(
        '-v',
        '--verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Display verbose progress to the console (from only errors to very verbose: -1, 0, 1, or 2)',
    )
    global_group.add_argument(
        '--syslog-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to syslog (from only errors to very verbose: -1, 0, 1, or 2). Ignored when console is interactive or --log-file is given',
    )
    global_group.add_argument(
        '--log-file-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to log file (from only errors to very verbose: -1, 0, 1, or 2). Only used when --log-file is given',
    )
    global_group.add_argument(
        '--monitoring-verbosity',
        type=int,
        choices=range(-1, 3),
        default=0,
        help='Log verbose progress to monitoring integrations that support logging (from only errors to very verbose: -1, 0, 1, or 2)',
    )
    global_group.add_argument(
        '--log-file',
        type=str,
        default=None,
        help='Write log messages to this file instead of syslog',
    )
    global_group.add_argument(
        '--override',
        metavar='SECTION.OPTION=VALUE',
        nargs='+',
        dest='overrides',
        action='extend',
        help='One or more configuration file options to override with specified values',
    )
    global_group.add_argument(
        '--no-environment-interpolation',
        dest='resolve_env',
        action='store_false',
        help='Do not resolve environment variables in configuration file',
    )
    global_group.add_argument(
        '--bash-completion',
        default=False,
        action='store_true',
        help='Show bash completion script and exit',
    )
    global_group.add_argument(
        '--version',
        dest='version',
        default=False,
        action='store_true',
        help='Display installed version number of borgmatic and exit',
    )

    top_level_parser = ArgumentParser(
        description='''
            Simple, configuration-driven backup software for servers and workstations. If none of
            the action options are given, then borgmatic defaults to: prune, compact, create, and
            check.
            ''',
        parents=[global_parser],
    )

    subparsers = top_level_parser.add_subparsers(
        title='actions',
        metavar='',
        help='Specify zero or more actions. Defaults to prune, compact, create, and check. Use --help with action for details:',
    )
    init_parser = subparsers.add_parser(
        'init',
        aliases=SUBPARSER_ALIASES['init'],
        help='Initialize an empty Borg repository',
        description='Initialize an empty Borg repository',
        add_help=False,
    )
    init_group = init_parser.add_argument_group('init arguments')
    init_group.add_argument(
        '-e',
        '--encryption',
        dest='encryption_mode',
        help='Borg repository encryption mode',
        required=True,
    )
    init_group.add_argument(
        '--append-only',
        dest='append_only',
        action='store_true',
        help='Create an append-only repository',
    )
    init_group.add_argument(
        '--storage-quota',
        dest='storage_quota',
        help='Create a repository with a fixed storage quota',
    )
    init_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    prune_parser = subparsers.add_parser(
        'prune',
        aliases=SUBPARSER_ALIASES['prune'],
        help='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
        description='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
        add_help=False,
    )
    prune_group = prune_parser.add_argument_group('prune arguments')
    prune_group.add_argument(
        '--stats',
        dest='stats',
        default=False,
        action='store_true',
        help='Display statistics of archive',
    )
    prune_group.add_argument(
        '--files', dest='files', default=False, action='store_true', help='Show per-file details'
    )
    prune_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    compact_parser = subparsers.add_parser(
        'compact',
        aliases=SUBPARSER_ALIASES['compact'],
        help='Compact segments to free space (Borg 1.2+ only)',
        description='Compact segments to free space (Borg 1.2+ only)',
        add_help=False,
    )
    compact_group = compact_parser.add_argument_group('compact arguments')
    compact_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress as each segment is compacted',
    )
    compact_group.add_argument(
        '--cleanup-commits',
        dest='cleanup_commits',
        default=False,
        action='store_true',
        help='Cleanup commit-only 17-byte segment files left behind by Borg 1.1',
    )
    compact_group.add_argument(
        '--threshold',
        type=int,
        dest='threshold',
        help='Minimum saved space percentage threshold for compacting a segment, defaults to 10',
    )
    compact_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    create_parser = subparsers.add_parser(
        'create',
        aliases=SUBPARSER_ALIASES['create'],
        help='Create archives (actually perform backups)',
        description='Create archives (actually perform backups)',
        add_help=False,
    )
    create_group = create_parser.add_argument_group('create arguments')
    create_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is backed up',
    )
    create_group.add_argument(
        '--stats',
        dest='stats',
        default=False,
        action='store_true',
        help='Display statistics of archive',
    )
    create_group.add_argument(
        '--files', dest='files', default=False, action='store_true', help='Show per-file details'
    )
    create_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    create_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    check_parser = subparsers.add_parser(
        'check',
        aliases=SUBPARSER_ALIASES['check'],
        help='Check archives for consistency',
        description='Check archives for consistency',
        add_help=False,
    )
    check_group = check_parser.add_argument_group('check arguments')
    check_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is checked',
    )
    check_group.add_argument(
        '--repair',
        dest='repair',
        default=False,
        action='store_true',
        help='Attempt to repair any inconsistencies found (for interactive use)',
    )
    check_group.add_argument(
        '--only',
        metavar='CHECK',
        choices=('repository', 'archives', 'data', 'extract'),
        dest='only',
        action='append',
        help='Run a particular consistency check (repository, archives, data, or extract) instead of configured checks (subject to configured frequency, can specify flag multiple times)',
    )
    check_group.add_argument(
        '--force',
        default=False,
        action='store_true',
        help='Ignore configured check frequencies and run checks unconditionally',
    )
    check_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    extract_parser = subparsers.add_parser(
        'extract',
        aliases=SUBPARSER_ALIASES['extract'],
        help='Extract files from a named archive to the current directory',
        description='Extract a named archive to the current directory',
        add_help=False,
    )
    extract_group = extract_parser.add_argument_group('extract arguments')
    extract_group.add_argument(
        '--repository',
        help='Path of repository to extract, defaults to the configured repository if there is only one',
    )
    extract_group.add_argument(
        '--archive', help='Name of archive to extract (or "latest")', required=True
    )
    extract_group.add_argument(
        '--path',
        '--restore-path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to extract from archive, defaults to the entire archive',
    )
    extract_group.add_argument(
        '--destination',
        metavar='PATH',
        dest='destination',
        help='Directory to extract files into, defaults to the current directory',
    )
    extract_group.add_argument(
        '--strip-components',
        type=int,
        metavar='NUMBER',
        dest='strip_components',
        help='Number of leading path components to remove from each extracted path. Skip paths with fewer elements',
    )
    extract_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is extracted',
    )
    extract_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    export_tar_parser = subparsers.add_parser(
        'export-tar',
        aliases=SUBPARSER_ALIASES['export-tar'],
        help='Export an archive to a tar-formatted file or stream',
        description='Export an archive to a tar-formatted file or stream',
        add_help=False,
    )
    export_tar_group = export_tar_parser.add_argument_group('export-tar arguments')
    export_tar_group.add_argument(
        '--repository',
        help='Path of repository to export from, defaults to the configured repository if there is only one',
    )
    export_tar_group.add_argument(
        '--archive', help='Name of archive to export (or "latest")', required=True
    )
    export_tar_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to export from archive, defaults to the entire archive',
    )
    export_tar_group.add_argument(
        '--destination',
        metavar='PATH',
        dest='destination',
        help='Path to destination export tar file, or "-" for stdout (but be careful about dirtying output with --verbosity or --files)',
        required=True,
    )
    export_tar_group.add_argument(
        '--tar-filter', help='Name of filter program to pipe data through'
    )
    export_tar_group.add_argument(
        '--files', default=False, action='store_true', help='Show per-file details'
    )
    export_tar_group.add_argument(
        '--strip-components',
        type=int,
        metavar='NUMBER',
        dest='strip_components',
        help='Number of leading path components to remove from each exported path. Skip paths with fewer elements',
    )
    export_tar_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    mount_parser = subparsers.add_parser(
        'mount',
        aliases=SUBPARSER_ALIASES['mount'],
        help='Mount files from a named archive as a FUSE filesystem',
        description='Mount a named archive as a FUSE filesystem',
        add_help=False,
    )
    mount_group = mount_parser.add_argument_group('mount arguments')
    mount_group.add_argument(
        '--repository',
        help='Path of repository to use, defaults to the configured repository if there is only one',
    )
    mount_group.add_argument('--archive', help='Name of archive to mount (or "latest")')
    mount_group.add_argument(
        '--mount-point',
        metavar='PATH',
        dest='mount_point',
        help='Path where filesystem is to be mounted',
        required=True,
    )
    mount_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths to mount from archive, defaults to the entire archive',
    )
    mount_group.add_argument(
        '--foreground',
        dest='foreground',
        default=False,
        action='store_true',
        help='Stay in foreground until ctrl-C is pressed',
    )
    mount_group.add_argument('--options', dest='options', help='Extra Borg mount options')
    mount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    umount_parser = subparsers.add_parser(
        'umount',
        aliases=SUBPARSER_ALIASES['umount'],
        help='Unmount a FUSE filesystem that was mounted with "borgmatic mount"',
        description='Unmount a mounted FUSE filesystem',
        add_help=False,
    )
    umount_group = umount_parser.add_argument_group('umount arguments')
    umount_group.add_argument(
        '--mount-point',
        metavar='PATH',
        dest='mount_point',
        help='Path of filesystem to unmount',
        required=True,
    )
    umount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    restore_parser = subparsers.add_parser(
        'restore',
        aliases=SUBPARSER_ALIASES['restore'],
        help='Restore database dumps from a named archive',
        description='Restore database dumps from a named archive. (To extract files instead, use "borgmatic extract".)',
        add_help=False,
    )
    restore_group = restore_parser.add_argument_group('restore arguments')
    restore_group.add_argument(
        '--repository',
        help='Path of repository to restore from, defaults to the configured repository if there is only one',
    )
    restore_group.add_argument(
        '--archive', help='Name of archive to restore from (or "latest")', required=True
    )
    restore_group.add_argument(
        '--database',
        metavar='NAME',
        nargs='+',
        dest='databases',
        help='Names of databases to restore from archive, defaults to all databases. Note that any databases to restore must be defined in borgmatic\'s configuration',
    )
    restore_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    list_parser = subparsers.add_parser(
        'list',
        aliases=SUBPARSER_ALIASES['list'],
        help='List archives',
        description='List archives or the contents of an archive',
        add_help=False,
    )
    list_group = list_parser.add_argument_group('list arguments')
    list_group.add_argument(
        '--repository', help='Path of repository to list, defaults to the configured repositories',
    )
    list_group.add_argument('--archive', help='Name of archive to list (or "latest")')
    list_group.add_argument(
        '--path',
        metavar='PATH',
        nargs='+',
        dest='paths',
        help='Paths or patterns to list from a single selected archive (via "--archive"), defaults to listing the entire archive',
    )
    list_group.add_argument(
        '--find',
        metavar='PATH',
        nargs='+',
        dest='find_paths',
        help='Partial paths or patterns to search for and list across multiple archives',
    )
    list_group.add_argument(
        '--short', default=False, action='store_true', help='Output only archive or path names'
    )
    list_group.add_argument('--format', help='Format for file listing')
    list_group.add_argument(
        '--json', default=False, action='store_true', help='Output results as JSON'
    )
    list_group.add_argument(
        '-P', '--prefix', help='Only list archive names starting with this prefix'
    )
    list_group.add_argument(
        '-a', '--glob-archives', metavar='GLOB', help='Only list archive names matching this glob'
    )
    list_group.add_argument(
        '--successful',
        default=True,
        action='store_true',
        help='Deprecated in favor of listing successful (non-checkpoint) backups by default in newer versions of Borg',
    )
    list_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    list_group.add_argument(
        '--first', metavar='N', help='List first N archives after other filters are applied'
    )
    list_group.add_argument(
        '--last', metavar='N', help='List last N archives after other filters are applied'
    )
    list_group.add_argument(
        '-e', '--exclude', metavar='PATTERN', help='Exclude paths matching the pattern'
    )
    list_group.add_argument(
        '--exclude-from', metavar='FILENAME', help='Exclude paths from exclude file, one per line'
    )
    list_group.add_argument('--pattern', help='Include or exclude paths matching a pattern')
    list_group.add_argument(
        '--patterns-from',
        metavar='FILENAME',
        help='Include or exclude paths matching patterns from pattern file, one per line',
    )
    list_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    info_parser = subparsers.add_parser(
        'info',
        aliases=SUBPARSER_ALIASES['info'],
        help='Display summary information on archives',
        description='Display summary information on archives',
        add_help=False,
    )
    info_group = info_parser.add_argument_group('info arguments')
    info_group.add_argument(
        '--repository',
        help='Path of repository to show info for, defaults to the configured repository if there is only one',
    )
    info_group.add_argument('--archive', help='Name of archive to show info for (or "latest")')
    info_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    info_group.add_argument(
        '-P', '--prefix', help='Only show info for archive names starting with this prefix'
    )
    info_group.add_argument(
        '-a',
        '--glob-archives',
        metavar='GLOB',
        help='Only show info for archive names matching this glob',
    )
    info_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    info_group.add_argument(
        '--first',
        metavar='N',
        help='Show info for first N archives after other filters are applied',
    )
    info_group.add_argument(
        '--last', metavar='N', help='Show info for last N archives after other filters are applied'
    )
    info_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    borg_parser = subparsers.add_parser(
        'borg',
        aliases=SUBPARSER_ALIASES['borg'],
        help='Run an arbitrary Borg command',
        description='Run an arbitrary Borg command based on borgmatic\'s configuration',
        add_help=False,
    )
    borg_group = borg_parser.add_argument_group('borg arguments')
    borg_group.add_argument(
        '--repository',
        help='Path of repository to pass to Borg, defaults to the configured repositories',
    )
    borg_group.add_argument('--archive', help='Name of archive to pass to Borg (or "latest")')
    borg_group.add_argument(
        '--',
        metavar='OPTION',
        dest='options',
        nargs='+',
        help='Options to pass to Borg, command first ("create", "list", etc). "--" is optional. To specify the repository or the archive, you must use --repository or --archive instead of providing them here.',
    )
    borg_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    return top_level_parser, subparsers


def parse_arguments(*unparsed_arguments):
    '''
    Given command-line arguments with which this script was invoked, parse the arguments and return
    them as a dict mapping from subparser name (or "global") to an argparse.Namespace instance.
    '''
    top_level_parser, subparsers = make_parsers()

    arguments, remaining_arguments = parse_subparser_arguments(
        unparsed_arguments, subparsers.choices
    )
    arguments['global'] = top_level_parser.parse_args(remaining_arguments)

    if arguments['global'].excludes_filename:
        raise ValueError(
            'The --excludes option has been replaced with exclude_patterns in configuration'
        )

    if 'init' in arguments and arguments['global'].dry_run:
        raise ValueError('The init action cannot be used with the --dry-run option')

    if (
        'list' in arguments
        and 'info' in arguments
        and arguments['list'].json
        and arguments['info'].json
    ):
        raise ValueError('With the --json option, list and info actions cannot be used together')

    return arguments



if __name__ == "__main__":
    args0=('list', '--json')
    args1={'init': ArgumentParser(prog='pytest init', usage=None, description='Initialize an empty Borg repository',
                             conflict_handler='error', add_help=False), '--init': ArgumentParser(
        prog='pytest init', usage=None, description='Initialize an empty Borg repository', conflict_handler='error', add_help=False), '-I': ArgumentParser(prog='pytest init',
                                                                                                     usage=None,
                                                                                                     description='Initialize an empty Borg repository',
                                                                                                      conflict_handler='error', add_help=False), 'prune': ArgumentParser(
        prog='pytest prune', usage=None,
        description='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
         conflict_handler='error', add_help=False), '--prune': ArgumentParser(
        prog='pytest prune', usage=None,
        description='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
         conflict_handler='error', add_help=False), '-p': ArgumentParser(
        prog='pytest prune', usage=None,
        description='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
         conflict_handler='error', add_help=False), 'compact': ArgumentParser(
        prog='pytest compact', usage=None, description='Compact segments to free space (Borg 1.2+ only)',
         conflict_handler='error', add_help=False), 'create': ArgumentParser(
        prog='pytest create', usage=None, description='Create archives (actually perform backups)',  conflict_handler='error', add_help=False), '--create': ArgumentParser(
        prog='pytest create', usage=None, description='Create archives (actually perform backups)',  conflict_handler='error', add_help=False), '-C': ArgumentParser(
        prog='pytest create', usage=None, description='Create archives (actually perform backups)',  conflict_handler='error', add_help=False), 'check': ArgumentParser(
        prog='pytest check', usage=None, description='Check archives for consistency',  conflict_handler='error', add_help=False), '--check': ArgumentParser(
        prog='pytest check', usage=None, description='Check archives for consistency',  conflict_handler='error', add_help=False), '-k': ArgumentParser(
        prog='pytest check', usage=None, description='Check archives for consistency',  conflict_handler='error', add_help=False), 'extract': ArgumentParser(
        prog='pytest extract', usage=None, description='Extract a named archive to the current directory',
         conflict_handler='error', add_help=False), '--extract': ArgumentParser(
        prog='pytest extract', usage=None, description='Extract a named archive to the current directory',
         conflict_handler='error', add_help=False), '-x': ArgumentParser(
        prog='pytest extract', usage=None, description='Extract a named archive to the current directory',
         conflict_handler='error', add_help=False), 'export-tar': ArgumentParser(
        prog='pytest export-tar', usage=None, description='Export an archive to a tar-formatted file or stream',
         conflict_handler='error', add_help=False), '--export-tar': ArgumentParser(
        prog='pytest export-tar', usage=None, description='Export an archive to a tar-formatted file or stream',
         conflict_handler='error', add_help=False), 'mount': ArgumentParser(
        prog='pytest mount', usage=None, description='Mount a named archive as a FUSE filesystem',  conflict_handler='error', add_help=False), '--mount': ArgumentParser(
        prog='pytest mount', usage=None, description='Mount a named archive as a FUSE filesystem',  conflict_handler='error', add_help=False), '-m': ArgumentParser(
        prog='pytest mount', usage=None, description='Mount a named archive as a FUSE filesystem',  conflict_handler='error', add_help=False), 'umount': ArgumentParser(
        prog='pytest umount', usage=None, description='Unmount a mounted FUSE filesystem',  conflict_handler='error', add_help=False), '--umount': ArgumentParser(
        prog='pytest umount', usage=None, description='Unmount a mounted FUSE filesystem',  conflict_handler='error', add_help=False), '-u': ArgumentParser(
        prog='pytest umount', usage=None, description='Unmount a mounted FUSE filesystem',  conflict_handler='error', add_help=False), 'restore': ArgumentParser(
        prog='pytest restore', usage=None,
        description='Restore database dumps from a named archive. (To extract files instead, use "borgmatic extract".)',
         conflict_handler='error', add_help=False), '--restore': ArgumentParser(
        prog='pytest restore', usage=None,
        description='Restore database dumps from a named archive. (To extract files instead, use "borgmatic extract".)',
         conflict_handler='error', add_help=False), '-r': ArgumentParser(
        prog='pytest restore', usage=None,
        description='Restore database dumps from a named archive. (To extract files instead, use "borgmatic extract".)',
         conflict_handler='error', add_help=False), 'list': ArgumentParser(
        prog='pytest list', usage=None, description='List archives or the contents of an archive',  conflict_handler='error', add_help=False), '--list': ArgumentParser(
        prog='pytest list', usage=None, description='List archives or the contents of an archive',  conflict_handler='error', add_help=False), '-l': ArgumentParser(prog='pytest list',
                                                                                                     usage=None,
                                                                                                     description='List archives or the contents of an archive',
                                                                                                      conflict_handler='error', add_help=False), 'info': ArgumentParser(
        prog='pytest info', usage=None, description='Display summary information on archives',  conflict_handler='error', add_help=False), '--info': ArgumentParser(
        prog='pytest info', usage=None, description='Display summary information on archives',  conflict_handler='error', add_help=False), '-i': ArgumentParser(prog='pytest info',
                                                                                                     usage=None,
                                                                                                     description='Display summary information on archives',
                                                                                                      conflict_handler='error', add_help=False), 'borg': ArgumentParser(
        prog='pytest borg', usage=None, description="Run an arbitrary Borg command based on borgmatic's configuration",
         conflict_handler='error', add_help=False)}
    out= collections.OrderedDict([('list', Namespace())]), ['--json']
    isT=parse_subparser_arguments(args0,args1)==out
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\\yh199\\Downloads\\witten---borgmatic\\witten---borgmatic/data_passk_platform1/62b438a266fea644fe22cc2d/"):
    #     f = open("C:\\Users\\yh199\\Downloads\\witten---borgmatic\\witten---borgmatic/data_passk_platform1/62b438a266fea644fe22cc2d/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     print(args0)
    #     print(args1)
    #     print("------------")
    #     res0 = parse_subparser_arguments(args0,args1)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")



----------------------------
/home/travis/builds/repos/witten---borgmatic/borgmatic/commands/arguments_make_parsers_passk_validte.py
import collections
import itertools
import sys
from argparse import ArgumentParser
sys.path.remove('/home/travis/builds/repos/witten---borgmatic/borgmatic/commands')
sys.path.append("/home/travis/builds/repos/witten---borgmatic")

from borgmatic.config import collect

ACTION_ALIASES = {
    'rcreate': ['init', '-I'],
    'prune': ['-p'],
    'compact': [],
    'create': ['-C'],
    'check': ['-k'],
    'config': [],
    'extract': ['-x'],
    'export-tar': [],
    'mount': ['-m'],
    'umount': ['-u'],
    'restore': ['-r'],
    'rlist': [],
    'list': ['-l'],
    'rinfo': [],
    'info': ['-i'],
    'transfer': [],
    'break-lock': [],
    'key': [],
    'borg': [],
}


def get_subaction_parsers(action_parser):
    '''
    Given an argparse.ArgumentParser instance, lookup the subactions in it and return a dict from
    subaction name to subaction parser.
    '''
    if not action_parser._subparsers:
        return {}

    return {
        subaction_name: subaction_parser
        for group_action in action_parser._subparsers._group_actions
        for subaction_name, subaction_parser in group_action.choices.items()
    }


def get_subactions_for_actions(action_parsers):
    '''
    Given a dict from action name to an argparse.ArgumentParser instance, make a map from action
    name to the names of contained sub-actions.
    '''
    return {
        action: tuple(
            subaction_name
            for group_action in action_parser._subparsers._group_actions
            for subaction_name in group_action.choices.keys()
        )
        for action, action_parser in action_parsers.items()
        if action_parser._subparsers
    }


def omit_values_colliding_with_action_names(unparsed_arguments, parsed_arguments):
    '''
    Given a sequence of string arguments and a dict from action name to parsed argparse.Namespace
    arguments, return the string arguments with any values omitted that happen to be the same as
    the name of a borgmatic action.

    This prevents, for instance, "check --only extract" from triggering the "extract" action.
    '''
    remaining_arguments = list(unparsed_arguments)

    for action_name, parsed in parsed_arguments.items():
        for value in vars(parsed).values():
            if isinstance(value, str):
                if value in ACTION_ALIASES.keys():
                    remaining_arguments.remove(value)
            elif isinstance(value, list):
                for item in value:
                    if item in ACTION_ALIASES.keys():
                        remaining_arguments.remove(item)

    return tuple(remaining_arguments)


def parse_and_record_action_arguments(
    unparsed_arguments, parsed_arguments, action_parser, action_name, canonical_name=None
):
    '''
    Given unparsed arguments as a sequence of strings, parsed arguments as a dict from action name
    to parsed argparse.Namespace, a parser to parse with, an action name, and an optional canonical
    action name (in case this the action name is an alias), parse the arguments and return a list of
    any remaining string arguments that were not parsed. Also record the parsed argparse.Namespace
    by setting it into the given parsed arguments. Return None if no parsing occurs because the
    given action doesn't apply to the given unparsed arguments.
    '''
    filtered_arguments = omit_values_colliding_with_action_names(
        unparsed_arguments, parsed_arguments
    )

    if action_name not in filtered_arguments:
        return tuple(unparsed_arguments)

    parsed, remaining = action_parser.parse_known_args(filtered_arguments)
    parsed_arguments[canonical_name or action_name] = parsed

    # Special case: If this is a "borg" action, greedily consume all arguments after (+1) the "borg"
    # argument.
    if action_name == 'borg':
        borg_options_index = remaining.index('borg') + 1
        parsed_arguments['borg'].options = remaining[borg_options_index:]
        remaining = remaining[:borg_options_index]

    return tuple(argument for argument in remaining if argument != action_name)


def get_unparsable_arguments(remaining_action_arguments):
    '''
    Given a sequence of argument tuples (one per action parser that parsed arguments), determine the
    remaining arguments that no action parsers have consumed.
    '''
    if not remaining_action_arguments:
        return ()

    return tuple(
        argument
        for argument in dict.fromkeys(
            itertools.chain.from_iterable(remaining_action_arguments)
        ).keys()
        if all(argument in action_arguments for action_arguments in remaining_action_arguments)
    )


def parse_arguments_for_actions(unparsed_arguments, action_parsers, global_parser):
    '''
    Given a sequence of arguments, a dict from action name to argparse.ArgumentParser instance,
    and the global parser as a argparse.ArgumentParser instance, give each requested action's
    parser a shot at parsing all arguments. This allows common arguments like "--repository" to be
    shared across multiple action parsers.

    Return the result as a tuple of: (a dict mapping from action name to an argparse.Namespace of
    parsed arguments, a tuple of argument tuples where each is the remaining arguments not claimed
    by any action parser).
    '''
    arguments = collections.OrderedDict()
    help_requested = bool('--help' in unparsed_arguments or '-h' in unparsed_arguments)
    remaining_action_arguments = []
    alias_to_action_name = {
        alias: action_name for action_name, aliases in ACTION_ALIASES.items() for alias in aliases
    }

    # If the "borg" action is used, skip all other action parsers. This avoids confusion like
    # "borg list" triggering borgmatic's own list action.
    if 'borg' in unparsed_arguments:
        action_parsers = {'borg': action_parsers['borg']}

    # Ask each action parser, one by one, to parse arguments.
    for argument in unparsed_arguments:
        action_name = argument
        canonical_name = alias_to_action_name.get(action_name, action_name)
        action_parser = action_parsers.get(action_name)

        if not action_parser:
            continue

        subaction_parsers = get_subaction_parsers(action_parser)

        # But first parse with subaction parsers, if any.
        if subaction_parsers:
            subactions_parsed = False

            for subaction_name, subaction_parser in subaction_parsers.items():
                remaining_action_arguments.append(
                    tuple(
                        argument
                        for argument in parse_and_record_action_arguments(
                            unparsed_arguments,
                            arguments,
                            subaction_parser,
                            subaction_name,
                        )
                        if argument != action_name
                    )
                )

                if subaction_name in arguments:
                    subactions_parsed = True

            if not subactions_parsed:
                if help_requested:
                    action_parser.print_help()
                    sys.exit(0)
                else:
                    raise ValueError(
                        f"Missing sub-action after {action_name} action. Expected one of: {', '.join(get_subactions_for_actions(action_parsers)[action_name])}"
                    )
        # Otherwise, parse with the main action parser.
        else:
            remaining_action_arguments.append(
                parse_and_record_action_arguments(
                    unparsed_arguments, arguments, action_parser, action_name, canonical_name
                )
            )

    # If no actions were explicitly requested, assume defaults.
    if not arguments and not help_requested:
        for default_action_name in ('create', 'prune', 'compact', 'check'):
            default_action_parser = action_parsers[default_action_name]
            remaining_action_arguments.append(
                parse_and_record_action_arguments(
                    tuple(unparsed_arguments) + (default_action_name,),
                    arguments,
                    default_action_parser,
                    default_action_name,
                )
            )

    arguments['global'], remaining = global_parser.parse_known_args(unparsed_arguments)
    remaining_action_arguments.append(remaining)

    return (
        arguments,
        tuple(remaining_action_arguments) if arguments else unparsed_arguments,
    )


def make_parsers():
    '''
    Build a global arguments parser, individual action parsers, and a combined parser containing
    both. Return them as a tuple. The global parser is useful for parsing just global arguments
    while ignoring actions, and the combined parser is handy for displaying help that includes
    everything: global flags, a list of actions, etc.
    '''
    config_paths = collect.get_default_config_paths(expand_home=True)
    unexpanded_config_paths = collect.get_default_config_paths(expand_home=False)

    global_parser = ArgumentParser(add_help=False)
    global_group = global_parser.add_argument_group('global arguments')

    global_group.add_argument(
        '-c',
        '--config',
        dest='config_paths',
        action='append',
        help=f"Configuration filename or directory, can specify flag multiple times, defaults to: {' '.join(unexpanded_config_paths)}",
    )
    global_group.add_argument(
        '-n',
        '--dry-run',
        dest='dry_run',
        action='store_true',
        help='Go through the motions, but do not actually write to any repositories',
    )
    global_group.add_argument(
        '-nc', '--no-color', dest='no_color', action='store_true', help='Disable colored output'
    )
    global_group.add_argument(
        '-v',
        '--verbosity',
        type=int,
        choices=range(-2, 3),
        default=0,
        help='Display verbose progress to the console (disabled, errors only, default, some, or lots: -2, -1, 0, 1, or 2)',
    )
    global_group.add_argument(
        '--syslog-verbosity',
        type=int,
        choices=range(-2, 3),
        default=0,
        help='Log verbose progress to syslog (disabled, errors only, default, some, or lots: -2, -1, 0, 1, or 2). Ignored when console is interactive or --log-file is given',
    )
    global_group.add_argument(
        '--log-file-verbosity',
        type=int,
        choices=range(-2, 3),
        default=0,
        help='Log verbose progress to log file (disabled, errors only, default, some, or lots: -2, -1, 0, 1, or 2). Only used when --log-file is given',
    )
    global_group.add_argument(
        '--monitoring-verbosity',
        type=int,
        choices=range(-2, 3),
        default=0,
        help='Log verbose progress to monitoring integrations that support logging (from disabled, errors only, default, some, or lots: -2, -1, 0, 1, or 2)',
    )
    global_group.add_argument(
        '--log-file',
        type=str,
        help='Write log messages to this file instead of syslog',
    )
    global_group.add_argument(
        '--log-file-format',
        type=str,
        help='Log format string used for log messages written to the log file',
    )
    global_group.add_argument(
        '--log-json',
        action='store_true',
        help='Write log messages and console output as one JSON object per log line instead of formatted text',
    )
    global_group.add_argument(
        '--override',
        metavar='OPTION.SUBOPTION=VALUE',
        dest='overrides',
        action='append',
        help='Configuration file option to override with specified value, can specify flag multiple times',
    )
    global_group.add_argument(
        '--no-environment-interpolation',
        dest='resolve_env',
        action='store_false',
        help='Do not resolve environment variables in configuration file',
    )
    global_group.add_argument(
        '--bash-completion',
        default=False,
        action='store_true',
        help='Show bash completion script and exit',
    )
    global_group.add_argument(
        '--fish-completion',
        default=False,
        action='store_true',
        help='Show fish completion script and exit',
    )
    global_group.add_argument(
        '--version',
        dest='version',
        default=False,
        action='store_true',
        help='Display installed version number of borgmatic and exit',
    )

    global_plus_action_parser = ArgumentParser(
        description='''
            Simple, configuration-driven backup software for servers and workstations. If no actions
            are given, then borgmatic defaults to: create, prune, compact, and check.
            ''',
        parents=[global_parser],
    )

    action_parsers = global_plus_action_parser.add_subparsers(
        title='actions',
        metavar='',
        help='Specify zero or more actions. Defaults to create, prune, compact, and check. Use --help with action for details:',
    )
    rcreate_parser = action_parsers.add_parser(
        'rcreate',
        aliases=ACTION_ALIASES['rcreate'],
        help='Create a new, empty Borg repository',
        description='Create a new, empty Borg repository',
        add_help=False,
    )
    rcreate_group = rcreate_parser.add_argument_group('rcreate arguments')
    rcreate_group.add_argument(
        '-e',
        '--encryption',
        dest='encryption_mode',
        help='Borg repository encryption mode',
        required=True,
    )
    rcreate_group.add_argument(
        '--source-repository',
        '--other-repo',
        metavar='KEY_REPOSITORY',
        help='Path to an existing Borg repository whose key material should be reused [Borg 2.x+ only]',
    )
    rcreate_group.add_argument(
        '--repository',
        help='Path of the new repository to create (must be already specified in a borgmatic configuration file), defaults to the configured repository if there is only one',
    )
    rcreate_group.add_argument(
        '--copy-crypt-key',
        action='store_true',
        help='Copy the crypt key used for authenticated encryption from the source repository, defaults to a new random key [Borg 2.x+ only]',
    )
    rcreate_group.add_argument(
        '--append-only',
        action='store_true',
        help='Create an append-only repository',
    )
    rcreate_group.add_argument(
        '--storage-quota',
        help='Create a repository with a fixed storage quota',
    )
    rcreate_group.add_argument(
        '--make-parent-dirs',
        action='store_true',
        help='Create any missing parent directories of the repository directory',
    )
    rcreate_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    transfer_parser = action_parsers.add_parser(
        'transfer',
        aliases=ACTION_ALIASES['transfer'],
        help='Transfer archives from one repository to another, optionally upgrading the transferred data [Borg 2.0+ only]',
        description='Transfer archives from one repository to another, optionally upgrading the transferred data [Borg 2.0+ only]',
        add_help=False,
    )
    transfer_group = transfer_parser.add_argument_group('transfer arguments')
    transfer_group.add_argument(
        '--repository',
        help='Path of existing destination repository to transfer archives to, defaults to the configured repository if there is only one',
    )
    transfer_group.add_argument(
        '--source-repository',
        help='Path of existing source repository to transfer archives from',
        required=True,
    )
    transfer_group.add_argument(
        '--archive',
        help='Name of single archive to transfer (or "latest"), defaults to transferring all archives',
    )
    transfer_group.add_argument(
        '--upgrader',
        help='Upgrader type used to convert the transferred data, e.g. "From12To20" to upgrade data from Borg 1.2 to 2.0 format, defaults to no conversion',
    )
    transfer_group.add_argument(
        '--progress',
        default=False,
        action='store_true',
        help='Display progress as each archive is transferred',
    )
    transfer_group.add_argument(
        '-a',
        '--match-archives',
        '--glob-archives',
        metavar='PATTERN',
        help='Only transfer archives with names matching this pattern',
    )
    transfer_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    transfer_group.add_argument(
        '--first',
        metavar='N',
        help='Only transfer first N archives after other filters are applied',
    )
    transfer_group.add_argument(
        '--last', metavar='N', help='Only transfer last N archives after other filters are applied'
    )
    transfer_group.add_argument(
        '--oldest',
        metavar='TIMESPAN',
        help='Transfer archives within a specified time range starting from the timestamp of the oldest archive (e.g. 7d or 12m) [Borg 2.x+ only]',
    )
    transfer_group.add_argument(
        '--newest',
        metavar='TIMESPAN',
        help='Transfer archives within a time range that ends at timestamp of the newest archive and starts a specified time range ago (e.g. 7d or 12m) [Borg 2.x+ only]',
    )
    transfer_group.add_argument(
        '--older',
        metavar='TIMESPAN',
        help='Transfer archives that are older than the specified time range (e.g. 7d or 12m) from the current time [Borg 2.x+ only]',
    )
    transfer_group.add_argument(
        '--newer',
        metavar='TIMESPAN',
        help='Transfer archives that are newer than the specified time range (e.g. 7d or 12m) from the current time [Borg 2.x+ only]',
    )
    transfer_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    prune_parser = action_parsers.add_parser(
        'prune',
        aliases=ACTION_ALIASES['prune'],
        help='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
        description='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',
        add_help=False,
    )
    prune_group = prune_parser.add_argument_group('prune arguments')
    prune_group.add_argument(
        '--repository',
        help='Path of specific existing repository to prune (must be already specified in a borgmatic configuration file)',
    )
    prune_group.add_argument(
        '--stats',
        dest='stats',
        default=False,
        action='store_true',
        help='Display statistics of archive',
    )
    prune_group.add_argument(
        '--list', dest='list_archives', action='store_true', help='List archives kept/pruned'
    )
    prune_group.add_argument(
        '--oldest',
        metavar='TIMESPAN',
        help='Prune archives within a specified time range starting from the timestamp of the oldest archive (e.g. 7d or 12m) [Borg 2.x+ only]',
    )
    prune_group.add_argument(
        '--newest',
        metavar='TIMESPAN',
        help='Prune archives within a time range that ends at timestamp of the newest archive and starts a specified time range ago (e.g. 7d or 12m) [Borg 2.x+ only]',
    )
    prune_group.add_argument(
        '--older',
        metavar='TIMESPAN',
        help='Prune archives that are older than the specified time range (e.g. 7d or 12m) from the current time [Borg 2.x+ only]',
    )
    prune_group.add_argument(
        '--newer',
        metavar='TIMESPAN',
        help='Prune archives that are newer than the specified time range (e.g. 7d or 12m) from the current time [Borg 2.x+ only]',
    )
    prune_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    compact_parser = action_parsers.add_parser(
        'compact',
        aliases=ACTION_ALIASES['compact'],
        help='Compact segments to free space [Borg 1.2+, borgmatic 1.5.23+ only]',
        description='Compact segments to free space [Borg 1.2+, borgmatic 1.5.23+ only]',
        add_help=False,
    )
    compact_group = compact_parser.add_argument_group('compact arguments')
    compact_group.add_argument(
        '--repository',
        help='Path of specific existing repository to compact (must be already specified in a borgmatic configuration file)',
    )
    compact_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress as each segment is compacted',
    )
    compact_group.add_argument(
        '--cleanup-commits',
        dest='cleanup_commits',
        default=False,
        action='store_true',
        help='Cleanup commit-only 17-byte segment files left behind by Borg 1.1 [flag in Borg 1.2 only]',
    )
    compact_group.add_argument(
        '--threshold',
        type=int,
        dest='threshold',
        help='Minimum saved space percentage threshold for compacting a segment, defaults to 10',
    )
    compact_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    create_parser = action_parsers.add_parser(
        'create',
        aliases=ACTION_ALIASES['create'],
        help='Create an archive (actually perform a backup)',
        description='Create an archive (actually perform a backup)',
        add_help=False,
    )
    create_group = create_parser.add_argument_group('create arguments')
    create_group.add_argument(
        '--repository',
        help='Path of specific existing repository to backup to (must be already specified in a borgmatic configuration file)',
    )
    create_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is backed up',
    )
    create_group.add_argument(
        '--stats',
        dest='stats',
        default=False,
        action='store_true',
        help='Display statistics of archive',
    )
    create_group.add_argument(
        '--list', '--files', dest='list_files', action='store_true', help='Show per-file details'
    )
    create_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    create_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    check_parser = action_parsers.add_parser(
        'check',
        aliases=ACTION_ALIASES['check'],
        help='Check archives for consistency',
        description='Check archives for consistency',
        add_help=False,
    )
    check_group = check_parser.add_argument_group('check arguments')
    check_group.add_argument(
        '--repository',
        help='Path of specific existing repository to check (must be already specified in a borgmatic configuration file)',
    )
    check_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is checked',
    )
    check_group.add_argument(
        '--repair',
        dest='repair',
        default=False,
        action='store_true',
        help='Attempt to repair any inconsistencies found (for interactive use)',
    )
    check_group.add_argument(
        '--only',
        metavar='CHECK',
        choices=('repository', 'archives', 'data', 'extract'),
        dest='only',
        action='append',
        help='Run a particular consistency check (repository, archives, data, or extract) instead of configured checks (subject to configured frequency, can specify flag multiple times)',
    )
    check_group.add_argument(
        '--force',
        default=False,
        action='store_true',
        help='Ignore configured check frequencies and run checks unconditionally',
    )
    check_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    extract_parser = action_parsers.add_parser(
        'extract',
        aliases=ACTION_ALIASES['extract'],
        help='Extract files from a named archive to the current directory',
        description='Extract a named archive to the current directory',
        add_help=False,
    )
    extract_group = extract_parser.add_argument_group('extract arguments')
    extract_group.add_argument(
        '--repository',
        help='Path of repository to extract, defaults to the configured repository if there is only one',
    )
    extract_group.add_argument(
        '--archive', help='Name of archive to extract (or "latest")', required=True
    )
    extract_group.add_argument(
        '--path',
        '--restore-path',
        metavar='PATH',
        dest='paths',
        action='append',
        help='Path to extract from archive, can specify flag multiple times, defaults to the entire archive',
    )
    extract_group.add_argument(
        '--destination',
        metavar='PATH',
        dest='destination',
        help='Directory to extract files into, defaults to the current directory',
    )
    extract_group.add_argument(
        '--strip-components',
        type=lambda number: number if number == 'all' else int(number),
        metavar='NUMBER',
        help='Number of leading path components to remove from each extracted path or "all" to strip all leading path components. Skip paths with fewer elements',
    )
    extract_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is extracted',
    )
    extract_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    config_parser = action_parsers.add_parser(
        'config',
        aliases=ACTION_ALIASES['config'],
        help='Perform configuration file related operations',
        description='Perform configuration file related operations',
        add_help=False,
    )

    config_group = config_parser.add_argument_group('config arguments')
    config_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    config_parsers = config_parser.add_subparsers(
        title='config sub-actions',
    )

    config_bootstrap_parser = config_parsers.add_parser(
        'bootstrap',
        help='Extract the borgmatic configuration files from a named archive',
        description='Extract the borgmatic configuration files from a named archive',
        add_help=False,
    )
    config_bootstrap_group = config_bootstrap_parser.add_argument_group(
        'config bootstrap arguments'
    )
    config_bootstrap_group.add_argument(
        '--repository',
        help='Path of repository to extract config files from',
        required=True,
    )
    config_bootstrap_group.add_argument(
        '--borgmatic-source-directory',
        help='Path that stores the config files used to create an archive and additional source files used for temporary internal state like borgmatic database dumps. Defaults to ~/.borgmatic',
    )
    config_bootstrap_group.add_argument(
        '--archive',
        help='Name of archive to extract config files from, defaults to "latest"',
        default='latest',
    )
    config_bootstrap_group.add_argument(
        '--destination',
        metavar='PATH',
        dest='destination',
        help='Directory to extract config files into, defaults to /',
        default='/',
    )
    config_bootstrap_group.add_argument(
        '--strip-components',
        type=lambda number: number if number == 'all' else int(number),
        metavar='NUMBER',
        help='Number of leading path components to remove from each extracted path or "all" to strip all leading path components. Skip paths with fewer elements',
    )
    config_bootstrap_group.add_argument(
        '--progress',
        dest='progress',
        default=False,
        action='store_true',
        help='Display progress for each file as it is extracted',
    )
    config_bootstrap_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    config_generate_parser = config_parsers.add_parser(
        'generate',
        help='Generate a sample borgmatic configuration file',
        description='Generate a sample borgmatic configuration file',
        add_help=False,
    )
    config_generate_group = config_generate_parser.add_argument_group('config generate arguments')
    config_generate_group.add_argument(
        '-s',
        '--source',
        dest='source_filename',
        help='Optional configuration file to merge into the generated configuration, useful for upgrading your configuration',
    )
    config_generate_group.add_argument(
        '-d',
        '--destination',
        dest='destination_filename',
        default=config_paths[0],
        help=f'Destination configuration file, default: {unexpanded_config_paths[0]}',
    )
    config_generate_group.add_argument(
        '--overwrite',
        default=False,
        action='store_true',
        help='Whether to overwrite any existing destination file, defaults to false',
    )
    config_generate_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    config_validate_parser = config_parsers.add_parser(
        'validate',
        help='Validate borgmatic configuration files specified with --config (see borgmatic --help)',
        description='Validate borgmatic configuration files specified with --config (see borgmatic --help)',
        add_help=False,
    )
    config_validate_group = config_validate_parser.add_argument_group('config validate arguments')
    config_validate_group.add_argument(
        '-s',
        '--show',
        action='store_true',
        help='Show the validated configuration after all include merging has occurred',
    )
    config_validate_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    export_tar_parser = action_parsers.add_parser(
        'export-tar',
        aliases=ACTION_ALIASES['export-tar'],
        help='Export an archive to a tar-formatted file or stream',
        description='Export an archive to a tar-formatted file or stream',
        add_help=False,
    )
    export_tar_group = export_tar_parser.add_argument_group('export-tar arguments')
    export_tar_group.add_argument(
        '--repository',
        help='Path of repository to export from, defaults to the configured repository if there is only one',
    )
    export_tar_group.add_argument(
        '--archive', help='Name of archive to export (or "latest")', required=True
    )
    export_tar_group.add_argument(
        '--path',
        metavar='PATH',
        dest='paths',
        action='append',
        help='Path to export from archive, can specify flag multiple times, defaults to the entire archive',
    )
    export_tar_group.add_argument(
        '--destination',
        metavar='PATH',
        dest='destination',
        help='Path to destination export tar file, or "-" for stdout (but be careful about dirtying output with --verbosity or --list)',
        required=True,
    )
    export_tar_group.add_argument(
        '--tar-filter', help='Name of filter program to pipe data through'
    )
    export_tar_group.add_argument(
        '--list', '--files', dest='list_files', action='store_true', help='Show per-file details'
    )
    export_tar_group.add_argument(
        '--strip-components',
        type=int,
        metavar='NUMBER',
        dest='strip_components',
        help='Number of leading path components to remove from each exported path. Skip paths with fewer elements',
    )
    export_tar_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    mount_parser = action_parsers.add_parser(
        'mount',
        aliases=ACTION_ALIASES['mount'],
        help='Mount files from a named archive as a FUSE filesystem',
        description='Mount a named archive as a FUSE filesystem',
        add_help=False,
    )
    mount_group = mount_parser.add_argument_group('mount arguments')
    mount_group.add_argument(
        '--repository',
        help='Path of repository to use, defaults to the configured repository if there is only one',
    )
    mount_group.add_argument('--archive', help='Name of archive to mount (or "latest")')
    mount_group.add_argument(
        '--mount-point',
        metavar='PATH',
        dest='mount_point',
        help='Path where filesystem is to be mounted',
        required=True,
    )
    mount_group.add_argument(
        '--path',
        metavar='PATH',
        dest='paths',
        action='append',
        help='Path to mount from archive, can specify multiple times, defaults to the entire archive',
    )
    mount_group.add_argument(
        '--foreground',
        dest='foreground',
        default=False,
        action='store_true',
        help='Stay in foreground until ctrl-C is pressed',
    )
    mount_group.add_argument(
        '--first',
        metavar='N',
        help='Mount first N archives after other filters are applied',
    )
    mount_group.add_argument(
        '--last', metavar='N', help='Mount last N archives after other filters are applied'
    )
    mount_group.add_argument(
        '--oldest',
        metavar='TIMESPAN',
        help='Mount archives within a specified time range starting from the timestamp of the oldest archive (e.g. 7d or 12m) [Borg 2.x+ only]',
    )
    mount_group.add_argument(
        '--newest',
        metavar='TIMESPAN',
        help='Mount archives within a time range that ends at timestamp of the newest archive and starts a specified time range ago (e.g. 7d or 12m) [Borg 2.x+ only]',
    )
    mount_group.add_argument(
        '--older',
        metavar='TIMESPAN',
        help='Mount archives that are older than the specified time range (e.g. 7d or 12m) from the current time [Borg 2.x+ only]',
    )
    mount_group.add_argument(
        '--newer',
        metavar='TIMESPAN',
        help='Mount archives that are newer than the specified time range (e.g. 7d or 12m) from the current time [Borg 2.x+ only]',
    )
    mount_group.add_argument('--options', dest='options', help='Extra Borg mount options')
    mount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    umount_parser = action_parsers.add_parser(
        'umount',
        aliases=ACTION_ALIASES['umount'],
        help='Unmount a FUSE filesystem that was mounted with "borgmatic mount"',
        description='Unmount a mounted FUSE filesystem',
        add_help=False,
    )
    umount_group = umount_parser.add_argument_group('umount arguments')
    umount_group.add_argument(
        '--mount-point',
        metavar='PATH',
        dest='mount_point',
        help='Path of filesystem to unmount',
        required=True,
    )
    umount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    restore_parser = action_parsers.add_parser(
        'restore',
        aliases=ACTION_ALIASES['restore'],
        help='Restore data source (e.g. database) dumps from a named archive',
        description='Restore data source (e.g. database) dumps from a named archive. (To extract files instead, use "borgmatic extract".)',
        add_help=False,
    )
    restore_group = restore_parser.add_argument_group('restore arguments')
    restore_group.add_argument(
        '--repository',
        help='Path of repository to restore from, defaults to the configured repository if there is only one',
    )
    restore_group.add_argument(
        '--archive', help='Name of archive to restore from (or "latest")', required=True
    )
    restore_group.add_argument(
        '--data-source',
        '--database',
        metavar='NAME',
        dest='data_sources',
        action='append',
        help="Name of data source (e.g. database) to restore from archive, must be defined in borgmatic's configuration, can specify flag multiple times, defaults to all data sources in the archive",
    )
    restore_group.add_argument(
        '--schema',
        metavar='NAME',
        dest='schemas',
        action='append',
        help='Name of schema to restore from the data source, can specify flag multiple times, defaults to all schemas. Schemas are only supported for PostgreSQL and MongoDB databases',
    )
    restore_group.add_argument(
        '--hostname',
        help='Database hostname to restore to. Defaults to the "restore_hostname" option in borgmatic\'s configuration',
    )
    restore_group.add_argument(
        '--port',
        help='Database port to restore to. Defaults to the "restore_port" option in borgmatic\'s configuration',
    )
    restore_group.add_argument(
        '--username',
        help='Username with which to connect to the database. Defaults to the "restore_username" option in borgmatic\'s configuration',
    )
    restore_group.add_argument(
        '--password',
        help='Password with which to connect to the restore database. Defaults to the "restore_password" option in borgmatic\'s configuration',
    )
    restore_group.add_argument(
        '--restore-path',
        help='Path to restore SQLite database dumps to. Defaults to the "restore_path" option in borgmatic\'s configuration',
    )
    restore_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    rlist_parser = action_parsers.add_parser(
        'rlist',
        aliases=ACTION_ALIASES['rlist'],
        help='List repository',
        description='List the archives in a repository',
        add_help=False,
    )
    rlist_group = rlist_parser.add_argument_group('rlist arguments')
    rlist_group.add_argument(
        '--repository',
        help='Path of repository to list, defaults to the configured repositories',
    )
    rlist_group.add_argument(
        '--short', default=False, action='store_true', help='Output only archive names'
    )
    rlist_group.add_argument('--format', help='Format for archive listing')
    rlist_group.add_argument(
        '--json', default=False, action='store_true', help='Output results as JSON'
    )
    rlist_group.add_argument(
        '-P', '--prefix', help='Deprecated. Only list archive names starting with this prefix'
    )
    rlist_group.add_argument(
        '-a',
        '--match-archives',
        '--glob-archives',
        metavar='PATTERN',
        help='Only list archive names matching this pattern',
    )
    rlist_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    rlist_group.add_argument(
        '--first', metavar='N', help='List first N archives after other filters are applied'
    )
    rlist_group.add_argument(
        '--last', metavar='N', help='List last N archives after other filters are applied'
    )
    rlist_group.add_argument(
        '--oldest',
        metavar='TIMESPAN',
        help='List archives within a specified time range starting from the timestamp of the oldest archive (e.g. 7d or 12m) [Borg 2.x+ only]',
    )
    rlist_group.add_argument(
        '--newest',
        metavar='TIMESPAN',
        help='List archives within a time range that ends at timestamp of the newest archive and starts a specified time range ago (e.g. 7d or 12m) [Borg 2.x+ only]',
    )
    rlist_group.add_argument(
        '--older',
        metavar='TIMESPAN',
        help='List archives that are older than the specified time range (e.g. 7d or 12m) from the current time [Borg 2.x+ only]',
    )
    rlist_group.add_argument(
        '--newer',
        metavar='TIMESPAN',
        help='List archives that are newer than the specified time range (e.g. 7d or 12m) from the current time [Borg 2.x+ only]',
    )
    rlist_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    list_parser = action_parsers.add_parser(
        'list',
        aliases=ACTION_ALIASES['list'],
        help='List archive',
        description='List the files in an archive or search for a file across archives',
        add_help=False,
    )
    list_group = list_parser.add_argument_group('list arguments')
    list_group.add_argument(
        '--repository',
        help='Path of repository containing archive to list, defaults to the configured repositories',
    )
    list_group.add_argument('--archive', help='Name of the archive to list (or "latest")')
    list_group.add_argument(
        '--path',
        metavar='PATH',
        dest='paths',
        action='append',
        help='Path or pattern to list from a single selected archive (via "--archive"), can specify flag multiple times, defaults to listing the entire archive',
    )
    list_group.add_argument(
        '--find',
        metavar='PATH',
        dest='find_paths',
        action='append',
        help='Partial path or pattern to search for and list across multiple archives, can specify flag multiple times',
    )
    list_group.add_argument(
        '--short', default=False, action='store_true', help='Output only path names'
    )
    list_group.add_argument('--format', help='Format for file listing')
    list_group.add_argument(
        '--json', default=False, action='store_true', help='Output results as JSON'
    )
    list_group.add_argument(
        '-P', '--prefix', help='Deprecated. Only list archive names starting with this prefix'
    )
    list_group.add_argument(
        '-a',
        '--match-archives',
        '--glob-archives',
        metavar='PATTERN',
        help='Only list archive names matching this pattern',
    )
    list_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    list_group.add_argument(
        '--first', metavar='N', help='List first N archives after other filters are applied'
    )
    list_group.add_argument(
        '--last', metavar='N', help='List last N archives after other filters are applied'
    )
    list_group.add_argument(
        '-e', '--exclude', metavar='PATTERN', help='Exclude paths matching the pattern'
    )
    list_group.add_argument(
        '--exclude-from', metavar='FILENAME', help='Exclude paths from exclude file, one per line'
    )
    list_group.add_argument('--pattern', help='Include or exclude paths matching a pattern')
    list_group.add_argument(
        '--patterns-from',
        metavar='FILENAME',
        help='Include or exclude paths matching patterns from pattern file, one per line',
    )
    list_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    rinfo_parser = action_parsers.add_parser(
        'rinfo',
        aliases=ACTION_ALIASES['rinfo'],
        help='Show repository summary information such as disk space used',
        description='Show repository summary information such as disk space used',
        add_help=False,
    )
    rinfo_group = rinfo_parser.add_argument_group('rinfo arguments')
    rinfo_group.add_argument(
        '--repository',
        help='Path of repository to show info for, defaults to the configured repository if there is only one',
    )
    rinfo_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    rinfo_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    info_parser = action_parsers.add_parser(
        'info',
        aliases=ACTION_ALIASES['info'],
        help='Show archive summary information such as disk space used',
        description='Show archive summary information such as disk space used',
        add_help=False,
    )
    info_group = info_parser.add_argument_group('info arguments')
    info_group.add_argument(
        '--repository',
        help='Path of repository containing archive to show info for, defaults to the configured repository if there is only one',
    )
    info_group.add_argument('--archive', help='Name of archive to show info for (or "latest")')
    info_group.add_argument(
        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'
    )
    info_group.add_argument(
        '-P',
        '--prefix',
        help='Deprecated. Only show info for archive names starting with this prefix',
    )
    info_group.add_argument(
        '-a',
        '--match-archives',
        '--glob-archives',
        metavar='PATTERN',
        help='Only show info for archive names matching this pattern',
    )
    info_group.add_argument(
        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'
    )
    info_group.add_argument(
        '--first',
        metavar='N',
        help='Show info for first N archives after other filters are applied',
    )
    info_group.add_argument(
        '--last', metavar='N', help='Show info for last N archives after other filters are applied'
    )
    info_group.add_argument(
        '--oldest',
        metavar='TIMESPAN',
        help='Show info for archives within a specified time range starting from the timestamp of the oldest archive (e.g. 7d or 12m) [Borg 2.x+ only]',
    )
    info_group.add_argument(
        '--newest',
        metavar='TIMESPAN',
        help='Show info for archives within a time range that ends at timestamp of the newest archive and starts a specified time range ago (e.g. 7d or 12m) [Borg 2.x+ only]',
    )
    info_group.add_argument(
        '--older',
        metavar='TIMESPAN',
        help='Show info for archives that are older than the specified time range (e.g. 7d or 12m) from the current time [Borg 2.x+ only]',
    )
    info_group.add_argument(
        '--newer',
        metavar='TIMESPAN',
        help='Show info for archives that are newer than the specified time range (e.g. 7d or 12m) from the current time [Borg 2.x+ only]',
    )
    info_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    break_lock_parser = action_parsers.add_parser(
        'break-lock',
        aliases=ACTION_ALIASES['break-lock'],
        help='Break the repository and cache locks left behind by Borg aborting',
        description='Break Borg repository and cache locks left behind by Borg aborting',
        add_help=False,
    )
    break_lock_group = break_lock_parser.add_argument_group('break-lock arguments')
    break_lock_group.add_argument(
        '--repository',
        help='Path of repository to break the lock for, defaults to the configured repository if there is only one',
    )
    break_lock_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    key_parser = action_parsers.add_parser(
        'key',
        aliases=ACTION_ALIASES['key'],
        help='Perform repository key related operations',
        description='Perform repository key related operations',
        add_help=False,
    )

    key_group = key_parser.add_argument_group('key arguments')
    key_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    key_parsers = key_parser.add_subparsers(
        title='key sub-actions',
    )

    key_export_parser = key_parsers.add_parser(
        'export',
        help='Export a copy of the repository key for safekeeping in case the original goes missing or gets damaged',
        description='Export a copy of the repository key for safekeeping in case the original goes missing or gets damaged',
        add_help=False,
    )
    key_export_group = key_export_parser.add_argument_group('key export arguments')
    key_export_group.add_argument(
        '--paper',
        action='store_true',
        help='Export the key in a text format suitable for printing and later manual entry',
    )
    key_export_group.add_argument(
        '--qr-html',
        action='store_true',
        help='Export the key in an HTML format suitable for printing and later manual entry or QR code scanning',
    )
    key_export_group.add_argument(
        '--repository',
        help='Path of repository to export the key for, defaults to the configured repository if there is only one',
    )
    key_export_group.add_argument(
        '--path',
        metavar='PATH',
        help='Path to export the key to, defaults to stdout (but be careful about dirtying the output with --verbosity)',
    )
    key_export_group.add_argument(
        '-h', '--help', action='help', help='Show this help message and exit'
    )

    borg_parser = action_parsers.add_parser(
        'borg',
        aliases=ACTION_ALIASES['borg'],
        help='Run an arbitrary Borg command',
        description="Run an arbitrary Borg command based on borgmatic's configuration",
        add_help=False,
    )
    borg_group = borg_parser.add_argument_group('borg arguments')
    borg_group.add_argument(
        '--repository',
        help='Path of repository to pass to Borg, defaults to the configured repositories',
    )
    borg_group.add_argument('--archive', help='Name of archive to pass to Borg (or "latest")')
    borg_group.add_argument(
        '--',
        metavar='OPTION',
        dest='options',
        nargs='+',
        help='Options to pass to Borg, command first ("create", "list", etc). "--" is optional. To specify the repository or the archive, you must use --repository or --archive instead of providing them here.',
    )
    borg_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')

    return global_parser, action_parsers, global_plus_action_parser


def parse_arguments(*unparsed_arguments):
    '''
    Given command-line arguments with which this script was invoked, parse the arguments and return
    them as a dict mapping from action name (or "global") to an argparse.Namespace instance.

    Raise ValueError if the arguments cannot be parsed.
    Raise SystemExit with an error code of 0 if "--help" was requested.
    '''
    global_parser, action_parsers, global_plus_action_parser = make_parsers()
    arguments, remaining_action_arguments = parse_arguments_for_actions(
        unparsed_arguments, action_parsers.choices, global_parser
    )

    if not arguments['global'].config_paths:
        arguments['global'].config_paths = collect.get_default_config_paths(expand_home=True)

    for action_name in ('bootstrap', 'generate', 'validate'):
        if (
            action_name in arguments.keys() and len(arguments.keys()) > 2
        ):  # 2 = 1 for 'global' + 1 for the action
            raise ValueError(
                f'The {action_name} action cannot be combined with other actions. Please run it separately.'
            )

    unknown_arguments = get_unparsable_arguments(remaining_action_arguments)

    if unknown_arguments:
        if '--help' in unknown_arguments or '-h' in unknown_arguments:
            global_plus_action_parser.print_help()
            sys.exit(0)

        global_plus_action_parser.print_usage()
        raise ValueError(
            f"Unrecognized argument{'s' if len(unknown_arguments) > 1 else ''}: {' '.join(unknown_arguments)}"
        )

    if 'create' in arguments and arguments['create'].list_files and arguments['create'].progress:
        raise ValueError(
            'With the create action, only one of --list (--files) and --progress flags can be used.'
        )
    if 'create' in arguments and arguments['create'].list_files and arguments['create'].json:
        raise ValueError(
            'With the create action, only one of --list (--files) and --json flags can be used.'
        )

    if (
        ('list' in arguments and 'rinfo' in arguments and arguments['list'].json)
        or ('list' in arguments and 'info' in arguments and arguments['list'].json)
        or ('rinfo' in arguments and 'info' in arguments and arguments['rinfo'].json)
    ):
        raise ValueError('With the --json flag, multiple actions cannot be used together.')

    if (
        'transfer' in arguments
        and arguments['transfer'].archive
        and arguments['transfer'].match_archives
    ):
        raise ValueError(
            'With the transfer action, only one of --archive and --match-archives flags can be used.'
        )

    if 'list' in arguments and (arguments['list'].prefix and arguments['list'].match_archives):
        raise ValueError(
            'With the list action, only one of --prefix or --match-archives flags can be used.'
        )

    if 'rlist' in arguments and (arguments['rlist'].prefix and arguments['rlist'].match_archives):
        raise ValueError(
            'With the rlist action, only one of --prefix or --match-archives flags can be used.'
        )

    if 'info' in arguments and (
        (arguments['info'].archive and arguments['info'].prefix)
        or (arguments['info'].archive and arguments['info'].match_archives)
        or (arguments['info'].prefix and arguments['info'].match_archives)
    ):
        raise ValueError(
            'With the info action, only one of --archive, --prefix, or --match-archives flags can be used.'
        )

    return arguments

if __name__ == "__main__":
    res0 = make_parsers()
    aa="arguments_make_parsers_passk_validte"
    if not res0[0].prog.startswith(aa) or not res0[0].allow_abbrev:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/standalone/infoblox_client-utils-paging.py
def paging(response, max_results):
    """Returns WAPI response page by page

    Args:
        response (list): WAPI response.
        max_results (int): Maximum number of objects to be returned in one page.
    Returns:
        Generator object with WAPI response split page by page.
    """
    i = 0
    while i < len(response):
        yield response[i:i + max_results]
        i = i + max_results


def test_paging():
    """
    Check the corretness of paging
    """
    assert list(paging([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)) == [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]
    assert list(paging([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4)) == [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10]]
    assert list(paging([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 5)) == [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]
    assert list(paging([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 6)) == [[1, 2, 3, 4, 5, 6], [7, 8, 9, 10]]
    assert list(paging([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 7)) == [[1, 2, 3, 4, 5, 6, 7], [8, 9, 10]]


if __name__ == "__main__":
    test_paging()


----------------------------
/home/travis/builds/repos/standalone/swh-lister-arch-lister-size_to_bytes.py
def size_to_bytes(size: str) -> int:
    """Convert human readable file size to bytes.

    Resulting value is an approximation as input value is in most case rounded.

    Args:
        size: A string representing a human readable file size (eg: '500K')

    Returns:
        A decimal representation of file size

        Examples::

            >>> size_to_bytes("500")
            500
            >>> size_to_bytes("1K")
            1000
    """
    units = {
        "K": 1000,
        "M": 1000 ** 2,
        "G": 1000 ** 3,
        "T": 1000 ** 4,
        "P": 1000 ** 5,
        "E": 1000 ** 6,
        "Z": 1000 ** 7,
        "Y": 1000 ** 8,
    }
    if size.endswith(tuple(units)):
        v, u = (size[:-1], size[-1])
        return int(v) * units[u]
    else:
        return int(size)


def test_size_to_bytes():
    """
    Check the corretness of size_to_bytes
    """
    assert size_to_bytes("500") == 500
    assert size_to_bytes("1K") == 1000
    assert size_to_bytes("1M") == 1000 ** 2
    assert size_to_bytes("1G") == 1000 ** 3
    assert size_to_bytes("1T") == 1000 ** 4
    assert size_to_bytes("1P") == 1000 ** 5


if __name__ == "__main__":
    test_size_to_bytes()


----------------------------
/home/travis/builds/repos/standalone/contrib-planb-swiftsync-_dictsum.py
def _dictsum(dicts):
    """
    Combine values of the dictionaries supplied by iterable dicts.

    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])
    {'a': 6, 'b': 2}
    """
    it = iter(dicts)
    first = next(it).copy()
    for d in it:
        for k, v in d.items():
            first[k] += v
    return first


def test__dictsum():
    """
    Check the corretness of _dictsum
    """
    assert _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}]) == {'a': 6, 'b': 2}
    assert _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}, {'a': 1, 'b': 2}]) == {'a': 7, 'b': 4}
    assert _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}, {'a': 1, 'b': 2}, {'a': 1, 'b': 2}]) == {'a': 8, 'b': 6}
    assert _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}, {'a': 1, 'b': 2}, {'a': 1, 'b': 2}, {'a': 1, 'b': 2}]) == {
        'a': 9, 'b': 8}
    assert _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}, {'a': 1, 'b': 2}, {'a': 1, 'b': 2}, {'a': 1, 'b': 2},
                     {'a': 1, 'b': 2}]) == {'a': 10, 'b': 10}


if __name__ == "__main__":
    test__dictsum()


----------------------------
/home/travis/builds/repos/standalone/pyseed-apibase-_replace_url_args.py
def _replace_url_args(url, url_args):
    """Replace any custom string URL items with values in args"""
    if url_args:
        for key, value in url_args.items():
            url = url.replace(f"{key}/", f"{value}/")
    return url


def test__replace_url_args():
    """
    Check the corretness of _replace_url_args
    """
    assert _replace_url_args("http://localhost:8080/test/", {}) == "http://localhost:8080/test/"
    assert _replace_url_args("http://localhost:8080/test/", {"test": "test"}) == "http://localhost:8080/test/"
    assert _replace_url_args("http://localhost:8080/test/",
                             {"test": "test", "test2": "test2"}) == "http://localhost:8080/test/"
    assert _replace_url_args("http://localhost:8080/test/",
                             {"test": "test", "test2": "test2", "test3": "test3"}) == "http://localhost:8080/test/"
    assert _replace_url_args("http://localhost:8080/test/", {"test": "test", "test2": "test2", "test3": "test3",
                                                             "test4": "test4"}) == "http://localhost:8080/test/"
    assert _replace_url_args("http://localhost:8080/test/",
                             {"test": "reverse", "test2": "test2", "test3": "test3", "test4": "test4",
                              "test5": "test5"}) == "http://localhost:8080/reverse/"
    assert _replace_url_args("http://localhost:8080/test/",
                             {"test": "reverse", "test2": "test2", "test3": "test3", "test4": "test4", "test5": "test5",
                              "test6": "test6"}) == "http://localhost:8080/reverse/"


if __name__ == "__main__":
    test__replace_url_args()


----------------------------
/home/travis/builds/repos/standalone/cinder-api-api_utils-is_none_string.py
def is_none_string(val: any) -> bool:
    """Check if a string represents a None value."""
    if not isinstance(val, str):
        return False

    return val.lower() == 'none'


def test_is_none_string():
    """
    Check the corretness of is_none_string
    """
    assert is_none_string('None') == True
    assert is_none_string('none') == True
    assert is_none_string('not none') == False
    assert is_none_string(None) == False
    assert is_none_string('') == False
    assert is_none_string(' ') == False


if __name__ == "__main__":
    test_is_none_string()


----------------------------
/home/travis/builds/repos/standalone/borgmatic-commands-completion-parser_flags.py
import argparse


def parser_flags(parser):
    '''
    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated
    string.
    '''
    return ' '.join(option for action in parser._actions for option in action.option_strings)


def test_parser_flags():
    """
    Check the corretness of parser_flags
    """
    assert parser_flags(argparse.ArgumentParser()) == '-h --help'
    assert parser_flags(argparse.ArgumentParser(add_help=False)) == ''
    assert parser_flags(argparse.ArgumentParser(prog='test')) == '-h --help'
    assert parser_flags(argparse.ArgumentParser(prog='test', add_help=False)) == ''
    assert parser_flags(argparse.ArgumentParser(prog='test', description='test')) == '-h --help'
    assert parser_flags(argparse.ArgumentParser(prog='test', description='test', add_help=False)) == ''
    assert parser_flags(argparse.ArgumentParser(prog='test', description='test', epilog='test')) == '-h --help'


if __name__ == "__main__":
    test_parser_flags()


----------------------------
/home/travis/builds/repos/standalone/makeprojects-util-was_processed.py
def was_processed(processed, path_name, verbose):
    """
    Check if a file or directory has already been processed.

    To prevent recursion, expand the path name to an absolution path
    call this function with a set that will store all the entries and
    the entry to test. If the entry is already in the set, report the issue
    and return ``True``. Otherwise, add the entry to the set and return
    ``False`` to allow the path to be processed.

    Args:
        processed: Set to store processed pathnames
        path_name: Path to a directory or file
        verbose: True if verbose output is requested

    Returns:
        True if it's already in the set. False if not.
    """

    # Test for recursion
    if path_name in processed:
        if verbose:
            print('{} has already been processed'.format(path_name))
        return True

    # Mark this list as "processed" to prevent recursion
    if verbose:
        print('Processing {}.'.format(path_name))
    processed.add(path_name)
    return False


def test_was_processed():
    """
    Check the corretness of was_processed
    """
    assert was_processed(set(), "A", True) == False
    assert was_processed(set(), "A", False) == False
    assert was_processed(set(), "A", True) == False
    assert was_processed(set("A"), "A", False) == True
    assert was_processed(set("A"), "A", True) == True


if __name__ == "__main__":
    test_was_processed()


----------------------------
/home/travis/builds/repos/standalone/eppy-geometry-surface-vertex3tuple.py
def vertex3tuple(vertices):
    """return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::

        polygon with vertices ABCD
        Will return
        DAB, ABC, BCD, CDA -> returns 3tuples
        #A    B    C    D  -> of vertices
    """
    asvertex_list = []
    for i in range(len(vertices)):
        try:
            asvertex_list.append((vertices[i - 1], vertices[i], vertices[i + 1]))
        except IndexError as e:
            asvertex_list.append((vertices[i - 1], vertices[i], vertices[0]))
    return asvertex_list


def test_vertex3tuple():
    """
    Check the corretness of vertex3tuple
    """
    assert set(vertex3tuple(["A", "B", "C", "D"])) == set(
        [("D", "A", "B"), ("A", "B", "C"), ("B", "C", "D"), ("C", "D", "A")])
    assert set(vertex3tuple(["A", "B", "C"])) == set([("A", "B", "C"), ("B", "C", "A"), ("C", "A", "B")])
    assert set(vertex3tuple(["A", "B", "C", "D", "E"])) == set(
        [("E", "A", "B"), ("A", "B", "C"), ("B", "C", "D"), ("C", "D", "E"), ("D", "E", "A")])
    assert set(vertex3tuple(["A", "B", "C", "D", "E", "F"])) == set(
        [("F", "A", "B"), ("A", "B", "C"), ("B", "C", "D"), ("C", "D", "E"), ("D", "E", "F"), ("E", "F", "A")])


if __name__ == "__main__":
    test_vertex3tuple()


----------------------------
/home/travis/builds/repos/standalone/shortuuid-main-int_to_string.py
def int_to_string(number: int, alphabet, padding= None) -> str:
    """
    Convert a number to a string, using the given alphabet.

    The output has the most significant digit first.
    """
    output = ""
    alpha_len = len(alphabet)
    while number:
        number, digit = divmod(number, alpha_len)
        output += alphabet[digit]
    if padding:
        remainder = max(padding - len(output), 0)
        output = output + alphabet[0] * remainder
    return output[::-1]


def test_int_to_string():
    """
    Check the corretness of int_to_string
    """
    assert int_to_string(1, ["a", "b", "c"]) == "b"
    assert int_to_string(1, ["a", "b", "c"], padding=3) == "aab"
    assert int_to_string(1, ["a", "b", "c"], padding=4) == "aaab"
    assert int_to_string(1, ["a", "b", "c"], padding=5) == "aaaab"
    assert int_to_string(1, ["a", "b", "c"], padding=6) == "aaaaab"
    assert int_to_string(1, ["a", "b", "c"], padding=7) == "aaaaaab"
    assert int_to_string(1, ["a", "b", "c"], padding=8) == "aaaaaaab"


if __name__ == "__main__":
    test_int_to_string()


----------------------------
/home/travis/builds/repos/standalone/neutron_lib-agent-common-utils-_replace_register.py
def _replace_register(flow_params, register_number, register_value):
    """Replace value from flows to given register number

    'register_value' key in dictionary will be replaced by register number
    given by 'register_number'

    :param flow_params: Dictionary containing defined flows
    :param register_number: The number of register where value will be stored
    :param register_value: Key to be replaced by register number

    """
    try:
        reg_port = flow_params[register_value]
        del flow_params[register_value]
        flow_params['reg{:d}'.format(register_number)] = reg_port
    except KeyError:
        pass
    return flow_params  # expose the observer to the test_module


def test__replace_register():
    """
    Check the corretness of _replace_register
    """
    assert _replace_register({'reg1': 1, 'reg2': 2, 'reg3': 3}, 1, 'reg1') == {'reg1': 1, 'reg2': 2, 'reg3': 3}
    assert _replace_register({'reg1': 1, 'reg2': 2, 'reg3': 3}, 2, 'reg2') == {'reg1': 1, 'reg2': 2, 'reg3': 3}
    assert _replace_register({'reg1': 1, 'reg2': 2, 'reg3': 3}, 3, 'reg3') == {'reg1': 1, 'reg2': 2, 'reg3': 3}
    assert _replace_register({'reg1': 1, 'reg2': 2, 'reg3': 3}, 1, 'reg2') == {'reg1': 2, 'reg3': 3}
    assert _replace_register({'reg1': 1, 'reg2': 2, 'reg3': 3}, 2, 'reg3') == {'reg1': 1, 'reg2': 3}
    assert _replace_register({'reg1': 1, 'reg2': 2, 'reg3': 3}, 3, 'reg1') == {'reg2': 2, 'reg3': 1}


if __name__ == "__main__":
    test__replace_register()


----------------------------
/home/travis/builds/repos/standalone/release_dashboard-templatetags-rd_extras-replace_dots.py
def replace_dots(value, arg):
    """Replaces all values of '.' to arg from the given string"""
    return value.replace(".", arg)


def test_replace_dots():
    """
    Check the corretness of replace_dots
    """
    assert replace_dots("test.txt", ".") == "test.txt"
    assert replace_dots("test.txt", " ") == "test txt"
    assert replace_dots("test.txt", "") == "testtxt"
    assert replace_dots("test.txt", ".") == "test.txt"


if __name__ == "__main__":
    test_replace_dots()


----------------------------
/home/travis/builds/repos/standalone/rows-utils-__init__-subclasses.py
def subclasses(cls):
    """Return all subclasses of a class, recursively"""
    children = cls.__subclasses__()
    return set(children).union(
        set(grandchild for child in children for grandchild in subclasses(child))
    )


def test_subclasses():
    """
    Check the corretness of subclasses
    """
    assert subclasses(set) == set()


if __name__ == "__main__":
    test_subclasses()


----------------------------
/home/travis/builds/repos/standalone/shortuuid-main-string_to_int.py
def string_to_int(string: str, alphabet) -> int:
    """
    Convert a string to a number, using the given alphabet.

    The input is assumed to have the most significant digit first.
    """
    number = 0
    alpha_len = len(alphabet)
    for char in string:
        number = number * alpha_len + alphabet.index(char)
    return number


def test_string_to_int():
    """
    Check the corretness of string_to_int
    """
    assert string_to_int("b", ["a", "b", "c"]) == 1
    assert string_to_int("c", ["a", "b", "c"]) == 2
    assert string_to_int("aab", ["a", "b", "c"]) == 1
    assert string_to_int("aaab", ["a", "b", "c"]) == 1
    assert string_to_int("aaaab", ["a", "b", "c"]) == 1
    assert string_to_int("aaaaab", ["a", "b", "c"]) == 1


if __name__ == "__main__":
    test_string_to_int()


----------------------------
/home/travis/builds/repos/standalone/swh-lister-arch-lister-get_repo_archive.py
import requests, tarfile
from pathlib import Path


def get_repo_archive(url: str, destination_path: Path) -> Path:
    """
    Given an url and a destination path, retrieve and extract .tar.gz archive
    which contains 'desc' file for each package.
    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').

    Args:
        url: url of the .tar.gz archive to download
        destination_path: the path on disk where to extract archive

    Returns:
        a directory Path where the archive has been extracted to.
    """
    res = requests.get(url)
    print(res.status_code)
    destination_path.parent.mkdir(parents=True, exist_ok=True)
    destination_path.write_bytes(res.content)

    extract_to = Path(str(destination_path).split(".tar.gz")[0])
    tar = tarfile.open(destination_path)
    tar.extractall(path=extract_to)
    tar.close()

    return extract_to


def test_get_repo_archive():
    """Check the correctness of get_repo_archive
    """
    assert get_repo_archive('https://files.pythonhosted.org/packages/bf/40/a1b1810a09e3e85567c17831fcc2fc8e48ad9a1d3b02e8be940c43b908a8/jsonlines-2.0.0.tar.gz',
                            Path('/tmp/jsonlines-2.0.0.tar.gz')) == Path('/tmp/jsonlines-2.0.0')


if __name__ == "__main__":
    test_get_repo_archive()
----------------------------
/home/travis/builds/repos/standalone/cloudmesh-common-systeminfo-os_is_mac.py
import platform


def os_is_mac():
    """
    Checks if the os is macOS

    :return: True is macOS
    :rtype: bool
    """
    return platform.system() == "Darwin"


def test_os_is_mac():
    """Check the correctness of os_is_mac
    """
    assert os_is_mac() == (platform.system() == "Darwin")


if __name__ == "__main__":
    test_os_is_mac()


----------------------------
/home/travis/builds/repos/standalone/makeprojects-util-regex_dict.py
import re, fnmatch


def regex_dict(item):
    """
    Convert *.cpp keys to regex keys

    Given a dict where the keys are all filenames with wildcards, convert only
    the keys into equivalent regexes and leave the values intact.

    Example:

    rules = {
        '*.cpp':
            {'a': 'arf', 'b': 'bark', 'c': 'coo'},
        '*.h':
            {'h': 'help'}
    }
    regex_keys = regex_dict(rules)

    Args:
        item: dict to convert
    Returns:
        dict with keys converted to regexes
    """

    output = {}
    for key in item:
        output[re.compile(fnmatch.translate(key)).match] = item[key]
    return output


def test_regex_dict():
    """Check the correctness of regex_dict
    """
    assert regex_dict({'*.cpp': {'a': 'arf', 'b': 'bark', 'c': 'coo'}}) == {
        re.compile(fnmatch.translate('*.cpp')).match: {'a': 'arf', 'b': 'bark', 'c': 'coo'}}
    assert regex_dict({'*.h': {'h': 'help'}}) == {re.compile(fnmatch.translate('*.h')).match: {'h': 'help'}}
    assert regex_dict({'*.cpp': {'a': 'arf', 'b': 'bark', 'c': 'coo'}, '*.h': {'h': 'help'}}) == {
        re.compile(fnmatch.translate('*.cpp')).match: {'a': 'arf', 'b': 'bark', 'c': 'coo'},
        re.compile(fnmatch.translate('*.h')).match: {'h': 'help'}}


if __name__ == "__main__":
    test_regex_dict()


----------------------------
/home/travis/builds/repos/standalone/rdiffweb-core-librdiff-unquote.py
import re


def unquote(name):
    """Remove quote from the given name."""
    assert isinstance(name, bytes)

    # This function just gives back the original text if it can decode it
    def unquoted_char(match):
        """For each ;000 return the corresponding byte."""
        if len(match.group()) != 4:
            return match.group
        try:
            return bytes([int(match.group()[1:])])
        except ValueError:
            return match.group

    # Remove quote using regex
    return re.sub(b";[0-9]{3}", unquoted_char, name, re.S)


def test_unquote():
    """Check the correctness of unquote
    """
    assert unquote(b"Hello") == b"Hello"
    assert unquote(b"Hello;000") == b'Hello\x00'
    assert unquote(b"Hello;001") == b'Hello\x01'
    assert unquote(b"Hello;002") == b'Hello\x02'
    assert unquote(b"Hello;003") == b'Hello\x03'
    assert unquote(b"Hello;004") == b'Hello\x04'


if __name__ == "__main__":
    test_unquote()


----------------------------
/home/travis/builds/repos/standalone/cloudmesh-common-shlex-split.py
import re, sys


def split(s, platform='this'):
    """Multi-platform variant of shlex.split() for command-line splitting.
    For use with subprocess, for argv injection etc. Using fast REGEX.

    platform: 'this' = auto from current platform;
              1 = POSIX;
              0 = Windows/CMD
              (other values reserved)
    """
    if platform == 'this':
        platform = (sys.platform != 'win32')
    if platform == 1:
        RE_CMD_LEX = r'''"((?:\\["\\]|[^"])*)"|'([^']*)'|(\\.)|(&&?|\|\|?|\d?\>|[<])|([^\s'"\\&|<>]+)|(\s+)|(.)'''
    elif platform == 0:
        RE_CMD_LEX = r'''"((?:""|\\["\\]|[^"])*)"?()|(\\\\(?=\\*")|\\")|(&&?|\|\|?|\d?>|[<])|([^\s"&|<>]+)|(\s+)|(.)'''
    else:
        raise AssertionError('unkown platform %r' % platform)

    args = []
    accu = None  # collects pieces of one arg
    for qs, qss, esc, pipe, word, white, fail in re.findall(RE_CMD_LEX, s):
        if word:
            pass  # most frequent
        elif esc:
            word = esc[1]
        elif white or pipe:
            if accu is not None:
                args.append(accu)
            if pipe:
                args.append(pipe)
            accu = None
            continue
        elif fail:
            raise ValueError("invalid or incomplete shell string")
        elif qs:
            word = qs.replace('\\"', '"').replace('\\\\', '\\')
            if platform == 0:
                word = word.replace('""', '"')
        else:
            word = qss  # may be even empty; must be last

        accu = (accu or '') + word

    if accu is not None:
        args.append(accu)

    return args


def test_split():
    """Check the correctness of split
    """
    assert split('"a" "b"') == ['a', 'b']
    assert split('"a" "b"', platform=0) == ['a', 'b']
    assert split('"a" "b"', platform=1) == ['a', 'b']
    assert split('"a" "b"', platform='this') == ['a', 'b']
    assert split('"a" "b"', platform=0) == ['a', 'b']


if __name__ == "__main__":
    test_split()


----------------------------
/home/travis/builds/repos/standalone/swh-lister-arch-tests-__init__-prepare_repository_from_archive.py
import subprocess, os
from typing import Optional, Union
from pathlib import PosixPath


def prepare_repository_from_archive(
        archive_path: str,
        filename: Optional[str] = None,
        tmp_path: Union[PosixPath, str] = "/tmp",
) -> str:
    """Given an existing archive_path, uncompress it.
    Returns a file repo url which can be used as origin url.

    This does not deal with the case where the archive passed along does not exist.
    """
    if not isinstance(tmp_path, str):
        tmp_path = str(tmp_path)
    # uncompress folder/repositories/dump for the loader to ingest
    subprocess.check_output(["tar", "xf", archive_path, "-C", tmp_path])
    # build the origin url (or some derivative form)
    _fname = filename if filename else os.path.basename(archive_path)
    repo_url = f"file://{tmp_path}/{_fname}"
    return repo_url


def test_prepare_repository_from_archive():
    """Check the correctness of prepare_repository_from_archive
    """
    #print(prepare_repository_from_archive("dump.tar.gz"))
    assert prepare_repository_from_archive("dump.tar.gz") == "file:///tmp/dump.tar.gz"


if __name__ == "__main__":
    os.chdir("/home/travis/builds/repos/standalone")
    test_prepare_repository_from_archive()
----------------------------
/home/travis/builds/repos/commandline---flashbake/src/flashbake/plugins/ignored_addignored_passk_validte.py
import subprocess


def addignored(ignored):
    ''' Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.'''
    import os
    os.chdir("/home/travis/builds/repos/zimeon---ocfl-py")
    fldr = subprocess.run(["git", "-C", ignored, "status", "-s", "--ignored"], capture_output=True,
                          text=True).stdout.strip("\n")
    x = fldr.splitlines()
    sub = "!"
    g = ([s for s in x if sub in s])
    i = [elem.replace(sub, '') for elem in g]
    t = ", ".join(i)
    return t


def test_addignored():
    """Check the correctness of addignored
    """
    # print(addignored("."))
    assert addignored(".") == " ocfl/__pycache__/"


if __name__ == "__main__":
    test_addignored()


----------------------------
/home/travis/builds/repos/standalone/docopt-__init__-match.py
import os


def match(filename):
    """
    Check if the filename is a type that this module supports

    Args:
        filename: Filename to match
    Returns:
        False if not a match, True if supported
    """

    base_name = os.path.basename(filename)
    base_name_lower = base_name.lower()
    return base_name_lower == 'doxyfile'


def test_match():
    """Check the correctness of match
    """
    assert match('doxyfile')
    assert not match('doxygen.conf')
    assert not match('doxygen.conf.dist')
    assert not match('doxygen.conf.dist.dist')
    assert not match('doxygen.conf.dist.dist.dist')
    assert match('DOXyFile')
    assert match('DOXyFILE')


if __name__ == "__main__":
    test_match()


----------------------------
/home/travis/builds/repos/standalone/borgmatic-borg-check-parse_frequency.py
import datetime


def parse_frequency(frequency):
    '''
    Given a frequency string with a number and a unit of time, return a corresponding
    datetime.timedelta instance or None if the frequency is None or "always".

    For instance, given "3 weeks", return datetime.timedelta(weeks=3)

    Raise ValueError if the given frequency cannot be parsed.
    '''
    if not frequency:
        return None

    frequency = frequency.strip().lower()

    if frequency == 'always':
        return None

    try:
        number, time_unit = frequency.split(' ')
        number = int(number)
    except ValueError:
        raise ValueError(f"Could not parse consistency check frequency '{frequency}'")

    if not time_unit.endswith('s'):
        time_unit += 's'

    if time_unit == 'months':
        number *= 4
        time_unit = 'weeks'
    elif time_unit == 'years':
        number *= 365
        time_unit = 'days'

    try:
        return datetime.timedelta(**{time_unit: number})
    except TypeError:
        raise ValueError(f"Could not parse consistency check frequency '{frequency}'")


def test_parse_frequency():
    """Check the correctness of parse_frequency
    """
    assert parse_frequency('1 day') == datetime.timedelta(days=1)
    assert parse_frequency('1 week') == datetime.timedelta(weeks=1)
    assert parse_frequency('1 month') == datetime.timedelta(weeks=4)
    assert parse_frequency('1 year') == datetime.timedelta(days=365)
    assert parse_frequency('1 day') == datetime.timedelta(days=1)
    assert parse_frequency('10 day') == datetime.timedelta(days=10)


if __name__ == "__main__":
    test_parse_frequency()


----------------------------
/home/travis/builds/repos/standalone/cloudmesh-common-util-is_local.py
import socket,platform


def is_local(host):
    """
    Checks if the host is the localhost

    :param host: The hostname or ip
    :return: True if the host is the localhost
    """
    return host in ["127.0.0.1",
                    "localhost",
                    socket.gethostname(),
                    # just in case socket.gethostname() does not work  we also try the following:
                    platform.node(),
                    socket.gethostbyaddr(socket.gethostname())[0]
                    ]


def test_is_local():
    """Check the correctness of is_local
    """
    assert is_local(' ') == False
    assert is_local('   ') == False
    assert is_local('127.0.0.1') == True
    assert is_local('localhost') == True
    assert is_local(' localhost ') == False
    assert is_local(platform.node()) == True
    assert is_local(socket.gethostbyaddr(socket.gethostname())[0]) == True
    assert is_local(socket.gethostname()) == True
    #print( is_local(' '))
    #print( is_local('   '))
    #print( is_local('127.0.0.1'))
    #print( is_local('localhost'))
    #print( is_local(' localhost '))
    #print( is_local(platform.node()))
    #print( is_local(socket.gethostbyaddr(socket.gethostname())[0]))
    #print( is_local(socket.gethostname()))

if __name__ == "__main__":
    test_is_local()


----------------------------
/home/travis/builds/repos/standalone/borgmatic-borg-list-make_find_paths.py
import re


def make_find_paths(find_paths):
    '''
    Given a sequence of path fragments or patterns as passed to `--find`, transform all path
    fragments into glob patterns. Pass through existing patterns untouched.

    For example, given find_paths of:

      ['foo.txt', 'pp:root/somedir']

    ... transform that into:

      ['sh:**/*foo.txt*/**', 'pp:root/somedir']
    '''

    return tuple(
        find_path
        if re.compile(r'([-!+RrPp] )|(\w\w:)').match(find_path)
        else f'sh:**/*{find_path}*/**'
        for find_path in find_paths
    )


def test_make_find_paths():
    """Check the correctness of make_find_paths
    """
    assert make_find_paths(('foo.txt', 'pp:root/somedir')) == ('sh:**/*foo.txt*/**', 'pp:root/somedir')
    assert make_find_paths(('foo.txt', 'pp:root/somedir', '-R')) == (
    'sh:**/*foo.txt*/**', 'pp:root/somedir', 'sh:**/*-R*/**')
    assert make_find_paths(('foo.txt', 'pp:root/somedir', '-R', '-r')) == (
    'sh:**/*foo.txt*/**', 'pp:root/somedir', 'sh:**/*-R*/**', 'sh:**/*-r*/**')
    assert make_find_paths(('foo.txt', 'pp:root/somedir', '-R', '-r', '-P')) == (
    'sh:**/*foo.txt*/**', 'pp:root/somedir', 'sh:**/*-R*/**', 'sh:**/*-r*/**', 'sh:**/*-P*/**')
    assert make_find_paths(('foo.txt', 'pp:root/somedir', '-R', '-r', '-P', '-p')) == (
    'sh:**/*foo.txt*/**', 'pp:root/somedir', 'sh:**/*-R*/**', 'sh:**/*-r*/**', 'sh:**/*-P*/**', 'sh:**/*-p*/**')


if __name__ == "__main__":
    test_make_find_paths()


----------------------------
/home/travis/builds/repos/standalone/cloudmesh-common-util-is_gitbash.py
import os


def is_gitbash():
    """
    returns True if you run in a Windows gitbash

    :return: True if gitbash
    """
    try:
        exepath = os.environ['EXEPATH']
        return "Git" in exepath
    except:
        return False


def test_is_gitbash():
    """Check the correctness of is_gitbash
    """
    assert is_gitbash() == False


if __name__ == "__main__":
    test_is_gitbash()


----------------------------
/home/travis/builds/repos/standalone/borgmatic-config-generate-write_configuration.py
import os


def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):
    '''
    Given a target config filename and rendered config YAML, write it out to file. Create any
    containing directories as needed. But if the file already exists and overwrite is False,
    abort before writing anything.
    '''
    if not overwrite and os.path.exists(config_filename):
        return FileExistsError
        # raise FileExistsError(
        #     '{} already exists. Aborting. Use --overwrite to replace the file.'.format(
        #         config_filename
        #     )
        # )

    try:
        os.makedirs(os.path.dirname(config_filename), mode=0o700)
    except (FileExistsError, FileNotFoundError):
        pass

    with open(config_filename, 'w') as config_file:
        config_file.write(rendered_config)

    os.chmod(config_filename, mode)
    return rendered_config


def test_write_configuration():
    """Check the correctness of write_configuration
    """
    assert write_configuration('test.yaml', 'test', overwrite=True) == 'test'
    assert write_configuration('test.yaml', 'test', overwrite=False) == FileExistsError
    assert write_configuration('test.yaml', 'hhhhh', overwrite=True) == 'hhhhh'
    assert write_configuration('test.yaml', 'hhhhh', overwrite=False) == FileExistsError


if __name__ == "__main__":
    test_write_configuration()


----------------------------
/home/travis/builds/repos/standalone/cloudmesh-common-Shell-oneline.py
import textwrap


def oneline(script, seperator=" && "):
    """
    converts a script to one line command.
    THis is useful to run a single ssh command and pass a one line script.

    :param script:
    :return:
    """
    return seperator.join(textwrap.dedent(script).strip().splitlines())


def test_oneline():
    """Check the correctness of oneline
    """
    assert oneline("hello") == "hello"
    assert oneline("hello\nworld") == "hello && world"
    assert oneline("hello\nworld\n") == "hello && world"
    assert oneline("hello\nworld\n", ";") == "hello;world"
    assert oneline("hello\nworld\n", "&&") == "hello&&world"
    assert oneline("hello\nworld\n", "||") == "hello||world"
    assert oneline("hello\nworld\n", ";|") == "hello;|world"


if __name__ == "__main__":
    test_oneline()


----------------------------
/home/travis/builds/repos/standalone/lib-matplotlib-testing-__init__-subprocess_run_helper.py
import subprocess, sys, os


def subprocess_run_helper(func, *args, timeout, extra_env=None):
    """
    Run a function in a sub-process.

    Parameters
    ----------
    func : function
        The function to be run.  It must be in a module that is importable.
    *args : str
        Any additional command line arguments to be passed in
        the first argument to ``subprocess.run``.
    extra_env : dict[str, str]
        Any additional environment variables to be set for the subprocess.
    """
    target = func.__name__
    module = func.__module__
    proc = subprocess.run(
        [sys.executable,
         "-c",
         f"from {module} import {target}; {target}()",
         *args],
        env={**os.environ, "SOURCE_DATE_EPOCH": "0", **(extra_env or {})},
        timeout=timeout, check=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        universal_newlines=True)
    return proc


def kk():
    print(10)

def test_subprocess_run_helper():
    """Check the correctness of subprocess_run_helper
    """
    # print("aaaa",subprocess_run_helper(kk, timeout=100).stdout)
    assert subprocess_run_helper(kk, timeout=100).stdout == "10\n"


----------------------------
/home/travis/builds/repos/standalone/borgmatic-config-override-_resolve_string.py
import os, re


def _resolve_string(matcher):
    '''
    Get the value from environment given a matcher containing a name and an optional default value.
    If the variable is not defined in environment and no default value is provided, an Error is raised.
    '''
    name, default = matcher.group("name"), matcher.group("default")
    # print(name)
    # print(default)
    out = os.getenv(name, default=default)
    if out is None:
        raise ValueError("Cannot find variable &{name} in envivonment".format(name=name))
    return out


def test__resolve_string():

    """Check the correctness of _resolve_string
    """
    # print(_resolve_string(re.compile(r"\&\{(?P<name>[a-zA-Z0-9_]+)(?P<default>\:.+)?\}").match("&{name}")))
    os.environ["AAA"] = "huawei"
    # assert _resolve_string(re.compile(r"\&\{(?P<name>[a-zA-Z0-9_]+)(?P<default>\:.+)?\}").match("&{name}")) == "huawei"
    assert _resolve_string(
    re.compile(r"\&\{(?P<name>[a-zA-Z0-9_]+)(?P<default>\:.+)?\}").match("&{AAA:huawei}")) == "huawei"

if __name__ == "__main__":
    test__resolve_string()

----------------------------
/home/travis/builds/repos/standalone/cinder-image-glance-_parse_image_ref.py
import urllib.parse
from typing import (Tuple)


def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:
    """Parse an image href into composite parts.

    :param image_href: href of an image
    :returns: a tuple of the form (image_id, netloc, use_ssl)
    :raises ValueError:

    """
    url = urllib.parse.urlparse(image_href)
    netloc = url.netloc
    image_id = url.path.split('/')[-1]
    use_ssl = (url.scheme == 'https')
    return (image_id, netloc, use_ssl)


def test__parse_image_ref():
    """Check the correctness of _parse_image_ref
    """
    assert _parse_image_ref('http://example.com/image_id') == ('image_id', 'example.com', False)
    assert _parse_image_ref('https://example.com/image_id') == ('image_id', 'example.com', True)
    assert _parse_image_ref('https://example.com/image_id.tar.gz') == ('image_id.tar.gz', 'example.com', True)
    assert _parse_image_ref('https://example.com/image_id.tar.gz.gz') == ('image_id.tar.gz.gz', 'example.com', True)
    assert _parse_image_ref('https://example.com/image_id.tar.gz.gz.gz') == (
    'image_id.tar.gz.gz.gz', 'example.com', True)


if __name__ == "__main__":
    test__parse_image_ref()


----------------------------
/home/travis/builds/repos/standalone/makeprojects-util-remove_ending_os_sep.py
import os


def remove_ending_os_sep(input_list):
    """
    Iterate over a string list and remove trailing os seperator characters.

    Each string is tested if its length is greater than one and if the last
    character is the pathname seperator. If so, the pathname seperator character
    is removed.

    Args:
        input_list: list of strings

    Returns:
        Processed list of strings

    Raises:
        TypeError
    """

    # Input could be None, so test for that case
    if input_list is None:
        return []

    return [item[:-1] if len(item) >= 2 and item.endswith(os.sep)
            else item for item in input_list]


def test_remove_ending_os_sep():
    """Check the correctness of remove_ending_os_sep
    """
    assert remove_ending_os_sep(['a', 'b', 'c']) == ['a', 'b', 'c']
    assert remove_ending_os_sep(['a', 'b', 'c' + os.sep]) == ['a', 'b', 'c']
    assert remove_ending_os_sep(['a', 'b', 'c' + os.sep * 2]) == ['a', 'b', 'c' + os.sep]
    assert remove_ending_os_sep(['a', 'b', 'c' + os.sep * 3]) == ['a', 'b', 'c' + os.sep * 2]


if __name__ == "__main__":
    test_remove_ending_os_sep()


----------------------------
/home/travis/builds/repos/standalone/shconfparser-search-get_pattern.py
import re


def get_pattern(pattern, strip=True):
    """
    This method converts the given string to regex pattern
    """
    if type(pattern) == re.Pattern:
        return pattern

    if strip and type(pattern) == str:
        pattern = pattern.strip()

    return re.compile(pattern)


def test_get_pattern():
    """Check the correctness of get_pattern
    """
    assert get_pattern('1.cpp', ) == re.compile('1.cpp')
    assert get_pattern('4.cpp') == re.compile('4.cpp')
    assert get_pattern('9.h') == re.compile('9.h')


if __name__ == "__main__":
    test_get_pattern()


----------------------------
/home/travis/builds/repos/standalone/makeprojects-core-run_command.py
import subprocess, sys, errno


def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):
    """Call the given command(s)."""
    assert isinstance(commands, list)
    process = None

    popen_kwargs = {}
    if sys.platform == "win32":
        # This hides the console window if pythonw.exe is used
        startupinfo = subprocess.STARTUPINFO()
        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
        popen_kwargs["startupinfo"] = startupinfo

    for command in commands:
        try:
            dispcmd = str([command] + args)
            # remember shell=False, so use git.cmd on windows, not just git
            process = subprocess.Popen(
                [command] + args,
                cwd=cwd,
                env=env,
                stdout=subprocess.PIPE,
                stderr=(subprocess.PIPE if hide_stderr else None),
                **popen_kwargs
            )
            break
        except OSError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print("unable to run %s" % dispcmd)
                print(e)
            return None, None
    else:
        if verbose:
            print("unable to find command, tried %s" % (commands,))
        return None, None
    stdout = process.communicate()[0].strip().decode()
    if process.returncode != 0:
        if verbose:
            print("unable to run %s (error)" % dispcmd)
            print("stdout was %s" % stdout)
        return None, process.returncode
    return stdout, process.returncode


def test_run_command():
    """Check the correctness of run_command
    """
    assert run_command(["echo"], ["hello"]) == ('hello', 0)
    assert run_command(["echo"], ["hello", "world"]) == ('hello world', 0)
    assert run_command(["echo"], ["hello", "world", 'hhh']) == ('hello world hhh', 0)
    assert run_command(["echo"], ["hello", "world", '111']) == ('hello world 111', 0)


if __name__ == "__main__":
    test_run_command()


----------------------------
/home/travis/builds/repos/standalone/gopad-rest-is_ipv4.py
import ipaddress


def is_ipv4(target):
    """ Test if IPv4 address or not
    """
    try:
        chk = ipaddress.IPv4Address(target)
        return True
    except ipaddress.AddressValueError:
        return False


def test_is_ipv4():
    """Check the correctness of is_ipv4
    """
    assert is_ipv4('127.0.0.1') == True
    assert is_ipv4('127.0.0.256') == False
    assert is_ipv4('fe80:0000:0001:0000:0440:44ff:1233:5678') == False
    assert is_ipv4('12.134.25.123') == True
    assert is_ipv4(' ') == False
    assert is_ipv4('ipv4') == False


if __name__ == "__main__":
    test_is_ipv4()


----------------------------
/home/travis/builds/repos/standalone/rdflib-util-find_roots.py
import rdflib
from typing import (
    Optional,
    Set,
)
from rdflib.term import  Node, URIRef


def find_roots(
        graph: "Graph", prop: "URIRef", roots: Optional[Set["Node"]] = None
) -> Set["Node"]:
    """
    Find the roots in some sort of transitive hierarchy.

    find_roots(graph, rdflib.RDFS.subClassOf)
    will return a set of all roots of the sub-class hierarchy

    Assumes triple of the form (child, prop, parent), i.e. the direction of
    RDFS.subClassOf or SKOS.broader

    """

    non_roots: Set[Node] = set()
    if roots is None:
        roots = set()
    for x, y in graph.subject_objects(prop):
        non_roots.add(x)
        if x in roots:
            roots.remove(x)
        if y not in non_roots:
            roots.add(y)
    return roots


def test_find_roots():
    """Check the correctness of find_roots
    """
    assert find_roots(rdflib.graph.Graph(), rdflib.RDFS.subClassOf) == set()


if __name__ == "__main__":
    test_find_roots()


----------------------------
/home/travis/builds/repos/standalone/tests-unit-mock-yaml_helper-_dump_string.py
import yaml
from ansible.module_utils.six import PY3
from ansible.parsing.yaml.dumper import AnsibleDumper


def _dump_string(obj, dumper=None):
    """Dump to a py2-unicode or py3-string"""
    if PY3:
        return yaml.dump(obj, Dumper=dumper)
    else:
        return yaml.dump(obj, Dumper=dumper, encoding=None)


def test__dump_string():
    """Check the correctness of _dump_string
    """
    assert _dump_string({"a": 1, "b": 2}, dumper=AnsibleDumper) == "a: 1\nb: 2\n"
    assert _dump_string({"a": 1, "b": 2, "c": 3, }, dumper=AnsibleDumper) == "a: 1\nb: 2\nc: 3\n"
    assert _dump_string({"a": 1, "b": 2, "d": 3, }, dumper=AnsibleDumper) == "a: 1\nb: 2\nd: 3\n"
    assert _dump_string({"f": 1, "b": 2, "d": 3, }, dumper=AnsibleDumper) == "b: 2\nd: 3\nf: 1\n"
    assert _dump_string({1, 2, 3}, dumper=AnsibleDumper) == "!!set\n1: null\n2: null\n3: null\n"
    assert _dump_string([1, 2, 3], dumper=AnsibleDumper) == "- 1\n- 2\n- 3\n"


if __name__ == "__main__":
    test__dump_string()


----------------------------
/home/travis/builds/repos/standalone/apphelpers-loggers-build_app_logger.py
import os, logging
from logging.handlers import RotatingFileHandler

def build_app_logger(name='app', logfile='app.log', debug=True):
    """
    General purpose application logger. Useful mainly for debugging
    """
    # level = logging.DEBUG if settings.DEBUG else logging.INFO
    level = logging.INFO

    logdir = './logs'  # TODO: move this to settings
    if not os.path.exists(logdir):
        os.mkdir(logdir)
    logpath = os.path.join(logdir, logfile)
    maxBytes = 1024 * 1024 * 10
    handler = RotatingFileHandler(logpath, maxBytes=maxBytes, backupCount=100)
    handler.setLevel(level)
    formatter = logging.Formatter('[%(levelname)s] %(asctime)s: %(message)s')
    handler.setFormatter(formatter)
    logger = logging.getLogger(name)
    logger.addHandler(handler)
    logger.setLevel(level)
    return logger


def test_build_app_logger():
    """Check the correctness of build_app_logger
    """

    app_logger = build_app_logger()
    assert app_logger.level == logging.INFO
    assert app_logger.handlers[0].level == logging.INFO
    assert app_logger.handlers[0].formatter.datefmt == None


if __name__ == "__main__":
    test_build_app_logger()


----------------------------
/home/travis/builds/repos/standalone/radiospectra-spectrogram-make_array.py
import numpy as np


def make_array(shape, dtype=np.dtype("float32")):
    """
    Function to create an array with shape and dtype.

    Parameters
    ----------
    shape : tuple
        shape of the array to create
    dtype : `numpy.dtype`
        data-type of the array to create
    """
    return np.zeros(shape, dtype=dtype)


def test_make_array():
    """Check the correctness of make_array
    """
    assert make_array((3, 4)).shape == (3, 4)
    assert make_array((3, 4), dtype=np.dtype("float64")).dtype == np.dtype("float64")
    assert make_array((3, 4), dtype=np.dtype("float64")).shape == (3, 4)
    assert make_array((3, 4), dtype=np.dtype("float64"))[0][0] == 0
    assert make_array((3, 4), dtype=np.dtype("float64"))[-1][-1] == 0


if __name__ == "__main__":
    test_make_array()


----------------------------
/home/travis/builds/repos/standalone/concert-tests-unit-devices-test_monochromator-gaussian.py
import numpy as np


def gaussian(x):
    """
    Gaussian centered around 0.2 with a sigma of 0.1.
    """
    mu = 0.2
    sigma = 0.1
    return np.exp(-(x - mu) ** 2 /(2*sigma ** 2))


def test_gaussian():
    """Check the correctness of gaussian
    """
    assert gaussian(0.1) == np.exp(-(0.1 - 0.2) ** 2 / (2*0.1 ** 2))
    assert gaussian(1) == np.exp(-(1 - 0.2) ** 2 / (2*0.1 ** 2))
    assert gaussian(-1) == np.exp(-(-1 - 0.2) ** 2 / (2*0.1 ** 2))
    assert gaussian(0) == np.exp(-(0.0 - 0.2) ** 2 / (2*0.1 ** 2))
    assert gaussian(10) == np.exp(-(10 - 0.2) ** 2 / (2*0.1 ** 2))


if __name__ == "__main__":
    test_gaussian()


----------------------------
/home/travis/builds/repos/standalone/borgmatic-commands-borgmatic-load_configurations.py
import collections
import logging

from borgmatic.config import validate


def load_configurations(config_filenames, overrides=None, resolve_env=True):
    '''
    Given a sequence of configuration filenames, load and validate each configuration file. Return
    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,
    and sequence of logging.LogRecord instances containing any parse errors.
    '''
    # Dict mapping from config filename to corresponding parsed config dict.
    configs = collections.OrderedDict()
    logs = []

    # Parse and load each configuration file.
    for config_filename in config_filenames:
        try:
            configs[config_filename] = validate.parse_configuration(
                config_filename, validate.schema_filename(), overrides, resolve_env
            )
        except PermissionError:
            logs.extend(
                [
                    logging.makeLogRecord(
                        dict(
                            levelno=logging.WARNING,
                            levelname='WARNING',
                            msg='{}: Insufficient permissions to read configuration file'.format(
                                config_filename
                            ),
                        )
                    ),
                ]
            )
        except (ValueError, OSError, validate.Validation_error) as error:
            logs.extend(
                [
                    logging.makeLogRecord(
                        dict(
                            levelno=logging.CRITICAL,
                            levelname='CRITICAL',
                            msg='{}: Error parsing configuration file'.format(config_filename),
                        )
                    ),
                    logging.makeLogRecord(
                        dict(levelno=logging.CRITICAL, levelname='CRITICAL', msg=error)
                    ),
                ]
            )

    return (configs, logs)


def test_load_configurations():
    """Check the correctness of load_configurations
    """

    assert load_configurations(['/etc/borgmatic/config'])[0] == collections.OrderedDict()


if __name__ == "__main__":
    test_load_configurations()


----------------------------
/home/travis/builds/repos/standalone/o2sclpy-utils-force_string.py
import numpy


def force_string(obj):
    """
    This function returns the bytes object corresponding to ``obj``
    in case it is a string using UTF-8.
    """
    if isinstance(obj, numpy.bytes_) == True or isinstance(obj, bytes) == True:
        return obj.decode('utf-8')
    return obj


def test_force_string():
    """Check the correctness of force_string
    """
    assert force_string(b'abc') == 'abc'
    assert force_string('abc') == 'abc'
    assert force_string(b'abcd') == 'abcd'
    assert force_string(numpy.bytes_(b'abc')) == 'abc'
    assert force_string(numpy.bytes_('abcd')) == 'abcd'


if __name__ == "__main__":
    test_force_string()


----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/time/__init___from_ticks_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


"""
This module contains the fundamental types used for temporal accounting as well
as a number of utility functions.
"""


from __future__ import annotations
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
import re
import typing
import typing as t
from datetime import (
    date,
    datetime,
    time,
    timedelta,
    timezone,
    tzinfo as _tzinfo,
)
from functools import total_ordering
from re import compile as re_compile
from time import (
    gmtime,
    mktime,
    struct_time,
)


if t.TYPE_CHECKING:
    import typing_extensions as te

from src.neo4j.time._arithmetic import (
    nano_add,
    nano_div,
    round_half_to_even,
    symmetric_divmod,
)
from src.neo4j.time._metaclasses import (
    DateTimeType,
    DateType,
    TimeType,
)


__all__ = [
    "MIN_INT64",
    "MAX_INT64",
    "MIN_YEAR",
    "MAX_YEAR",
    "Duration",
    "Date",
    "ZeroDate",
    "Time",
    "Midnight",
    "Midday",
    "DateTime",
    "Never",
    "UnixEpoch",
]


MIN_INT64 = -(2 ** 63)
MAX_INT64 = (2 ** 63) - 1

#: The smallest year number allowed in a :class:`.Date` or :class:`.DateTime`
#: object to be compatible with :class:`datetime.date` and
#: :class:`datetime.datetime`.
MIN_YEAR: te.Final[int] = 1

#: The largest year number allowed in a :class:`.Date` or :class:`.DateTime`
#: object to be compatible with :class:`datetime.date` and
#: :class:`datetime.datetime`.
MAX_YEAR: te.Final[int] = 9999

DATE_ISO_PATTERN = re_compile(r"^(\d{4})-(\d{2})-(\d{2})$")
TIME_ISO_PATTERN = re_compile(
    r"^(\d{2})(:(\d{2})(:((\d{2})"
    r"(\.\d*)?))?)?(([+-])(\d{2}):(\d{2})(:((\d{2})(\.\d*)?))?)?$"
)
DURATION_ISO_PATTERN = re_compile(
    r"^P((\d+)Y)?((\d+)M)?((\d+)D)?"
    r"(T((\d+)H)?((\d+)M)?(((\d+)(\.\d+)?)?S)?)?$"
)

NANO_SECONDS = 1000000000
AVERAGE_SECONDS_IN_MONTH = 2629746
AVERAGE_SECONDS_IN_DAY = 86400

FORMAT_F_REPLACE = re.compile(r"(?<!%)%f")


def _is_leap_year(year):
    if year % 4 != 0:
        return False
    if year % 100 != 0:
        return True
    return year % 400 == 0


IS_LEAP_YEAR = {year: _is_leap_year(year) for year in range(MIN_YEAR, MAX_YEAR + 1)}


def _days_in_year(year):
    return 366 if IS_LEAP_YEAR[year] else 365


DAYS_IN_YEAR = {year: _days_in_year(year) for year in range(MIN_YEAR, MAX_YEAR + 1)}


def _days_in_month(year, month):
    if month in (9, 4, 6, 11):
        return 30
    elif month != 2:
        return 31
    else:
        return 29 if IS_LEAP_YEAR[year] else 28


DAYS_IN_MONTH = {(year, month): _days_in_month(year, month)
                 for year in range(MIN_YEAR, MAX_YEAR + 1) for month in range(1, 13)}


def _normalize_day(year, month, day):
    """ Coerce the day of the month to an internal value that may or
    may not match the "public" value.

    With the exception of the last three days of every month, all
    days are stored as-is. The last three days are instead stored
    as -1 (the last), -2 (the second to last) and -3 (the third to
    last).

    Therefore, for a 28-day month, the last week is as follows:

        Day   | 22 23 24 25 26 27 28
        Value | 22 23 24 25 -3 -2 -1

    For a 29-day month, the last week is as follows:

        Day   | 23 24 25 26 27 28 29
        Value | 23 24 25 26 -3 -2 -1

    For a 30-day month, the last week is as follows:

        Day   | 24 25 26 27 28 29 30
        Value | 24 25 26 27 -3 -2 -1

    For a 31-day month, the last week is as follows:

        Day   | 25 26 27 28 29 30 31
        Value | 25 26 27 28 -3 -2 -1

    This slightly unintuitive system makes some temporal arithmetic
    produce a more desirable outcome.

    :param year:
    :param month:
    :param day:
    :returns:
    """
    if year < MIN_YEAR or year > MAX_YEAR:
        raise ValueError("Year out of range (%d..%d)" % (MIN_YEAR, MAX_YEAR))
    if month < 1 or month > 12:
        raise ValueError("Month out of range (1..12)")
    days_in_month = DAYS_IN_MONTH[(year, month)]
    if day in (days_in_month, -1):
        return year, month, -1
    if day in (days_in_month - 1, -2):
        return year, month, -2
    if day in (days_in_month - 2, -3):
        return year, month, -3
    if 1 <= day <= days_in_month - 3:
        return year, month, int(day)
    # TODO improve this error message
    raise ValueError("Day %d out of range (1..%d, -1, -2 ,-3)" % (day, days_in_month))


class ClockTime(tuple):
    """ A count of `seconds` and `nanoseconds`. This class can be used to
    mark a particular point in time, relative to an externally-specified
    epoch.

    The `seconds` and `nanoseconds` values provided to the constructor can
    can have any sign but will be normalized internally into a positive or
    negative `seconds` value along with a positive `nanoseconds` value
    between `0` and `999,999,999`. Therefore ``ClockTime(-1, -1)`` is
    normalized to ``ClockTime(-2, 999999999)``.

    Note that the structure of a :class:`.ClockTime` object is similar to
    the ``timespec`` struct in C.
    """

    def __new__(cls, seconds: float = 0, nanoseconds: int = 0) -> ClockTime:
        seconds, nanoseconds = divmod(
            int(NANO_SECONDS * seconds) + int(nanoseconds), NANO_SECONDS
        )
        return tuple.__new__(cls, (seconds, nanoseconds))

    def __add__(self, other):
        if isinstance(other, (int, float)):
            other = ClockTime(other)
        if isinstance(other, ClockTime):
            return ClockTime(self.seconds + other.seconds, self.nanoseconds + other.nanoseconds)
        if isinstance(other, Duration):
            if other.months or other.days:
                raise ValueError("Cannot add Duration with months or days")
            return ClockTime(self.seconds + other.seconds, self.nanoseconds +
                             int(other.nanoseconds))
        return NotImplemented

    def __sub__(self, other):
        if isinstance(other, (int, float)):
            other = ClockTime(other)
        if isinstance(other, ClockTime):
            return ClockTime(self.seconds - other.seconds, self.nanoseconds - other.nanoseconds)
        if isinstance(other, Duration):
            if other.months or other.days:
                raise ValueError("Cannot subtract Duration with months or days")
            return ClockTime(self.seconds - other.seconds, self.nanoseconds - int(other.nanoseconds))
        return NotImplemented

    def __repr__(self):
        return "ClockTime(seconds=%r, nanoseconds=%r)" % self

    @property
    def seconds(self):
        return self[0]

    @property
    def nanoseconds(self):
        return self[1]


class Clock:
    """ Accessor for time values. This class is fulfilled by implementations
    that subclass :class:`.Clock`. These implementations are contained within
    the ``neo4j.time.clock_implementations`` module, and are not intended to be
    accessed directly.

    Creating a new :class:`.Clock` instance will produce the highest
    precision clock implementation available.

        >>> clock = Clock()
        >>> type(clock)                                         # doctest: +SKIP
        neo4j.time.clock_implementations.LibCClock
        >>> clock.local_time()                                  # doctest: +SKIP
        ClockTime(seconds=1525265942, nanoseconds=506844026)

    """

    __implementations = None

    def __new__(cls):
        if cls.__implementations is None:
            # Find an available clock with the best precision
            import neo4j.time._clock_implementations
            cls.__implementations = sorted((clock for clock in Clock.__subclasses__() if clock.available()),
                                           key=lambda clock: clock.precision(), reverse=True)
        if not cls.__implementations:
            raise RuntimeError("No clock implementations available")
        instance = object.__new__(cls.__implementations[0])
        return instance

    @classmethod
    def precision(cls):
        """ The precision of this clock implementation, represented as a
        number of decimal places. Therefore, for a nanosecond precision
        clock, this function returns `9`.
        """
        raise NotImplementedError("No clock implementation selected")

    @classmethod
    def available(cls):
        """ A boolean flag to indicate whether or not this clock
        implementation is available on this platform.
        """
        raise NotImplementedError("No clock implementation selected")

    @classmethod
    def local_offset(cls):
        """The offset from UTC for local time read from this clock.
        This may raise OverflowError if not supported, because of platform depending C libraries.

        :returns:
        :rtype:

        :raises OverflowError:
        """
        # Adding and subtracting two days to avoid passing a pre-epoch time to
        # `mktime`, which can cause a `OverflowError` on some platforms (e.g.,
        # Windows).
        return ClockTime(-int(mktime(gmtime(172800))) + 172800)

    def local_time(self):
        """ Read and return the current local time from this clock, measured relative to the Unix Epoch.
        This may raise OverflowError if not supported, because of platform depending C libraries.

        :returns:
        :rtype:

        :raises OverflowError:
        """
        return self.utc_time() + self.local_offset()

    def utc_time(self):
        """ Read and return the current UTC time from this clock, measured
        relative to the Unix Epoch.
        """
        raise NotImplementedError("No clock implementation selected")


if t.TYPE_CHECKING:
    # make typechecker believe that Duration subclasses datetime.timedelta
    # https://github.com/python/typeshed/issues/8409#issuecomment-1197704527
    duration_base_class = timedelta
else:
    duration_base_class = object


class Duration(t.Tuple[int, int, int, int],  # type: ignore[misc]
               duration_base_class):
    """A difference between two points in time.

    A :class:`.Duration` represents the difference between two points in time.
    Duration objects store a composite value of `months`, `days`, `seconds`,
    and `nanoseconds`. Unlike :class:`datetime.timedelta` however, days, and
    seconds/nanoseconds are never interchanged. All values except seconds and
    nanoseconds are applied separately in calculations (element-wise).

    A :class:`.Duration` stores four primary instance attributes internally:
    `months`, `days`, `seconds` and `nanoseconds`. These are maintained as
    individual values and are immutable. Each of these four attributes can carry
    its own sign, with the exception of `nanoseconds`, which always has the same
    sign as `seconds`. The constructor will establish this state, should the
    duration be initialized with conflicting `seconds` and `nanoseconds` signs.
    This structure allows the modelling of durations such as
    `3 months minus 2 days`.

    To determine if a :class:`Duration` `d` is overflowing the accepted values
    of the database, first, all `nanoseconds` outside the range -999_999_999 and
    999_999_999 are transferred into the seconds field. Then, `months`, `days`,
    and `seconds` are summed up like so:
    `months * 2629746 + days * 86400 + d.seconds + d.nanoseconds // 1000000000`.
    (Like the integer division in Python, this one is to be understood as
    rounding down rather than towards 0.)
    This value must be between -(2\\ :sup:`63`) and (2\\ :sup:`63` - 1)
    inclusive.

    :param years: will be added times 12 to `months`
    :param months: will be truncated to :class:`int` (`int(months)`)
    :param weeks: will be added times 7 to `days`
    :param days: will be truncated to :class:`int` (`int(days)`)
    :param hours: will be added times 3,600,000,000,000 to `nanoseconds`
    :param minutes: will be added times 60,000,000,000 to `nanoseconds`
    :param seconds: will be added times 1,000,000,000 to `nanoseconds``
    :param milliseconds: will be added times 1,000,000 to `nanoseconds`
    :param microseconds: will be added times 1,000 to `nanoseconds`
    :param nanoseconds: will be truncated to :class:`int` (`int(nanoseconds)`)

    :raises ValueError: the components exceed the limits as described above.
    """

    # i64: i64:i64: i32

    min: te.Final[Duration] = None  # type: ignore
    """The lowest duration value possible."""

    max: te.Final[Duration] = None  # type: ignore
    """The highest duration value possible."""

    def __new__(
        cls,
        years: float = 0,
        months: float = 0,
        weeks: float = 0,
        days: float = 0,
        hours: float = 0,
        minutes: float = 0,
        seconds: float = 0,
        milliseconds: float = 0,
        microseconds: float = 0,
        nanoseconds: float = 0
    ) -> Duration:
        mo = int(12 * years + months)
        if mo < MIN_INT64 or mo > MAX_INT64:
            raise ValueError("Months value out of range")
        d = int(7 * weeks + days)
        ns = (int(3600000000000 * hours) +
              int(60000000000 * minutes) +
              int(1000000000 * seconds) +
              int(1000000 * milliseconds) +
              int(1000 * microseconds) +
              int(nanoseconds))
        s, ns = symmetric_divmod(ns, NANO_SECONDS)
        avg_total_seconds = (mo * AVERAGE_SECONDS_IN_MONTH
                             + d * AVERAGE_SECONDS_IN_DAY
                             + s
                             - (1 if ns < 0 else 0))
        if not MIN_INT64 <= avg_total_seconds <= MAX_INT64:
            raise ValueError("Duration value out of range: %r",
                             tuple.__repr__((mo, d, s, ns)))
        # mypy issue https://github.com/python/mypy/issues/14890
        return tuple.__new__(cls, (mo, d, s, ns))  # type: ignore[type-var]

    def __bool__(self) -> bool:
        """Falsy if all primary instance attributes are."""
        return any(map(bool, self))

    __nonzero__ = __bool__

    def __add__(  # type: ignore[override]
        self, other: t.Union[Duration, timedelta]
    ) -> Duration:
        """Add a :class:`.Duration` or :class:`datetime.timedelta`."""
        if isinstance(other, Duration):
            return Duration(
                months=self[0] + int(other.months),
                days=self[1] + int(other.days),
                seconds=self[2] + int(other.seconds),
                nanoseconds=self[3] + int(other.nanoseconds)
            )
        if isinstance(other, timedelta):
            return Duration(
                months=self[0], days=self[1] + other.days,
                seconds=self[2] + other.seconds,
                nanoseconds=self[3] + other.microseconds * 1000
            )
        return NotImplemented

    def __sub__(self, other: t.Union[Duration, timedelta]) -> Duration:
        """Subtract a :class:`.Duration` or :class:`datetime.timedelta`."""
        if isinstance(other, Duration):
            return Duration(
                months=self[0] - int(other.months),
                days=self[1] - int(other.days),
                seconds=self[2] - int(other.seconds),
                nanoseconds=self[3] - int(other.nanoseconds)
            )
        if isinstance(other, timedelta):
            return Duration(
                months=self[0],
                days=self[1] - other.days,
                seconds=self[2] - other.seconds,
                nanoseconds=self[3] - other.microseconds * 1000
            )
        return NotImplemented

    def __mul__(self, other: float) -> Duration:  # type: ignore[override]
        """Multiply by an :class:`int` or :class:`float`.

        The operation is performed element-wise on
        ``(months, days, nanaoseconds)`` where

        * years go into months,
        * weeks go into days,
        * seconds and all sub-second units go into nanoseconds.

        Each element will be rounded to the nearest integer (.5 towards even).
        """
        if isinstance(other, (int, float)):
            return Duration(
                months=round_half_to_even(self[0] * other),
                days=round_half_to_even(self[1] * other),
                nanoseconds=round_half_to_even(
                    self[2] * NANO_SECONDS * other
                    + self[3] * other
                )
            )
        return NotImplemented

    def __floordiv__(self, other: int) -> Duration:  # type: ignore[override]
        """Integer division by an :class:`int`.

        The operation is performed element-wise on
        ``(months, days, nanaoseconds)`` where

        * years go into months,
        * weeks go into days,
        * seconds and all sub-second units go into nanoseconds.

        Each element will be rounded towards -inf.
        """
        if isinstance(other, int):
            return Duration(
                months=self[0] // other, days=self[1] // other,
                nanoseconds=(self[2] * NANO_SECONDS + self[3]) // other
            )
        return NotImplemented

    def __mod__(self, other: int) -> Duration:  # type: ignore[override]
        """Modulo operation by an :class:`int`.

        The operation is performed element-wise on
        ``(months, days, nanaoseconds)`` where

        * years go into months,
        * weeks go into days,
        * seconds and all sub-second units go into nanoseconds.
        """
        if isinstance(other, int):
            return Duration(
                months=self[0] % other, days=self[1] % other,
                nanoseconds=(self[2] * NANO_SECONDS + self[3]) % other
            )
        return NotImplemented

    def __divmod__(  # type: ignore[override]
        self, other: int
    ) -> t.Tuple[Duration, Duration]:
        """Division and modulo operation by an :class:`int`.

        See :meth:`__floordiv__` and :meth:`__mod__`.
        """
        if isinstance(other, int):
            return self.__floordiv__(other), self.__mod__(other)
        return NotImplemented

    def __truediv__(self, other: float) -> Duration:  # type: ignore[override]
        """Division by an :class:`int` or :class:`float`.

        The operation is performed element-wise on
        ``(months, days, nanaoseconds)`` where

        * years go into months,
        * weeks go into days,
        * seconds and all sub-second units go into nanoseconds.

        Each element will be rounded to the nearest integer (.5 towards even).
        """
        if isinstance(other, (int, float)):
            return Duration(
                months=round_half_to_even(self[0] / other),
                days=round_half_to_even(self[1] / other),
                nanoseconds=round_half_to_even(
                    self[2] * NANO_SECONDS / other
                    + self[3] / other
                )
            )
        return NotImplemented

    def __pos__(self) -> Duration:
        """"""
        return self

    def __neg__(self) -> Duration:
        """"""
        return Duration(months=-self[0], days=-self[1], seconds=-self[2],
                        nanoseconds=-self[3])

    def __abs__(self) -> Duration:
        """"""
        return Duration(months=abs(self[0]), days=abs(self[1]),
                        seconds=abs(self[2]), nanoseconds=abs(self[3]))

    def __repr__(self) -> str:
        """"""
        return "Duration(months=%r, days=%r, seconds=%r, nanoseconds=%r)" % self

    def __str__(self) -> str:
        """"""
        return self.iso_format()

    def __copy__(self) -> Duration:
        return self.__new__(self.__class__, months=self[0], days=self[1],
                            seconds=self[2], nanoseconds=self[3])

    def __deepcopy__(self, memo) -> Duration:
        return self.__copy__()

    @classmethod
    def from_iso_format(cls, s: str) -> Duration:
        """Parse a ISO formatted duration string.

        Accepted formats (all lowercase letters are placeholders):
            'P', a zero length duration
            'PyY', y being a number of years
            'PmM', m being a number of months
            'PdD', d being a number of days

            Any combination of the above, e.g., 'P25Y1D' for 25 years and 1 day.

            'PThH', h being a number of hours
            'PTmM', h being a number of minutes
            'PTsS', h being a number of seconds
            'PTs.sss...S', h being a fractional number of seconds

            Any combination of the above, e.g. 'PT5H1.2S' for 5 hours and 1.2
            seconds.
            Any combination of all options, e.g. 'P13MT100M' for 13 months and
            100 minutes.

        :param s: String to parse

        :raises ValueError: if the string does not match the required format.
        """
        match = DURATION_ISO_PATTERN.match(s)
        if match:
            ns = 0
            if match.group(15):
                ns = int(match.group(15)[1:10].ljust(9, "0"))
            return cls(
                years=int(match.group(2) or 0),
                months=int(match.group(4) or 0),
                days=int(match.group(6) or 0),
                hours=int(match.group(9) or 0),
                minutes=int(match.group(11) or 0),
                seconds=int(match.group(14) or 0),
                nanoseconds=ns
            )
        raise ValueError("Duration string must be in ISO format")

    fromisoformat = from_iso_format

    def iso_format(self, sep: str = "T") -> str:
        """Return the :class:`Duration` as ISO formatted string.

        :param sep: the separator before the time components.
        """
        parts = []
        hours, minutes, seconds, nanoseconds = \
            self.hours_minutes_seconds_nanoseconds
        if hours:
            parts.append("%dH" % hours)
        if minutes:
            parts.append("%dM" % minutes)
        if nanoseconds:
            if seconds >= 0 and nanoseconds >= 0:
                parts.append("%d.%sS" %
                             (seconds,
                              str(nanoseconds).rjust(9, "0").rstrip("0")))
            elif seconds <= 0 and nanoseconds <= 0:
                parts.append("-%d.%sS" %
                             (abs(seconds),
                              str(abs(nanoseconds)).rjust(9, "0").rstrip("0")))

            else:
                assert False and "Please report this issue"
        elif seconds:
            parts.append("%dS" % seconds)
        if parts:
            parts.insert(0, sep)
        years, months, days = self.years_months_days
        if days:
            parts.insert(0, "%dD" % days)
        if months:
            parts.insert(0, "%dM" % months)
        if years:
            parts.insert(0, "%dY" % years)
        if parts:
            parts.insert(0, "P")
            return "".join(parts)
        else:
            return "PT0S"

    @property
    def months(self) -> int:
        """The months of the :class:`Duration`."""
        return self[0]

    @property
    def days(self) -> int:
        """The days of the :class:`Duration`."""
        return self[1]

    @property
    def seconds(self) -> int:
        """The seconds of the :class:`Duration`."""
        return self[2]

    @property
    def nanoseconds(self) -> int:
        """The nanoseconds of the :class:`Duration`."""
        return self[3]

    @property
    def years_months_days(self) -> t.Tuple[int, int, int]:
        """Months and days components as a 3-tuple.

        t.Tuple of years, months and days.
        """
        years, months = symmetric_divmod(self[0], 12)
        return years, months, self[1]

    @property
    def hours_minutes_seconds_nanoseconds(self) -> t.Tuple[int, int, int, int]:
        """Seconds and nanoseconds components as a 4-tuple.

        t.Tuple of hours, minutes, seconds and nanoseconds.
        """
        minutes, seconds = symmetric_divmod(self[2], 60)
        hours, minutes = symmetric_divmod(minutes, 60)
        return hours, minutes, seconds, self[3]


Duration.min = Duration(  # type: ignore
    seconds=MIN_INT64, nanoseconds=0
)

Duration.max = Duration(  # type: ignore
    seconds=MAX_INT64,
    nanoseconds=999999999
)


if t.TYPE_CHECKING:
    # make typechecker believe that Date subclasses datetime.date
    # https://github.com/python/typeshed/issues/8409#issuecomment-1197704527
    date_base_class = date
else:
    date_base_class = object


class Date(date_base_class, metaclass=DateType):
    """Idealized date representation.

    A :class:`.Date` object represents a date (year, month, and day) in the
    `proleptic Gregorian Calendar
    <https://en.wikipedia.org/wiki/Proleptic_Gregorian_calendar>`_.

    Years between `0001` and `9999` are supported, with additional support for
    the "zero date" used in some contexts.

    Each date is based on a proleptic Gregorian ordinal, which models
    1 Jan 0001 as `day 1` and counts each subsequent day up to, and including,
    31 Dec 9999. The standard `year`, `month` and `day` value of each date is
    also available.

    Internally, the day of the month is always stored as-is, with the exception
    of the last three days of that month. These are always stored as
    -1, -2 and -3 (counting from the last day). This system allows some temporal
    arithmetic (particularly adding or subtracting months) to produce a more
    desirable outcome than would otherwise be produced. Externally, the day
    number is always the same as would be written on a calendar.

    :param year: the year. Minimum :attr:`.MIN_YEAR` (0001), maximum
        :attr:`.MAX_YEAR` (9999).
    :type year: int
    :param month: the month. Minimum 1, maximum 12.
    :type month: int
    :param day: the day. Minimum 1, maximum
        :attr:`Date.days_in_month(year, month) <Date.days_in_month>`.
    :type day: int

    A zero date can also be acquired by passing all zeroes to the
    :class:`neo4j.time.Date` constructor or by using the :attr:`ZeroDate`
    constant.
    """

    # CONSTRUCTOR #

    def __new__(cls, year: int, month: int, day: int) -> Date:
        if year == month == day == 0:
            return ZeroDate
        year, month, day = _normalize_day(year, month, day)
        ordinal = cls.__calc_ordinal(year, month, day)
        return cls.__new(ordinal, year, month, day)

    @classmethod
    def __new(cls, ordinal: int, year: int, month: int, day: int) -> Date:
        instance = object.__new__(cls)
        instance.__ordinal = int(ordinal)
        instance.__year = int(year)
        instance.__month = int(month)
        instance.__day = int(day)
        return instance

    # CLASS METHODS #

    @classmethod
    def today(cls, tz: t.Optional[_tzinfo] = None) -> Date:
        """Get the current date.

        :param tz: timezone or None to get the local :class:`.Date`.

        :raises OverflowError: if the timestamp is out of the range of values
            supported by the platform C localtime() function. It’s common for
            this to be restricted to years from 1970 through 2038.
        """
        if tz is None:
            return cls.from_clock_time(Clock().local_time(), UnixEpoch)
        else:
            return (
                DateTime.utc_now()
                .replace(tzinfo=timezone.utc).astimezone(tz)
                .date()
            )

    @classmethod
    def utc_today(cls) -> Date:
        """Get the current date as UTC local date."""
        return cls.from_clock_time(Clock().utc_time(), UnixEpoch)

    @classmethod
    def from_timestamp(
        cls, timestamp: float, tz: t.Optional[_tzinfo] = None
    ) -> Date:
        """:class:`.Date` from a time stamp (seconds since unix epoch).

        :param timestamp: the unix timestamp (seconds since unix epoch).
        :param tz: timezone. Set to None to create a local :class:`.Date`.

        :raises OverflowError: if the timestamp is out of the range of values
            supported by the platform C localtime() function. It’s common for
            this to be restricted to years from 1970 through 2038.
        """
        return cls.from_native(datetime.fromtimestamp(timestamp, tz))

    @classmethod
    def utc_from_timestamp(cls, timestamp: float) -> Date:
        """:class:`.Date` from a time stamp (seconds since unix epoch).

        :returns: the `Date` as local date `Date` in UTC.
        """
        return cls.from_clock_time((timestamp, 0), UnixEpoch)

    @classmethod
    def from_ordinal(cls, ordinal: int) -> Date:
        """
        The :class:`.Date` that corresponds to the proleptic Gregorian ordinal.

        `0001-01-01` has ordinal 1 and `9999-12-31` has ordinal 3,652,059.
        Values outside of this range trigger a :exc:`ValueError`.
        The corresponding instance method for the reverse date-to-ordinal
        transformation is :meth:`.to_ordinal`.
        The ordinal 0 has a special semantic and will return :attr:`ZeroDate`.

        :raises ValueError: if the ordinal is outside the range [0, 3652059]
            (both values included).
        """
        if ordinal == 0:
            return ZeroDate
        if ordinal >= 736695:
            year = 2018     # Project release year
            month = 1
            day = int(ordinal - 736694)
        elif ordinal >= 719163:
            year = 1970     # Unix epoch
            month = 1
            day = int(ordinal - 719162)
        else:
            year = 1
            month = 1
            day = int(ordinal)
        if day < 1 or day > 3652059:
            # Note: this requires a maximum of 22 bits for storage
            # Could be transferred in 3 bytes.
            raise ValueError("Ordinal out of range (1..3652059)")
        if year < MIN_YEAR or year > MAX_YEAR:
            raise ValueError("Year out of range (%d..%d)" % (MIN_YEAR, MAX_YEAR))
        days_in_year = DAYS_IN_YEAR[year]
        while day > days_in_year:
            day -= days_in_year
            year += 1
            days_in_year = DAYS_IN_YEAR[year]
        days_in_month = DAYS_IN_MONTH[(year, month)]
        while day > days_in_month:
            day -= days_in_month
            month += 1
            days_in_month = DAYS_IN_MONTH[(year, month)]
        year, month, day = _normalize_day(year, month, day)
        return cls.__new(ordinal, year, month, day)

    @classmethod
    def parse(cls, s: str) -> Date:
        """Parse a string to produce a :class:`.Date`.

        Accepted formats:
            'Y-M-D'

        :param s: the string to be parsed.

        :raises ValueError: if the string could not be parsed.
        """
        try:
            numbers = list(map(int, s.split("-")))
        except (ValueError, AttributeError):
            raise ValueError("Date string must be in format YYYY-MM-DD")
        else:
            if len(numbers) == 3:
                return cls(*numbers)
            raise ValueError("Date string must be in format YYYY-MM-DD")

    @classmethod
    def from_iso_format(cls, s: str) -> Date:
        """Parse a ISO formatted Date string.

        Accepted formats:
            'YYYY-MM-DD'

        :param s: the string to be parsed.

        :raises ValueError: if the string could not be parsed.
        """
        m = DATE_ISO_PATTERN.match(s)
        if m:
            year = int(m.group(1))
            month = int(m.group(2))
            day = int(m.group(3))
            return cls(year, month, day)
        raise ValueError("Date string must be in format YYYY-MM-DD")

    @classmethod
    def from_native(cls, d: date) -> Date:
        """Convert from a native Python `datetime.date` value.

        :param d: the date to convert.
        """
        return Date.from_ordinal(d.toordinal())

    @classmethod
    def from_clock_time(
        cls,
        clock_time: t.Union[ClockTime, t.Tuple[float, int]],
        epoch: DateTime
    ) -> Date:
        """Convert from a ClockTime relative to a given epoch.

        :param clock_time: the clock time as :class:`.ClockTime` or as tuple of
            (seconds, nanoseconds)
        :param epoch: the epoch to which `clock_time` is relative
        """
        try:
            clock_time = ClockTime(*clock_time)
        except (TypeError, ValueError):
            raise ValueError("Clock time must be a 2-tuple of (s, ns)")
        else:
            ordinal = clock_time.seconds // 86400
            return Date.from_ordinal(ordinal + epoch.date().to_ordinal())

    @classmethod
    def is_leap_year(cls, year: int) -> bool:
        """Indicates whether or not `year` is a leap year.

        :param year: the year to look up

        :raises ValueError: if `year` is out of range:
            :attr:`MIN_YEAR` <= year <= :attr:`MAX_YEAR`
        """
        if year < MIN_YEAR or year > MAX_YEAR:
            raise ValueError("Year out of range (%d..%d)" % (MIN_YEAR, MAX_YEAR))
        return IS_LEAP_YEAR[year]

    @classmethod
    def days_in_year(cls, year: int) -> int:
        """Return the number of days in `year`.

        :param year: the year to look up

        :raises ValueError: if `year` is out of range:
            :attr:`MIN_YEAR` <= year <= :attr:`MAX_YEAR`
        """
        if year < MIN_YEAR or year > MAX_YEAR:
            raise ValueError("Year out of range (%d..%d)" % (MIN_YEAR, MAX_YEAR))
        return DAYS_IN_YEAR[year]

    @classmethod
    def days_in_month(cls, year: int, month: int) -> int:
        """Return the number of days in `month` of `year`.

        :param year: the year to look up
        :param month: the month to look up

        :raises ValueError: if `year` or `month` is out of range:
            :attr:`MIN_YEAR` <= year <= :attr:`MAX_YEAR`;
            1 <= year <= 12
        """
        if year < MIN_YEAR or year > MAX_YEAR:
            raise ValueError("Year out of range (%d..%d)" % (MIN_YEAR, MAX_YEAR))
        if month < 1 or month > 12:
            raise ValueError("Month out of range (1..12)")
        return DAYS_IN_MONTH[(year, month)]

    @classmethod
    def __calc_ordinal(cls, year, month, day):
        if day < 0:
            day = cls.days_in_month(year, month) + int(day) + 1
        # The built-in date class does this faster than a
        # long-hand pure Python algorithm could
        return date(year, month, day).toordinal()

    # CLASS METHOD ALIASES #

    if t.TYPE_CHECKING:
        @classmethod
        def fromisoformat(cls, s: str) -> Date:
            ...

        @classmethod
        def fromordinal(cls, ordinal: int) -> Date:
            ...

        @classmethod
        def fromtimestamp(
            cls, timestamp: float, tz: t.Optional[_tzinfo] = None
        ) -> Date:
            ...

        @classmethod
        def utcfromtimestamp(cls, timestamp: float) -> Date:
            ...

    # CLASS ATTRIBUTES #

    min: te.Final[Date] = None  # type: ignore
    """The earliest date value possible."""

    max: te.Final[Date] = None  # type: ignore
    """The latest date value possible."""

    resolution: te.Final[Duration] = None  # type: ignore
    """The minimum resolution supported."""

    # INSTANCE ATTRIBUTES #

    __ordinal = 0

    __year = 0

    __month = 0

    __day = 0

    @property
    def year(self) -> int:
        """The year of the date.

        :type: int
        """
        return self.__year

    @property
    def month(self) -> int:
        """The month of the date.

        :type: int
        """
        return self.__month

    @property
    def day(self) -> int:
        """The day of the date.

        :type: int
        """
        if self.__day == 0:
            return 0
        if self.__day >= 1:
            return self.__day
        return self.days_in_month(self.__year, self.__month) + self.__day + 1

    @property
    def year_month_day(self) -> t.Tuple[int, int, int]:
        """3-tuple of (year, month, day) describing the date.
        """
        return self.year, self.month, self.day

    @property
    def year_week_day(self) -> t.Tuple[int, int, int]:
        """3-tuple of (year, week_of_year, day_of_week) describing the date.

        `day_of_week` will be 1 for Monday and 7 for Sunday.
        """
        ordinal = self.__ordinal
        year = self.__year

        def day_of_week(o):
            return ((o - 1) % 7) + 1

        def iso_week_1(y):
            j4 = Date(y, 1, 4)
            return j4 + Duration(days=(1 - day_of_week(j4.to_ordinal())))

        if ordinal >= Date(year, 12, 29).to_ordinal():
            week1 = iso_week_1(year + 1)
            if ordinal < week1.to_ordinal():
                week1 = iso_week_1(year)
            else:
                year += 1
        else:
            week1 = iso_week_1(year)
            if ordinal < week1.to_ordinal():
                year -= 1
                week1 = iso_week_1(year)
        return (year, int((ordinal - week1.to_ordinal()) / 7 + 1),
                day_of_week(ordinal))

    @property
    def year_day(self) -> t.Tuple[int, int]:
        """2-tuple of (year, day_of_the_year) describing the date.

        This is the number of the day relative to the start of the year,
        with `1 Jan` corresponding to `1`.
        """
        return (self.__year,
                self.toordinal() - Date(self.__year, 1, 1).toordinal() + 1)

    # OPERATIONS #

    def __hash__(self):
        """"""
        return hash(self.toordinal())

    def __eq__(self, other: object) -> bool:
        """``==`` comparison with :class:`.Date` or :class:`datetime.date`."""
        if isinstance(other, (Date, date)):
            return self.toordinal() == other.toordinal()
        return False

    def __ne__(self, other: object) -> bool:
        """``!=`` comparison with :class:`.Date` or :class:`datetime.date`."""
        return not self.__eq__(other)

    def __lt__(self, other: t.Union[Date, date]) -> bool:
        """``<`` comparison with :class:`.Date` or :class:`datetime.date`."""
        if isinstance(other, (Date, date)):
            return self.toordinal() < other.toordinal()
        raise TypeError("'<' not supported between instances of 'Date' and %r" % type(other).__name__)

    def __le__(self, other: t.Union[Date, date]) -> bool:
        """``<=`` comparison with :class:`.Date` or :class:`datetime.date`."""
        if isinstance(other, (Date, date)):
            return self.toordinal() <= other.toordinal()
        raise TypeError("'<=' not supported between instances of 'Date' and %r" % type(other).__name__)

    def __ge__(self, other: t.Union[Date, date]) -> bool:
        """``>=`` comparison with :class:`.Date` or :class:`datetime.date`."""
        if isinstance(other, (Date, date)):
            return self.toordinal() >= other.toordinal()
        raise TypeError("'>=' not supported between instances of 'Date' and %r" % type(other).__name__)

    def __gt__(self, other: t.Union[Date, date]) -> bool:
        """``>`` comparison with :class:`.Date` or :class:`datetime.date`."""
        if isinstance(other, (Date, date)):
            return self.toordinal() > other.toordinal()
        raise TypeError("'>' not supported between instances of 'Date' and %r" % type(other).__name__)

    def __add__(self, other: Duration) -> Date:  # type: ignore[override]
        """Add a :class:`.Duration`.

        :raises ValueError: if the added duration has a time component.
        """
        def add_months(d, months):
            years, months = symmetric_divmod(months, 12)
            year = d.__year + years
            month = d.__month + months
            while month > 12:
                year += 1
                month -= 12
            while month < 1:
                year -= 1
                month += 12
            d.__year = year
            d.__month = month

        def add_days(d, days):
            assert 1 <= d.__day <= 28 or -28 <= d.__day <= -1
            if d.__day >= 1:
                new_days = d.__day + days
                if 1 <= new_days <= 27:
                    d.__day = new_days
                    return
            d0 = Date.from_ordinal(d.__ordinal + days)
            d.__year, d.__month, d.__day = d0.__year, d0.__month, d0.__day

        if isinstance(other, Duration):
            if other.seconds or other.nanoseconds:
                raise ValueError("Cannot add a Duration with seconds or "
                                 "nanoseconds to a Date")
            if other.months == other.days == 0:
                return self
            new_date = self.replace()
            # Add days before months as the former sometimes
            # requires the current ordinal to be correct.
            if other.days:
                add_days(new_date, other.days)
            if other.months:
                add_months(new_date, other.months)
            new_date.__ordinal = self.__calc_ordinal(new_date.year, new_date.month, new_date.day)
            return new_date
        return NotImplemented

    @t.overload  # type: ignore[override]
    def __sub__(self, other: t.Union[Date, date]) -> Duration:
        ...

    @t.overload
    def __sub__(self, other: Duration) -> Date:
        ...

    def __sub__(self, other):
        """Subtract a :class:`.Date` or :class:`.Duration`.

        :returns: If a :class:`.Date` is subtracted, the time between the two
            dates is returned as :class:`.Duration`. If a :class:`.Duration` is
            subtracted, a new :class:`.Date` is returned.
        :rtype: Date or Duration

        :raises ValueError: if the added duration has a time component.
        """
        if isinstance(other, (Date, date)):
            return Duration(days=(self.toordinal() - other.toordinal()))
        try:
            return self.__add__(-other)
        except TypeError:
            return NotImplemented

    def __copy__(self) -> Date:
        return self.__new(self.__ordinal, self.__year, self.__month, self.__day)

    def __deepcopy__(self, *args, **kwargs) -> Date:
        return self.__copy__()

    # INSTANCE METHODS #

    if t.TYPE_CHECKING:

        def replace(
            self,
            year: te.SupportsIndex = ...,
            month: te.SupportsIndex = ...,
            day: te.SupportsIndex = ...,
            **kwargs: object
        ) -> Date:
            ...

    else:

        def replace(self, **kwargs) -> Date:
            """Return a :class:`.Date` with one or more components replaced.

            :Keyword Arguments:
               * **year** (:class:`typing.SupportsIndex`):
                 overwrite the year - default: `self.year`
               * **month** (:class:`typing.SupportsIndex`):
                 overwrite the month - default: `self.month`
               * **day** (:class:`typing.SupportsIndex`):
                 overwrite the day - default: `self.day`
            """
            return Date(int(kwargs.get("year", self.__year)),
                        int(kwargs.get("month", self.__month)),
                        int(kwargs.get("day", self.__day)))

    def time_tuple(self) -> struct_time:
        """Convert the date to :class:`time.struct_time`."""
        _, _, day_of_week = self.year_week_day
        _, day_of_year = self.year_day
        return struct_time((self.year, self.month, self.day, 0, 0, 0, day_of_week - 1, day_of_year, -1))

    def to_ordinal(self) -> int:
        """The date's proleptic Gregorian ordinal.

        The corresponding class method for the reverse ordinal-to-date
        transformation is :meth:`.Date.from_ordinal`.
        """
        return self.__ordinal

    def to_clock_time(self, epoch: t.Union[Date, DateTime]) -> ClockTime:
        """Convert the date to :class:`ClockTime` relative to `epoch`.

        :param epoch: the epoch to which the date is relative
        """
        try:
            return ClockTime(86400 * (self.to_ordinal() - epoch.to_ordinal()))
        except AttributeError:
            raise TypeError("Epoch has no ordinal value")

    def to_native(self) -> date:
        """Convert to a native Python :class:`datetime.date` value.
        """
        return date.fromordinal(self.to_ordinal())

    def weekday(self) -> int:
        """The day of the week where Monday is 0 and Sunday is 6."""
        return self.year_week_day[2] - 1

    def iso_weekday(self) -> int:
        """The day of the week where Monday is 1 and Sunday is 7."""
        return self.year_week_day[2]

    def iso_calendar(self) -> t.Tuple[int, int, int]:
        """Alias for :attr:`.year_week_day`"""
        return self.year_week_day

    def iso_format(self) -> str:
        """Return the :class:`.Date` as ISO formatted string."""
        if self.__ordinal == 0:
            return "0000-00-00"
        return "%04d-%02d-%02d" % self.year_month_day

    def __repr__(self) -> str:
        """"""
        if self.__ordinal == 0:
            return "neo4j.time.ZeroDate"
        return "neo4j.time.Date(%r, %r, %r)" % self.year_month_day

    def __str__(self) -> str:
        """"""
        return self.iso_format()

    def __format__(self, format_spec):
        """"""
        if not format_spec:
            return self.iso_format()
        format_spec = FORMAT_F_REPLACE.sub("000000000", format_spec)
        return self.to_native().__format__(format_spec)

    # INSTANCE METHOD ALIASES #

    def __getattr__(self, name):
        """ Map standard library attribute names to local attribute names,
        for compatibility.
        """
        try:
            return {
                "isocalendar": self.iso_calendar,
                "isoformat": self.iso_format,
                "isoweekday": self.iso_weekday,
                "strftime": self.__format__,
                "toordinal": self.to_ordinal,
                "timetuple": self.time_tuple,
            }[name]
        except KeyError:
            raise AttributeError("Date has no attribute %r" % name)

    if t.TYPE_CHECKING:
        def iso_calendar(self) -> t.Tuple[int, int, int]:
            ...

        isoformat = iso_format
        isoweekday = iso_weekday
        strftime = __format__
        toordinal = to_ordinal
        timetuple = time_tuple


Date.min = Date.from_ordinal(1)  # type: ignore
Date.max = Date.from_ordinal(3652059)  # type: ignore
Date.resolution = Duration(days=1)  # type: ignore

#: A :class:`neo4j.time.Date` instance set to `0000-00-00`.
#: This has an ordinal value of `0`.
ZeroDate = object.__new__(Date)


if t.TYPE_CHECKING:
    # make typechecker believe that Time subclasses datetime.time
    # https://github.com/python/typeshed/issues/8409#issuecomment-1197704527
    time_base_class = time
else:
    time_base_class = object


class Time(time_base_class, metaclass=TimeType):
    """Time of day.

    The :class:`.Time` class is a nanosecond-precision drop-in replacement for
    the standard library :class:`datetime.time` class.

    A high degree of API compatibility with the standard library classes is
    provided.

    :class:`neo4j.time.Time` objects introduce the concept of ``ticks``.
    This is simply a count of the number of nanoseconds since midnight,
    in many ways analogous to the :class:`neo4j.time.Date` ordinal.
    `ticks` values are integers, with a minimum value of `0` and a maximum
    of `86_399_999_999_999`.

    Local times are represented by :class:`.Time` with no ``tzinfo``.

    :param hour: the hour of the time. Must be in range 0 <= hour < 24.
    :param minute: the minute of the time. Must be in range 0 <= minute < 60.
    :param second: the second of the time. Must be in range 0 <= second < 60.
    :param nanosecond: the nanosecond of the time.
        Must be in range 0 <= nanosecond < 999999999.
    :param tzinfo: timezone or None to get a local :class:`.Time`.

    :raises ValueError: if one of the parameters is out of range.

    .. versionchanged:: 5.0
        The parameter ``second`` no longer accepts :class:`float` values.
    """

    # CONSTRUCTOR #

    def __new__(
        cls,
        hour: int = 0,
        minute: int = 0,
        second: int = 0,
        nanosecond: int = 0,
        tzinfo: t.Optional[_tzinfo] = None
    ) -> Time:
        hour, minute, second, nanosecond = cls.__normalize_nanosecond(
            hour, minute, second, nanosecond
        )
        ticks = (3600000000000 * hour
                 + 60000000000 * minute
                 + 1000000000 * second
                 + nanosecond)
        return cls.__new(ticks, hour, minute, second, nanosecond, tzinfo)

    @classmethod
    def __new(cls, ticks, hour, minute, second, nanosecond, tzinfo):
        instance = object.__new__(cls)
        instance.__ticks = int(ticks)
        instance.__hour = int(hour)
        instance.__minute = int(minute)
        instance.__second = int(second)
        instance.__nanosecond = int(nanosecond)
        instance.__tzinfo = tzinfo
        return instance

    # CLASS METHODS #

    @classmethod
    def now(cls, tz: t.Optional[_tzinfo] = None) -> Time:
        """Get the current time.

        :param tz: optional timezone

        :raises OverflowError: if the timestamp is out of the range of values
            supported by the platform C localtime() function. It’s common for
            this to be restricted to years from 1970 through 2038.
        """
        if tz is None:
            return cls.from_clock_time(Clock().local_time(), UnixEpoch)
        else:
            return (
                DateTime.utc_now()
                .replace(tzinfo=timezone.utc).astimezone(tz)
                .timetz()
            )

    @classmethod
    def utc_now(cls) -> Time:
        """Get the current time as UTC local time."""
        return cls.from_clock_time(Clock().utc_time(), UnixEpoch)

    @classmethod
    def from_iso_format(cls, s: str) -> Time:
        """Parse a ISO formatted time string.

        Accepted formats:
            Local times:
                'hh'
                'hh:mm'
                'hh:mm:ss'
                'hh:mm:ss.ssss...'
            Times with timezones (UTC offset):
                '<local time>+hh:mm'
                '<local time>+hh:mm:ss'
                '<local time>+hh:mm:ss.ssss....'
                '<local time>-hh:mm'
                '<local time>-hh:mm:ss'
                '<local time>-hh:mm:ss.ssss....'

                Where the UTC offset will only respect hours and minutes.
                Seconds and sub-seconds are ignored.

        :param s: String to parse

        :raises ValueError: if the string does not match the required format.
        """
        from pytz import FixedOffset  # type: ignore
        m = TIME_ISO_PATTERN.match(s)
        if m:
            hour = int(m.group(1))
            minute = int(m.group(3) or 0)
            second = int(m.group(6) or 0)
            nanosecond = m.group(7)
            if nanosecond:
                nanosecond = int(nanosecond[1:10].ljust(9, "0"))
            else:
                nanosecond = 0
            if m.group(8) is None:
                return cls(hour, minute, second, nanosecond)
            else:
                offset_multiplier = 1 if m.group(9) == "+" else -1
                offset_hour = int(m.group(10))
                offset_minute = int(m.group(11))
                # pytz only supports offsets of minute resolution
                # so we can ignore this part
                # offset_second = float(m.group(13) or 0.0)
                offset = 60 * offset_hour + offset_minute
                return cls(hour, minute, second, nanosecond,
                           tzinfo=FixedOffset(offset_multiplier * offset))
        raise ValueError("Time string is not in ISO format")

    @classmethod
    def from_ticks(cls, ticks: int, tz: t.Optional[_tzinfo] = None) -> Time:
        """Create a time from ticks (nanoseconds since midnight).

        :param ticks: nanoseconds since midnight
        :param tz: optional timezone

        :raises ValueError: if ticks is out of bounds
            (0 <= ticks < 86400000000000)

        .. versionchanged:: 5.0
            The parameter ``ticks`` no longer accepts :class:`float` values
            but only :class:`int`. It's now nanoseconds since midnight instead
            of seconds.
        """
        if not isinstance(ticks, int):
            raise TypeError("Ticks must be int")
        if 0 <= ticks < 86400000000000:
            second, nanosecond = divmod(ticks, NANO_SECONDS)
            minute, second = divmod(second, 60)
            hour, minute = divmod(minute, 60)
            return cls.__new(ticks, hour, minute, second, nanosecond, tz)
        raise ValueError("Ticks out of range (0..86400000000000)")

    @classmethod
    def from_native(cls, t: time) -> Time:
        """Convert from a native Python :class:`datetime.time` value.

        :param t: time to convert from
        """
        nanosecond = t.microsecond * 1000
        return Time(t.hour, t.minute, t.second, nanosecond, t.tzinfo)

    @classmethod
    def from_clock_time(
        cls,
        clock_time: t.Union[ClockTime, t.Tuple[float, int]],
        epoch: DateTime
    ) -> Time:
        """Convert from a :class:`.ClockTime` relative to a given epoch.

        This method, in contrast to most others of this package, assumes days of
        exactly 24 hours.

        :param clock_time: the clock time as :class:`.ClockTime` or as tuple of
            (seconds, nanoseconds)
        :param epoch: the epoch to which `clock_time` is relative
        """
        clock_time = ClockTime(*clock_time)
        ts = clock_time.seconds % 86400
        nanoseconds = int(NANO_SECONDS * ts + clock_time.nanoseconds)
        ticks = (epoch.time().ticks + nanoseconds) % (86400 * NANO_SECONDS)
        return Time.from_ticks(ticks)

    @classmethod
    def __normalize_hour(cls, hour):
        hour = int(hour)
        if 0 <= hour < 24:
            return hour
        raise ValueError("Hour out of range (0..23)")

    @classmethod
    def __normalize_minute(cls, hour, minute):
        hour = cls.__normalize_hour(hour)
        minute = int(minute)
        if 0 <= minute < 60:
            return hour, minute
        raise ValueError("Minute out of range (0..59)")

    @classmethod
    def __normalize_second(cls, hour, minute, second):
        hour, minute = cls.__normalize_minute(hour, minute)
        second = int(second)
        if 0 <= second < 60:
            return hour, minute, second
        raise ValueError("Second out of range (0..59)")

    @classmethod
    def __normalize_nanosecond(cls, hour, minute, second, nanosecond):
        hour, minute, second = cls.__normalize_second(hour, minute, second)
        if 0 <= nanosecond < NANO_SECONDS:
            return hour, minute, second, nanosecond
        raise ValueError("Nanosecond out of range (0..%s)" % (NANO_SECONDS - 1))

    # CLASS METHOD ALIASES #

    if t.TYPE_CHECKING:

        @classmethod
        def from_iso_format(cls, s: str) -> Time:
            ...

        @classmethod
        def utc_now(cls) -> Time:
            ...

    # CLASS ATTRIBUTES #

    min: te.Final[Time] = None  # type: ignore
    """The earliest time value possible."""

    max: te.Final[Time] = None  # type: ignore
    """The latest time value possible."""

    resolution: te.Final[Duration] = None  # type: ignore
    """The minimum resolution supported."""

    # INSTANCE ATTRIBUTES #

    __ticks = 0

    __hour = 0

    __minute = 0

    __second = 0

    __nanosecond = 0

    __tzinfo = None

    @property
    def ticks(self) -> int:
        """The total number of nanoseconds since midnight.

        .. versionchanged:: 5.0
            The property's type changed from :class:`float` to :class:`int`.
            It's now nanoseconds since midnight instead of seconds.
        """
        return self.__ticks

    @property
    def hour(self) -> int:
        """The hours of the time."""
        return self.__hour

    @property
    def minute(self) -> int:
        """The minutes of the time."""
        return self.__minute

    @property
    def second(self) -> int:
        """The seconds of the time.

        .. versionchanged:: 4.4
            The property's type changed from :class:`float` to
            :class:`decimal.Decimal` to mitigate rounding issues.

        .. versionchanged:: 5.0
            The  property's type changed from :class:`decimal.Decimal` to
            :class:`int`. It does not longer cary sub-second information.
            Use `attr:`nanosecond` instead.
        """
        return self.__second

    @property
    def nanosecond(self) -> int:
        """The nanoseconds of the time."""
        return self.__nanosecond

    @property
    def hour_minute_second_nanosecond(self) -> t.Tuple[int, int, int, int]:
        """The time as a tuple of (hour, minute, second, nanosecond)."""
        return self.__hour, self.__minute, self.__second, self.__nanosecond

    @property
    def tzinfo(self) -> t.Optional[_tzinfo]:
        """The timezone of this time."""
        return self.__tzinfo

    # OPERATIONS #

    def _get_both_normalized_ticks(self, other: object, strict=True):
        if (isinstance(other, (time, Time))
                and ((self.utc_offset() is None)
                     ^ (other.utcoffset() is None))):
            if strict:
                raise TypeError("can't compare offset-naive and offset-aware "
                                "times")
            else:
                return None, None
        other_ticks: int
        if isinstance(other, Time):
            other_ticks = other.__ticks
        elif isinstance(other, time):
            other_ticks = int(3600000000000 * other.hour
                              + 60000000000 * other.minute
                              + NANO_SECONDS * other.second
                              + 1000 * other.microsecond)
        else:
            return None, None
        assert isinstance(other, (Time, time))
        utc_offset: t.Optional[timedelta] = other.utcoffset()
        if utc_offset is not None:
            other_ticks -= int(utc_offset.total_seconds() * NANO_SECONDS)
        self_ticks = self.__ticks
        utc_offset = self.utc_offset()
        if utc_offset is not None:
            self_ticks -= int(utc_offset.total_seconds() * NANO_SECONDS)
        return self_ticks, other_ticks

    def __hash__(self):
        """"""
        if self.__nanosecond % 1000 == 0:
            return hash(self.to_native())
        self_ticks = self.__ticks
        if self.utc_offset() is not None:
            self_ticks -= self.utc_offset().total_seconds() * NANO_SECONDS
        return hash(self_ticks)

    def __eq__(self, other: object) -> bool:
        """`==` comparison with :class:`.Time` or :class:`datetime.time`."""
        self_ticks, other_ticks = self._get_both_normalized_ticks(other,
                                                                  strict=False)
        if self_ticks is None:
            return False
        return self_ticks == other_ticks

    def __ne__(self, other: object) -> bool:
        """`!=` comparison with :class:`.Time` or :class:`datetime.time`."""
        return not self.__eq__(other)

    def __lt__(self, other: t.Union[Time, time]) -> bool:
        """`<` comparison with :class:`.Time` or :class:`datetime.time`."""
        self_ticks, other_ticks = self._get_both_normalized_ticks(other)
        if self_ticks is None:
            return NotImplemented
        return self_ticks < other_ticks

    def __le__(self, other: t.Union[Time, time]) -> bool:
        """`<=` comparison with :class:`.Time` or :class:`datetime.time`."""
        self_ticks, other_ticks = self._get_both_normalized_ticks(other)
        if self_ticks is None:
            return NotImplemented
        return self_ticks <= other_ticks

    def __ge__(self, other: t.Union[Time, time]) -> bool:
        """`>=` comparison with :class:`.Time` or :class:`datetime.time`."""
        self_ticks, other_ticks = self._get_both_normalized_ticks(other)
        if self_ticks is None:
            return NotImplemented
        return self_ticks >= other_ticks

    def __gt__(self, other: t.Union[Time, time]) -> bool:
        """`>` comparison with :class:`.Time` or :class:`datetime.time`."""
        self_ticks, other_ticks = self._get_both_normalized_ticks(other)
        if self_ticks is None:
            return NotImplemented
        return self_ticks > other_ticks

    def __copy__(self) -> Time:
        return self.__new(self.__ticks, self.__hour, self.__minute,
                          self.__second, self.__nanosecond, self.__tzinfo)

    def __deepcopy__(self, *args, **kwargs) -> Time:
        return self.__copy__()

    # INSTANCE METHODS #

    if t.TYPE_CHECKING:

        def replace(  # type: ignore[override]
            self,
            hour: te.SupportsIndex = ...,
            minute: te.SupportsIndex = ...,
            second: te.SupportsIndex = ...,
            nanosecond: te.SupportsIndex = ...,
            tzinfo: t.Optional[_tzinfo] = ...,
            **kwargs: object
        ) -> Time:
            ...

    else:

        def replace(self, **kwargs) -> Time:
            """Return a :class:`.Time` with one or more components replaced.

            :Keyword Arguments:
               * **hour** (:class:`typing.SupportsIndex`):
                 overwrite the hour - default: `self.hour`
               * **minute** (:class:`typing.SupportsIndex`):
                 overwrite the minute - default: `self.minute`
               * **second** (:class:`typing.SupportsIndex`):
                 overwrite the second - default: `int(self.second)`
               * **nanosecond** (:class:`typing.SupportsIndex`):
                 overwrite the nanosecond - default: `self.nanosecond`
               * **tzinfo** (:class:`datetime.tzinfo` or `None`):
                 overwrite the timezone - default: `self.tzinfo`
            """
            return Time(hour=int(kwargs.get("hour", self.__hour)),
                        minute=int(kwargs.get("minute", self.__minute)),
                        second=int(kwargs.get("second", self.__second)),
                        nanosecond=int(kwargs.get("nanosecond",
                                                  self.__nanosecond)),
                        tzinfo=kwargs.get("tzinfo", self.__tzinfo))

    def _utc_offset(self, dt=None):
        if self.tzinfo is None:
            return None
        try:
            value = self.tzinfo.utcoffset(dt)
        except TypeError:
            # For timezone implementations not compatible with the custom
            # datetime implementations, we can't do better than this.
            value = self.tzinfo.utcoffset(dt.to_native())
        if value is None:
            return None
        if isinstance(value, timedelta):
            s = value.total_seconds()
            if not (-86400 < s < 86400):
                raise ValueError("utcoffset must be less than a day")
            if s % 60 != 0 or value.microseconds != 0:
                raise ValueError("utcoffset must be a whole number of minutes")
            return value
        raise TypeError("utcoffset must be a timedelta")

    def utc_offset(self) -> t.Optional[timedelta]:
        """Return the UTC offset of this time.

        :returns: None if this is a local time (:attr:`.tzinfo` is None), else
            returns `self.tzinfo.utcoffset(self)`.

        :raises ValueError: if `self.tzinfo.utcoffset(self)` is not None and a
            :class:`timedelta` with a magnitude greater equal 1 day or that is
            not a whole number of minutes.
        :raises TypeError: if `self.tzinfo.utcoffset(self)` does return anything but
            None or a :class:`datetime.timedelta`.
        """
        return self._utc_offset()

    def dst(self) -> t.Optional[timedelta]:
        """Get the daylight saving time adjustment (DST).

        :returns: None if this is a local time (:attr:`.tzinfo` is None), else
            returns `self.tzinfo.dst(self)`.

        :raises ValueError: if `self.tzinfo.dst(self)` is not None and a
            :class:`timedelta` with a magnitude greater equal 1 day or that is
            not a whole number of minutes.
        :raises TypeError: if `self.tzinfo.dst(self)` does return anything but
            None or a :class:`datetime.timedelta`.
        """
        if self.tzinfo is None:
            return None
        try:
            value = self.tzinfo.dst(self)  # type: ignore
        except TypeError:
            # For timezone implementations not compatible with the custom
            # datetime implementations, we can't do better than this.
            value = self.tzinfo.dst(self.to_native())  # type: ignore
        if value is None:
            return None
        if isinstance(value, timedelta):
            if value.days != 0:
                raise ValueError("dst must be less than a day")
            if value.seconds % 60 != 0 or value.microseconds != 0:
                raise ValueError("dst must be a whole number of minutes")
            return value
        raise TypeError("dst must be a timedelta")

    def tzname(self) -> t.Optional[str]:
        """Get the name of the :class:`.Time`'s timezone.

        :returns: None if the time is local (i.e., has no timezone), else return
            `self.tzinfo.tzname(self)`
        """
        if self.tzinfo is None:
            return None
        try:
            return self.tzinfo.tzname(self)  # type: ignore
        except TypeError:
            # For timezone implementations not compatible with the custom
            # datetime implementations, we can't do better than this.
            return self.tzinfo.tzname(self.to_native())  # type: ignore

    def to_clock_time(self) -> ClockTime:
        """Convert to :class:`.ClockTime`."""
        seconds, nanoseconds = divmod(self.ticks, NANO_SECONDS)
        return ClockTime(seconds, nanoseconds)

    def to_native(self) -> time:
        """Convert to a native Python `datetime.time` value.

        This conversion is lossy as the native time implementation only
        supports a resolution of microseconds instead of nanoseconds.
        """
        h, m, s, ns = self.hour_minute_second_nanosecond
        µs = round_half_to_even(ns / 1000)
        tz = self.tzinfo
        return time(h, m, s, µs, tz)

    def iso_format(self) -> str:
        """Return the :class:`.Time` as ISO formatted string."""
        s = "%02d:%02d:%02d.%09d" % self.hour_minute_second_nanosecond
        offset = self.utc_offset()
        if offset is not None:
            s += "%+03d:%02d" % divmod(offset.total_seconds() // 60, 60)
        return s

    def __repr__(self) -> str:
        """"""
        if self.tzinfo is None:
            return "neo4j.time.Time(%r, %r, %r, %r)" % \
                   self.hour_minute_second_nanosecond
        else:
            return "neo4j.time.Time(%r, %r, %r, %r, tzinfo=%r)" % \
                   (self.hour_minute_second_nanosecond + (self.tzinfo,))

    def __str__(self) -> str:
        """"""
        return self.iso_format()

    def __format__(self, format_spec):
        """"""
        if not format_spec:
            return self.iso_format()
        format_spec = FORMAT_F_REPLACE.sub(f"{self.__nanosecond:09}",
                                           format_spec)
        return self.to_native().__format__(format_spec)

    # INSTANCE METHOD ALIASES #

    def __getattr__(self, name):
        """Map standard library attribute names to local attribute names,
        for compatibility.
        """
        try:
            return {
                "isoformat": self.iso_format,
                "utcoffset": self.utc_offset,
            }[name]
        except KeyError:
            raise AttributeError("Date has no attribute %r" % name)

    if t.TYPE_CHECKING:

        def isoformat(self) -> str:  # type: ignore[override]
            ...

        utcoffset = utc_offset


Time.min = Time(  # type: ignore
    hour=0, minute=0, second=0, nanosecond=0
)
Time.max = Time(  # type: ignore
    hour=23, minute=59, second=59, nanosecond=999999999
)
Time.resolution = Duration(  # type: ignore
    nanoseconds=1
)

#: A :class:`.Time` instance set to `00:00:00`.
#: This has a :attr:`.ticks` value of `0`.
Midnight: te.Final[Time] = Time.min

#: A :class:`.Time` instance set to `12:00:00`.
#: This has a :attr:`.ticks` value of `43200000000000`.
Midday: te.Final[Time] = Time(hour=12)


if t.TYPE_CHECKING:
    # make typechecker believe that DateTime subclasses datetime.datetime
    # https://github.com/python/typeshed/issues/8409#issuecomment-1197704527
    date_time_base_class = datetime
else:
    date_time_base_class = object


@total_ordering
class DateTime(date_time_base_class, metaclass=DateTimeType):
    """A point in time represented as a date and a time.

    The :class:`.DateTime` class is a nanosecond-precision drop-in replacement
    for the standard library :class:`datetime.datetime` class.

    As such, it contains both :class:`.Date` and :class:`.Time` information and
    draws functionality from those individual classes.

    A :class:`.DateTime` object is fully compatible with the Python time zone
    library `pytz <https://pypi.org/project/pytz/>`_. Functions such as
    `normalize` and `localize` can be used in the same way as they are with the
    standard library classes.

    Regular construction of a :class:`.DateTime` object requires at
    least the `year`, `month` and `day` arguments to be supplied. The
    optional `hour`, `minute` and `second` arguments default to zero and
    `tzinfo` defaults to :data:`None`.

    `year`, `month`, and `day` are passed to the constructor of :class:`.Date`.
    `hour`, `minute`, `second`, `nanosecond`, and `tzinfo` are passed to the
    constructor of :class:`.Time`. See their documentation for more details.

        >>> dt = DateTime(2018, 4, 30, 12, 34, 56, 789123456); dt
        neo4j.time.DateTime(2018, 4, 30, 12, 34, 56, 789123456)
        >>> dt.second
        56
    """

    __date: Date
    __time: Time

    # CONSTRUCTOR #

    def __new__(
        cls,
        year: int,
        month: int,
        day: int,
        hour: int = 0,
        minute: int = 0,
        second: int = 0,
        nanosecond: int = 0,
        tzinfo: t.Optional[_tzinfo] = None
    ) -> DateTime:
        return cls.combine(Date(year, month, day),
                           Time(hour, minute, second, nanosecond, tzinfo))

    # CLASS METHODS #

    @classmethod
    def now(cls, tz: t.Optional[_tzinfo] = None) -> DateTime:
        """Get the current date and time.

        :param tz: timezone. Set to None to create a local :class:`.DateTime`.

        :raises OverflowError: if the timestamp is out of the range of values
            supported by the platform C localtime() function. It’s common for
            this to be restricted to years from 1970 through 2038.
        """
        if tz is None:
            return cls.from_clock_time(Clock().local_time(), UnixEpoch)
        else:
            try:
                return tz.fromutc(  # type: ignore
                    cls.from_clock_time(  # type: ignore
                        Clock().utc_time(), UnixEpoch
                    ).replace(tzinfo=tz)
                )
            except TypeError:
                # For timezone implementations not compatible with the custom
                # datetime implementations, we can't do better than this.
                utc_now = cls.from_clock_time(
                    Clock().utc_time(), UnixEpoch
                )
                utc_now_native = utc_now.to_native()
                now_native = tz.fromutc(utc_now_native)
                now = cls.from_native(now_native)
                return now.replace(
                    nanosecond=(now.nanosecond
                                + utc_now.nanosecond
                                - utc_now_native.microsecond * 1000)
                )

    @classmethod
    def utc_now(cls) -> DateTime:
        """Get the current date and time in UTC."""
        return cls.from_clock_time(Clock().utc_time(), UnixEpoch)

    @classmethod
    def from_iso_format(cls, s) -> DateTime:
        """Parse a ISO formatted date with time string.

        :param s: String to parse

        :raises ValueError: if the string does not match the ISO format.
        """
        try:
            return cls.combine(Date.from_iso_format(s[0:10]),
                               Time.from_iso_format(s[11:]))
        except ValueError:
            raise ValueError("DateTime string is not in ISO format")

    @classmethod
    def from_timestamp(
        cls, timestamp: float, tz: t.Optional[_tzinfo] = None
    ) -> DateTime:
        """:class:`.DateTime` from a time stamp (seconds since unix epoch).

        :param timestamp: the unix timestamp (seconds since unix epoch).
        :param tz: timezone. Set to None to create a local :class:`.DateTime`.

        :raises OverflowError: if the timestamp is out of the range of values
            supported by the platform C localtime() function. It’s common for
            this to be restricted to years from 1970 through 2038.
        """
        if tz is None:
            return cls.from_clock_time(
                ClockTime(timestamp) + Clock().local_offset(), UnixEpoch
            )
        else:
            return (
                cls.utc_from_timestamp(timestamp)
                .replace(tzinfo=timezone.utc).astimezone(tz)
            )

    @classmethod
    def utc_from_timestamp(cls, timestamp: float) -> DateTime:
        """:class:`.DateTime` from a time stamp (seconds since unix epoch).

        Returns the `DateTime` as local date `DateTime` in UTC.
        """
        return cls.from_clock_time((timestamp, 0), UnixEpoch)

    @classmethod
    def from_ordinal(cls, ordinal: int) -> DateTime:
        """:class:`.DateTime` from an ordinal.

        For more info about ordinals see :meth:`.Date.from_ordinal`.
        """
        return cls.combine(Date.from_ordinal(ordinal), Midnight)

    @classmethod
    def combine(  # type: ignore[override]
        cls, date: Date, time: Time
    ) -> DateTime:
        """Combine a :class:`.Date` and a :class:`.Time` to a :class:`DateTime`.

        :param date: the date
        :param time: the time

        :raises AssertionError: if the parameter types don't match.
        """
        assert isinstance(date, Date)
        assert isinstance(time, Time)
        instance = object.__new__(cls)
        instance.__date = date
        instance.__time = time
        return instance

    @classmethod
    def parse(cls, date_string, format):
        raise NotImplementedError()

    @classmethod
    def from_native(cls, dt: datetime) -> DateTime:
        """Convert from a native Python :class:`datetime.datetime` value.

        :param dt: the datetime to convert
        """
        return cls.combine(Date.from_native(dt.date()),
                           Time.from_native(dt.timetz()))

    @classmethod
    def from_clock_time(
        cls,
        clock_time: t.Union[ClockTime, t.Tuple[float, int]],
        epoch: DateTime
    ) -> DateTime:
        """Convert from a :class:`ClockTime` relative to a given epoch.

        :param clock_time: the clock time as :class:`.ClockTime` or as tuple of
            (seconds, nanoseconds)
        :param epoch: the epoch to which `clock_time` is relative

        :raises ValueError: if `clock_time` is invalid.
        """
        try:
            seconds, nanoseconds = ClockTime(*clock_time)
        except (TypeError, ValueError):
            raise ValueError("Clock time must be a 2-tuple of (s, ns)")
        else:
            ordinal, seconds = divmod(seconds, 86400)
            ticks = epoch.time().ticks + seconds * NANO_SECONDS + nanoseconds
            days, ticks = divmod(ticks, 86400 * NANO_SECONDS)
            ordinal += days
            date_ = Date.from_ordinal(ordinal + epoch.date().to_ordinal())
            time_ = Time.from_ticks(ticks)
            return cls.combine(date_, time_)

    # CLASS METHOD ALIASES #

    if t.TYPE_CHECKING:

        @classmethod
        def fromisoformat(cls, s) -> DateTime:
            ...

        @classmethod
        def fromordinal(cls, ordinal: int) -> DateTime:
            ...

        @classmethod
        def fromtimestamp(
            cls, timestamp: float, tz: t.Optional[_tzinfo] = None
        ) -> DateTime:
            ...

        # alias of parse
        @classmethod
        def strptime(cls, date_string, format):
            ...

        # alias of now
        @classmethod
        def today(cls, tz: t.Optional[_tzinfo] = None) -> DateTime:
            ...

        @classmethod
        def utcfromtimestamp(cls, timestamp: float) -> DateTime:
            ...

        @classmethod
        def utcnow(cls) -> DateTime:
            ...

    # CLASS ATTRIBUTES #

    min: te.Final[DateTime] = None  # type: ignore
    """The earliest date time value possible."""

    max: te.Final[DateTime] = None  # type: ignore
    """The latest date time value possible."""

    resolution: te.Final[Duration] = None  # type: ignore
    """The minimum resolution supported."""

    # INSTANCE ATTRIBUTES #

    @property
    def year(self) -> int:
        """The year of the :class:`.DateTime`.

        See :attr:`.Date.year`.
        """
        return self.__date.year

    @property
    def month(self) -> int:
        """The year of the :class:`.DateTime`.

        See :attr:`.Date.year`."""
        return self.__date.month

    @property
    def day(self) -> int:
        """The day of the :class:`.DateTime`'s date.

        See :attr:`.Date.day`."""
        return self.__date.day

    @property
    def year_month_day(self) -> t.Tuple[int, int, int]:
        """The year_month_day of the :class:`.DateTime`'s date.

        See :attr:`.Date.year_month_day`."""
        return self.__date.year_month_day

    @property
    def year_week_day(self) -> t.Tuple[int, int, int]:
        """The year_week_day of the :class:`.DateTime`'s date.

        See :attr:`.Date.year_week_day`."""
        return self.__date.year_week_day

    @property
    def year_day(self) -> t.Tuple[int, int]:
        """The year_day of the :class:`.DateTime`'s date.

        See :attr:`.Date.year_day`."""
        return self.__date.year_day

    @property
    def hour(self) -> int:
        """The hour of the :class:`.DateTime`'s time.

        See :attr:`.Time.hour`."""
        return self.__time.hour

    @property
    def minute(self) -> int:
        """The minute of the :class:`.DateTime`'s time.

        See :attr:`.Time.minute`."""
        return self.__time.minute

    @property
    def second(self) -> int:
        """The second of the :class:`.DateTime`'s time.

        See :attr:`.Time.second`."""
        return self.__time.second

    @property
    def nanosecond(self) -> int:
        """The nanosecond of the :class:`.DateTime`'s time.

        See :attr:`.Time.nanosecond`."""
        return self.__time.nanosecond

    @property
    def tzinfo(self) -> t.Optional[_tzinfo]:
        """The tzinfo of the :class:`.DateTime`'s time.

        See :attr:`.Time.tzinfo`."""
        return self.__time.tzinfo

    @property
    def hour_minute_second_nanosecond(self) -> t.Tuple[int, int, int, int]:
        """The hour_minute_second_nanosecond of the :class:`.DateTime`'s time.

        See :attr:`.Time.hour_minute_second_nanosecond`."""
        return self.__time.hour_minute_second_nanosecond

    # OPERATIONS #

    def _get_both_normalized(self, other, strict=True):
        if (isinstance(other, (datetime, DateTime))
                and ((self.utc_offset() is None)
                     ^ (other.utcoffset() is None))):
            if strict:
                raise TypeError("can't compare offset-naive and offset-aware "
                                "datetimes")
            else:
                return None, None
        self_norm = self
        utc_offset = self.utc_offset()
        if utc_offset is not None:
            self_norm -= utc_offset
        self_norm = self_norm.replace(tzinfo=None)
        other_norm = other
        if isinstance(other, (datetime, DateTime)):
            utc_offset = other.utcoffset()
            if utc_offset is not None:
                other_norm -= utc_offset
            other_norm = other_norm.replace(tzinfo=None)
        else:
            return None, None
        return self_norm, other_norm

    def __hash__(self):
        """"""
        if self.nanosecond % 1000 == 0:
            return hash(self.to_native())
        self_norm = self
        utc_offset = self.utc_offset()
        if utc_offset is not None:
            self_norm -= utc_offset
        return hash(self_norm.date()) ^ hash(self_norm.time())

    def __eq__(self, other: object) -> bool:
        """
        ``==`` comparison with :class:`.DateTime` or :class:`datetime.datetime`.
        """
        if not isinstance(other, (datetime, DateTime)):
            return NotImplemented
        if self.utc_offset() == other.utcoffset():
            return self.date() == other.date() and self.time() == other.time()
        self_norm, other_norm = self._get_both_normalized(other, strict=False)
        if self_norm is None:
            return False
        return self_norm == other_norm

    def __ne__(self, other: object) -> bool:
        """
        ``!=`` comparison with :class:`.DateTime` or :class:`datetime.datetime`.
        """
        return not self.__eq__(other)

    def __lt__(  # type: ignore[override]
        self, other: datetime
    ) -> bool:
        """
        ``<`` comparison with :class:`.DateTime` or :class:`datetime.datetime`.
        """
        if not isinstance(other, (datetime, DateTime)):
            return NotImplemented
        if self.utc_offset() == other.utcoffset():
            if self.date() == other.date():
                return self.time() < other.time()
            return self.date() < other.date()
        self_norm, other_norm = self._get_both_normalized(other)
        return (self_norm.date() < other_norm.date()
                or self_norm.time() < other_norm.time())

    def __le__(  # type: ignore[override]
        self, other: t.Union[datetime, DateTime]
    ) -> bool:
        """
        ``<=`` comparison with :class:`.DateTime` or :class:`datetime.datetime`.
        """
        if not isinstance(other, (datetime, DateTime)):
            return NotImplemented
        if self.utc_offset() == other.utcoffset():
            if self.date() == other.date():
                return self.time() <= other.time()
            return self.date() <= other.date()
        self_norm, other_norm = self._get_both_normalized(other)
        return self_norm <= other_norm

    def __ge__(  # type: ignore[override]
        self, other: t.Union[datetime, DateTime]
    ) -> bool:
        """
        ``>=`` comparison with :class:`.DateTime` or :class:`datetime.datetime`.
        """
        if not isinstance(other, (datetime, DateTime)):
            return NotImplemented
        if self.utc_offset() == other.utcoffset():
            if self.date() == other.date():
                return self.time() >= other.time()
            return self.date() >= other.date()
        self_norm, other_norm = self._get_both_normalized(other)
        return self_norm >= other_norm

    def __gt__(  # type: ignore[override]
        self, other: t.Union[datetime, DateTime]
    ) -> bool:
        """
        ``>`` comparison with :class:`.DateTime` or :class:`datetime.datetime`.
        """
        if not isinstance(other, (datetime, DateTime)):
            return NotImplemented
        if self.utc_offset() == other.utcoffset():
            if self.date() == other.date():
                return self.time() > other.time()
            return self.date() > other.date()
        self_norm, other_norm = self._get_both_normalized(other)
        return (self_norm.date() > other_norm.date()
                or self_norm.time() > other_norm.time())

    def __add__(self, other: t.Union[timedelta, Duration]) -> DateTime:
        """Add a :class:`datetime.timedelta`."""
        if isinstance(other, Duration):
            t = (self.to_clock_time()
                 + ClockTime(other.seconds, other.nanoseconds))
            days, seconds = symmetric_divmod(t.seconds, 86400)
            date_ = self.date() + Duration(months=other.months,
                                           days=days + other.days)
            time_ = Time.from_ticks(seconds * NANO_SECONDS + t.nanoseconds)
            return self.combine(date_, time_).replace(tzinfo=self.tzinfo)
        if isinstance(other, timedelta):
            t = (self.to_clock_time()
                 + ClockTime(86400 * other.days + other.seconds,
                             other.microseconds * 1000))
            days, seconds = symmetric_divmod(t.seconds, 86400)
            date_ = Date.from_ordinal(days + 1)
            time_ = Time.from_ticks(round_half_to_even(
                seconds * NANO_SECONDS + t.nanoseconds
            ))
            return self.combine(date_, time_).replace(tzinfo=self.tzinfo)
        return NotImplemented

    @t.overload  # type: ignore[override]
    def __sub__(self, other: DateTime) -> Duration:
        ...

    @t.overload
    def __sub__(self, other: datetime) -> timedelta:
        ...

    @t.overload
    def __sub__(self, other: t.Union[Duration, timedelta]) -> DateTime:
        ...

    def __sub__(self, other):
        """Subtract a datetime/DateTime or a timedelta/Duration.

        Subtracting a :class:`.DateTime` yields the duration between the two
        as a :class:`.Duration`.

        Subtracting a :class:`datetime.datetime` yields the duration between
        the two as a :class:`datetime.timedelta`.

        Subtracting a :class:`datetime.timedelta` or a :class:`.Duration`
        yields the :class:`.DateTime` that's the given duration away.
        """
        if isinstance(other, DateTime):
            self_month_ordinal = 12 * (self.year - 1) + self.month
            other_month_ordinal = 12 * (other.year - 1) + other.month
            months = self_month_ordinal - other_month_ordinal
            days = self.day - other.day
            t = self.time().to_clock_time() - other.time().to_clock_time()
            return Duration(months=months, days=days, seconds=t.seconds,
                            nanoseconds=t.nanoseconds)
        if isinstance(other, datetime):
            days = self.to_ordinal() - other.toordinal()
            t = (self.time().to_clock_time()
                 - ClockTime(
                       3600 * other.hour + 60 * other.minute + other.second,
                       other.microsecond * 1000
                    ))
            return timedelta(days=days, seconds=t.seconds,
                             microseconds=(t.nanoseconds // 1000))
        if isinstance(other, Duration):
            return self.__add__(-other)
        if isinstance(other, timedelta):
            return self.__add__(-other)
        return NotImplemented

    def __copy__(self) -> DateTime:
        return self.combine(self.__date, self.__time)

    def __deepcopy__(self, memo) -> DateTime:
        return self.__copy__()

    # INSTANCE METHODS #

    def date(self) -> Date:
        """The date."""
        return self.__date

    def time(self) -> Time:
        """The time without timezone info."""
        return self.__time.replace(tzinfo=None)

    def timetz(self) -> Time:
        """The time with timezone info."""
        return self.__time

    if t.TYPE_CHECKING:

        def replace(  # type: ignore[override]
            self,
            year: te.SupportsIndex = ...,
            month: te.SupportsIndex = ...,
            day: te.SupportsIndex = ...,
            hour: te.SupportsIndex = ...,
            minute: te.SupportsIndex = ...,
            second: te.SupportsIndex = ...,
            nanosecond: te.SupportsIndex = ...,
            tzinfo: t.Optional[_tzinfo] = ...,
            **kwargs: object
        ) -> DateTime:
            ...

    else:

        def replace(self, **kwargs) -> DateTime:
            """Return a ``DateTime`` with one or more components replaced.

            See :meth:`.Date.replace` and :meth:`.Time.replace` for available
            arguments.
            """
            date_ = self.__date.replace(**kwargs)
            time_ = self.__time.replace(**kwargs)
            return self.combine(date_, time_)

    def as_timezone(self, tz: _tzinfo) -> DateTime:
        """Convert this :class:`.DateTime` to another timezone.

        :param tz: the new timezone

        :returns: the same object if ``tz`` is :data:``None``.
            Else, a new :class:`.DateTime` that's the same point in time but in
            a different timezone.
        """
        if self.tzinfo is None:
            return self
        offset = t.cast(timedelta, self.utcoffset())
        utc = (self - offset).replace(tzinfo=tz)
        try:
            return tz.fromutc(utc)  # type: ignore
        except TypeError:
            # For timezone implementations not compatible with the custom
            # datetime implementations, we can't do better than this.
            native_utc = utc.to_native()
            native_res = tz.fromutc(native_utc)
            res = self.from_native(native_res)
            return res.replace(
                nanosecond=(native_res.microsecond * 1000
                            + self.nanosecond % 1000)
            )

    def utc_offset(self) -> t.Optional[timedelta]:
        """Get the date times utc offset.

        See :meth:`.Time.utc_offset`.
        """

        return self.__time._utc_offset(self)

    def dst(self) -> t.Optional[timedelta]:
        """Get the daylight saving time adjustment (DST).

        See :meth:`.Time.dst`.
        """
        return self.__time.dst()

    def tzname(self) -> t.Optional[str]:
        """Get the timezone name.

        See :meth:`.Time.tzname`.
        """
        return self.__time.tzname()

    def time_tuple(self):
        raise NotImplementedError()

    def utc_time_tuple(self):
        raise NotImplementedError()

    def to_ordinal(self) -> int:
        """Get the ordinal of the :class:`.DateTime`'s date.

        See :meth:`.Date.to_ordinal`
        """
        return self.__date.to_ordinal()

    def to_clock_time(self) -> ClockTime:
        """Convert to :class:`.ClockTime`."""
        ordinal_seconds = 86400 * (self.__date.to_ordinal() - 1)
        time_seconds, nanoseconds = divmod(self.__time.ticks, NANO_SECONDS)
        return ClockTime(ordinal_seconds + time_seconds, nanoseconds)

    def to_native(self) -> datetime:
        """Convert to a native Python :class:`datetime.datetime` value.

        This conversion is lossy as the native time implementation only supports
        a resolution of microseconds instead of nanoseconds.
        """
        y, mo, d = self.year_month_day
        h, m, s, ns = self.hour_minute_second_nanosecond
        ms = int(ns / 1000)
        tz = self.tzinfo
        return datetime(y, mo, d, h, m, s, ms, tz)

    def weekday(self) -> int:
        """Get the weekday.

        See :meth:`.Date.weekday`
        """
        return self.__date.weekday()

    def iso_weekday(self) -> int:
        """Get the ISO weekday.

        See :meth:`.Date.iso_weekday`
        """
        return self.__date.iso_weekday()

    def iso_calendar(self) -> t.Tuple[int, int, int]:
        """Get date as ISO tuple.

        See :meth:`.Date.iso_calendar`
        """
        return self.__date.iso_calendar()

    def iso_format(self, sep: str = "T") -> str:
        """Return the :class:`.DateTime` as ISO formatted string.

        This method joins `self.date().iso_format()` (see
        :meth:`.Date.iso_format`) and `self.timetz().iso_format()` (see
        :meth:`.Time.iso_format`) with `sep` in between.

        :param sep: the separator between the formatted date and time.
        """
        s = "%s%s%s" % (self.date().iso_format(), sep,
                        self.timetz().iso_format())
        time_tz = self.timetz()
        offset = time_tz.utc_offset()
        if offset is not None:
            # the time component will have taken care of formatting the offset
            return s
        offset = self.utc_offset()
        if offset is not None:
            s += "%+03d:%02d" % divmod(offset.total_seconds() // 60, 60)
        return s

    def __repr__(self) -> str:
        """"""
        fields: tuple
        if self.tzinfo is None:
            fields = (*self.year_month_day,
                      *self.hour_minute_second_nanosecond)
            return "neo4j.time.DateTime(%r, %r, %r, %r, %r, %r, %r)" % fields
        else:
            fields = (*self.year_month_day,
                      *self.hour_minute_second_nanosecond, self.tzinfo)
            return ("neo4j.time.DateTime(%r, %r, %r, %r, %r, %r, %r, tzinfo=%r)"
                    % fields)

    def __str__(self) -> str:
        """"""
        return self.iso_format()

    def __format__(self, format_spec):
        """"""
        if not format_spec:
            return self.iso_format()
        format_spec = FORMAT_F_REPLACE.sub(f"{self.__time.nanosecond:09}",
                                           format_spec)
        return self.to_native().__format__(format_spec)

    # INSTANCE METHOD ALIASES #

    def __getattr__(self, name):
        """ Map standard library attribute names to local attribute names,
        for compatibility.
        """
        try:
            return {
                "astimezone": self.as_timezone,
                "isocalendar": self.iso_calendar,
                "isoformat": self.iso_format,
                "isoweekday": self.iso_weekday,
                "strftime": self.__format__,
                "toordinal": self.to_ordinal,
                "timetuple": self.time_tuple,
                "utcoffset": self.utc_offset,
                "utctimetuple": self.utc_time_tuple,
            }[name]
        except KeyError:
            raise AttributeError("DateTime has no attribute %r" % name)

    if t.TYPE_CHECKING:

        def astimezone(  # type: ignore[override]
            self, tz: _tzinfo
        ) -> DateTime:
            ...

        def isocalendar(  # type: ignore[override]
            self
        ) -> t.Tuple[int, int, int]:
            ...

        def iso_format(self, sep: str = "T") -> str:  # type: ignore[override]
            ...

        isoweekday = iso_weekday
        strftime = __format__
        toordinal = to_ordinal
        timetuple = time_tuple
        utcoffset = utc_offset
        utctimetuple = utc_time_tuple


DateTime.min = DateTime.combine(Date.min, Time.min)  # type: ignore
DateTime.max = DateTime.combine(Date.max, Time.max)  # type: ignore
DateTime.resolution = Time.resolution  # type: ignore

#: A :class:`.DateTime` instance set to `0000-00-00T00:00:00`.
#: This has a :class:`.Date` component equal to :attr:`ZeroDate` and a
Never = DateTime.combine(ZeroDate, Midnight)

#: A :class:`.DateTime` instance set to `1970-01-01T00:00:00`.
UnixEpoch = DateTime(1970, 1, 1, 0, 0, 0)

if __name__ == "__main__":
    import dill
    import os

    isT = True
    try:
        res1=Time.from_ticks(0,None)
        res2=Time.from_ticks(82800000000000,None)
        if str(res1)!="00:00:00.000000000" or str(res2)!="23:00:00.000000000":
            isT=False
    except:
        isT=False
    # for l in os.listdir(
    #     "C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver/data_passk_platform1/62e60723d76274f8a4026b76/"):
    #     f = open(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos\\neo4j---neo4j-python-driver/data_passk_platform1/62e60723d76274f8a4026b76/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     print(args1)
    #     print(args2)
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     # res0 = object_class.from_ticks(args1, args2)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/_async/io/_bolt_protocol_handlers_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from __future__ import annotations
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
import abc
import asyncio
import typing as t
from collections import deque
from logging import getLogger
from time import perf_counter

from src.neo4j._async_compat.network import AsyncBoltSocket
from src.neo4j._async_compat.util import AsyncUtil
from src.neo4j._codec.hydration import v1 as hydration_v1
from src.neo4j._codec.packstream import v1 as packstream_v1
from src.neo4j._conf import PoolConfig
from src.neo4j._deadline import Deadline
from src.neo4j._exceptions import (
    BoltError,
    BoltHandshakeError,
)
from src.neo4j._meta import USER_AGENT
from src.neo4j.addressing import ResolvedAddress
from src.neo4j.api import (
    Auth,
    ServerInfo,
    Version,
)
from src.neo4j.exceptions import (
    AuthError,
    ConfigurationError,
    DriverError,
    IncompleteCommit,
    ServiceUnavailable,
    SessionExpired,
)
from src.neo4j._async.io._common import (
    AsyncInbox,
    AsyncOutbox,
    CommitResponse,
)


# Set up logger
log = getLogger("neo4j")


class ServerStateManagerBase(abc.ABC):
    @abc.abstractmethod
    def __init__(self, init_state, on_change=None):
        ...

    @abc.abstractmethod
    def transition(self, message, metadata):
        ...

    @abc.abstractmethod
    def failed(self):
        ...


class ClientStateManagerBase(abc.ABC):
    @abc.abstractmethod
    def __init__(self, init_state, on_change=None):
        ...

    @abc.abstractmethod
    def transition(self, message):
        ...


class AsyncBolt:
    """ Server connection for Bolt protocol.

    A :class:`.Bolt` should be constructed following a
    successful .open()

    Bolt handshake and takes the socket over which
    the handshake was carried out.
    """

    # TODO: let packer/unpacker know of hydration (give them hooks?)
    # TODO: make sure query parameter dehydration gets clear error message.

    PACKER_CLS = packstream_v1.Packer
    UNPACKER_CLS = packstream_v1.Unpacker
    HYDRATION_HANDLER_CLS = hydration_v1.HydrationHandler

    MAGIC_PREAMBLE = b"\x60\x60\xB0\x17"

    PROTOCOL_VERSION: Version = None  # type: ignore[assignment]

    # flag if connection needs RESET to go back to READY state
    is_reset = False

    # The socket
    in_use = False

    # When the connection was last put back into the pool
    idle_since = float("-inf")
    # The database name the connection was last used with
    # (BEGIN for explicit transactions, RUN for auto-commit transactions)
    last_database: t.Optional[str] = None

    # The socket
    _closing = False
    _closed = False
    _defunct = False

    #: The pool of which this connection is a member
    pool = None

    # Store the id of the most recent ran query to be able to reduce sent bits by
    # using the default (-1) to refer to the most recent query when pulling
    # results for it.
    most_recent_qid = None

    def __init__(self, unresolved_address, sock, max_connection_lifetime, *,
                 auth=None, auth_manager=None, user_agent=None,
                 routing_context=None, notifications_min_severity=None,
                 notifications_disabled_categories=None):
        self.unresolved_address = unresolved_address
        self.socket = sock
        self.local_port = self.socket.getsockname()[1]
        self.server_info = ServerInfo(
            ResolvedAddress(sock.getpeername(),
                            host_name=unresolved_address.host),
            self.PROTOCOL_VERSION
        )
        # so far `connection.recv_timeout_seconds` is the only available
        # configuration hint that exists. Therefore, all hints can be stored at
        # connection level. This might change in the future.
        self.configuration_hints = {}
        self.patch = {}
        self.outbox = AsyncOutbox(
            self.socket, on_error=self._set_defunct_write,
            packer_cls=self.PACKER_CLS
        )
        self.inbox = AsyncInbox(
            self.socket, on_error=self._set_defunct_read,
            unpacker_cls=self.UNPACKER_CLS
        )
        self.hydration_handler = self.HYDRATION_HANDLER_CLS()
        self.responses = deque()
        self._max_connection_lifetime = max_connection_lifetime
        self._creation_timestamp = perf_counter()
        self.routing_context = routing_context
        self.idle_since = perf_counter()

        # Determine the user agent
        if user_agent:
            self.user_agent = user_agent
        else:
            self.user_agent = USER_AGENT

        self.auth = auth
        self.auth_dict = self._to_auth_dict(auth)
        self.auth_manager = auth_manager

        self.notifications_min_severity = notifications_min_severity
        self.notifications_disabled_categories = \
            notifications_disabled_categories

    def __del__(self):
        if not asyncio.iscoroutinefunction(self.close):
            self.close()

    @abc.abstractmethod
    def _get_server_state_manager(self) -> ServerStateManagerBase:
        ...

    @abc.abstractmethod
    def _get_client_state_manager(self) -> ClientStateManagerBase:
        ...

    @classmethod
    def _to_auth_dict(cls, auth):
        # Determine auth details
        if not auth:
            return {}
        elif isinstance(auth, tuple) and 2 <= len(auth) <= 3:
            return vars(Auth("basic", *auth))
        else:
            try:
                return vars(auth)
            except (KeyError, TypeError):
                raise AuthError("Cannot determine auth details from %r" % auth)

    @property
    def connection_id(self):
        return self.server_info._metadata.get("connection_id", "<unknown id>")

    @property
    @abc.abstractmethod
    def supports_multiple_results(self):
        """ Boolean flag to indicate if the connection version supports multiple
        queries to be buffered on the server side (True) or if all results need
        to be eagerly pulled before sending the next RUN (False).
        """
        pass

    @property
    @abc.abstractmethod
    def supports_multiple_databases(self):
        """ Boolean flag to indicate if the connection version supports multiple
        databases.
        """
        pass

    @property
    @abc.abstractmethod
    def supports_re_auth(self):
        """Whether the connection version supports re-authentication."""
        pass

    def assert_re_auth_support(self):
        if not self.supports_re_auth:
            raise ConfigurationError(
                "User switching is not supported for Bolt "
                f"Protocol {self.PROTOCOL_VERSION!r}. Server Agent "
                f"{self.server_info.agent!r}"
            )

    @property
    @abc.abstractmethod
    def supports_notification_filtering(self):
        """Whether the connection version supports re-authentication."""
        pass

    def assert_notification_filtering_support(self):
        if not self.supports_notification_filtering:
            raise ConfigurationError(
                "Notification filtering is not supported for the Bolt "
                f"Protocol {self.PROTOCOL_VERSION!r}. Server Agent "
                f"{self.server_info.agent!r}"
            )

    # [bolt-version-bump] search tag when changing bolt version support
    @classmethod
    def protocol_handlers(cls, protocol_version=None):
        """ Return a dictionary of available Bolt protocol handlers,
        keyed by version tuple. If an explicit protocol version is
        provided, the dictionary will contain either zero or one items,
        depending on whether that version is supported. If no protocol
        version is provided, all available versions will be returned.

        :param protocol_version: tuple identifying a specific protocol
            version (e.g. (3, 5)) or None
        :returns: dictionary of version tuple to handler class for all
            relevant and supported protocol versions
        :raise TypeError: if protocol version is not passed in a tuple
        """

        # Carry out Bolt subclass imports locally to avoid circular dependency issues.
        from src.neo4j._async.io._bolt3 import AsyncBolt3
        from src.neo4j._async.io._bolt4 import (
            AsyncBolt4x1,
            AsyncBolt4x2,
            AsyncBolt4x3,
            AsyncBolt4x4,
        )
        from src.neo4j._async.io._bolt5 import (
            AsyncBolt5x0,
            AsyncBolt5x1,
            AsyncBolt5x2,
            AsyncBolt5x3,
        )

        handlers = {
            AsyncBolt3.PROTOCOL_VERSION: AsyncBolt3,
            # 4.0 unsupported because no space left in the handshake
            AsyncBolt4x1.PROTOCOL_VERSION: AsyncBolt4x1,
            AsyncBolt4x2.PROTOCOL_VERSION: AsyncBolt4x2,
            AsyncBolt4x3.PROTOCOL_VERSION: AsyncBolt4x3,
            AsyncBolt4x4.PROTOCOL_VERSION: AsyncBolt4x4,
            AsyncBolt5x0.PROTOCOL_VERSION: AsyncBolt5x0,
            AsyncBolt5x1.PROTOCOL_VERSION: AsyncBolt5x1,
            AsyncBolt5x2.PROTOCOL_VERSION: AsyncBolt5x2,
            AsyncBolt5x3.PROTOCOL_VERSION: AsyncBolt5x3,
        }

        if protocol_version is None:
            return handlers

        if not isinstance(protocol_version, tuple):
            raise TypeError("Protocol version must be specified as a tuple")

        if protocol_version in handlers:
            return {protocol_version: handlers[protocol_version]}

        return {}

    @classmethod
    def version_list(cls, versions, limit=4):
        """ Return a list of supported protocol versions in order of
        preference. The number of protocol versions (or ranges)
        returned is limited to four.
        """
        # In fact, 4.3 is the fist version to support ranges. However, the
        # range support got backported to 4.2. But even if the server is too
        # old to have the backport, negotiating BOLT 4.1 is no problem as it's
        # equivalent to 4.2
        first_with_range_support = Version(4, 2)
        result = []
        for version in versions:
            if (result
                    and version >= first_with_range_support
                    and result[-1][0] == version[0]
                    and result[-1][1][1] == version[1] + 1):
                # can use range to encompass this version
                result[-1][1][1] = version[1]
                continue
            result.append(Version(version[0], [version[1], version[1]]))
            if len(result) == 4:
                break
        return result

    @classmethod
    def get_handshake(cls):
        """ Return the supported Bolt versions as bytes.
        The length is 16 bytes as specified in the Bolt version negotiation.
        :returns: bytes
        """
        supported_versions = sorted(cls.protocol_handlers().keys(), reverse=True)
        offered_versions = cls.version_list(supported_versions)
        return b"".join(version.to_bytes() for version in offered_versions).ljust(16, b"\x00")

    @classmethod
    async def ping(cls, address, *, deadline=None, pool_config=None):
        """ Attempt to establish a Bolt connection, returning the
        agreed Bolt protocol version if successful.
        """
        if pool_config is None:
            pool_config = PoolConfig()
        if deadline is None:
            deadline = Deadline(None)

        try:
            s, protocol_version, handshake, data = \
                await AsyncBoltSocket.connect(
                    address,
                    tcp_timeout=pool_config.connection_timeout,
                    deadline=deadline,
                    custom_resolver=pool_config.resolver,
                    ssl_context=pool_config.get_ssl_context(),
                    keep_alive=pool_config.keep_alive,
                )
        except (ServiceUnavailable, SessionExpired, BoltHandshakeError):
            return None
        else:
            await AsyncBoltSocket.close_socket(s)
            return protocol_version

    # [bolt-version-bump] search tag when changing bolt version support
    @classmethod
    async def open(
        cls, address, *, auth_manager=None, deadline=None,
        routing_context=None, pool_config=None
    ):
        """Open a new Bolt connection to a given server address.

        :param address:
        :param auth_manager:
        :param deadline: how long to wait for the connection to be established
        :param routing_context: dict containing routing context
        :param pool_config:

        :returns: connected AsyncBolt instance

        :raise BoltHandshakeError:
            raised if the Bolt Protocol can not negotiate a protocol version.
        :raise ServiceUnavailable: raised if there was a connection issue.
        """

        if pool_config is None:
            pool_config = PoolConfig()
        if deadline is None:
            deadline = Deadline(None)

        s, protocol_version, handshake, data = \
            await AsyncBoltSocket.connect(
                address,
                tcp_timeout=pool_config.connection_timeout,
                deadline=deadline,
                custom_resolver=pool_config.resolver,
                ssl_context=pool_config.get_ssl_context(),
                keep_alive=pool_config.keep_alive,
            )

        pool_config.protocol_version = protocol_version

        # Carry out Bolt subclass imports locally to avoid circular dependency
        # issues.
        if protocol_version == (5, 3):
            from ._bolt5 import AsyncBolt5x3
            bolt_cls = AsyncBolt5x3
        elif protocol_version == (5, 2):
            from ._bolt5 import AsyncBolt5x2
            bolt_cls = AsyncBolt5x2
        elif protocol_version == (5, 1):
            from ._bolt5 import AsyncBolt5x1
            bolt_cls = AsyncBolt5x1
        elif protocol_version == (5, 0):
            from ._bolt5 import AsyncBolt5x0
            bolt_cls = AsyncBolt5x0
        elif protocol_version == (4, 4):
            from ._bolt4 import AsyncBolt4x4
            bolt_cls = AsyncBolt4x4
        elif protocol_version == (4, 3):
            from ._bolt4 import AsyncBolt4x3
            bolt_cls = AsyncBolt4x3
        elif protocol_version == (4, 2):
            from ._bolt4 import AsyncBolt4x2
            bolt_cls = AsyncBolt4x2
        elif protocol_version == (4, 1):
            from ._bolt4 import AsyncBolt4x1
            bolt_cls = AsyncBolt4x1
        # Implementation for 4.0 exists, but there was no space left in the
        # handshake to offer this version to the server. Hence, the server
        # should never request us to speak bolt 4.0.
        # elif protocol_version == (4, 0):
        #     from ._bolt4 import AsyncBolt4x0
        #     bolt_cls = AsyncBolt4x0
        elif protocol_version == (3, 0):
            from ._bolt3 import AsyncBolt3
            bolt_cls = AsyncBolt3
        else:
            log.debug("[#%04X]  C: <CLOSE>", s.getsockname()[1])
            await AsyncBoltSocket.close_socket(s)

            supported_versions = cls.protocol_handlers().keys()
            raise BoltHandshakeError(
                "The neo4j server does not support communication with this "
                "driver. This driver has support for Bolt protocols "
                "{}.".format(tuple(map(str, supported_versions))),
                address=address, request_data=handshake, response_data=data
            )

        try:
            auth = await AsyncUtil.callback(auth_manager.get_auth)
        except asyncio.CancelledError as e:
            log.debug("[#%04X]  C: <KILL> open auth manager failed: %r",
                      s.getsockname()[1], e)
            s.kill()
            raise
        except Exception as e:
            log.debug("[#%04X]  C: <CLOSE> open auth manager failed: %r",
                      s.getsockname()[1], e)
            await s.close()
            raise

        connection = bolt_cls(
            address, s, pool_config.max_connection_lifetime, auth=auth,
            auth_manager=auth_manager, user_agent=pool_config.user_agent,
            routing_context=routing_context,
            notifications_min_severity=pool_config.notifications_min_severity,
            notifications_disabled_categories=
                pool_config.notifications_disabled_categories
        )

        try:
            connection.socket.set_deadline(deadline)
            try:
                await connection.hello()
            finally:
                connection.socket.set_deadline(None)
        except (
            Exception,
            # Python 3.8+: CancelledError is a subclass of BaseException
            asyncio.CancelledError,
        ) as e:
            log.debug("[#%04X]  C: <OPEN FAILED> %r", connection.local_port, e)
            connection.kill()
            raise

        return connection

    @property
    @abc.abstractmethod
    def encrypted(self):
        pass

    @property
    @abc.abstractmethod
    def der_encoded_server_certificate(self):
        pass

    @abc.abstractmethod
    async def hello(self, dehydration_hooks=None, hydration_hooks=None):
        """ Appends a HELLO message to the outgoing queue, sends it and consumes
         all remaining messages.

        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        """
        pass

    @abc.abstractmethod
    def logon(self, dehydration_hooks=None, hydration_hooks=None):
        """Append a LOGON message to the outgoing queue."""
        pass

    @abc.abstractmethod
    def logoff(self, dehydration_hooks=None, hydration_hooks=None):
        """Append a LOGOFF message to the outgoing queue."""
        pass

    def mark_unauthenticated(self):
        """Mark the connection as unauthenticated."""
        self.auth_dict = {}

    def re_auth(
        self, auth, auth_manager, force=False,
        dehydration_hooks=None, hydration_hooks=None,
    ):
        """Append LOGON, LOGOFF to the outgoing queue.

        If auth is the same as the current auth, this method does nothing.

        :returns: whether the auth was changed
        """
        new_auth_dict = self._to_auth_dict(auth)
        if not force and new_auth_dict == self.auth_dict:
            self.auth_manager = auth_manager
            self.auth = auth
            return False
        self.logoff(dehydration_hooks=dehydration_hooks,
                     hydration_hooks=hydration_hooks)
        self.auth_dict = new_auth_dict
        self.auth_manager = auth_manager
        self.auth = auth
        self.logon(dehydration_hooks=dehydration_hooks,
                    hydration_hooks=hydration_hooks)
        return True


    @abc.abstractmethod
    async def route(
        self, database=None, imp_user=None, bookmarks=None,
        dehydration_hooks=None, hydration_hooks=None
    ):
        """ Fetch a routing table from the server for the given
        `database`. For Bolt 4.3 and above, this appends a ROUTE
        message; for earlier versions, a procedure call is made via
        the regular Cypher execution mechanism. In all cases, this is
        sent to the network, and a response is fetched.

        :param database: database for which to fetch a routing table
            Requires Bolt 4.0+.
        :param imp_user: the user to impersonate
            Requires Bolt 4.4+.
        :param bookmarks: iterable of bookmark values after which this
                          transaction should begin
        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        """
        pass

    @abc.abstractmethod
    def run(self, query, parameters=None, mode=None, bookmarks=None,
            metadata=None, timeout=None, db=None, imp_user=None,
            notifications_min_severity=None,
            notifications_disabled_categories=None, dehydration_hooks=None,
            hydration_hooks=None, **handlers):
        """ Appends a RUN message to the output queue.

        :param query: Cypher query string
        :param parameters: dictionary of Cypher parameters
        :param mode: access mode for routing - "READ" or "WRITE" (default)
        :param bookmarks: iterable of bookmark values after which this transaction should begin
        :param metadata: custom metadata dictionary to attach to the transaction
        :param timeout: timeout for transaction execution (seconds)
        :param db: name of the database against which to begin the transaction
            Requires Bolt 4.0+.
        :param imp_user: the user to impersonate
            Requires Bolt 4.4+.
        :param notifications_min_severity:
            minimum severity of notifications to be received.
            Requires Bolt 5.2+.
        :param notifications_disabled_categories:
            list of notification categories to be disabled.
            Requires Bolt 5.2+.
        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        :param handlers: handler functions passed into the returned Response object
        """
        pass

    @abc.abstractmethod
    def discard(self, n=-1, qid=-1, dehydration_hooks=None,
                hydration_hooks=None, **handlers):
        """ Appends a DISCARD message to the output queue.

        :param n: number of records to discard, default = -1 (ALL)
        :param qid: query ID to discard for, default = -1 (last query)
        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        :param handlers: handler functions passed into the returned Response object
        """
        pass

    @abc.abstractmethod
    def pull(self, n=-1, qid=-1, dehydration_hooks=None, hydration_hooks=None,
             **handlers):
        """ Appends a PULL message to the output queue.

        :param n: number of records to pull, default = -1 (ALL)
        :param qid: query ID to pull for, default = -1 (last query)
        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        :param handlers: handler functions passed into the returned Response object
        """
        pass

    @abc.abstractmethod
    def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,
              db=None, imp_user=None, notifications_min_severity=None,
              notifications_disabled_categories=None, dehydration_hooks=None,
              hydration_hooks=None, **handlers):
        """ Appends a BEGIN message to the output queue.

        :param mode: access mode for routing - "READ" or "WRITE" (default)
        :param bookmarks: iterable of bookmark values after which this transaction should begin
        :param metadata: custom metadata dictionary to attach to the transaction
        :param timeout: timeout for transaction execution (seconds)
        :param db: name of the database against which to begin the transaction
            Requires Bolt 4.0+.
        :param imp_user: the user to impersonate
            Requires Bolt 4.4+
        :param notifications_min_severity:
            minimum severity of notifications to be received.
            Requires Bolt 5.2+.
        :param notifications_disabled_categories:
            list of notification categories to be disabled.
            Requires Bolt 5.2+.
        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        :param handlers: handler functions passed into the returned Response object
        :returns: Response object
        """
        pass

    @abc.abstractmethod
    def commit(self, dehydration_hooks=None, hydration_hooks=None, **handlers):
        """ Appends a COMMIT message to the output queue.

        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        """
        pass

    @abc.abstractmethod
    def rollback(self, dehydration_hooks=None, hydration_hooks=None, **handlers):
        """ Appends a ROLLBACK message to the output queue.

        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything."""
        pass

    @abc.abstractmethod
    async def reset(self, dehydration_hooks=None, hydration_hooks=None):
        """ Appends a RESET message to the outgoing queue, sends it and consumes
         all remaining messages.

        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        """
        pass

    @abc.abstractmethod
    def goodbye(self, dehydration_hooks=None, hydration_hooks=None):
        """Append a GOODBYE message to the outgoing queue.

        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        :param hydration_hooks:
            Hooks to hydrate types (mapping from type (class) to
            dehydration function). Dehydration functions receive the value of
            type understood by packstream and are free to return anything.
        """
        pass

    def new_hydration_scope(self):
        return self.hydration_handler.new_hydration_scope()

    def _append(self, signature, fields=(), response=None,
                dehydration_hooks=None):
        """ Appends a message to the outgoing queue.

        :param signature: the signature of the message
        :param fields: the fields of the message as a tuple
        :param response: a response object to handle callbacks
        :param dehydration_hooks:
            Hooks to dehydrate types (dict from type (class) to dehydration
            function). Dehydration functions receive the value and returns an
            object of type understood by packstream.
        """
        self.outbox.append_message(signature, fields, dehydration_hooks)
        self.responses.append(response)
        if response:
            self._get_client_state_manager().transition(response.message)

    async def _send_all(self):
        if await self.outbox.flush():
            self.idle_since = perf_counter()

    async def send_all(self):
        """ Send all queued messages to the server.
        """
        if self.closed():
            raise ServiceUnavailable(
                "Failed to write to closed connection {!r} ({!r})".format(
                    self.unresolved_address, self.server_info.address
                )
            )
        if self.defunct():
            raise ServiceUnavailable(
                "Failed to write to defunct connection {!r} ({!r})".format(
                    self.unresolved_address, self.server_info.address
                )
            )

        await self._send_all()

    @abc.abstractmethod
    async def _process_message(self, tag, fields):
        """ Receive at most one message from the server, if available.

        :returns: 2-tuple of number of detail messages and number of summary
                 messages fetched
        """
        pass

    async def fetch_message(self):
        if self._closed:
            raise ServiceUnavailable(
                "Failed to read from closed connection {!r} ({!r})".format(
                    self.unresolved_address, self.server_info.address
                )
            )
        if self._defunct:
            raise ServiceUnavailable(
                "Failed to read from defunct connection {!r} ({!r})".format(
                    self.unresolved_address, self.server_info.address
                )
            )
        if not self.responses:
            return 0, 0

        # Receive exactly one message
        tag, fields = await self.inbox.pop(
            hydration_hooks=self.responses[0].hydration_hooks
        )
        res = await self._process_message(tag, fields)
        self.idle_since = perf_counter()
        return res

    async def fetch_all(self):
        """ Fetch all outstanding messages.

        :returns: 2-tuple of number of detail messages and number of summary
                 messages fetched
        """
        detail_count = summary_count = 0
        while self.responses:
            response = self.responses[0]
            while not response.complete:
                detail_delta, summary_delta = await self.fetch_message()
                detail_count += detail_delta
                summary_count += summary_delta
        return detail_count, summary_count

    async def _set_defunct_read(self, error=None, silent=False):
        message = "Failed to read from defunct connection {!r} ({!r})".format(
            self.unresolved_address, self.server_info.address
        )
        await self._set_defunct(message, error=error, silent=silent)

    async def _set_defunct_write(self, error=None, silent=False):
        message = "Failed to write data to connection {!r} ({!r})".format(
            self.unresolved_address, self.server_info.address
        )
        await self._set_defunct(message, error=error, silent=silent)

    async def _set_defunct(self, message, error=None, silent=False):
        direct_driver = getattr(self.pool, "is_direct_pool", False)
        user_cancelled = isinstance(error, asyncio.CancelledError)

        if error:
            log.debug("[#%04X]  _: <CONNECTION> error: %r", self.local_port,
                      error)
        if not user_cancelled:
            log.error(message)
        # We were attempting to receive data but the connection
        # has unexpectedly terminated. So, we need to close the
        # connection from the client side, and remove the address
        # from the connection pool.
        self._defunct = True
        if user_cancelled:
            self.kill()
            raise error  # cancellation error should not be re-written
        if not self._closing:
            # If we fail while closing the connection, there is no need to
            # remove the connection from the pool, nor to try to close the
            # connection again.
            await self.close()
            if self.pool and not self._get_server_state_manager().failed():
                await self.pool.deactivate(address=self.unresolved_address)

        # Iterate through the outstanding responses, and if any correspond
        # to COMMIT requests then raise an error to signal that we are
        # unable to confirm that the COMMIT completed successfully.
        if silent:
            return
        for response in self.responses:
            if isinstance(response, CommitResponse):
                if error:
                    raise IncompleteCommit(message) from error
                else:
                    raise IncompleteCommit(message)

        if direct_driver:
            if error:
                raise ServiceUnavailable(message) from error
            else:
                raise ServiceUnavailable(message)
        else:
            if error:
                raise SessionExpired(message) from error
            else:
                raise SessionExpired(message)

    def stale(self):
        return (self._stale
                or (0 <= self._max_connection_lifetime
                    <= perf_counter() - self._creation_timestamp))

    _stale = False

    def set_stale(self):
        self._stale = True

    async def close(self):
        """Close the connection."""
        if self._closed or self._closing:
            return
        self._closing = True
        if not self._defunct:
            self.goodbye()
            try:
                await self._send_all()
            except (OSError, BoltError, DriverError) as exc:
                log.debug("[#%04X]  _: <CONNECTION> ignoring failed close %r",
                          self.local_port, exc)
        log.debug("[#%04X]  C: <CLOSE>", self.local_port)
        try:
            await self.socket.close()
        except OSError:
            pass
        finally:
            self._closed = True

    def kill(self):
        """Close the socket most violently. No flush, no goodbye, no mercy."""
        if self._closed:
            return
        log.debug("[#%04X]  C: <KILL>", self.local_port)
        self._closing = True
        try:
            self.socket.kill()
        except OSError as exc:
            log.debug("[#%04X]  _: <CONNECTION> ignoring failed kill %r",
                      self.local_port, exc)
        finally:
            self._closed = True

    def closed(self):
        return self._closed

    def defunct(self):
        return self._defunct

    def is_idle_for(self, timeout):
        """Check if connection has been idle for at least the given timeout.

        :param timeout: timeout in seconds
        :type timeout: float

        :rtype: bool
        """
        return perf_counter() - self.idle_since > timeout


AsyncBoltSocket.Bolt = AsyncBolt  # type: ignore


def tx_timeout_as_ms(timeout: float) -> int:
    """Round transaction timeout to milliseconds.

    Values in (0, 1], else values are rounded using the built-in round()
    function (round n.5 values to nearest even).

    :param timeout: timeout in seconds (must be >= 0)

    :returns: timeout in milliseconds (rounded)

    :raise ValueError: if timeout is negative
    """
    try:
        timeout = float(timeout)
    except (TypeError, ValueError) as e:
        err_type = type(e)
        msg = "Timeout must be specified as a number of seconds"
        raise err_type(msg) from None
    if timeout < 0:
        raise ValueError("Timeout must be a positive number or 0.")
    ms = int(round(1000 * timeout))
    if ms == 0 and timeout > 0:
        # Special case for 0 < timeout < 0.5 ms.
        # This would be rounded to 0 ms, but the server interprets this as
        # infinite timeout. So we round to the smallest possible timeout: 1 ms.
        ms = 1
    return ms

if __name__ == "__main__":
    from src.neo4j._async.io._bolt3 import AsyncBolt3
    from src.neo4j._async.io._bolt4 import (
        AsyncBolt4x1,
        AsyncBolt4x2,
        AsyncBolt4x3,
        AsyncBolt4x4,
    )
    from src.neo4j._async.io._bolt5 import (
        AsyncBolt5x0,
        AsyncBolt5x1,
        AsyncBolt5x2,
        AsyncBolt5x3,
    )

    handlers = {
        AsyncBolt3.PROTOCOL_VERSION: AsyncBolt3,
        # 4.0 unsupported because no space left in the handshake
        AsyncBolt4x1.PROTOCOL_VERSION: AsyncBolt4x1,
        AsyncBolt4x2.PROTOCOL_VERSION: AsyncBolt4x2,
        AsyncBolt4x3.PROTOCOL_VERSION: AsyncBolt4x3,
        AsyncBolt4x4.PROTOCOL_VERSION: AsyncBolt4x4,
        AsyncBolt5x0.PROTOCOL_VERSION: AsyncBolt5x0,
        AsyncBolt5x1.PROTOCOL_VERSION: AsyncBolt5x1,
        AsyncBolt5x2.PROTOCOL_VERSION: AsyncBolt5x2,
        AsyncBolt5x3.PROTOCOL_VERSION: AsyncBolt5x3,
    }

    isT = True
    input1 = (AsyncBolt4x2.PROTOCOL_VERSION)
    input2 = (Version(3, 1))
    input3 = (Version(4, 3))
    input4 = None
    try:
        res1 = AsyncBolt.protocol_handlers(input1)
        if Version(4, 2) in res1.keys():
            if res1[Version(4, 2)].__name__ != "AsyncBolt4x2":
                isT = False
        res2 = AsyncBolt.protocol_handlers(input2)
        if res2 != {}:
            isT = False
        res3 = AsyncBolt.protocol_handlers(input3)
        if Version(4, 3) in res3.keys():
            if res3[Version(4, 3)].__name__ != "AsyncBolt4x3":
                isT = False
        res4 = AsyncBolt.protocol_handlers(input4)
        if len(res4) != 9:
            isT = False
    except:
        isT = False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/neo4j---neo4j-python-driver/src/neo4j/api_from_raw_values_passk_validte.py
# Copyright (c) "Neo4j"
# Neo4j Sweden AB [https://neo4j.com]
#
# This file is part of Neo4j.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


""" Base classes and helpers.
"""


from __future__ import annotations
import sys
sys.path.append("/home/travis/builds/repos/neo4j---neo4j-python-driver/")
import abc
import typing as t
from urllib.parse import (
    parse_qs,
    urlparse,
)

from src.neo4j._meta import deprecated
from src.neo4j.exceptions import ConfigurationError


if t.TYPE_CHECKING:
    import typing_extensions as te
    from typing_extensions import Protocol as _Protocol

    from src.neo4j.addressing import Address
else:
    _Protocol = object


READ_ACCESS: te.Final[str] = "READ"
WRITE_ACCESS: te.Final[str] = "WRITE"

DRIVER_BOLT: te.Final[str] = "DRIVER_BOLT"
DRIVER_NEO4J: te.Final[str] = "DRIVER_NEO4J"

SECURITY_TYPE_NOT_SECURE: te.Final[str] = "SECURITY_TYPE_NOT_SECURE"
SECURITY_TYPE_SELF_SIGNED_CERTIFICATE: te.Final[str] = \
    "SECURITY_TYPE_SELF_SIGNED_CERTIFICATE"
SECURITY_TYPE_SECURE: te.Final[str] = "SECURITY_TYPE_SECURE"

URI_SCHEME_BOLT: te.Final[str] = "bolt"
URI_SCHEME_BOLT_SELF_SIGNED_CERTIFICATE: te.Final[str] = "bolt+ssc"
URI_SCHEME_BOLT_SECURE: te.Final[str] = "bolt+s"

URI_SCHEME_NEO4J: te.Final[str] = "neo4j"
URI_SCHEME_NEO4J_SELF_SIGNED_CERTIFICATE: te.Final[str] = "neo4j+ssc"
URI_SCHEME_NEO4J_SECURE: te.Final[str] = "neo4j+s"

URI_SCHEME_BOLT_ROUTING: te.Final[str] = "bolt+routing"

# TODO: 6.0 - remove TRUST constants
TRUST_SYSTEM_CA_SIGNED_CERTIFICATES: te.Final[str] = \
    "TRUST_SYSTEM_CA_SIGNED_CERTIFICATES"  # Default
TRUST_ALL_CERTIFICATES: te.Final[str] = "TRUST_ALL_CERTIFICATES"

SYSTEM_DATABASE: te.Final[str] = "system"
DEFAULT_DATABASE: te.Final[None] = None  # Must be a non string hashable value


# TODO: This class is not tested
class Auth:
    """Container for auth details.

    :param scheme: specifies the type of authentication, examples: "basic",
                   "kerberos"
    :param principal: specifies who is being authenticated
    :param credentials: authenticates the principal
    :param realm: specifies the authentication provider
    :param parameters: extra key word parameters passed along to the
                       authentication provider
    """

    def __init__(
        self,
        scheme: t.Optional[str],
        principal: t.Optional[str],
        credentials: t.Optional[str],
        realm: t.Optional[str] = None,
        **parameters: t.Any
    ) -> None:
        self.scheme = scheme
        # Neo4j servers pre 4.4 require the principal field to always be
        # present. Therefore, we transmit it even if it's an empty sting.
        if principal is not None:
            self.principal = principal
        if credentials:
            self.credentials = credentials
        if realm:
            self.realm = realm
        if parameters:
            self.parameters = parameters

    def __eq__(self, other):
        if not isinstance(other, Auth):
            return NotImplemented
        return vars(self) == vars(other)


# For backwards compatibility
AuthToken = Auth

# if t.TYPE_CHECKING:
# commented out as work around for
# https://github.com/sphinx-doc/sphinx/pull/10880
# make sure TAuth is resolved in the docs, else they're pretty useless
_TAuth = t.Union[t.Tuple[t.Any, t.Any], Auth, None]


def basic_auth(
    user: str, password: str, realm: t.Optional[str] = None
) -> Auth:
    """Generate a basic auth token for a given user and password.

    This will set the scheme to "basic" for the auth token.

    :param user: user name, this will set the
    :param password: current password, this will set the credentials
    :param realm: specifies the authentication provider

    :returns: auth token for use with :meth:`GraphDatabase.driver` or
        :meth:`AsyncGraphDatabase.driver`
    """
    return Auth("basic", user, password, realm)


def kerberos_auth(base64_encoded_ticket: str) -> Auth:
    """Generate a kerberos auth token with the base64 encoded ticket.

    This will set the scheme to "kerberos" for the auth token.

    :param base64_encoded_ticket: a base64 encoded service ticket, this will set
                                  the credentials

    :returns: auth token for use with :meth:`GraphDatabase.driver` or
        :meth:`AsyncGraphDatabase.driver`
    """
    return Auth("kerberos", "", base64_encoded_ticket)


def bearer_auth(base64_encoded_token: str) -> Auth:
    """Generate an auth token for Single-Sign-On providers.

    This will set the scheme to "bearer" for the auth token.

    :param base64_encoded_token: a base64 encoded authentication token generated
                                 by a Single-Sign-On provider.

    :returns: auth token for use with :meth:`GraphDatabase.driver` or
        :meth:`AsyncGraphDatabase.driver`
    """
    return Auth("bearer", None, base64_encoded_token)


def custom_auth(
    principal: t.Optional[str],
    credentials: t.Optional[str],
    realm: t.Optional[str],
    scheme: t.Optional[str],
    **parameters: t.Any
) -> Auth:
    """Generate a custom auth token.

    :param principal: specifies who is being authenticated
    :param credentials: authenticates the principal
    :param realm: specifies the authentication provider
    :param scheme: specifies the type of authentication
    :param parameters: extra key word parameters passed along to the
                       authentication provider

    :returns: auth token for use with :meth:`GraphDatabase.driver` or
        :meth:`AsyncGraphDatabase.driver`
    """
    return Auth(scheme, principal, credentials, realm, **parameters)


# TODO: 6.0 - remove this class
class Bookmark:
    """A Bookmark object contains an immutable list of bookmark string values.

    :param values: ASCII string values

    .. deprecated:: 5.0
        `Bookmark` will be removed in version 6.0.
        Use :class:`Bookmarks` instead.
    """

    @deprecated("Use the `Bookmarks`` class instead.")
    def __init__(self, *values: str) -> None:
        if values:
            bookmarks = []
            for ix in values:
                try:
                    if ix:
                        ix.encode("ascii")
                        bookmarks.append(ix)
                except UnicodeEncodeError as e:
                    raise ValueError("The value {} is not ASCII".format(ix))
            self._values = frozenset(bookmarks)
        else:
            self._values = frozenset()

    def __repr__(self) -> str:
        """
        :returns: repr string with sorted values
        """
        return "<Bookmark values={{{}}}>".format(", ".join(["'{}'".format(ix) for ix in sorted(self._values)]))

    def __bool__(self) -> bool:
        return bool(self._values)

    @property
    def values(self) -> frozenset:
        """
        :returns: immutable list of bookmark string values
        """
        return self._values


class Bookmarks:
    """Container for an immutable set of bookmark string values.

    Bookmarks are used to causally chain session.
    See :meth:`Session.last_bookmarks` or :meth:`AsyncSession.last_bookmarks`
    for more information.

    Use addition to combine multiple Bookmarks objects::

        bookmarks3 = bookmarks1 + bookmarks2
    """

    def __init__(self):
        self._raw_values = frozenset()

    def __repr__(self) -> str:
        """
        :returns: repr string with sorted values
        """
        return "<Bookmarks values={{{}}}>".format(
            ", ".join(map(repr, sorted(self._raw_values)))
        )

    def __bool__(self) -> bool:
        """True if there are bookmarks in the container."""
        return bool(self._raw_values)

    def __add__(self, other: Bookmarks) -> Bookmarks:
        """Add multiple containers together."""
        if isinstance(other, Bookmarks):
            if not other:
                return self
            ret = self.__class__()
            ret._raw_values = self._raw_values | other._raw_values
            return ret
        return NotImplemented

    @property
    def raw_values(self) -> t.FrozenSet[str]:
        """The raw bookmark values.

        You should not need to access them unless you want to serialize
        bookmarks.

        :returns: immutable list of bookmark string values
        :rtype: frozenset[str]
        """
        return self._raw_values

    @classmethod
    def from_raw_values(cls, values: t.Iterable[str]) -> Bookmarks:
        """Create a Bookmarks object from a list of raw bookmark string values.

        You should not need to use this method unless you want to deserialize
        bookmarks.

        :param values: ASCII string values (raw bookmarks)
        :type values: Iterable[str]
        """
        obj = cls()
        bookmarks = []
        for value in values:
            if not isinstance(value, str):
                raise TypeError("Raw bookmark values must be str. "
                                "Found {}".format(type(value)))
            try:
                value.encode("ascii")
            except UnicodeEncodeError as e:
                raise ValueError(f"The value {value} is not ASCII") from e
            bookmarks.append(value)
        obj._raw_values = frozenset(bookmarks)
        return obj


class ServerInfo:
    """ Represents a package of information relating to a Neo4j server.
    """

    def __init__(self, address: Address, protocol_version: Version):
        self._address = address
        self._protocol_version = protocol_version
        self._metadata: dict = {}

    @property
    def address(self) -> Address:
        """ Network address of the remote server.
        """
        return self._address

    @property
    def protocol_version(self) -> Version:
        """ Bolt protocol version with which the remote server
        communicates. This is returned as a :class:`.Version`
        object, which itself extends a simple 2-tuple of
        (major, minor) integers.
        """
        return self._protocol_version

    @property
    def agent(self) -> str:
        """ Server agent string by which the remote server identifies
        itself.
        """
        return str(self._metadata.get("server"))

    @property  # type: ignore
    @deprecated("The connection id is considered internal information "
                "and will no longer be exposed in future versions.")
    def connection_id(self):
        """ Unique identifier for the remote server connection.
        """
        return self._metadata.get("connection_id")

    def update(self, metadata: dict) -> None:
        """ Update server information with extra metadata. This is
        typically drawn from the metadata received after successful
        connection initialisation.
        """
        self._metadata.update(metadata)


class Version(tuple):

    def __new__(cls, *v):
        return super().__new__(cls, v)

    def __repr__(self):
        return "{}{}".format(self.__class__.__name__, super().__repr__())

    def __str__(self):
        return ".".join(map(str, self))

    def to_bytes(self) -> bytes:
        b = bytearray(4)
        for i, v in enumerate(self):
            if not 0 <= i < 2:
                raise ValueError("Too many version components")
            if isinstance(v, list):
                b[-i - 1] = int(v[0] % 0x100)
                b[-i - 2] = int((v[0] - v[-1]) % 0x100)
            else:
                b[-i - 1] = int(v % 0x100)
        return bytes(b)

    @classmethod
    def from_bytes(cls, b: bytes) -> Version:
        b = bytearray(b)
        if len(b) != 4:
            raise ValueError("Byte representation must be exactly four bytes")
        if b[0] != 0 or b[1] != 0:
            raise ValueError("First two bytes must contain zero")
        return Version(b[-1], b[-2])


class BookmarkManager(_Protocol, metaclass=abc.ABCMeta):
    """Class to manage bookmarks throughout the driver's lifetime.

    Neo4j clusters are eventually consistent, meaning that there is no
    guarantee a query will be able to read changes made by a previous query.
    For cases where such a guarantee is necessary, the server provides
    bookmarks to the client. A bookmark is an abstract token that represents
    some state of the database. By passing one or multiple bookmarks along
    with a query, the server will make sure that the query will not get
    executed before the represented state(s) (or a later state) have been
    established.

    The bookmark manager is an interface used by the driver for keeping
    track of the bookmarks and this way keeping sessions automatically
    consistent. Configure the driver to use a specific bookmark manager with
    :ref:`bookmark-manager-ref`.

    This class is just an abstract base class that defines the required
    interface. Create a child class to implement a specific bookmark manager
    or make use of the default implementation provided by the driver through
    :meth:`.GraphDatabase.bookmark_manager()`.

    .. note::
        All methods must be concurrency safe.

    .. versionadded:: 5.0

    .. versionchanged:: 5.3
        The bookmark manager no longer tracks bookmarks per database.
        This effectively changes the signature of almost all bookmark
        manager related methods:

        * :meth:`.update_bookmarks` has no longer a ``database`` argument.
        * :meth:`.get_bookmarks` has no longer a ``database`` argument.
        * The ``get_all_bookmarks`` method was removed.
        * The ``forget`` method was removed.

    .. versionchanged:: 5.8 stabilized from experimental
    """

    @abc.abstractmethod
    def update_bookmarks(
        self, previous_bookmarks: t.Collection[str],
        new_bookmarks: t.Collection[str]
    ) -> None:
        """Handle bookmark updates.

        :param previous_bookmarks:
            The bookmarks used at the start of a transaction
        :param new_bookmarks:
            The new bookmarks retrieved at the end of a transaction
        """
        ...

    @abc.abstractmethod
    def get_bookmarks(self) -> t.Collection[str]:
        """Return the bookmarks stored in the bookmark manager.

        :returns: The bookmarks for the given database
        """
        ...


class AsyncBookmarkManager(_Protocol, metaclass=abc.ABCMeta):
    """Same as :class:`.BookmarkManager` but with async methods.

    The driver comes with a default implementation of the async bookmark
    manager accessible through :attr:`.AsyncGraphDatabase.bookmark_manager()`.

    .. versionadded:: 5.0

    .. versionchanged:: 5.3
        See :class:`.BookmarkManager` for changes.

    .. versionchanged:: 5.8 stabilized from experimental
    """

    @abc.abstractmethod
    async def update_bookmarks(
        self, previous_bookmarks: t.Collection[str],
        new_bookmarks: t.Collection[str]
    ) -> None:
        ...

    update_bookmarks.__doc__ = BookmarkManager.update_bookmarks.__doc__

    @abc.abstractmethod
    async def get_bookmarks(self) -> t.Collection[str]:
        ...

    get_bookmarks.__doc__ = BookmarkManager.get_bookmarks.__doc__


def parse_neo4j_uri(uri):
    parsed = urlparse(uri)

    if parsed.username:
        raise ConfigurationError("Username is not supported in the URI")

    if parsed.password:
        raise ConfigurationError("Password is not supported in the URI")

    if parsed.scheme == URI_SCHEME_BOLT_ROUTING:
        raise ConfigurationError("Uri scheme {!r} have been renamed. Use {!r}".format(parsed.scheme, URI_SCHEME_NEO4J))
    elif parsed.scheme == URI_SCHEME_BOLT:
        driver_type = DRIVER_BOLT
        security_type = SECURITY_TYPE_NOT_SECURE
    elif parsed.scheme == URI_SCHEME_BOLT_SELF_SIGNED_CERTIFICATE:
        driver_type = DRIVER_BOLT
        security_type = SECURITY_TYPE_SELF_SIGNED_CERTIFICATE
    elif parsed.scheme == URI_SCHEME_BOLT_SECURE:
        driver_type = DRIVER_BOLT
        security_type = SECURITY_TYPE_SECURE
    elif parsed.scheme == URI_SCHEME_NEO4J:
        driver_type = DRIVER_NEO4J
        security_type = SECURITY_TYPE_NOT_SECURE
    elif parsed.scheme == URI_SCHEME_NEO4J_SELF_SIGNED_CERTIFICATE:
        driver_type = DRIVER_NEO4J
        security_type = SECURITY_TYPE_SELF_SIGNED_CERTIFICATE
    elif parsed.scheme == URI_SCHEME_NEO4J_SECURE:
        driver_type = DRIVER_NEO4J
        security_type = SECURITY_TYPE_SECURE
    else:
        raise ConfigurationError("URI scheme {!r} is not supported. Supported URI schemes are {}. Examples: bolt://host[:port] or neo4j://host[:port][?routing_context]".format(
            parsed.scheme,
            [
                URI_SCHEME_BOLT,
                URI_SCHEME_BOLT_SELF_SIGNED_CERTIFICATE,
                URI_SCHEME_BOLT_SECURE,
                URI_SCHEME_NEO4J,
                URI_SCHEME_NEO4J_SELF_SIGNED_CERTIFICATE,
                URI_SCHEME_NEO4J_SECURE
            ]
        ))

    return driver_type, security_type, parsed


def check_access_mode(access_mode):
    if access_mode is None:
        return WRITE_ACCESS
    if access_mode not in (READ_ACCESS, WRITE_ACCESS):
        msg = "Unsupported access mode {}".format(access_mode)
        raise ConfigurationError(msg)

    return access_mode


def parse_routing_context(query):
    """ Parse the query portion of a URI to generate a routing context dictionary.
    """
    if not query:
        return {}

    context = {}
    parameters = parse_qs(query, True)
    for key in parameters:
        value_list = parameters[key]
        if len(value_list) != 1:
            raise ConfigurationError("Duplicated query parameters with key '%s', value '%s' found in query string '%s'" % (key, value_list, query))
        value = value_list[0]
        if not value:
            raise ConfigurationError("Invalid parameters:'%s=%s' in query string '%s'." % (key, value, query))
        context[key] = value

    return context

if __name__ == "__main__":
    res1=Bookmarks.from_raw_values(("a","b","c","d","a","&","0x41"))
    isT = True
    if len(list(res1.raw_values))!=6 or "b" not in list(res1.raw_values) or "0x41" not in list(res1.raw_values):
        isT=False

    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/core/split__get_seq_with_type_passk_validte.py
"""Split data flow and run analysis in parallel."""
import copy
import itertools
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")
from lena.core import fill_compute_seq
from lena.core  import check_sequence_type as ct
from lena.core  import fill_request_seq
from lena.core  import sequence
from lena.core  import exceptions
from lena.core  import source
from lena.core  import meta


def _get_seq_with_type(seq, bufsize=None):
    """Return a (sequence, type) pair.
    Sequence is derived from *seq*
    (or is *seq*, if that is of a sequence type).
    """
    seq_type = ""
    if isinstance(seq, source.Source):
        seq_type = "source"
    elif isinstance(seq, fill_compute_seq.FillComputeSeq):
        seq_type = "fill_compute"
    elif isinstance(seq, fill_request_seq.FillRequestSeq):
        seq_type = "fill_request"
    elif isinstance(seq, sequence.Sequence):
        seq_type = "sequence"

    if seq_type:
        # append later
        pass
    ## If no explicit type is given, check seq's methods
    elif ct.is_fill_compute_seq(seq):
        seq_type = "fill_compute"
        if not ct.is_fill_compute_el(seq):
            seq = fill_compute_seq.FillComputeSeq(*seq)
    elif ct.is_fill_request_seq(seq):
        seq_type = "fill_request"
        if not ct.is_fill_request_el(seq):
            seq = fill_request_seq.FillRequestSeq(
                *seq, bufsize=bufsize,
                # if we have a FillRequest element inside,
                # it decides itself when to reset.
                reset=False,
                # todo: change the interface, because
                # no difference with buffer_output: we fill
                # without a buffer
                buffer_input=True
            )
    # Source is not checked,
    # because it must be Source explicitly.
    else:
        try:
            if isinstance(seq, tuple):
                seq = sequence.Sequence(*seq)
            else:
                seq = sequence.Sequence(seq)
        except exceptions.LenaTypeError:
            raise exceptions.LenaTypeError(
                "unknown argument type. Must be a "
                "FillComputeSeq, FillRequestSeq or Source, "
                "{} provided".format(seq)
            )
        else:
            seq_type = "sequence"
    return (seq, seq_type)


class Split(object):
    """Split data flow and run analysis in parallel."""

    def __init__(self, seqs, bufsize=1000, copy_buf=True):
        """*seqs* must be a list of Sequence, Source, FillComputeSeq
        or FillRequestSeq sequences.
        If *seqs* is empty, *Split* acts as an empty *Sequence* and
        yields all values it receives.

        *bufsize* is the size of the buffer for the input flow.
        If *bufsize* is ``None``,
        whole input flow is materialized in the buffer.
        *bufsize* must be a natural number or ``None``.

        *copy_buf* sets whether the buffer should be copied
        during :meth:`run`.
        This is important if different sequences can change input data
        and thus interfere with each other.

        Common type:
            If each sequence from *seqs* has a common type,
            *Split* creates methods corresponding to this type.
            For example, if each sequence is *FillCompute*,
            *Split* creates methods *fill* and *compute*
            and can be used as a *FillCompute* sequence.
            *fill* fills all its subsequences (with copies
            if *copy_buf* is True), and *compute*
            yields values from all sequences in turn
            (as would also do *request* or *Source.__call__*).
            Common type is not implemented for *Call* element.

        In case of wrong initialization arguments, :exc:`.LenaTypeError`
        or :exc:`.LenaValueError` is raised.
        """
        # todo: copy_buf must be always True. Isn't that?
        if not isinstance(seqs, list):
            raise exceptions.LenaTypeError(
                "seqs must be a list of sequences, "
                "{} provided".format(seqs)
            )
        seqs = [meta.alter_sequence(seq) for seq in seqs]
        self._sequences = []
        self._seq_types = []

        for sequence in seqs:
            try:
                seq, seq_type = _get_seq_with_type(sequence, bufsize)
            except exceptions.LenaTypeError:
                raise exceptions.LenaTypeError(
                    "unknown argument type. Must be one of "
                    "FillComputeSeq, FillRequestSeq or Source, "
                    "{} provided".format(sequence)
                )
            self._sequences.append(seq)
            self._seq_types.append(seq_type)

        different_seq_types = set(self._seq_types)
        self._n_seq_types = len(different_seq_types)
        if self._n_seq_types == 1:
            seq_type = different_seq_types.pop()
            # todo: probably remove run to avoid duplication?
            if seq_type == "fill_compute":
                self.fill = self._fill
                self.compute = self._compute
            elif seq_type == "fill_request":
                self.fill = self._fill
                self.request = self._request
            elif seq_type == "source":
                pass
        elif self._n_seq_types == 0:
            self.run = self._empty_run

        self._copy_buf = bool(copy_buf)

        if bufsize is not None:
            if bufsize != int(bufsize) or bufsize < 1:
                raise exceptions.LenaValueError(
                    "bufsize should be a natural number "
                    "or None, {} provided".format(bufsize)
                )
        self._bufsize = bufsize

    def __call__(self):
        """Each initialization sequence generates flow.
        After its flow is empty, next sequence is called, etc.

        This method is available only if each self sequence is a
        :class:`.Source`,
        otherwise runtime :exc:`.LenaAttributeError` is raised.
        """
        if self._n_seq_types != 1 or not ct.is_source(self._sequences[0]):
            raise exceptions.LenaAttributeError(
                "Split has no method '__call__'. It should contain "
                "only Source sequences to be callable"
            )
        # todo: use itertools.chain and check performance difference
        for seq in self._sequences:
            for result in seq():
                yield result

    def _fill(self, val):
        for seq in self._sequences[:-1]:
            if self._copy_buf:
                seq.fill(copy.deepcopy(val))
            else:
                seq.fill(val)
        self._sequences[-1].fill(val)

    def _compute(self):
        for seq in self._sequences:
            for val in seq.compute():
                yield val

    def _request(self):
        for seq in self._sequences:
            for val in seq.request():
                yield val

    def _empty_run(self, flow):
        """If self sequence is empty, yield all flow unchanged."""
        for val in flow:
            yield val

    def run(self, flow):
        """Iterate input *flow* and yield results.

        The *flow* is divided into subslices of *bufsize*.
        Each subslice is processed by sequences
        in the order of their initializer list.

        If a sequence is a *Source*,
        it doesn't accept the incoming *flow*,
        but produces its own complete flow
        and becomes inactive (is not called any more).

        A *FillRequestSeq* is filled with the buffer contents.
        After the buffer is finished,
        it yields all values from *request()*.

        A *FillComputeSeq* is filled with values from each buffer,
        but yields values from *compute* only after the whole *flow*
        is finished.

        A *Sequence* is called with *run(buffer)*
        instead of the whole flow. The results are yielded
        for each buffer (and also if the *flow* was empty).
        If the whole flow must be analysed at once,
        don't use such a sequence in *Split*.

        If the *flow* was empty, each *call*, *compute*,
        *request* or *run* is called nevertheless.

        If *copy_buf* is True,
        then the buffer for each sequence except the last one is a deep copy
        of the current buffer.
        """
        active_seqs = self._sequences[:]
        active_seq_types = self._seq_types[:]

        n_of_active_seqs = len(active_seqs)
        ind = 0
        flow = iter(flow)
        flow_was_empty = True
        while True:
            ## iterate on flow
            # If stop is None, then iteration continues
            # until the iterator is exhausted, if at all
            # https://docs.python.org/3/library/itertools.html#itertools.islice
            orig_buf = list(itertools.islice(flow, self._bufsize))
            if orig_buf:
                flow_was_empty = False
            else:
                break

            # iterate on active sequences
            ind = 0
            while ind < n_of_active_seqs:
                if self._copy_buf and n_of_active_seqs - ind > 1:
                    # last sequence doesn't need a copy of the buffer
                    buf = copy.deepcopy(orig_buf)
                else:
                    buf = orig_buf
                seq = active_seqs[ind]
                seq_type = active_seq_types[ind]

                if seq_type == "source":
                    for val in seq():
                        yield val
                    del active_seqs[ind]
                    del active_seq_types[ind]
                    n_of_active_seqs -= 1
                    continue
                elif seq_type == "fill_compute":
                    stopped = False
                    for val in buf:
                        try:
                            seq.fill(val)
                        except exceptions.LenaStopFill:
                            stopped = True
                            break
                    if stopped:
                        for result in seq.compute():
                            yield result
                        # we don't have goto in Python,
                        # so we have to repeat this
                        # each time we break double cycle.
                        del active_seqs[ind]
                        del active_seq_types[ind]
                        n_of_active_seqs -= 1
                        continue
                elif seq_type == "fill_request":
                    stopped = False
                    for val in buf:
                        try:
                            seq.fill(val)
                        except exceptions.LenaStopFill:
                            stopped = True
                            break
                    # FillRequest yields each time after buffer is filled
                    for result in seq.request():
                        yield result
                    if stopped:
                        del active_seqs[ind]
                        del active_seq_types[ind]
                        n_of_active_seqs -= 1
                        continue
                elif seq_type == "sequence":
                    # run buf as a whole flow.
                    # this may be very wrong if seq has internal state,
                    # e.g. contains a Cache
                    for res in seq.run(buf):
                        yield res
                # this is not needed, because can't be tested.
                # else:
                #     raise exceptions.LenaRuntimeError(
                #         "unknown sequence type {}".format(seq_type)
                #     )

                ind += 1
                # end internal while on sequences
            # end while on flow

        # yield computed data
        for seq, seq_type in zip(active_seqs, active_seq_types):
            if seq_type == "source":
                # otherwise it is a logic error
                assert flow_was_empty
                for val in seq():
                    yield val
            elif seq_type == "fill_compute":
                for val in seq.compute():
                    yield val
            elif seq_type == "fill_request":
                # otherwise FillRequest yielded after each buffer
                if flow_was_empty:
                    for val in seq.request():
                        yield val
            elif seq_type == "sequence":
                if flow_was_empty:
                    for val in seq.run([]):
                        yield val

    def _repr_nested(self, base_indent="", indent=" "*4, el_separ=",\n"):
        # copied from LenaSequence, see the diffs
        def repr_maybe_nested(el, base_indent, indent):
            if hasattr(el, "_repr_nested"):
                return el._repr_nested(base_indent=base_indent+indent, indent=indent)
            else:
                return base_indent + indent + repr(el)

        elems = el_separ.join((repr_maybe_nested(el, base_indent=base_indent,
                                                 indent=indent)
                               # diff here
                               for el in self._sequences))

        if "\n" in el_separ and self._sequences:
            # maybe new line
            mnl = "\n"
            # maybe base indent
            mbi = base_indent
        else:
            mnl = ""
            mbi = ""
        # diff here in name and brackets
        return "".join([base_indent, "Split",
                        "([", mnl, elems, mnl, mbi, "])"])

    def __repr__(self):
        return self._repr_nested()

if __name__ == "__main__":
    isT = True
    from lena.core.adapters import FillRequest
    from lena.math.elements import Sum, DSum, Mean
    from lena.core.source import Source
    from lena.flow import CountFrom

    res1 = _get_seq_with_type(Sum())[1]=="fill_compute"
    res2 = _get_seq_with_type(DSum())[1]=="fill_compute"
    res3 = _get_seq_with_type(Mean())[1]=="fill_compute"
    res4 = _get_seq_with_type(FillRequest(Sum(), reset=False, bufsize=10, buffer_input=True))[1]=="fill_request"
    res5 = _get_seq_with_type(Source(CountFrom()))[1]=="source"
    if not res1 or not res2 or not res3 or not res4 or not res5:
        isT=False
    # for l in os.listdir("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87b199a0c4fa8b80b354c/"):
    #     f = open("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87b199a0c4fa8b80b354c/" + l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"], bytes):
    #         args0 = dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0 = content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     res0 = _get_seq_with_type(args0, args1)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/structures/histogram_scale_passk_validte.py
"""Histogram structure *histogram* and element *Histogram*."""
import copy
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")
import lena.context
import lena.core
import lena.flow
import lena.math
from lena.structures import hist_functions as hf


class histogram():
    """A multidimensional histogram.

    Arbitrary dimension, variable bin size and weights are supported.
    Lower bin edge is included, upper edge is excluded.
    Underflow and overflow values are skipped.
    Bin content can be of arbitrary type,
    which is defined during initialization.

    Examples:

    >>> # a two-dimensional histogram
    >>> hist = histogram([[0, 1, 2], [0, 1, 2]])
    >>> hist.fill([0, 1])
    >>> hist.bins
    [[0, 1], [0, 0]]
    >>> values = [[0, 0], [1, 0], [1, 1]]
    >>> # fill the histogram with values
    >>> for v in values:
    ...     hist.fill(v)
    >>> hist.bins
    [[1, 1], [1, 1]]
    """
    # Note the differences from existing packages.
    # Numpy 1.16 (numpy.histogram): all but the last
    # (righthand-most) bin is half-open.
    # This histogram class has bin limits as in ROOT
    # (but without overflow and underflow).

    # Numpy: the first element of the range must be less than or equal to the second.
    # This histogram requires strictly increasing edges.
    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html
    # https://root.cern.ch/root/htmldoc/guides/users-guide/Histograms.html#bin-numbering

    def __init__(self, edges, bins=None, initial_value=0):
        """*edges* is a sequence of one-dimensional arrays,
        each containing strictly increasing bin edges.

        Histogram's bins by default
        are initialized with *initial_value*.
        It can be any object that supports addition with *weight*
        during *fill* (but that is not necessary
        if you don't plan to fill the histogram).
        If the *initial_value* is compound and requires special copying,
        create initial bins yourself (see :func:`.init_bins`).

        A histogram can be created from existing *bins* and *edges*.
        In this case a simple check of the shape of *bins* is done
        (raising :exc:`.LenaValueError` if failed).

        **Attributes**

        :attr:`edges` is a list of edges on each dimension.
        Edges mark the borders of the bin.
        Edges along each dimension are one-dimensional lists,
        and the multidimensional bin is the result of all intersections
        of one-dimensional edges.
        For example, a 3-dimensional histogram has edges of the form
        *[x_edges, y_edges, z_edges]*,
        and the 0th bin has borders
        *((x[0], x[1]), (y[0], y[1]), (z[0], z[1]))*.

        Index in the edges is a tuple, where a given position corresponds
        to a dimension, and the content at that position
        to the bin along that dimension.
        For example, index *(0, 1, 3)* corresponds to the bin
        with lower edges *(x[0], y[1], z[3])*.

        :attr:`bins` is a list of nested lists.
        Same index as for edges can be used to get bin content:
        bin at *(0, 1, 3)* can be obtained as *bins[0][1][3]*.
        Most nested arrays correspond to highest
        (further from x) coordinates.
        For example, for a 3-dimensional histogram bins equal to
        *[[[1, 1], [0, 0]], [[0, 0], [0, 0]]]*
        mean that the only filled bins are those
        where x and y indices are 0, and z index is 0 and 1.

        :attr:`dim` is the dimension of a histogram
        (length of its *edges* for a multidimensional histogram).

        If subarrays of *edges* are not increasing
        or if any of them has length less than 2,
        :exc:`.LenaValueError` is raised.

        .. admonition:: Programmer's note

            one- and multidimensional histograms
            have different *bins* and *edges* format.
            To be unified, 1-dimensional edges should be
            nested in a list (like *[[1, 2, 3]]*).
            Instead, they are simply the x-edges list,
            because it is more intuitive and one-dimensional histograms
            are used more often.
            To unify the interface for bins and edges in your code,
            use :func:`.unify_1_md` function.
        """
        # todo: allow creation of *edges* from tuples
        # (without lena.math.mesh). Allow bin_size in this case.
        hf.check_edges_increasing(edges)
        self.edges = edges
        self._scale = None

        if hasattr(edges[0], "__iter__"):
            self.dim = len(edges)
        else:
            self.dim = 1

        # todo: add a kwarg no_check=False to disable bins testing
        if bins is None:
            self.bins = hf.init_bins(self.edges, initial_value)
        else:
            self.bins = bins
            # We can't make scale for an arbitrary histogram,
            # because it may contain compound values.
            # self._scale = self.make_scale()
            wrong_bins_error = lena.core.LenaValueError(
                "bins of incorrect shape given, {}".format(bins)
            )
            if self.dim == 1:
                if len(bins) != len(edges) - 1:
                    raise wrong_bins_error
            else:
                if len(bins) != len(edges[0]) - 1:
                    raise wrong_bins_error
        if self.dim > 1:
            self.ranges = [(axis[0], axis[-1]) for axis in edges]
            self.nbins =  [len(axis) - 1 for axis in edges]
        else:
            self.ranges = [(edges[0], edges[-1])]
            self.nbins = [len(edges)-1]

    def __eq__(self, other):
        """Two histograms are equal, if and only if they have
        equal bins and equal edges.

        If *other* is not a :class:`.histogram`, return ``False``.

        Note that floating numbers should be compared
        approximately (using :func:`math.isclose`).
        """
        if not isinstance(other, histogram):
            # in Python comparison between different types is allowed
            return False
        return self.bins == other.bins and self.edges == other.edges

    def fill(self, coord, weight=1):
        """Fill histogram at *coord* with the given *weight*.

        Coordinates outside the histogram edges are ignored.
        """
        indices = hf.get_bin_on_value(coord, self.edges)
        subarr = self.bins
        for ind in indices[:-1]:
            # underflow
            if ind < 0:
                return
            try:
                subarr = subarr[ind]
            # overflow
            except IndexError:
                return
        ind = indices[-1]
        # underflow
        if ind < 0:
            return

        # fill
        try:
            subarr[ind] += weight
        except IndexError:
            return

    def __repr__(self):
        return "histogram({}, bins={})".format(self.edges, self.bins)

    def scale(self, other=None, recompute=False):
        """Compute or set scale (integral of the histogram).

        If *other* is ``None``, return scale of this histogram.
        If its scale was not computed before,
        it is computed and stored for subsequent use
        (unless explicitly asked to *recompute*).
        Note that after changing (filling) the histogram
        one must explicitly recompute the scale
        if it was computed before.

        If a float *other* is provided, rescale self to *other*.

        Histograms with scale equal to zero can't be rescaled.
        :exc:`.LenaValueError` is raised if one tries to do that.
        """
        # see graph.scale comments why this is called simply "scale"
        # (not set_scale, get_scale, etc.)
        if other is None:
            # return scale
            if self._scale is None or recompute:
                self._scale = hf.integral(
                    *hf.unify_1_md(self.bins, self.edges)
                )
            return self._scale
        else:
            # rescale from other
            scale = self.scale()
            if scale == 0:
                raise lena.core.LenaValueError(
                    "can not rescale histogram with zero scale"
                )
            self.bins = lena.math.md_map(lambda binc: binc*float(other) / scale,
                                         self.bins)
            self._scale = other
            return None

    def _update_context(self, context):
        """Update *context* with the properties of this histogram.

        *context.histogram* is updated with "dim", "nbins"
        and "ranges" with values for this histogram.
        If this histogram has a computed scale, it is also added
        to the context.

        Called on "destruction" of the histogram structure (for example,
        in :class:`.ToCSV`). See graph._update_context for more details.
        """

        hist_context = {
            "dim": self.dim,
            "nbins": self.nbins,
            "ranges": self.ranges
        }

        if self._scale is not None:
            hist_context["scale"] = self._scale

        lena.context.update_recursively(context, {"histogram": hist_context})


class Histogram():
    """An element to produce histograms."""

    def __init__(self, edges, bins=None, make_bins=None, initial_value=0):
        """*edges*, *bins* and *initial_value* have the same meaning
        as during creation of a :class:`histogram`.

        *make_bins* is a function without arguments
        that creates new bins
        (it will be called during :meth:`__init__` and :meth:`reset`).
        *initial_value* in this case is ignored, but bin check is made.
        If both *bins* and *make_bins* are provided,
        :exc:`.LenaTypeError` is raised.
        """
        self._hist = histogram(edges, bins)

        if make_bins is not None and bins is not None:
            raise lena.core.LenaTypeError(
                "either initial bins or make_bins must be provided, "
                "not both: {} and {}".format(bins, make_bins)
            )

        # may be None
        self._initial_bins = copy.deepcopy(bins)

        # todo: bins, make_bins, initial_value look redundant
        # and may be reconsidered when really using reset().
        if make_bins:
            bins = make_bins()
        self._make_bins = make_bins

        self._cur_context = {}

    def fill(self, value):
        """Fill the histogram with *value*.

        *value* can be a *(data, context)* pair. 
        Values outside the histogram edges are ignored.
        """
        data, self._cur_context = lena.flow.get_data_context(value)
        self._hist.fill(data)
        # filling with weight is only allowed in histogram structure
        # self._hist.fill(data, weight)

    def compute(self):
        """Yield histogram with context."""
        yield (self._hist, self._cur_context)

    def reset(self):
        """Reset the histogram.

        Current context is reset to an empty dict.
        Bins are reinitialized with the *initial_value*
        or with *make_bins()* (depending on the initialization).
        """
        if self._make_bins is not None:
            self.bins = self._make_bins()
        elif self._initial_bins is not None:
            self.bins = copy.deepcopy(self._initial_bins)
        else:
            self.bins = hf.init_bins(self.edges, self._initial_value)

        self._cur_context = {}

if __name__ == "__main__":
    isT=True
    hist = histogram([0, 1, 2])
    hist.fill(-10)
    hist.fill(10)

    hist = histogram([0, 0.5, 1])
    hist.fill(0.5)

    # _update_context without scale works
    context = {}
    hist._update_context(context)


    ## scale works
    # not initialized scale is set to None
    res1= hist._scale is None

    # scale is computed correctly
    res2= hist.scale() == 0.5

    # computed scale is saved
    res3= hist._scale == 0.5

    # _update_context works with scale
    hist._update_context(context)
    res4= context == {"histogram":
                           {"dim": 1, "nbins": [2], "ranges": [(0, 1)], "scale": hist.scale()}
                       }
    if not res1 or not res2 or not res3 or not res4:
        isT=False
    # for l in os.listdir("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87b4f9a0c4fa8b80b3581/"):
    #     f = open("/home/travis/builds/repos/ynikitenko---lena/data_passk_platform/62b87b4f9a0c4fa8b80b3581/" + l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = histogram([[0, 1, 2], [0, 1, 2]])
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.scale(args1, args2)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/structures/graph_scale_passk_validte.py
"""A graph is a function at given coordinates."""
import copy
import functools
import operator
import re
import warnings
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")
import lena.core
import lena.context
import lena.flow


class graph():
    """Numeric arrays of equal size."""

    def __init__(self, coords, field_names=("x", "y"), scale=None):
        """This structure generally corresponds
        to the graph of a function
        and represents arrays of coordinates and the function values
        of arbitrary dimensions.

        *coords* is a list of one-dimensional
        coordinate and value sequences (usually lists).
        There is little to no distinction between them,
        and "values" can also be called "coordinates".

        *field_names* provide the meaning of these arrays.
        For example, a 3-dimensional graph could be distinguished
        from a 2-dimensional graph with errors by its fields
        ("x", "y", "z") versus ("x", "y", "error_y").
        Field names don't affect drawing graphs:
        for that :class:`~Variable`-s should be used.
        Default field names,
        provided for the most used 2-dimensional graphs,
        are "x" and "y".

        *field_names* can be a string separated by whitespace
        and/or commas or a tuple of strings, such as ("x", "y").
        *field_names* must have as many elements
        as *coords* and each field name must be unique.
        Otherwise field names are arbitrary.
        Error fields must go after all other coordinates.
        Name of a coordinate error is "error\\_"
        appended by coordinate name. Further error details
        are appended after '_'. They could be arbitrary depending
        on the problem: "low", "high", "low_90%_cl", etc. Example:
        ("E", "time", "error_E_low", "error_time").

        *scale* of the graph is a kind of its norm. It could be
        the integral of the function or its other property.
        A scale of a normalised probability density
        function would be one.
        An initialized *scale* is required if one needs
        to renormalise the graph in :meth:`scale`
        (for example, to plot it with other graphs).

        Coordinates of a function graph would usually be arrays
        of increasing values, which is not required here.
        Neither is it checked that coordinates indeed
        contain one-dimensional numeric values.
        However, non-standard graphs
        will likely lead to errors during plotting
        and will require more programmer's work and caution,
        so use them only if you understand what you are doing.

        A graph can be iterated yielding tuples of numbers
        for each point.

        **Attributes**

        :attr:`coords` is a list \
            of one-dimensional lists of coordinates.

        :attr:`field_names`

        :attr:`dim` is the dimension of the graph,
        that is of all its coordinates without errors.

        In case of incorrect initialization arguments,
        :exc:`~.LenaTypeError` or :exc:`~.LenaValueError` is raised.

        .. versionadded:: 0.5
        """
        if not coords:
            raise lena.core.LenaValueError(
                "coords must be a non-empty sequence "
                "of coordinate sequences"
            )

        # require coords to be of the same size
        pt_len = len(coords[0])
        for arr in coords[1:]:
            if len(arr) != pt_len:
                raise lena.core.LenaValueError(
                    "coords must have subsequences of equal lengths"
                )

        # Unicode (Python 2) field names would be just bad,
        # so we don't check for it here.
        if isinstance(field_names, str):
            # split(', ') won't work.
            # From https://stackoverflow.com/a/44785447/952234:
            # \s stands for whitespace.
            field_names = tuple(re.findall(r'[^,\s]+', field_names))
        elif not isinstance(field_names, tuple):
            # todo: why field_names are a tuple,
            # while coords are a list?
            # It might be non-Pythonic to require a tuple
            # (to prohibit a list), but it's important
            # for comparisons and uniformity
            raise lena.core.LenaTypeError(
                "field_names must be a string or a tuple"
            )

        if len(field_names) != len(coords):
            raise lena.core.LenaValueError(
                "field_names must have must have the same size as coords"
            )

        if len(set(field_names)) != len(field_names):
            raise lena.core.LenaValueError(
                "field_names contains duplicates"
            )

        self.coords = coords
        self._scale = scale

        # field_names are better than fields,
        # because they are unambigous (as in namedtuple).
        self.field_names = field_names

        # decided to use "error_x_low" (like in ROOT).
        # Other versions were x_error (looked better than x_err),
        # but x_err_low looked much better than x_error_low).
        try:
            parsed_error_names = self._parse_error_names(field_names)
        except lena.core.LenaValueError as err:
            raise err
            # in Python 3
            # raise err from None
        self._parsed_error_names = parsed_error_names

        dim = len(field_names) - len(parsed_error_names)
        self._coord_names = field_names[:dim]
        self.dim = dim

        # todo: add subsequences of coords as attributes
        # with field names.
        # In case if someone wants to create a graph of another function
        # at the same coordinates.
        # Should a) work when we rescale the graph
        #        b) not interfere with other fields and methods

        # Probably we won't add methods __del__(n), __add__(*coords),
        # since it might change the scale.

    def __eq__(self, other):
        """Two graphs are equal, if and only if they have
        equal coordinates, field names and scales.

        If *other* is not a :class:`.graph`, return ``False``.

        Note that floating numbers should be compared
        approximately (using :func:`math.isclose`).
        Therefore this comparison may give false negatives.
        """
        if not isinstance(other, graph):
            # in Python comparison between different types is allowed
            return False
        return (self.coords == other.coords and self._scale == other._scale
                and self.field_names == other.field_names)

    def _get_err_indices(self, coord_name):
        """Get error indices corresponding to a coordinate."""
        err_indices = []
        dim = self.dim
        for ind, err in enumerate(self._parsed_error_names):
            if err[1] == coord_name:
                err_indices.append(ind+dim)
        return err_indices

    def __iter__(self):
        """Iterate graph coords one by one."""
        for val in zip(*self.coords):
            yield val

    def __repr__(self):
        return """graph({}, field_names={}, scale={})""".format(
            self.coords, self.field_names, self._scale
        )

    def scale(self, other=None):
        """Get or set the scale of the graph.

        If *other* is ``None``, return the scale of this graph.

        If a numeric *other* is provided, rescale to that value.
        If the graph has unknown or zero scale,
        rescaling that will raise :exc:`~.LenaValueError`.

        To get meaningful results, graph's fields are used.
        Only the last coordinate is rescaled.
        For example, if the graph has *x* and *y* coordinates,
        then *y* will be rescaled, and for a 3-dimensional graph
        *z* will be rescaled.
        All errors are rescaled together with their coordinate.
        """
        # this method is called scale() for uniformity with histograms
        # And this looks really good: explicit for computations
        # (not a subtle graph.scale, like a constant field (which is,
        #  however, the case in graph - but not in other structures))
        # and easy to remember (set_scale? rescale? change_scale_to?..)

        # We modify the graph in place,
        # because that would be redundant (not optimal)
        # to create a new graph
        # if we only want to change the scale of the existing one.

        if other is None:
            return self._scale

        if not self._scale:
            raise lena.core.LenaValueError(
                "can't rescale a graph with zero or unknown scale"
            )

        last_coord_ind = self.dim - 1
        last_coord_name = self.field_names[last_coord_ind]

        last_coord_indices = ([last_coord_ind] +
                self._get_err_indices(last_coord_name)
        )

        # In Python 2 3/2 is 1, so we want to be safe;
        # the downside is that integer-valued graphs
        # will become floating, but that is doubtfully an issue.
        # Remove when/if dropping support for Python 2.
        rescale = float(other) / self._scale

        mul = operator.mul
        partial = functools.partial

        # a version with lambda is about 50% slower:
        # timeit.timeit('[*map(lambda val: val*2, vals)]', \
        #     setup='vals = list(range(45)); from operator import mul; \
        #     from functools import partial')
        # 3.159
        # same setup for
        # timeit.timeit('[*map(partial(mul, 2), vals)]',...):
        # 2.075
        # 
        # [*map(...)] is very slightly faster than list(map(...)),
        # but it's unavailable in Python 2 (and anyway less readable).

        # rescale arrays of values and errors
        for ind, arr in enumerate(self.coords):
            if ind in last_coord_indices:
                # Python lists are faster than arrays,
                # https://stackoverflow.com/a/62399645/952234
                # (because each time taking a value from an array
                #  creates a Python object)
                self.coords[ind] = list(map(partial(mul, rescale),
                                            arr))

        self._scale = other

        # as suggested in PEP 8
        return None

    def _parse_error_names(self, field_names):
        # field_names is a parameter for easier testing,
        # usually object's field_names are used.
        errors = []

        # collect all error fields and check that they are
        # strictly after other fields
        in_error_fields = False
        # there is at least one field
        last_coord_ind = 0
        for ind, field in enumerate(field_names):
            if field.startswith("error_"):
                in_error_fields = True
                errors.append((field, ind))
            else:
                last_coord_ind = ind
                if in_error_fields:
                    raise lena.core.LenaValueError(
                        "errors must go after coordinate fields"
                    )

        coords = set(field_names[:last_coord_ind+1])
        parsed_errors = []

        for err, ind in errors:
            err_coords = []
            for coord in coords:
                err_main = err[6:]  # all after "error_"
                if err_main == coord or err_main.startswith(coord + "_"):
                    err_coords.append(coord)
                    err_tail = err_main[len(coord)+1:]
            if not err_coords:
                raise lena.core.LenaValueError(
                    "no coordinate corresponding to {} given".format(err)
                )
            elif len(err_coords) > 1:
                raise lena.core.LenaValueError(
                    "ambiguous error " + err +\
                    " corresponding to several coordinates given"
                )
            # "error" may be redundant, but it is explicit.
            parsed_errors.append(("error", err_coords[0], err_tail, ind))

        return parsed_errors

    def _update_context(self, context):
        """Update *context* with the properties of this graph.

        *context.error* is appended with indices of errors.
        Example subcontext for a graph with fields "E,t,error_E_low":
        {"error": {"x_low": {"index": 2}}}.
        Note that error names are called "x", "y" and "z"
        (this corresponds to first three coordinates,
        if they are present), which allows to simplify plotting.
        Existing values are not removed
        from *context.value* and its subcontexts.

        Called on "destruction" of the graph (for example,
        in :class:`.ToCSV`). By destruction we mean conversion
        to another structure (like text) in the flow.
        The graph object is not really destroyed in this process.
        """
        # this method is private, because we encourage users to yield
        # graphs into the flow and process them with ToCSV element
        # (not manually).

        if not self._parsed_error_names:
            # no error fields present
            return

        dim = self.dim

        xyz_coord_names = self._coord_names[:3]
        for name, coord_name in zip(["x", "y", "z"], xyz_coord_names):
            for err in self._parsed_error_names:
                if err[1] == coord_name:
                    error_ind = err[3]
                    if err[2]:
                        # add error suffix
                        error_name = name + "_" + err[2]
                    else:
                        error_name = name
                    lena.context.update_recursively(
                        context,
                        "error.{}.index".format(error_name),
                        # error can correspond both to variable and
                        # value, so we put it outside value.
                        # "value.error.{}.index".format(error_name),
                        error_ind
                    )

    # emulating numeric types
    def __add__(self, other):
        """Add last (highest) coordinates of two graphs.

        A new graph is returned. Error fields are ignored.
        """
        # todo: make it method add(.., calculate_error=...)
        if not isinstance(other, graph):
            return NotImplemented
        # but their errors may be different
        assert self.dim == other.dim
        dim = self.dim
        # copied from scale
        last_coord_ind = self.dim - 1
        last_coord_name = self.field_names[last_coord_ind]

        last_coord_indices = (
            [last_coord_ind] + self._get_err_indices(last_coord_name)
        )

        all_same = all(((len(self.coords[i]) == len(other.coords[i]))
                        for i in range(dim - 1)))
        assert all_same
        new_coords = [copy.copy(self.coords[i]) for i in range(dim - 1)]
        new_vals = [
            self.coords[last_coord_ind][i] + other.coords[last_coord_ind][i]
            for i in range(len(self.coords[last_coord_ind]))
        ]
        # add can't use zipped values
        # new_vals = list(map(operator.add, zip(self.coords[last_coord_ind], 
        #                                       other.coords[last_coord_ind])))
        new_coords.append(new_vals)
        try:
            scale0 = self.scale()
            scale1 = other.scale()
        except lena.core.LenaValueError:
            scale = None
        else:
            if scale0 is not None and scale1 is not None:
                scale = scale0 + scale1
            else:
                scale = None
        return graph(coords=new_coords, field_names=self.field_names,
                     scale=scale)

        # for ind, arr in enumerate(self.coords):
        #     if ind in last_coord_indices:
        #         self.coords[ind] = list(map(partial(mul, rescale),
        #                                     arr))


# used in deprecated Graph
def _rescale_value(rescale, value):
    return rescale * lena.flow.get_data(value)


class Graph(object):
    """
    .. deprecated:: 0.5
       use :class:`graph`.
       This class may be used in the future,
       but with a changed interface.

    Function at given coordinates (arbitraty dimensions).

    Graph points can be set during the initialization and
    during :meth:`fill`. It can be rescaled (producing a new :class:`Graph`).
    A point is a tuple of *(coordinate, value)*, where both *coordinate*
    and *value* can be tuples of numbers.
    *Coordinate* corresponds to a point in N-dimensional space,
    while *value* is some function's value at this point
    (the function can take a value in M-dimensional space).
    Coordinate and value dimensions must be the same for all points.

    One can get graph points as :attr:`Graph.points` attribute.
    They will be sorted each time before return
    if *sort* was set to ``True``.
    An attempt to change points
    (use :attr:`Graph.points` on the left of '=')
    will raise Python's :exc:`AttributeError`.
    """

    def __init__(self, points=None, context=None, scale=None, sort=True):
        """*points* is an array of *(coordinate, value)* tuples.

        *context* is the same as the most recent context
        during *fill*. Use it to provide a context
        when initializing a :class:`Graph` from existing points.

        *scale* sets the scale of the graph.
        It is used during plotting if rescaling is needed.

        Graph coordinates are sorted by default.
        This is usually needed to plot graphs of functions.
        If you need to keep the order of insertion, set *sort* to ``False``.

        By default, sorting is done using standard Python
        lists and functions. You can disable *sort* and provide your own
        sorting container for *points*.
        Some implementations are compared
        `here <http://www.grantjenks.com/docs/sortedcontainers/performance.html>`_.
        Note that a rescaled graph uses a default list.

        Note that :class:`Graph` does not reduce data.
        All filled values will be stored in it.
        To reduce data, use histograms.
        """
        warnings.warn("Graph is deprecated since Lena 0.5. Use graph.",
                      DeprecationWarning, stacklevel=2)

        self._points = points if points is not None else []
        # todo: add some sanity checks for points
        self._scale = scale
        self._init_context = {"scale": scale}
        if context is None:
            self._cur_context = {}
        elif not isinstance(context, dict):
            raise lena.core.LenaTypeError(
                "context must be a dict, {} provided".format(context)
            )
        else:
            self._cur_context = context
        self._sort = sort

        # todo: probably, scale from context is not needed.

        ## probably this function is not needed.
        ## it can't be copied, graphs won't be possible to compare.
        # *rescale_value* is a function, which can be used to scale
        # complex graph values.
        # It must accept a rescale parameter and the value at a data point.
        # By default, it is multiplication of rescale and the value
        # (which must be a number).
        # if rescale_value is None:
        #     self._rescale_value = _rescale_value
        self._rescale_value = _rescale_value
        self._update()

    def fill(self, value):
        """Fill the graph with *value*.

        *Value* can be a *(data, context)* tuple.
        *Data* part must be a *(coordinates, value)* pair,
        where both coordinates and value are also tuples.
        For example, *value* can contain the principal number
        and its precision.
        """
        point, self._cur_context = lena.flow.get_data_context(value)
        # coords, val = point
        self._points.append(point)

    def request(self):
        """Yield graph with context.

        If *sort* was initialized ``True``, graph points will be sorted.
        """
        # If flow contained *scale* it the context, it is set now.
        self._update()
        yield (self, self._context)

    # compute method shouldn't be in this class,
    # because it is a pure FillRequest.
    # def compute(self):
    #     """Yield graph with context (as in :meth:`request`),
    #     and :meth:`reset`."""
    #     self._update()
    #     yield (self, self._context)
    #     self.reset()

    @property
    def points(self):
        """Get graph points (read only)."""
        # sort points before giving them
        self._update()
        return self._points

    def reset(self):
        """Reset points to an empty list
        and current context to an empty dict.
        """
        self._points = []
        self._cur_context = {}

    def __repr__(self):
        self._update()
        return ("Graph(points={}, scale={}, sort={})"
                .format(self._points, self._scale, self._sort))

    def scale(self, other=None):
        """Get or set the scale.

        Graph's scale comes from an external source.
        For example, if the graph was computed from a function,
        this may be its integral passed via context during :meth:`fill`.
        Once the scale is set, it is stored in the graph.
        If one attempts to use scale which was not set,
        :exc:`.LenaAttributeError` is raised.

        If *other* is None, return the scale.

        If a ``float`` *other* is provided, rescale to *other*.
        A new graph with the scale equal to *other*
        is returned, the original one remains unchanged.
        Note that in this case its *points* will be a simple list
        and new graph *sort* parameter will be ``True``.

        Graphs with scale equal to zero can't be rescaled. 
        Attempts to do that raise :exc:`.LenaValueError`.
        """
        if other is None:
            # return scale
            self._update()
            if self._scale is None:
                raise lena.core.LenaAttributeError(
                    "scale must be explicitly set before using that"
                )
            return self._scale
        else:
            # rescale from other
            scale = self.scale()
            if scale == 0:
                raise lena.core.LenaValueError(
                    "can't rescale graph with 0 scale"
                )

            # new_init_context = copy.deepcopy(self._init_context)
            # new_init_context.update({"scale": other})

            rescale = float(other) / scale
            new_points = []
            for coord, val in self._points:
                # probably not needed, because tuples are immutable:
                # make a deep copy so that new values
                # are completely independent from old ones.
                new_points.append((coord, self._rescale_value(rescale, val)))
            # todo: should it inherit context?
            # Probably yes, but watch out scale.
            new_graph = Graph(points=new_points, scale=other,
                              sort=self._sort)
            return new_graph

    def to_csv(self, separator=",", header=None):
        """.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.
              Iterables are converted to tables.

        Convert graph's points to CSV.

        *separator* delimits values, the default is comma.

        *header*, if not ``None``, is the first string of the output
        (new line is added automatically).

        Since a graph can be multidimensional,
        for each point first its coordinate is converted to string
        (separated by *separator*), then each part of its value.

        To convert :class:`Graph` to CSV inside a Lena sequence,
        use :class:`lena.output.ToCSV`.
        """
        if self._sort:
            self._update()

        def unpack_pt(pt):
            coord = pt[0]
            value = pt[1]
            if isinstance(coord, tuple):
                unpacked = list(coord)
            else:
                unpacked = [coord]
            if isinstance(value, tuple):
                unpacked += list(value)
            else:
                unpacked.append(value)
            return unpacked

        def pt_to_str(pt, separ):
            return separ.join([str(val) for val in unpack_pt(pt)])

        if header is not None:
            # if one needs an empty header line, they may provide ""
            lines = header + "\n"
        else:
            lines = ""
        lines += "\n".join([pt_to_str(pt, separator) for pt in self.points])

        return lines

    #     *context* will be added to graph context.
    #     If it contains "scale", :meth:`scale` method will be available.
    #     Otherwise, if "scale" is contained in the context
    #     during :meth:`fill`, it will be used.
    #     In this case it is assumed that this scale
    #     is same for all values (only the last filled context is checked).
    #     Context from flow takes precedence over the initialized one.

    def _update(self):
        """Sort points if needed, update context."""
        # todo: probably remove this context_scale?
        context_scale = self._cur_context.get("scale")
        if context_scale is not None:
            # this complex check is fine with rescale,
            # because that returns a new graph (this scale unchanged).
            if self._scale is not None and self._scale != context_scale:
                raise lena.core.LenaRuntimeError(
                    "Initialization and context scale differ, "
                    "{} and {} from context {}"
                    .format(self._scale, context_scale, self._cur_context)
                )
            self._scale = context_scale
        if self._sort:
            self._points = sorted(self._points)

        self._context = copy.deepcopy(self._cur_context)
        self._context.update(self._init_context)
        # why this? Not *graph.scale*?
        self._context.update({"scale": self._scale})
        # self._context.update(lena.context.make_context(self, "_scale"))

        # todo: make this check during fill. Probably initialize self._dim
        # with kwarg dim. (dim of coordinates or values?)
        if self._points:
            # check points correctness
            points = self._points
            def coord_dim(coord):
                if not hasattr(coord, "__len__"):
                    return 1
                return len(coord)
            first_coord = points[0][0]
            dim = coord_dim(first_coord)
            same_dim = all(coord_dim(point[0]) == dim for point in points)
            if not same_dim:
                raise lena.core.LenaValueError(
                    "coordinates tuples must have same dimension, "
                    "{} given".format(points)
                )
            self.dim = dim
            self._context["dim"] = self.dim

    def __eq__(self, other):
        if not isinstance(other, Graph):
            return False
        if self.points != other.points:
            return False
        if self._scale is None and other._scale is None:
            return True
        try:
            result = self.scale() == other.scale()
        except lena.core.LenaAttributeError:
            # one scale couldn't be computed
            return False
        else:
            return result

if __name__ == "__main__":
    isT=True
    temp_class = graph([[0, 1], [1, 2]], scale=1)
    res1 = temp_class.scale(4)
    ist1=temp_class.coords==[[0, 1], [4.0, 8.0]]
    res2 = temp_class.scale()
    ist2=res2==4
    if not ist1 or not ist2:
        isT=False
    # import dill
    # import os
    #
    # isT = True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena/data_passk_platform1/62b87b519a0c4fa8b80b3583/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena/data_passk_platform1/62b87b519a0c4fa8b80b3583/" + l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = graph([[0, 1], [1, 2]],scale=1)
    #     # temp_class.__dict__.update(object_class)
    #     print(args1)
    #     print(temp_class.coords)
    #     res0 = temp_class.scale(args1)
    #     print(res0)
    #     print(temp_class.coords)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/ynikitenko---lena/lena/structures/hist_functions_hist_to_graph_passk_validte.py
"""Functions for histograms.

These functions are used for low-level work
with histograms and their contents.
They are not needed for normal usage.
"""
import collections
import copy
import itertools
import operator
import re
import sys
import sys
sys.path.append("/home/travis/builds/repos/ynikitenko---lena/")
from lena.math import mesh
from lena.structures import histogram

if sys.version_info.major == 3:
    from functools import reduce as _reduce
else:
    _reduce = reduce

import lena.core
from lena.structures.graph import graph as _graph, graph


class HistCell(collections.namedtuple("HistCell", ("edges, bin, index"))):
    """A namedtuple with fields *edges, bin, index*."""
    # from Aaron Hall's answer https://stackoverflow.com/a/28568351/952234
    __slots__ = ()


def cell_to_string(
        cell_edges, var_context=None, coord_names=None,
        coord_fmt="{}_lte_{}_lt_{}", coord_join="_", reverse=False):
    """Transform cell edges into a string.

    *cell_edges* is a tuple of pairs *(lower bound, upper bound)*
    for each coordinate.

    *coord_names* is a list of coordinates names.

    *coord_fmt* is a string,
    which defines how to format individual coordinates.

    *coord_join* is a string, which joins coordinate pairs.

    If *reverse* is True, coordinates are joined in reverse order.
    """
    # todo: do we really need var_context?
    # todo: even if so, why isn't that a {}? Is that dangerous?
    if coord_names is None:
        if var_context is None:
            coord_names = [
                "coord{}".format(ind) for ind in range(len(cell_edges))
            ]
        else:
            if "combine" in var_context:
                coord_names = [var["name"]
                               for var in var_context["combine"]]
            else:
                coord_names = [var_context["name"]]
    if len(cell_edges) != len(coord_names):
        raise lena.core.LenaValueError(
            "coord_names must have same length as cell_edges, "
            "{} and {} given".format(coord_names, cell_edges)
        )
    coord_strings = [coord_fmt.format(edge[0], coord_names[ind], edge[1])
                     for (ind, edge) in enumerate(cell_edges)]
    if reverse:
        coord_strings = reversed(coord_strings)
    coord_str = coord_join.join(coord_strings)
    return coord_str


def _check_edges_increasing_1d(arr):
    if len(arr) <= 1:
        raise lena.core.LenaValueError("size of edges should be more than one,"
                                       " {} provided".format(arr))
    increasing = (tup[0] < tup[1] for tup in zip(arr, arr[1:]))
    if not all(increasing):
        raise lena.core.LenaValueError(
            "expected strictly increasing values, "
            "{} provided".format(arr)
        )


def check_edges_increasing(edges):
    """Assure that multidimensional *edges* are increasing.

    If length of *edges* or its subarray is less than 2
    or if some subarray of *edges*
    contains not strictly increasing values,
    :exc:`.LenaValueError` is raised.
    """
    if not len(edges):
        raise lena.core.LenaValueError("edges must be non-empty")
    elif not hasattr(edges[0], '__iter__'):
        _check_edges_increasing_1d(edges)
        return
    for arr in edges:
        if len(arr) <= 1:
            raise lena.core.LenaValueError(
                "size of edges should be more than one. "
                "{} provided".format(arr)
            )
        _check_edges_increasing_1d(arr)


def get_bin_edges(index, edges):
    """Return edges of the bin for the given *edges* of a histogram.

    In one-dimensional case *index* must be an integer and a tuple
    of *(x_low_edge, x_high_edge)* for that bin is returned.

    In a multidimensional case *index* is a container of numeric indices
    in each dimension.
    A list of bin edges in each dimension is returned."""
    # todo: maybe give up this 1- and multidimensional unification
    # and write separate functions for each case.
    if not hasattr(edges[0], '__iter__'):
        # 1-dimensional edges
        if hasattr(index, '__iter__'):
            index = index[0]
        return (edges[index], edges[index+1])
    # multidimensional edges
    return [(edges[coord][i], edges[coord][i+1])
            for coord, i in enumerate(index)]


def get_bin_on_index(index, bins):
    """Return bin corresponding to multidimensional *index*.

    *index* can be a number or a list/tuple.
    If *index* length is less than dimension of *bins*,
    a subarray of *bins* is returned.

    In case of an index error, :exc:`.LenaIndexError` is raised.

    Example:

    >>> from lena.structures import histogram, get_bin_on_index
    >>> hist = histogram([0, 1], [0])
    >>> get_bin_on_index(0, hist.bins)
    0
    >>> get_bin_on_index((0, 1), [[0, 1], [0, 0]])
    1
    >>> get_bin_on_index(0, [[0, 1], [0, 0]])
    [0, 1]
    """
    if not isinstance(index, (list, tuple)):
        index = [index]
    subarr = bins
    for ind in index:
        try:
            subarr = subarr[ind]
        except IndexError:
            raise lena.core.LenaIndexError(
                "bad index: {}, bins = {}".format(index, bins)
            )
    return subarr


def get_bin_on_value_1d(val, arr):
    """Return index for value in one-dimensional array.

    *arr* must contain strictly increasing values
    (not necessarily equidistant),
    it is not checked.

    "Linear binary search" is used,
    that is our array search by default assumes
    the array to be split on equidistant steps.

    Example:

    >>> from lena.structures import get_bin_on_value_1d
    >>> arr = [0, 1, 4, 5, 7, 10]
    >>> get_bin_on_value_1d(0, arr)
    0
    >>> get_bin_on_value_1d(4.5, arr)
    2
    >>> # upper range is excluded
    >>> get_bin_on_value_1d(10, arr)
    5
    >>> # underflow
    >>> get_bin_on_value_1d(-10, arr)
    -1
    """
    # may also use numpy.searchsorted
    # https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.searchsorted.html
    ind_min = 0
    ind_max = len(arr) - 1
    while True:
        if ind_max - ind_min <= 1:
            # lower bound is close
            if val < arr[ind_min]:
                return ind_min - 1
            # upper bound is open
            elif val >= arr[ind_max]:
                return ind_max
            else:
                return ind_min
        if val == arr[ind_min]:
            return ind_min
        if val < arr[ind_min]:
            return ind_min - 1
        elif val >= arr[ind_max]:
            return ind_max
        else:
            shift = int(
                (ind_max - ind_min) * (
                    float(val - arr[ind_min]) / (arr[ind_max] - arr[ind_min])
                ))
            ind_guess = ind_min + shift

            if ind_min == ind_guess:
                ind_min += 1
                continue
            # ind_max is always more that ind_guess,
            # because val < arr[ind_max] (see the formula for shift).
            # This branch is not needed and can't be tested.
            # But for the sake of numerical inaccuracies, let us keep this
            # so that we never get into an infinite loop.
            elif ind_max == ind_guess:
                ind_max -= 1
                continue

            if val < arr[ind_guess]:
                ind_max = ind_guess
            else:
                ind_min = ind_guess


def get_bin_on_value(arg, edges):
    """Get the bin index for *arg* in a multidimensional array *edges*.

    *arg* is a 1-dimensional array of numbers
    (or a number for 1-dimensional *edges*),
    and corresponds to a point in N-dimensional space.

    *edges* is an array of N-1 dimensional arrays (lists or tuples) of numbers.
    Each 1-dimensional subarray consists of increasing numbers.

    *arg* and *edges* must have the same length
    (otherwise :exc:`.LenaValueError` is raised).
    *arg* and *edges* must be iterable and support *len()*.

    Return list of indices in *edges* corresponding to *arg*.

    If any coordinate is out of its corresponding edge range,
    its index will be ``-1`` for underflow
    or ``len(edge)-1`` for overflow.

    Examples:

    >>> from lena.structures import get_bin_on_value
    >>> edges = [[1, 2, 3], [1, 3.5]]
    >>> get_bin_on_value((1.5, 2), edges)
    [0, 0]
    >>> get_bin_on_value((1.5, 0), edges)
    [0, -1]
    >>> # the upper edge is excluded
    >>> get_bin_on_value((3, 2), edges)
    [2, 0]
    >>> # one-dimensional edges
    >>> edges = [1, 2, 3]
    >>> get_bin_on_value(2, edges)
    [1]
    """
    # arg is a one-dimensional index
    if not isinstance(arg, (tuple, list)):
        return [get_bin_on_value_1d(arg, edges)]
    # arg is a multidimensional index
    if len(arg) != len(edges):
        raise lena.core.LenaValueError(
            "argument should have same dimension as edges. "
            "arg = {}, edges = {}".format(arg, edges)
        )
    indices = []
    for ind, array in enumerate(edges):
        cur_bin = get_bin_on_value_1d(arg[ind], array)
        indices.append(cur_bin)
    return indices


def get_example_bin(struct):
    """Return bin with zero index on each axis of the histogram bins.

    For example, if the histogram is two-dimensional, return hist[0][0].

    *struct* can be a :class:`.histogram`
    or an array of bins.
    """
    if isinstance(struct, lena.structures.histogram):
        return lena.structures.get_bin_on_index([0] * struct.dim, struct.bins)
    else:
        bins = struct
        while isinstance(bins, list):
            bins = bins[0]
        return bins


def hist_to_graph(hist, make_value=None, get_coordinate="left",
                  field_names=("x", "y"), scale=None):
    """Convert a :class:`.histogram` to a :class:`.graph`.

    *make_value* is a function to set the value of a graph's point.
    By default it is bin content.
    *make_value* accepts a single value (bin content) without context.

    This option could be used to create graph's error bars.
    For example, to create a graph with errors
    from a histogram where bins contain
    a named tuple with fields *mean*, *mean_error* and a context
    one could use

    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)

    *get_coordinate* defines what the coordinate
    of a graph point created from a histogram bin will be.
    It can be "left" (default), "right" and "middle".

    *field_names* set field names of the graph. Their number
    must be the same as the dimension of the result.
    For a *make_value* above they would be
    *("x", "y_mean", "y_mean_error")*.

    *scale* becomes the graph's scale (unknown by default).
    If it is ``True``, it uses the histogram scale.

    *hist* must contain only numeric bins (without context)
    or *make_value* must remove context when creating a numeric graph.

    Return the resulting graph.
    """
    ## Could have allowed get_coordinate to be callable
    # (for generality), but 1) first find a use case,
    # 2) histogram bins could be adjusted in the first place.
    # -- don't understand 2.
    if get_coordinate == "left":
        get_coord = lambda edges: tuple(coord[0] for coord in edges)
    elif get_coordinate == "right":
        get_coord = lambda edges: tuple(coord[1] for coord in edges)
    # *middle* between the two edges, not the *center* of the bin
    # as a whole (because the graph corresponds to a point)
    elif get_coordinate == "middle":
        get_coord = lambda edges: tuple(0.5*(coord[0] + coord[1])
                                        for coord in edges)
    else:
        raise lena.core.LenaValueError(
            'get_coordinate must be one of "left", "right" or "middle"; '
            '"{}" provided'.format(get_coordinate)
        )

    # todo: make_value may be bad design.
    # Maybe allow to change the graph in the sequence.
    # However, make_value allows not to recreate a graph
    # or its coordinates (if that is not needed).

    if isinstance(field_names, str):
        # copied from graph.__init__
        field_names = tuple(re.findall(r'[^,\s]+', field_names))
    elif not isinstance(field_names, tuple):
        raise lena.core.LenaTypeError(
            "field_names must be a string or a tuple"
        )
    coords = [[] for _ in field_names]

    chain = itertools.chain

    if scale is True:
        scale = hist.scale()

    for value, edges in iter_bins_with_edges(hist.bins, hist.edges):
        coord = get_coord(edges)

        # Since we never use contexts here, it will be optimal
        # to ignore them completely (remove them elsewhere).
        # bin_value = lena.flow.get_data(value)
        bin_value = value

        if make_value is None:
            graph_value = bin_value
        else:
            graph_value = make_value(bin_value)

        # for iteration below
        if not hasattr(graph_value, "__iter__"):
            graph_value = (graph_value,)

        # add each coordinate to respective array
        for arr, coord_ in zip(coords, chain(coord, graph_value)):
            arr.append(coord_)

    return _graph(coords, field_names=field_names, scale=scale)


def init_bins(edges, value=0, deepcopy=False):
    """Initialize cells of the form *edges* with the given *value*.

    Return bins filled with copies of *value*.

    *Value* must be copyable, usual numbers will suit.
    If the value is mutable, use *deepcopy =* ``True``
    (or the content of cells will be identical).

    Examples:

    >>> edges = [[0, 1], [0, 1]]
    >>> # one cell
    >>> init_bins(edges)
    [[0]]
    >>> # no need to use floats,
    >>> # because integers will automatically be cast to floats
    >>> # when used together
    >>> init_bins(edges, 0.0)
    [[0.0]]
    >>> init_bins([[0, 1, 2], [0, 1, 2]])
    [[0, 0], [0, 0]]
    >>> init_bins([0, 1, 2])
    [0, 0]
    """
    nbins = len(edges) - 1
    if not isinstance(edges[0], (list, tuple)):
        # edges is one-dimensional
        if deepcopy:
            return [copy.deepcopy(value) for _ in range(nbins)]
        else:
            return [value] * nbins
    for ind, arr in enumerate(edges):
        if ind == nbins:
            if deepcopy:
                return [copy.deepcopy(value) for _ in range(len(arr)-1)]
            else:
                return list([value] * (len(arr)-1))
        bins = []
        for _ in range(len(arr)-1):
            bins.append(init_bins(edges[ind+1:], value, deepcopy))
        return bins


def integral(bins, edges):
    """Compute integral (scale for a histogram).

    *bins* contain values, and *edges* form the mesh
    for the integration.
    Their format is defined in :class:`.histogram` description.
    """
    total = 0
    for ind, bin_content in iter_bins(bins):
        bin_lengths = [
            edges[coord][i+1] - edges[coord][i]
            for coord, i in enumerate(ind)
        ]
        # product
        vol = _reduce(operator.mul, bin_lengths, 1)
        cell_integral = vol * bin_content
        total += cell_integral
    return total


def iter_bins(bins):
    """Iterate on *bins*. Yield *(index, bin content)*.

    Edges with higher index are iterated first
    (that is z, then y, then x for a 3-dimensional histogram).
    """
    # if not isinstance(bins, (list, tuple)):
    if not hasattr(bins, '__iter__'):
        # cell
        yield ((), bins)
    else:
        for ind, _ in enumerate(bins):
            for sub_ind, val in iter_bins(bins[ind]):
                yield (((ind,) + sub_ind), val)


def iter_bins_with_edges(bins, edges):
    """Generate *(bin content, bin edges)* pairs.

    Bin edges is a tuple, such that
    its item at index i is *(lower bound, upper bound)*
    of the bin at i-th coordinate.

    Examples:

    >>> from lena.math import mesh
    >>> list(iter_bins_with_edges([0, 1, 2], edges=mesh((0, 3), 3)))
    [(0, ((0, 1.0),)), (1, ((1.0, 2.0),)), (2, ((2.0, 3),))]
    >>>
    >>> # 2-dimensional histogram
    >>> list(iter_bins_with_edges(
    ...     bins=[[2]], edges=mesh(((0, 1), (0, 1)), (1, 1))
    ... ))
    [(2, ((0, 1), (0, 1)))]

    .. versionadded:: 0.5
       made public.
    """
    # todo: only a list or also a tuple, an array?
    if not isinstance(edges[0], list):
        edges = [edges]
    bins_sizes = [len(edge)-1 for edge in edges]
    indices = [list(range(nbins)) for nbins in bins_sizes]
    for index in itertools.product(*indices):
        bin_ = lena.structures.get_bin_on_index(index, bins)
        edges_low = []
        edges_high = []
        for var, var_ind in enumerate(index):
            edges_low.append(edges[var][var_ind])
            edges_high.append(edges[var][var_ind+1])
        yield (bin_, tuple(zip(edges_low, edges_high)))


def iter_cells(hist, ranges=None, coord_ranges=None):
    """Iterate cells of a histogram *hist*, possibly in a subrange.

    For each bin, yield a :class:`HistCell`
    containing *bin edges, bin content* and *bin index*.
    The order of iteration is the same as for :func:`iter_bins`.

    *ranges* are the ranges of bin indices to be used
    for each coordinate
    (the lower value is included, the upper value is excluded).

    *coord_ranges* set real coordinate ranges based on histogram edges.
    Obviously, they can be not exactly bin edges.
    If one of the ranges for the given coordinate
    is outside the histogram edges,
    then only existing histogram edges within the range are selected.
    If the coordinate range is completely outside histogram edges,
    nothing is yielded.
    If a lower or upper *coord_range*
    falls within a bin, this bin is yielded.
    Note that if a coordinate range falls on a bin edge,
    the number of generated bins can be unstable
    because of limited float precision.

    *ranges* and *coord_ranges* are tuples of tuples of limits
    in corresponding dimensions. 
    For one-dimensional histogram it must be a tuple 
    containing a tuple, for example
    *((None, None),)*.

    ``None`` as an upper or lower *range* means no limit
    (*((None, None),)* is equivalent to *((0, len(bins)),)*
    for a 1-dimensional histogram).

    If a *range* index is lower than 0 or higher than possible index,
    :exc:`.LenaValueError` is raised.
    If both *coord_ranges* and *ranges* are provided,
    :exc:`.LenaTypeError` is raised.
    """
    # for bin_ind, bin_ in iter_bins(hist.bins):
    #     yield HistCell(get_bin_edges(bin_ind, hist.edges), bin_, bin_ind)
    # if bins and edges are calculated each time, save the result now
    bins, edges = hist.bins, hist.edges
    # todo: hist.edges must be same
    # for 1- and multidimensional histograms.
    if hist.dim == 1:
        edges = (edges,)

    if coord_ranges is not None:
        if ranges is not None:
            raise lena.core.LenaTypeError(
                "only ranges or coord_ranges can be provided, not both"
            )
        ranges = []
        if not isinstance(coord_ranges[0], (tuple, list)):
            coord_ranges = (coord_ranges, )
        for coord, coord_range in enumerate(coord_ranges):
            # todo: (dis?)allow None as an infinite range.
            # todo: raise or transpose unordered coordinates?
            # todo: change the order of function arguments.
            lower_bin_ind = get_bin_on_value_1d(coord_range[0], edges[coord])
            if lower_bin_ind == -1:
                 lower_bin_ind = 0
            upper_bin_ind = get_bin_on_value_1d(coord_range[1], edges[coord])
            max_ind = len(edges[coord])
            if upper_bin_ind == max_ind:
                 upper_bin_ind -= 1
            if lower_bin_ind >= max_ind or upper_bin_ind <= 0:
                 # histogram edges are outside the range.
                 return
            ranges.append((lower_bin_ind, upper_bin_ind))

    if not ranges:
        ranges = ((None, None),) * hist.dim

    real_ind_ranges = []
    for coord, coord_range in enumerate(ranges):
        low, up = coord_range
        if low is None:
            low = 0
        else:
            # negative indices should not be supported
            if low < 0:
                raise lena.core.LenaValueError(
                    "low must be not less than 0 if provided"
                )
        max_ind = len(edges[coord]) - 1
        if up is None:
            up = max_ind
        else:
            # huge indices should not be supported as well.
            if up > max_ind:
                raise lena.core.LenaValueError(
                    "up must not be greater than len(edges)-1, if provided"
                )
        real_ind_ranges.append(list(range(low, up)))

    indices = list(itertools.product(*real_ind_ranges))
    for ind in indices:
        yield HistCell(get_bin_edges(ind, edges),
                       get_bin_on_index(ind, bins),
                       ind)


def make_hist_context(hist, context):
    """Update a deep copy of *context* with the context
    of a :class:`.histogram` *hist*.

    .. deprecated:: 0.5
       histogram context is updated automatically
       during conversion in :class:`~.output.ToCSV`.
       Use histogram._update_context explicitly if needed.
    """
    # absolutely unnecessary.
    context = copy.deepcopy(context)

    hist_context = {
        "histogram": {
            "dim": hist.dim,
            "nbins": hist.nbins,
            "ranges": hist.ranges
        }
    }
    context.update(hist_context)
    # just bad.
    return context


def unify_1_md(bins, edges):
    """Unify 1- and multidimensional bins and edges.

    Return a tuple of *(bins, edges)*.  
    Bins and multidimensional *edges* return unchanged,
    while one-dimensional *edges* are inserted into a list.
    """
    if hasattr(edges[0], '__iter__'):
    # if isinstance(edges[0], (list, tuple)):
        return (bins, edges)
    else:
        return (bins, [edges])

if __name__ == "__main__":
    isT=True
    val_with_error = collections.namedtuple("val_with_error",
                                            ["value", "error"])
    hist1 = histogram(mesh((0, 1), 1))
    val = val_with_error(1, 2)
    hist1.bins = lena.structures.init_bins(hist1.edges, val)

    # hist_to_graph works for 1-dimensional histograms
    res1=hist_to_graph(histogram([0, 1], bins=[1])) == graph([[0], [1]])

    # scale works
    # True == 1 in Python, so better to test scale 2.
    res2= hist_to_graph(histogram([0, 1], bins=[2]), scale=True) \
            == graph([[0], [2]], scale=2)

    # 2-dimensional histograms work
    hist2 = histogram(mesh(((0, 1), (0, 1)), (1, 1)), bins=[[2]])
    res3= hist_to_graph(hist2, scale=True, field_names="x,y,z") \
            == graph([[0], [0], [2]], scale=2, field_names="x,y,z")
    if not res1 or not res2 or not res3:
        isT=False
    # import dill
    # import os
    #
    # isT = True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena/data_passk_platform1/62b87b869a0c4fa8b80b35e1/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\ynikitenko---lena/data_passk_platform1/62b87b869a0c4fa8b80b35e1/" + l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"], bytes):
    #         args0 = dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0 = content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     if isinstance(content["input"]["args"][3]["bytes"], bytes):
    #         args3 = dill.loads(content["input"]["args"][3]["bytes"])
    #     else:
    #         args3 = content["input"]["args"][3]["bytes"]
    #     if isinstance(content["input"]["args"][4]["bytes"], bytes):
    #         args4 = dill.loads(content["input"]["args"][4]["bytes"])
    #     else:
    #         args4 = content["input"]["args"][4]["bytes"]
    #     print(args0, args1, args2, args3, args4)
    #     res0 = hist_to_graph(args0, args1, args2, args3, args4)
    #     print(res0)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/verify__verify_passk_validte.py
##############################################################################
#
# Copyright (c) 2001, 2002 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""Verify interface implementations
"""
from __future__ import print_function
import inspect
import sys
sys.path.append("/home/travis/builds/repos/pexip---os-zope")

from types import FunctionType
from types import MethodType

from src.zope.interface._compat import PYPY2

from zope.interface.exceptions import BrokenImplementation
from zope.interface.exceptions import BrokenMethodImplementation
from zope.interface.exceptions import DoesNotImplement
from zope.interface.exceptions import Invalid
from zope.interface.exceptions import MultipleInvalid

from zope.interface.interface import fromMethod, fromFunction, Method

__all__ = [
    'verifyObject',
    'verifyClass',
]

# This will be monkey-patched when running under Zope 2, so leave this
# here:
MethodTypes = (MethodType, )


def _verify(iface, candidate, tentative=False, vtype=None):
    """
    Verify that *candidate* might correctly provide *iface*.

    This involves:

    - Making sure the candidate claims that it provides the
      interface using ``iface.providedBy`` (unless *tentative* is `True`,
      in which case this step is skipped). This means that the candidate's class
      declares that it `implements <zope.interface.implementer>` the interface,
      or the candidate itself declares that it `provides <zope.interface.provider>`
      the interface

    - Making sure the candidate defines all the necessary methods

    - Making sure the methods have the correct signature (to the
      extent possible)

    - Making sure the candidate defines all the necessary attributes

    :return bool: Returns a true value if everything that could be
       checked passed.
    :raises zope.interface.Invalid: If any of the previous
       conditions does not hold.

    .. versionchanged:: 5.0
        If multiple methods or attributes are invalid, all such errors
        are collected and reported. Previously, only the first error was reported.
        As a special case, if only one such error is present, it is raised
        alone, like before.
    """

    if vtype == 'c':
        tester = iface.implementedBy
    else:
        tester = iface.providedBy

    excs = []
    if not tentative and not tester(candidate):
        excs.append(DoesNotImplement(iface, candidate))

    for name, desc in iface.namesAndDescriptions(all=True):
        try:
            _verify_element(iface, name, desc, candidate, vtype)
        except Invalid as e:
            excs.append(e)

    if excs:
        if len(excs) == 1:
            raise excs[0]
        raise MultipleInvalid(iface, candidate, excs)

    return True

def _verify_element(iface, name, desc, candidate, vtype):
    # Here the `desc` is either an `Attribute` or `Method` instance
    try:
        attr = getattr(candidate, name)
    except AttributeError:
        if (not isinstance(desc, Method)) and vtype == 'c':
            # We can't verify non-methods on classes, since the
            # class may provide attrs in it's __init__.
            return
        # TODO: On Python 3, this should use ``raise...from``
        raise BrokenImplementation(iface, desc, candidate)

    if not isinstance(desc, Method):
        # If it's not a method, there's nothing else we can test
        return

    if inspect.ismethoddescriptor(attr) or inspect.isbuiltin(attr):
        # The first case is what you get for things like ``dict.pop``
        # on CPython (e.g., ``verifyClass(IFullMapping, dict))``). The
        # second case is what you get for things like ``dict().pop`` on
        # CPython (e.g., ``verifyObject(IFullMapping, dict()))``.
        # In neither case can we get a signature, so there's nothing
        # to verify. Even the inspect module gives up and raises
        # ValueError: no signature found. The ``__text_signature__`` attribute
        # isn't typically populated either.
        #
        # Note that on PyPy 2 or 3 (up through 7.3 at least), these are
        # not true for things like ``dict.pop`` (but might be true for C extensions?)
        return

    if isinstance(attr, FunctionType):
        if sys.version_info[0] >= 3 and isinstance(candidate, type) and vtype == 'c':
            # This is an "unbound method" in Python 3.
            # Only unwrap this if we're verifying implementedBy;
            # otherwise we can unwrap @staticmethod on classes that directly
            # provide an interface.
            meth = fromFunction(attr, iface, name=name,
                                imlevel=1)
        else:
            # Nope, just a normal function
            meth = fromFunction(attr, iface, name=name)
    elif (isinstance(attr, MethodTypes)
          and type(attr.__func__) is FunctionType):
        meth = fromMethod(attr, iface, name)
    elif isinstance(attr, property) and vtype == 'c':
        # Without an instance we cannot be sure it's not a
        # callable.
        # TODO: This should probably check inspect.isdatadescriptor(),
        # a more general form than ``property``
        return

    else:
        if not callable(attr):
            raise BrokenMethodImplementation(desc, "implementation is not a method",
                                             attr, iface, candidate)
        # sigh, it's callable, but we don't know how to introspect it, so
        # we have to give it a pass.
        return

    # Make sure that the required and implemented method signatures are
    # the same.
    mess = _incompat(desc.getSignatureInfo(), meth.getSignatureInfo())
    if mess:
        if PYPY2 and _pypy2_false_positive(mess, candidate, vtype):
            return
        raise BrokenMethodImplementation(desc, mess, attr, iface, candidate)



def verifyClass(iface, candidate, tentative=False):
    """
    Verify that the *candidate* might correctly provide *iface*.
    """
    return _verify(iface, candidate, tentative, vtype='c')

def verifyObject(iface, candidate, tentative=False):
    return _verify(iface, candidate, tentative, vtype='o')

verifyObject.__doc__ = _verify.__doc__

_MSG_TOO_MANY = 'implementation requires too many arguments'
_KNOWN_PYPY2_FALSE_POSITIVES = frozenset((
    _MSG_TOO_MANY,
))


def _pypy2_false_positive(msg, candidate, vtype):
    # On PyPy2, builtin methods and functions like
    # ``dict.pop`` that take pseudo-optional arguments
    # (those with no default, something you can't express in Python 2
    # syntax; CPython uses special internal APIs to implement these methods)
    # return false failures because PyPy2 doesn't expose any way
    # to detect this pseudo-optional status. PyPy3 doesn't have this problem
    # because of __defaults_count__, and CPython never gets here because it
    # returns true for ``ismethoddescriptor`` or ``isbuiltin``.
    #
    # We can't catch all such cases, but we can handle the common ones.
    #
    if msg not in _KNOWN_PYPY2_FALSE_POSITIVES:
        return False

    known_builtin_types = vars(__builtins__).values()
    candidate_type = candidate if vtype == 'c' else type(candidate)
    if candidate_type in known_builtin_types:
        return True

    return False


def _incompat(required, implemented):
    #if (required['positional'] !=
    #    implemented['positional'][:len(required['positional'])]
    #    and implemented['kwargs'] is None):
    #    return 'imlementation has different argument names'
    if len(implemented['required']) > len(required['required']):
        return _MSG_TOO_MANY
    if ((len(implemented['positional']) < len(required['positional']))
        and not implemented['varargs']):
        return "implementation doesn't allow enough arguments"
    if required['kwargs'] and not implemented['kwargs']:
        return "implementation doesn't support keyword arguments"
    if required['varargs'] and not implemented['varargs']:
        return "implementation doesn't support variable arguments"
def _getTargetClass():
    from registry import AdapterRegistration
    return AdapterRegistration
def _makeOne( component=None):
    from declarations import InterfaceClass

    class IFoo(InterfaceClass):
        pass
    ifoo = IFoo('IFoo')
    ibar = IFoo('IBar')
    class _Registry(object):
        def __repr__(self):
            return '_REGISTRY'
    registry = _Registry()
    name = u'name'
    doc = 'DOCSTRING'
    klass = _getTargetClass()
    return (klass(registry, (ibar,), ifoo, name, component, doc),
            registry,
            name,
           )
if __name__ == "__main__":
    isT=True
    try:
        from zope.interface.verify import verifyClass
        from zope.interface.interfaces import IAdapterRegistration

        res1 = verifyClass(IAdapterRegistration, _getTargetClass())
        from zope.interface.verify import verifyObject
        from zope.interface.interfaces import IAdapterRegistration

        ar, _, _ = _makeOne()
        res2 = verifyObject(IAdapterRegistration, ar)
        if not res1 or not res2:
            isT = False
    except:
        isT=False
    # import dill
    # import os
    # from zope.interface.common.idatetime import IDate, IDateClass
    # isT = True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\pexip---os-zope\\data_passk_platform1/62b8b4baeb7e40a82d2d1136/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\pexip---os-zope\\data_passk_platform1/62b8b4baeb7e40a82d2d1136/" + l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"], bytes):
    #         args0 = dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0 = content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     if isinstance(content["input"]["args"][3]["bytes"], bytes):
    #         args3 = dill.loads(content["input"]["args"][3]["bytes"])
    #     else:
    #         args3 = content["input"]["args"][3]["bytes"]
    #     print(args0, args1, args2, args3)
    #     # Registered
    #     res0 = _verify(IDateClass, args1, args2, args3)
    #     # print(res0)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/verify_verifyObject_passk_validte.py
##############################################################################
#
# Copyright (c) 2001, 2002 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""Verify interface implementations
"""
from __future__ import print_function
import inspect
import sys
sys.path.append("/home/travis/builds/repos/pexip---os-zope")


from types import FunctionType
from types import MethodType

from src.zope.interface._compat import PYPY2

from zope.interface.exceptions import BrokenImplementation
from zope.interface.exceptions import BrokenMethodImplementation
from zope.interface.exceptions import DoesNotImplement
from zope.interface.exceptions import Invalid
from zope.interface.exceptions import MultipleInvalid

from zope.interface.interface import fromMethod, fromFunction, Method

__all__ = [
    'verifyObject',
    'verifyClass',
]

# This will be monkey-patched when running under Zope 2, so leave this
# here:
MethodTypes = (MethodType, )


def _verify(iface, candidate, tentative=False, vtype=None):
    """
    Verify that *candidate* might correctly provide *iface*.

    This involves:

    - Making sure the candidate claims that it provides the
      interface using ``iface.providedBy`` (unless *tentative* is `True`,
      in which case this step is skipped). This means that the candidate's class
      declares that it `implements <zope.interface.implementer>` the interface,
      or the candidate itself declares that it `provides <zope.interface.provider>`
      the interface

    - Making sure the candidate defines all the necessary methods

    - Making sure the methods have the correct signature (to the
      extent possible)

    - Making sure the candidate defines all the necessary attributes

    :return bool: Returns a true value if everything that could be
       checked passed.
    :raises zope.interface.Invalid: If any of the previous
       conditions does not hold.

    .. versionchanged:: 5.0
        If multiple methods or attributes are invalid, all such errors
        are collected and reported. Previously, only the first error was reported.
        As a special case, if only one such error is present, it is raised
        alone, like before.
    """

    if vtype == 'c':
        tester = iface.implementedBy
    else:
        tester = iface.providedBy

    excs = []
    if not tentative and not tester(candidate):
        excs.append(DoesNotImplement(iface, candidate))

    for name, desc in iface.namesAndDescriptions(all=True):
        try:
            _verify_element(iface, name, desc, candidate, vtype)
        except Invalid as e:
            excs.append(e)

    if excs:
        if len(excs) == 1:
            raise excs[0]
        raise MultipleInvalid(iface, candidate, excs)

    return True

def _verify_element(iface, name, desc, candidate, vtype):
    # Here the `desc` is either an `Attribute` or `Method` instance
    try:
        attr = getattr(candidate, name)
    except AttributeError:
        if (not isinstance(desc, Method)) and vtype == 'c':
            # We can't verify non-methods on classes, since the
            # class may provide attrs in it's __init__.
            return
        # TODO: On Python 3, this should use ``raise...from``
        raise BrokenImplementation(iface, desc, candidate)

    if not isinstance(desc, Method):
        # If it's not a method, there's nothing else we can test
        return

    if inspect.ismethoddescriptor(attr) or inspect.isbuiltin(attr):
        # The first case is what you get for things like ``dict.pop``
        # on CPython (e.g., ``verifyClass(IFullMapping, dict))``). The
        # second case is what you get for things like ``dict().pop`` on
        # CPython (e.g., ``verifyObject(IFullMapping, dict()))``.
        # In neither case can we get a signature, so there's nothing
        # to verify. Even the inspect module gives up and raises
        # ValueError: no signature found. The ``__text_signature__`` attribute
        # isn't typically populated either.
        #
        # Note that on PyPy 2 or 3 (up through 7.3 at least), these are
        # not true for things like ``dict.pop`` (but might be true for C extensions?)
        return

    if isinstance(attr, FunctionType):
        if sys.version_info[0] >= 3 and isinstance(candidate, type) and vtype == 'c':
            # This is an "unbound method" in Python 3.
            # Only unwrap this if we're verifying implementedBy;
            # otherwise we can unwrap @staticmethod on classes that directly
            # provide an interface.
            meth = fromFunction(attr, iface, name=name,
                                imlevel=1)
        else:
            # Nope, just a normal function
            meth = fromFunction(attr, iface, name=name)
    elif (isinstance(attr, MethodTypes)
          and type(attr.__func__) is FunctionType):
        meth = fromMethod(attr, iface, name)
    elif isinstance(attr, property) and vtype == 'c':
        # Without an instance we cannot be sure it's not a
        # callable.
        # TODO: This should probably check inspect.isdatadescriptor(),
        # a more general form than ``property``
        return

    else:
        if not callable(attr):
            raise BrokenMethodImplementation(desc, "implementation is not a method",
                                             attr, iface, candidate)
        # sigh, it's callable, but we don't know how to introspect it, so
        # we have to give it a pass.
        return

    # Make sure that the required and implemented method signatures are
    # the same.
    mess = _incompat(desc.getSignatureInfo(), meth.getSignatureInfo())
    if mess:
        if PYPY2 and _pypy2_false_positive(mess, candidate, vtype):
            return
        raise BrokenMethodImplementation(desc, mess, attr, iface, candidate)



def verifyClass(iface, candidate, tentative=False):
    """
    Verify that the *candidate* might correctly provide *iface*.
    """
    return _verify(iface, candidate, tentative, vtype='c')

def verifyObject(iface, candidate, tentative=False):
    return _verify(iface, candidate, tentative, vtype='o')

verifyObject.__doc__ = _verify.__doc__

_MSG_TOO_MANY = 'implementation requires too many arguments'
_KNOWN_PYPY2_FALSE_POSITIVES = frozenset((
    _MSG_TOO_MANY,
))


def _pypy2_false_positive(msg, candidate, vtype):
    # On PyPy2, builtin methods and functions like
    # ``dict.pop`` that take pseudo-optional arguments
    # (those with no default, something you can't express in Python 2
    # syntax; CPython uses special internal APIs to implement these methods)
    # return false failures because PyPy2 doesn't expose any way
    # to detect this pseudo-optional status. PyPy3 doesn't have this problem
    # because of __defaults_count__, and CPython never gets here because it
    # returns true for ``ismethoddescriptor`` or ``isbuiltin``.
    #
    # We can't catch all such cases, but we can handle the common ones.
    #
    if msg not in _KNOWN_PYPY2_FALSE_POSITIVES:
        return False

    known_builtin_types = vars(__builtins__).values()
    candidate_type = candidate if vtype == 'c' else type(candidate)
    if candidate_type in known_builtin_types:
        return True

    return False


def _incompat(required, implemented):
    #if (required['positional'] !=
    #    implemented['positional'][:len(required['positional'])]
    #    and implemented['kwargs'] is None):
    #    return 'imlementation has different argument names'
    if len(implemented['required']) > len(required['required']):
        return _MSG_TOO_MANY
    if ((len(implemented['positional']) < len(required['positional']))
        and not implemented['varargs']):
        return "implementation doesn't allow enough arguments"
    if required['kwargs'] and not implemented['kwargs']:
        return "implementation doesn't support keyword arguments"
    if required['varargs'] and not implemented['varargs']:
        return "implementation doesn't support variable arguments"
def _getTargetClass():
    from registry import AdapterRegistration
    return AdapterRegistration
def _makeOne( component=None):
    from declarations import InterfaceClass

    class IFoo(InterfaceClass):
        pass
    ifoo = IFoo('IFoo')
    ibar = IFoo('IBar')
    class _Registry(object):
        def __repr__(self):
            return '_REGISTRY'
    registry = _Registry()
    name = u'name'
    doc = 'DOCSTRING'
    klass = _getTargetClass()
    return (klass(registry, (ibar,), ifoo, name, component, doc),
            registry,
            name,
           )
if __name__ == "__main__":
    isT=True
    try:
        from zope.interface.verify import verifyObject
        from zope.interface.interfaces import IAdapterRegistration

        ar, _, _ = _makeOne()
        res1=verifyObject(IAdapterRegistration, ar)
        if not res1:
            isT=False
    except:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b4baeb7e40a82d2d1137/"):
    #     f = open("/home/travis/builds/repos/pexip---os-zope/data_passk_platform/62b8b4baeb7e40a82d2d1137/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"],bytes):
    #         args2=dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2=content["input"]["args"][2]["bytes"]
    #     res0 = verifyObject(args0,args1,args2)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/verify_verifyClass_passk_validte.py
##############################################################################
#
# Copyright (c) 2001, 2002 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""Verify interface implementations
"""

from __future__ import print_function
import sys
sys.path.append("/home/travis/builds/repos/pexip---os-zope")


import inspect
from types import FunctionType
from types import MethodType

from src.zope.interface._compat import PYPY2

from zope.interface.exceptions import BrokenImplementation
from zope.interface.exceptions import BrokenMethodImplementation
from zope.interface.exceptions import DoesNotImplement
from zope.interface.exceptions import Invalid
from zope.interface.exceptions import MultipleInvalid

from zope.interface.interface import fromMethod, fromFunction, Method

__all__ = [
    'verifyObject',
    'verifyClass',
]

# This will be monkey-patched when running under Zope 2, so leave this
# here:
MethodTypes = (MethodType, )


def _verify(iface, candidate, tentative=False, vtype=None):
    """
    Verify that *candidate* might correctly provide *iface*.

    This involves:

    - Making sure the candidate claims that it provides the
      interface using ``iface.providedBy`` (unless *tentative* is `True`,
      in which case this step is skipped). This means that the candidate's class
      declares that it `implements <implementer>` the interface,
      or the candidate itself declares that it `provides <provider>`
      the interface

    - Making sure the candidate defines all the necessary methods

    - Making sure the methods have the correct signature (to the
      extent possible)

    - Making sure the candidate defines all the necessary attributes

    :return bool: Returns a true value if everything that could be
       checked passed.
    :raises Invalid: If any of the previous
       conditions does not hold.

    .. versionchanged:: 5.0
        If multiple methods or attributes are invalid, all such errors
        are collected and reported. Previously, only the first error was reported.
        As a special case, if only one such error is present, it is raised
        alone, like before.
    """

    if vtype == 'c':
        tester = iface.implementedBy
    else:
        tester = iface.providedBy

    excs = []
    if not tentative and not tester(candidate):
        excs.append(DoesNotImplement(iface, candidate))

    for name, desc in iface.namesAndDescriptions(all=True):
        try:
            _verify_element(iface, name, desc, candidate, vtype)
        except Invalid as e:
            excs.append(e)

    if excs:
        if len(excs) == 1:
            raise excs[0]
        raise MultipleInvalid(iface, candidate, excs)

    return True

def _verify_element(iface, name, desc, candidate, vtype):
    # Here the `desc` is either an `Attribute` or `Method` instance
    try:
        attr = getattr(candidate, name)
    except AttributeError:
        if (not isinstance(desc, Method)) and vtype == 'c':
            # We can't verify non-methods on classes, since the
            # class may provide attrs in it's __init__.
            return
        # TODO: On Python 3, this should use ``raise...from``
        raise BrokenImplementation(iface, desc, candidate)

    if not isinstance(desc, Method):
        # If it's not a method, there's nothing else we can test
        return

    if inspect.ismethoddescriptor(attr) or inspect.isbuiltin(attr):
        # The first case is what you get for things like ``dict.pop``
        # on CPython (e.g., ``verifyClass(IFullMapping, dict))``). The
        # second case is what you get for things like ``dict().pop`` on
        # CPython (e.g., ``verifyObject(IFullMapping, dict()))``.
        # In neither case can we get a signature, so there's nothing
        # to verify. Even the inspect module gives up and raises
        # ValueError: no signature found. The ``__text_signature__`` attribute
        # isn't typically populated either.
        #
        # Note that on PyPy 2 or 3 (up through 7.3 at least), these are
        # not true for things like ``dict.pop`` (but might be true for C extensions?)
        return

    if isinstance(attr, FunctionType):
        if sys.version_info[0] >= 3 and isinstance(candidate, type) and vtype == 'c':
            # This is an "unbound method" in Python 3.
            # Only unwrap this if we're verifying implementedBy;
            # otherwise we can unwrap @staticmethod on classes that directly
            # provide an interface.
            meth = fromFunction(attr, iface, name=name,
                                imlevel=1)
        else:
            # Nope, just a normal function
            meth = fromFunction(attr, iface, name=name)
    elif (isinstance(attr, MethodTypes)
          and type(attr.__func__) is FunctionType):
        meth = fromMethod(attr, iface, name)
    elif isinstance(attr, property) and vtype == 'c':
        # Without an instance we cannot be sure it's not a
        # callable.
        # TODO: This should probably check inspect.isdatadescriptor(),
        # a more general form than ``property``
        return

    else:
        if not callable(attr):
            raise BrokenMethodImplementation(desc, "implementation is not a method",
                                             attr, iface, candidate)
        # sigh, it's callable, but we don't know how to introspect it, so
        # we have to give it a pass.
        return

    # Make sure that the required and implemented method signatures are
    # the same.
    mess = _incompat(desc.getSignatureInfo(), meth.getSignatureInfo())
    if mess:
        if PYPY2 and _pypy2_false_positive(mess, candidate, vtype):
            return
        raise BrokenMethodImplementation(desc, mess, attr, iface, candidate)



def verifyClass(iface, candidate, tentative=False):
    """
    Verify that the *candidate* might correctly provide *iface*.
    """
    return _verify(iface, candidate, tentative, vtype='c')

def verifyObject(iface, candidate, tentative=False):
    return _verify(iface, candidate, tentative, vtype='o')

verifyObject.__doc__ = _verify.__doc__

_MSG_TOO_MANY = 'implementation requires too many arguments'
_KNOWN_PYPY2_FALSE_POSITIVES = frozenset((
    _MSG_TOO_MANY,
))


def _pypy2_false_positive(msg, candidate, vtype):
    # On PyPy2, builtin methods and functions like
    # ``dict.pop`` that take pseudo-optional arguments
    # (those with no default, something you can't express in Python 2
    # syntax; CPython uses special internal APIs to implement these methods)
    # return false failures because PyPy2 doesn't expose any way
    # to detect this pseudo-optional status. PyPy3 doesn't have this problem
    # because of __defaults_count__, and CPython never gets here because it
    # returns true for ``ismethoddescriptor`` or ``isbuiltin``.
    #
    # We can't catch all such cases, but we can handle the common ones.
    #
    if msg not in _KNOWN_PYPY2_FALSE_POSITIVES:
        return False

    known_builtin_types = vars(__builtins__).values()
    candidate_type = candidate if vtype == 'c' else type(candidate)
    if candidate_type in known_builtin_types:
        return True

    return False


def _incompat(required, implemented):
    #if (required['positional'] !=
    #    implemented['positional'][:len(required['positional'])]
    #    and implemented['kwargs'] is None):
    #    return 'imlementation has different argument names'
    if len(implemented['required']) > len(required['required']):
        return _MSG_TOO_MANY
    if ((len(implemented['positional']) < len(required['positional']))
        and not implemented['varargs']):
        return "implementation doesn't allow enough arguments"
    if required['kwargs'] and not implemented['kwargs']:
        return "implementation doesn't support keyword arguments"
    if required['varargs'] and not implemented['varargs']:
        return "implementation doesn't support variable arguments"
def _getTargetClass():
    from registry import AdapterRegistration
    return AdapterRegistration
if __name__ == "__main__":
    isT=True
    try:
        from zope.interface.verify import verifyClass
        from zope.interface.interfaces import IAdapterRegistration

        res1=verifyClass(IAdapterRegistration, _getTargetClass())
        if not res1:
            isT=False
    except:
        isT=False
    # import dill
    # import os
    # isT=True
    # import common.numbers.IReal
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\pexip---os-zope/data_passk_platform1/62b8b4c1eb7e40a82d2d1139/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\pexip---os-zope/data_passk_platform1/62b8b4c1eb7e40a82d2d1139/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"],bytes):
    #         args2=dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2=content["input"]["args"][2]["bytes"]
    #     print(args0,args1,args2)
    #     # res0 = verifyClass(args0,args1,args2)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-zope/src/zope/interface/advice_determineMetaclass_passk_validte.py
##############################################################################
#
# Copyright (c) 2003 Zope Foundation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""Class advice.

This module was adapted from 'protocols.advice', part of the Python
Enterprise Application Kit (PEAK).  Please notify the PEAK authors
(pje@telecommunity.com and tsarna@sarna.org) if bugs are found or
Zope-specific changes are required, so that the PEAK version of this module
can be kept in sync.

PEAK is a Python application framework that interoperates with (but does
not require) Zope 3 and Twisted.  It provides tools for manipulating UML
models, object-relational persistence, aspect-oriented programming, and more.
Visit the PEAK home page at http://peak.telecommunity.com for more information.
"""

from types import FunctionType
try:
    from types import ClassType
except ImportError:
    __python3 = True
else:
    __python3 = False

__all__ = [
    'addClassAdvisor',
    'determineMetaclass',
    'getFrameInfo',
    'isClassAdvisor',
    'minimalBases',
]

import sys

def getFrameInfo(frame):
    """Return (kind,module,locals,globals) for a frame

    'kind' is one of "exec", "module", "class", "function call", or "unknown".
    """

    f_locals = frame.f_locals
    f_globals = frame.f_globals

    sameNamespace = f_locals is f_globals
    hasModule = '__module__' in f_locals
    hasName = '__name__' in f_globals

    sameName = hasModule and hasName
    sameName = sameName and f_globals['__name__']==f_locals['__module__']

    module = hasName and sys.modules.get(f_globals['__name__']) or None

    namespaceIsModule = module and module.__dict__ is f_globals

    if not namespaceIsModule:
        # some kind of funky exec
        kind = "exec"
    elif sameNamespace and not hasModule:
        kind = "module"
    elif sameName and not sameNamespace:
        kind = "class"
    elif not sameNamespace:
        kind = "function call"
    else: # pragma: no cover
        # How can you have f_locals is f_globals, and have '__module__' set?
        # This is probably module-level code, but with a '__module__' variable.
        kind = "unknown"
    return kind, module, f_locals, f_globals


def addClassAdvisor(callback, depth=2):
    """Set up 'callback' to be passed the containing class upon creation

    This function is designed to be called by an "advising" function executed
    in a class suite.  The "advising" function supplies a callback that it
    wishes to have executed when the containing class is created.  The
    callback will be given one argument: the newly created containing class.
    The return value of the callback will be used in place of the class, so
    the callback should return the input if it does not wish to replace the
    class.

    The optional 'depth' argument to this function determines the number of
    frames between this function and the targeted class suite.  'depth'
    defaults to 2, since this skips this function's frame and one calling
    function frame.  If you use this function from a function called directly
    in the class suite, the default will be correct, otherwise you will need
    to determine the correct depth yourself.

    This function works by installing a special class factory function in
    place of the '__metaclass__' of the containing class.  Therefore, only
    callbacks *after* the last '__metaclass__' assignment in the containing
    class will be executed.  Be sure that classes using "advising" functions
    declare any '__metaclass__' *first*, to ensure all callbacks are run."""
    # This entire approach is invalid under Py3K.  Don't even try to fix
    # the coverage for this block there. :(
    if __python3: # pragma: no cover
        raise TypeError('Class advice impossible in Python3')

    frame = sys._getframe(depth)
    kind, module, caller_locals, caller_globals = getFrameInfo(frame)

    # This causes a problem when zope interfaces are used from doctest.
    # In these cases, kind == "exec".
    #
    #if kind != "class":
    #    raise SyntaxError(
    #        "Advice must be in the body of a class statement"
    #    )

    previousMetaclass = caller_locals.get('__metaclass__')
    if __python3:   # pragma: no cover
        defaultMetaclass  = caller_globals.get('__metaclass__', type)
    else:
        defaultMetaclass  = caller_globals.get('__metaclass__', ClassType)


    def advise(name, bases, cdict):

        if '__metaclass__' in cdict:
            del cdict['__metaclass__']

        if previousMetaclass is None:
            if bases:
                # find best metaclass or use global __metaclass__ if no bases
                meta = determineMetaclass(bases)
            else:
                meta = defaultMetaclass

        elif isClassAdvisor(previousMetaclass):
            # special case: we can't compute the "true" metaclass here,
            # so we need to invoke the previous metaclass and let it
            # figure it out for us (and apply its own advice in the process)
            meta = previousMetaclass

        else:
            meta = determineMetaclass(bases, previousMetaclass)

        newClass = meta(name,bases,cdict)

        # this lets the callback replace the class completely, if it wants to
        return callback(newClass)

    # introspection data only, not used by inner function
    advise.previousMetaclass = previousMetaclass
    advise.callback = callback

    # install the advisor
    caller_locals['__metaclass__'] = advise


def isClassAdvisor(ob):
    """True if 'ob' is a class advisor function"""
    return isinstance(ob,FunctionType) and hasattr(ob,'previousMetaclass')


def determineMetaclass(bases, explicit_mc=None):
    """Determine metaclass from 1+ bases and optional explicit __metaclass__"""

    meta = [getattr(b,'__class__',type(b)) for b in bases]
    if explicit_mc is not None:
        # The explicit metaclass needs to be verified for compatibility
        # as well, and allowed to resolve the incompatible bases, if any
        meta.append(explicit_mc)
    if len(meta)==1:
        # easy case
        return meta[0]

    candidates = minimalBases(meta) # minimal set of metaclasses

    if not candidates: # pragma: no cover
        # they're all "classic" classes
        assert(not __python3) # This should not happen under Python 3
        return ClassType

    elif len(candidates)>1:
        # We could auto-combine, but for now we won't...
        raise TypeError("Incompatible metatypes",bases)

    # Just one, return it
    return candidates[0]


def minimalBases(classes):
    """Reduce a list of base classes to its ordered minimum equivalent"""

    if not __python3: # pragma: no cover
        classes = [c for c in classes if c is not ClassType]
    candidates = []

    for m in classes:
        for n in classes:
            if issubclass(n,m) and m is not n:
                break
        else:
            # m has no subclasses in 'classes'
            if m in candidates:
                candidates.remove(m)    # ensure that we're later in the list
            candidates.append(m)

    return candidates


class Metameta(type):
    pass


class Meta(type):
    __metaclass__ = Metameta
class A():
    def __init__(self):
        aaa=1
        self.b = 1
        b=1
class B():
    def __init__(self):
        aaa=1
if __name__ == "__main__":
    isT=True
    try:
        res1 = determineMetaclass([Meta,A,A,B],type)==type
        res2 = determineMetaclass([A, B])==type
        res3 = determineMetaclass([B])==type
        if not res1 or not res2 or not res3:
            isT=False
    except:
        isT=False


    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\pexip---os-zope\\data_passk_platform1/62b8b559eb7e40a82d2d11f6/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\pexip---os-zope\\data_passk_platform1/62b8b559eb7e40a82d2d11f6/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     print(args0,args1)
    #     # res0 = determineMetaclass(args0,args1)
    #
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/cache_pop_passk_validte.py
from collections.abc import MutableMapping


class _DefaultSize(object):

    __slots__ = ()

    def __getitem__(self, _):
        return 1

    def __setitem__(self, _, value):
        assert value == 1

    def pop(self, _):
        return 1


class Cache(MutableMapping):
    """Mutable mapping to serve as a simple cache or cache base class."""

    __marker = object()

    __size = _DefaultSize()

    def __init__(self, maxsize, getsizeof=None):
        if getsizeof:
            self.getsizeof = getsizeof
        if self.getsizeof is not Cache.getsizeof:
            self.__size = dict()
        self.__data = dict()
        self.__currsize = 0
        self.__maxsize = maxsize

    def __repr__(self):
        return '%s(%r, maxsize=%r, currsize=%r)' % (
            self.__class__.__name__,
            list(self.__data.items()),
            self.__maxsize,
            self.__currsize,
        )

    def __getitem__(self, key):
        try:
            return self.__data[key]
        except KeyError:
            return self.__missing__(key)

    def __setitem__(self, key, value):
        maxsize = self.__maxsize
        size = self.getsizeof(value)
        if size > maxsize:
            raise ValueError('value too large')
        if key not in self.__data or self.__size[key] < size:
            while self.__currsize + size > maxsize:
                self.popitem()
        if key in self.__data:
            diffsize = size - self.__size[key]
        else:
            diffsize = size
        self.__data[key] = value
        self.__size[key] = size
        self.__currsize += diffsize

    def __delitem__(self, key):
        size = self.__size.pop(key)
        del self.__data[key]
        self.__currsize -= size

    def __contains__(self, key):
        return key in self.__data

    def __missing__(self, key):
        raise KeyError(key)

    def __iter__(self):
        return iter(self.__data)

    def __len__(self):
        return len(self.__data)

    def get(self, key, default=None):
        if key in self:
            return self[key]
        else:
            return default

    def pop(self, key, default=__marker):
        if key in self:
            value = self[key]
            del self[key]
        elif default is self.__marker:
            raise KeyError(key)
        else:
            value = default
        return value

    def setdefault(self, key, default=None):
        if key in self:
            value = self[key]
        else:
            self[key] = value = default
        return value

    @property
    def maxsize(self):
        """The maximum size of the cache."""
        return self.__maxsize

    @property
    def currsize(self):
        """The current size of the cache."""
        return self.__currsize

    @staticmethod
    def getsizeof(value):
        """Return the size of a cache element's value."""
        return 1

if __name__ == "__main__":
    isT = True
    try:
        cache = Cache(10000)
        cache.__setitem__(4,10)
        cache.__setitem__(5, "five")
        cache.__setitem__(6, 600)
        res1 = cache.pop(4, 1)
        res2 = cache.pop(5, 2)
        res3 = cache.pop(7, 2)
        res4 = cache.pop(10,7)
        res5 = cache.pop(6, 11)
        res6 = cache.pop(6, 20)
        if res1!=10 or res2!="five" or res3 != 2 or res4 != 7 or res5 != 600 or res6 != 20:
            isT = False
    except:
        isT = False
    # for l in os.listdir(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos\pexip---os-python-cachetools\\data_passk_platform1/62b8d22a48ba5a41d1c3f47d/"):
    #     f = open(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos\pexip---os-python-cachetools\\data_passk_platform1/62b8d22a48ba5a41d1c3f47d/" + l,
    #         "rb")
    #
    #     content = dill.load(f)
    #     if len(content["input"]["args"])!=3:
    #         continue
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     print(args1,args2)
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     # temp_class = Cache(10000)
    #     # temp_class.__dict__.update(object_class)
    #     # res0 = temp_class.pop(args1, args2)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/lfu_popitem_passk_validte.py
import collections
import sys
sys.path.append("..")
from cache import Cache


class LFUCache(Cache):
    """Least Frequently Used (LFU) cache implementation."""

    def __init__(self, maxsize, getsizeof=None):
        Cache.__init__(self, maxsize, getsizeof)
        self.__counter = collections.Counter()

    def __getitem__(self, key, cache_getitem=Cache.__getitem__):
        value = cache_getitem(self, key)
        if key in self:  # __missing__ may not store item
            self.__counter[key] -= 1
        return value

    def __setitem__(self, key, value, cache_setitem=Cache.__setitem__):
        cache_setitem(self, key, value)
        self.__counter[key] -= 1

    def __delitem__(self, key, cache_delitem=Cache.__delitem__):
        cache_delitem(self, key)
        del self.__counter[key]

    def popitem(self):
        """Remove and return the `(key, value)` pair least frequently used."""
        try:
            (key, _), = self.__counter.most_common(1)
        except ValueError:
            raise KeyError('%s is empty' % type(self).__name__) from None
        else:
            return (key, self.pop(key))

if __name__ == "__main__":
    isT = True
    try:
        temp_class = LFUCache(10000)
        temp_class.__setitem__(4, 10)
        temp_class.__setitem__(5, "five")
        temp_class.__setitem__(6, 600)
        listt=[(4, 10),(5, 'five'),(6, 600)]
        p1 = temp_class.popitem()# == (4, 10) or
        p2 = temp_class.popitem() #== (5, 'five')
        p3 = temp_class.popitem() #== (6, 600)
        res1=p1 in listt
        res2=p2 in listt and p2!=p1
        res3=p3 in listt and p3!=p1 and p3!=p2

        if not res1 or not res2 or not res3:
            isT = False
    except:
        isT = False


    # for l in os.listdir(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d23748ba5a41d1c3f497/"):
    #     f = open(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d23748ba5a41d1c3f497/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = LFUCache(10000)
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.popitem()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/lru_popitem_passk_validte.py
import collections
import sys
sys.path.append("..")
from cache import Cache


class LRUCache(Cache):
    """Least Recently Used (LRU) cache implementation."""

    def __init__(self, maxsize, getsizeof=None):
        Cache.__init__(self, maxsize, getsizeof)
        self.__order = collections.OrderedDict()

    def __getitem__(self, key, cache_getitem=Cache.__getitem__):
        value = cache_getitem(self, key)
        if key in self:  # __missing__ may not store item
            self.__update(key)
        return value

    def __setitem__(self, key, value, cache_setitem=Cache.__setitem__):
        cache_setitem(self, key, value)
        self.__update(key)

    def __delitem__(self, key, cache_delitem=Cache.__delitem__):
        cache_delitem(self, key)
        del self.__order[key]

    def popitem(self):
        """Remove and return the `(key, value)` pair least recently used."""
        try:
            key = next(iter(self.__order))
        except StopIteration:
            raise KeyError('%s is empty' % type(self).__name__) from None
        else:
            return (key, self.pop(key))

    def __update(self, key):
        try:
            self.__order.move_to_end(key)
        except KeyError:
            self.__order[key] = None

if __name__ == "__main__":
    isT = True
    try:
        temp_class = LRUCache(10000)
        temp_class.__setitem__(4, 10)
        temp_class.__setitem__(5, "five")
        temp_class.__setitem__(6, 600)
        res1 = temp_class.popitem() == (4, 10)
        res2 = temp_class.popitem() == (5, 'five')
        res3 = temp_class.popitem() == (6, 600)
        if not res1 or not res2 or not res3:
            isT = False
    except:
        isT = False
    # for l in os.listdir(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d23a48ba5a41d1c3f499/"):
    #     f = open(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d23a48ba5a41d1c3f499/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = LRUCache(10000)
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.popitem()
    #     # print(res0)
    #     # print(content["output"][0])
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/mru_popitem_passk_validte.py
import collections
import sys
sys.path.append("..")

from cache import Cache


class MRUCache(Cache):
    """Most Recently Used (MRU) cache implementation."""

    def __init__(self, maxsize, getsizeof=None):
        Cache.__init__(self, maxsize, getsizeof)
        self.__order = collections.OrderedDict()

    def __getitem__(self, key, cache_getitem=Cache.__getitem__):
        value = cache_getitem(self, key)
        if key in self:  # __missing__ may not store item
            self.__update(key)
        return value

    def __setitem__(self, key, value, cache_setitem=Cache.__setitem__):
        cache_setitem(self, key, value)
        self.__update(key)

    def __delitem__(self, key, cache_delitem=Cache.__delitem__):
        cache_delitem(self, key)
        del self.__order[key]

    def popitem(self):
        """Remove and return the `(key, value)` pair most recently used."""
        try:
            key = next(iter(self.__order))
        except StopIteration:
            raise KeyError('%s is empty' % type(self).__name__) from None
        else:
            return (key, self.pop(key))

    def __update(self, key):
        try:
            self.__order.move_to_end(key, last=False)
        except KeyError:
            self.__order[key] = None

if __name__ == "__main__":
    isT = True
    try:
        temp_class = MRUCache(10000)
        temp_class.__setitem__(4, 10)
        temp_class.__setitem__(5, "five")
        temp_class.__setitem__(6, 600)
        res1 = temp_class.popitem() == (4, 10)
        res2 = temp_class.popitem() == (5, 'five')
        res3 = temp_class.popitem() == (6, 600)
        if not res1 or not res2 or not res3:
            isT = False
    except:
        isT = False
    # for l in os.listdir(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d23c48ba5a41d1c3f49b/"):
    #     f = open(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d23c48ba5a41d1c3f49b/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = MRUCache(10000)
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.popitem()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-cachetools/cachetools/rr_popitem_passk_validte.py
import random
import sys
sys.path.append("..")

from cache import Cache


# random.choice cannot be pickled in Python 2.7
def _choice(seq):
    return random.choice(seq)


class RRCache(Cache):
    """Random Replacement (RR) cache implementation."""

    def __init__(self, maxsize, choice=random.choice, getsizeof=None):
        Cache.__init__(self, maxsize, getsizeof)
        # TODO: use None as default, assing to self.choice directly?
        if choice is random.choice:
            self.__choice = _choice
        else:
            self.__choice = choice

    @property
    def choice(self):
        """The `choice` function used by the cache."""
        return self.__choice

    def popitem(self):
        """Remove and return a random `(key, value)` pair."""
        try:
            key = self.__choice(list(self))
        except IndexError:
            raise KeyError('%s is empty' % type(self).__name__) from None
        else:
            return (key, self.pop(key))

if __name__ == "__main__":
    isT = True
    try:
        temp_class = RRCache(10000)
        temp_class.__setitem__(4, 10)
        temp_class.__setitem__(5, "five")
        temp_class.__setitem__(6, 600)
        listt = [(4, 10), (5, 'five'), (6, 600)]
        p1 = temp_class.popitem()  # == (4, 10) or
        p2 = temp_class.popitem()  # == (5, 'five')
        p3 = temp_class.popitem()  # == (6, 600)
        res1 = p1 in listt
        res2 = p2 in listt and p2 != p1
        res3 = p3 in listt and p3 != p1 and p3 != p2

        if not res1 or not res2 or not res3:
            isT = False
    except:
        isT = False
    # for l in os.listdir(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d23e48ba5a41d1c3f49e/"):
    #     # print(l)
    #     # if l=="data10" or l=="data"
    #     f = open(
    #         "/home/travis/builds/repos/pexip---os-python-cachetools/data_passk_platform/62b8d23e48ba5a41d1c3f49e/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     # temp_class = RRCache(10000)
    #     # temp_class.__dict__.update(object_class)
    #     # res0 = temp_class.popitem()
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = RRCache(10000)
    #     temp_class.__dict__.update(object_class)
    #
    #     before=len(temp_class)
    #     # print(len(temp_class))
    #     res0 = temp_class.popitem()
    #     # print(temp_class.maxsize)
    #     # print(len(temp_class))
    #     after=len(temp_class)
    #     if before==0:
    #         if after!=0:
    #             raise Exception("Result not True!!!")
    #     else:
    #         if after+1!=before:
    #             raise Exception("Result not True!!!")
    #     # print(str(res0))
    #     # print(str(content["output"][0]))
    # #     if str(res0)!= str(content["output"][0]):
    # #         isT=False
    # #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/cpburnz---python-sql-parameters/sqlparams/__init____create_in_regex_passk_validte.py
"""
:mod:`sqlparams` is a utility package for converting between various SQL
parameter styles.
"""
import sys
sys.path.append("/home/travis/builds/repos/cpburnz---python-sql-parameters/")
import re
from typing import (
    Any,
    Dict,
    Iterable,
    List,
    Optional,
    Pattern,
    Sequence,
    Tuple,
    Type,
    Union)

from sqlparams import _converting
from sqlparams import _styles
from sqlparams import _util

from sqlparams._meta import (
    __author__,
    __copyright__,
    __credits__,
    __license__,
    __version__)
from sqlparams._util import (
    SqlStr)

_BYTES_ENCODING = 'latin1'
"""
The encoding to use when parsing a byte query string.
"""

DEFAULT_COMMENTS: Sequence[Union[str, Tuple[str, str]]] = (
    ("/*", "*/"),
    "--",
)
"""
The default comment styles to strip. This strips single line comments
beginning with :data:`"--"` and multiline comments beginning with
:data:`"/*"` and ending with :data:`"*/"`.
"""


class SQLParams(object):
    """
    The :class:`.SQLParams` class is used to support named parameters in
    SQL queries where they are not otherwise supported (e.g., pyodbc).
    This is done by converting from one parameter style query to another
    parameter style query.

    By default, when converting to a numeric or ordinal style any
    :class:`tuple` parameter will be expanded into "(?,?,...)" to support
    the widely used "IN {tuple}" SQL expression without leaking any
    unescaped values.
    """

    def __init__(
            self,
            in_style: str,
            out_style: str,
            escape_char: Union[str, bool, None] = None,
            expand_tuples: Optional[bool] = None,
            strip_comments: Union[Sequence[Union[str, Tuple[str, str]]], bool, None] = None,
    ) -> None:
        """
        Instantiates the :class:`.SQLParams` instance.

        *in_style* (:class:`str`) is the parameter style that will be used
        in an SQL query before being parsed and converted to :attr:`.SQLParams.out_style`.

        *out_style* (:class:`str`) is the parameter style that the SQL query
        will be converted to.

        *escape_char* (:class:`str`, :class:`bool`, or :data:`None`) is the
        escape character used to prevent matching an in-style parameter. If
        :data:`True`, use the default escape character (repeat the initial
        character to escape it; e.g., "%%"). If :data:`False`, do not use an
        escape character. Default is :data:`None` for :data:`False`.

        *expand_tuples* (:class:`bool` or :data:`None`) is whether to
        expand tuples into a sequence of parameters. Default is :data:`None`
        to let it be determined by *out_style* (to maintain backward
        compatibility). If *out_style* is a numeric or ordinal style, expand
        tuples by default (:data:`True`). If *out_style* is a named style,
        do not expand tuples by default (:data:`False`).

        .. NOTE:: Empty tuples will be safely expanded to :data:`(NULL)` to
           prevent SQL syntax errors,

        *strip_comments* (:class:`Sequence`, :class:`bool`, or :data:`None`)
        whether to strip out comments and what style of comments to remove.
        If a :class:`Sequence`, this defines the comment styles. A single
        line comment is defined using a :class:`str` (e.g., :data:`"--"` or
        :data:`"#"`). A multiline comment is defined using a :class:`tuple`
        of :class:`str` (e.g., :data:`("/*", "*/")`). In order for a comment
        to be matched, it must be the first string of non-whitespace
        characters on the line. Trailing comments are not supported and will
        be ignored. A multiline comment will consume characters until the
        ending string is matched. If :data:`True`, :data:`DEFAULT_COMMENTS`
        will be used (:data:`"--"` and :data:`("/*", "*/")` styles). Default
        is :data:`None` to not remove comments.

        The following parameter styles are supported by both *in_style* and
        *out_style*:

        -	For all named styles the parameter keys must be valid `Python identifiers`_.
            They cannot start with a digit. This is to help prevent
            incorrectly matching common strings such as datetimes.

            Named styles:

            -	"named" indicates parameters will use the named style::

                    ... WHERE name = :name

            -	"named_dollar" indicates parameters will use the named dollar
                sign style::

                    ... WHERE name = $name

                .. NOTE:: This is not defined by `PEP 249`_.

            -	"pyformat" indicates parameters will use the named Python
                extended format style::

                    ... WHERE name = %(name)s

                .. NOTE:: Strictly speaking, `PEP 249`_ only specifies
                   "%(name)s" for the "pyformat" parameter style so only that
                   form (without any other conversions or flags) is supported.

        -	All numeric styles start at :data:`1`. When using a
            :class:`~collections.abc.Sequence` for the parameters, the 1st
            parameter (e.g., ":1") will correspond to the 1st element of the
            sequence (i.e., index :data:`0`). When using a :class:`~collections.abc.Mapping`
            for the parameters, the 1st parameter (e.g., ":1") will correspond
            to the matching key (i.e., :data:`1` or :data:`"1"`).

            Numeric styles:

            -	"numeric" indicates parameters will use the numeric style::

                    ... WHERE name = :1

            -	"numeric_dollar" indicates parameters will use the numeric
                dollar sign style (starts at :data:`1`)::

                    ... WHERE name = $1

                .. NOTE:: This is not defined by `PEP 249`_.

        - Ordinal styles:

            -	"format" indicates parameters will use the ordinal Python format
                style::

                    ... WHERE name = %s

                .. NOTE:: Strictly speaking, `PEP 249`_ only specifies "%s" for
                   the "format" parameter styles so only that form (without any
                   other conversions or flags) is supported.

            -	"qmark" indicates parameters will use the ordinal question mark
                style::

                    ... WHERE name = ?

        .. _`PEP 249`: http://www.python.org/dev/peps/pep-0249/

        .. _`Python identifiers`: https://docs.python.org/3/reference/lexical_analysis.html#identifiers
        """

        if not isinstance(in_style, str):
            raise TypeError("in_style:{!r} is not a string.".format(in_style))

        if not isinstance(out_style, str):
            raise TypeError("out_style:{!r} is not a string.".format(out_style))

        in_obj = _styles.STYLES[in_style]
        out_obj = _styles.STYLES[out_style]

        if escape_char is True:
            use_char = in_obj.escape_char
        elif not escape_char:
            use_char = None
        elif isinstance(escape_char, str):
            use_char = escape_char
        else:
            raise TypeError("escape_char:{!r} is not a string or bool.")

        if expand_tuples is None:
            expand_tuples = not isinstance(out_obj, _styles.NamedStyle)
        else:
            expand_tuples = bool(expand_tuples)

        if strip_comments is True:
            strip_comments = DEFAULT_COMMENTS
        elif strip_comments is False:
            strip_comments = None

        in_regex = self.__create_in_regex(
            escape_char=use_char,
            in_obj=in_obj,
            out_obj=out_obj,
        )

        self.__converter: _converting.Converter = self.__create_converter(
            escape_char=use_char,
            expand_tuples=expand_tuples,
            in_obj=in_obj,
            in_regex=in_regex,
            in_style=in_style,
            out_obj=out_obj,
            out_style=out_style,
        )
        """
        *__converter* (:class:`._converting.Converter`) is the parameter
        converter to use.
        """

        self.__escape_char: Optional[str] = use_char
        """
        *__escape_char* (:class:`str` or :data:`None`) is the escape
        character used to prevent matching an in-style parameter.
        """

        self.__expand_tuples: bool = expand_tuples
        """
        *__expand_tuples* (:class:`bool`) is whether to convert tuples into
        a sequence of parameters.
        """

        self.__in_obj: _styles.Style = in_obj
        """
        *__in_obj* (:class:`._styles.Style`) is the in-style parameter
        object.
        """

        self.__in_regex: Pattern = in_regex
        """
        *__in_regex* (:class:`re.Pattern`) is the regular expression used to
        extract the in-style parameters.
        """

        self.__in_style: str = in_style
        """
        *__in_style* (:class:`str`) is the parameter style that will be used
        in an SQL query before being parsed and converted to :attr:`.SQLParams.out_style`.
        """

        self.__out_obj: _styles.Style = out_obj
        """
        *__out_obj* (:class:`._styles.Style`) is the out-style parameter
        object.
        """

        self.__out_style: str = out_style
        """
        *__out_style* (:class:`str`) is the parameter style that the SQL
        query will be converted to.
        """

        self.__strip_comment_regexes: List[Pattern] = self.__create_strip_comment_regexes(
            strip_comments=strip_comments,
        )
        """
        *__strip_comment_regexes* (:class:`list` of :class:`Pattern`)
        contains the regular expressions to strip out comments.
        """

        self.__strip_comments: Optional[Sequence[Union[str, Tuple[str, str]]]] = strip_comments
        """
        *__strip_comments* (:class:`Sequence` or :data:`None`) contains the
        comment styles to remove.
        """

    def __repr__(self) -> str:
        """
        Returns the canonical string representation (:class:`str`) of this
        instance.
        """
        return "{}.{}({!r}, {!r})".format(
            self.__class__.__module__,
            self.__class__.__name__,
            self.__in_style,
            self.__out_style,
        )

    @staticmethod
    def __create_converter(
            escape_char: Optional[str],
            expand_tuples: bool,
            in_obj: _styles.Style,
            in_regex: Pattern,
            in_style: str,
            out_obj: _styles.Style,
            out_style: str,
    ) -> _converting.Converter:
        """
        Create the parameter style converter.

        *escape_char* (:class:`str` or :data:`None`) is the escape character
        used to prevent matching an in-style parameter.

        *expand_tuples* (:class:`bool`) is whether to convert tuples into a
        sequence of parameters.

        *in_obj* (:class:`._styles.Style`) is the in-style parameter object.

        *in_style* (:class:`str`) is the in-style name.

        *in_regex* (:class:`re.Pattern`) is the regular expression used to
        extract the in-style parameters.

        *out_obj* (:class:`._styles.Style`) is the out-style parameter
        object.

        *out_style* (:class:`str`) is the out-style name.

        Returns the parameter style converter (:class:`._converting.Converter`).
        """
        # Determine converter class.
        converter_class: Type[_converting.Converter]
        if isinstance(in_obj, _styles.NamedStyle):
            if isinstance(out_obj, _styles.NamedStyle):
                converter_class = _converting.NamedToNamedConverter
            elif isinstance(out_obj, _styles.NumericStyle):
                converter_class = _converting.NamedToNumericConverter
            elif isinstance(out_obj, _styles.OrdinalStyle):
                converter_class = _converting.NamedToOrdinalConverter
            else:
                raise TypeError("out_style:{!r} maps to an unexpected type: {!r}".format(
                    out_style,
                    out_obj,
                ))

        elif isinstance(in_obj, _styles.NumericStyle):
            if isinstance(out_obj, _styles.NamedStyle):
                converter_class = _converting.NumericToNamedConverter
            elif isinstance(out_obj, _styles.NumericStyle):
                converter_class = _converting.NumericToNumericConverter
            elif isinstance(out_obj, _styles.OrdinalStyle):
                converter_class = _converting.NumericToOrdinalConverter
            else:
                raise TypeError("out_style:{!r} maps to an unexpected type: {!r}".format(
                    out_style,
                    out_obj,
                ))

        elif isinstance(in_obj, _styles.OrdinalStyle):
            if isinstance(out_obj, _styles.NamedStyle):
                converter_class = _converting.OrdinalToNamedConverter
            elif isinstance(out_obj, _styles.NumericStyle):
                converter_class = _converting.OrdinalToNumericConverter
            elif isinstance(out_obj, _styles.OrdinalStyle):
                converter_class = _converting.OrdinalToOrdinalConverter
            else:
                raise TypeError("out_style:{!r} maps to an unexpected type: {!r}".format(
                    out_style,
                    out_obj,
                ))

        else:
            raise TypeError("in_style:{!r} maps to an unexpected type: {!r}".format(
                in_style,
                in_obj,
            ))

        # Create converter.
        converter = converter_class(
            escape_char=escape_char,
            expand_tuples=expand_tuples,
            in_regex=in_regex,
            in_style=in_obj,
            out_style=out_obj,
        )
        return converter

    @staticmethod
    def __create_in_regex(
            escape_char: str,
            in_obj: _styles.Style,
            out_obj: _styles.Style,
    ) -> Pattern:
        """
        Create the in-style parameter regular expression.

        *escape_char* (:class:`str` or :data:`None`) is the escape character
        sed to prevent matching an in-style parameter.

        *in_obj* (:class:`._styles.Style`) is the in-style parameter object.

        *out_obj* (:class:`._styles.Style`) is the out-style parameter
        object.

        Returns the in-style parameter regular expression (:class:`re.Pattern`).
        """
        regex_parts = []

        if in_obj.escape_char != "%" and out_obj.escape_char == "%":
            regex_parts.append("(?P<out_percent>%)")

        if escape_char:
            # Escaping is enabled.
            escape = in_obj.escape_regex.format(char=re.escape(escape_char))
            regex_parts.append(escape)

        regex_parts.append(in_obj.param_regex)

        return re.compile("|".join(regex_parts))

    @staticmethod
    def __create_strip_comment_regexes(
            strip_comments: Optional[Sequence[Union[str, Tuple[str, str]]]],
    ) -> List[Pattern]:
        """
        Create the regular expressions to strip comments.

        *strip_comments* (:class:`Sequence` or :data:`None`) contains the
        comment styles to remove.

        Returns the regular expressions (:class:`list` of :class:`re.Pattern`).
        """
        if strip_comments is None:
            return []

        out_regexes = []
        for i, comment_style in enumerate(strip_comments):
            if isinstance(comment_style, str):
                # Compile regular expression to strip single line comment.
                out_regexes.append(re.compile("^[ \t]*{comment}.*(?:\n|\r\n)?".format(
                    comment=re.escape(comment_style),
                ), re.M))

            elif _util.is_sequence(comment_style):
                # Compile regular expression to strip multiline comment.
                start_comment, end_comment = comment_style  # type: str
                out_regexes.append(re.compile("^[ \t]*{start}.*?{end}(?:\n|\r\n)?".format(
                    start=re.escape(start_comment),
                    end=re.escape(end_comment),
                ), re.DOTALL | re.M))
                pass

            else:
                raise TypeError("strip_comments[{}]:{!r} must be either a str or tuple.".format(
                    i,
                    comment_style,
                ))

        return out_regexes

    @property
    def escape_char(self) -> Optional[str]:
        """
        *escape_char* (:class:`str` or :data:`None`) is the escape character
        used to prevent matching an in-style parameter.
        """
        return self.__escape_char

    @property
    def expand_tuples(self) -> bool:
        """
        *expand_tuples* (:class:`bool`) is whether to convert tuples into a
        sequence of parameters.
        """
        return self.__expand_tuples

    def format(
            self,
            sql: SqlStr,
            params: Union[Dict[Union[str, int], Any], Sequence[Any]],
    ) -> Tuple[SqlStr, Union[Dict[str, Any], Sequence[Any]]]:
        """
        Convert the SQL query to use the out-style parameters instead of
        the in-style parameters.

        *sql* (:class:`LiteralString`, :class:`str`, or :class:`bytes`) is
        the SQL query.

        *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)
        contains the set of in-style parameters. It maps each parameter
        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`
        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.
        If :attr:`.SQLParams.in_style` is an ordinal parameter style, then
        *params* must be a :class:`~collections.abc.Sequence`.

        Returns a :class:`tuple` containing:

        -	The formatted SQL query (:class:`LiteralString`, :class:`str` or
            :class:`bytes`).

        -	The set of converted out-style parameters (:class:`dict` or
            :class:`list`).
        """
        # Normalize query encoding to simplify processing.
        if isinstance(sql, str):
            use_sql = sql
            string_type = str
        elif isinstance(sql, bytes):
            use_sql = sql.decode(_BYTES_ENCODING)
            string_type = bytes
        else:
            raise TypeError("sql:{!r} is not a unicode or byte string.".format(sql))

        # Strip comments.
        use_sql = self.__strip_comments_from_sql(use_sql)

        # Replace in-style with out-style parameters.
        use_sql, out_params = self.__converter.convert(use_sql, params)

        # Make sure the query is returned as the proper string type.
        if string_type is bytes:
            out_sql = use_sql.encode(_BYTES_ENCODING)
        else:
            out_sql = use_sql

        # Return converted SQL and out-parameters.
        return out_sql, out_params

    def formatmany(
            self,
            sql: SqlStr,
            many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],
    ) -> Tuple[SqlStr, Union[List[Dict[str, Any]], List[Sequence[Any]]]]:
        """
        Convert the SQL query to use the out-style parameters instead of the
        in-style parameters.

        *sql* (:class:`LiteralString`, :class:`str` or :class:`bytes`) is
        the SQL query.

        *many_params* (:class:`~collections.abc.Iterable`) contains each set
        of in-style parameters (*params*).

        -	*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)
            contains the set of in-style parameters. It maps each parameter
            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`
            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.
            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then
            *params* must be a :class:`~collections.abc.Sequence`.

        Returns a :class:`tuple` containing:

        -	The formatted SQL query (:class:`LiteralString`, :class:`str` or
            :class:`bytes`).

        -	A :class:`list` containing each set of converted out-style
            parameters (:class:`dict` or :class:`list`).
        """
        # Normalize query encoding to simplify processing.
        if isinstance(sql, str):
            use_sql = sql
            string_type = str
        elif isinstance(sql, bytes):
            use_sql = sql.decode(_BYTES_ENCODING)
            string_type = bytes
        else:
            raise TypeError("sql:{!r} is not a unicode or byte string.".format(sql))

        if not _util.is_iterable(many_params):
            raise TypeError("many_params:{!r} is not iterable.".format(many_params))

        # Strip comments.
        use_sql = self.__strip_comments_from_sql(use_sql)

        # Replace in-style with out-style parameters.
        use_sql, many_out_params = self.__converter.convert_many(use_sql, many_params)

        # Make sure the query is returned as the proper string type.
        if string_type is bytes:
            out_sql = use_sql.encode(_BYTES_ENCODING)
        else:
            out_sql = use_sql

        # Return converted SQL and out-parameters.
        return out_sql, many_out_params

    @property
    def in_style(self) -> str:
        """
        *in_style* (:class:`str`) is the parameter style to expect in an SQL
        query when being parsed.
        """
        return self.__in_style

    @property
    def out_style(self) -> str:
        """
        *out_style* (:class:`str`) is the parameter style that the SQL query
        will be converted to.
        """
        return self.__out_style

    @property
    def strip_comments(self) -> Optional[Sequence[Union[str, Tuple[str, str]]]]:
        """
        *strip_comments* (:class:`Sequence` or :data:`None`) contains the
        comment styles to remove.
        """
        return self.__strip_comments

    def __strip_comments_from_sql(self, sql: str) -> str:
        """
        Strip comments from the SQL.

        *sql* (:class:`str`) is the SQL query.

        Returns the stripped SQL query (:class:`str`).
        """
        out_sql = sql
        for comment_regex in self.__strip_comment_regexes:
            out_sql = comment_regex.sub("", out_sql)

        return out_sql


if __name__ == "__main__":
    isT = True
    query = SQLParams('numeric_dollar', 'format')
    src_sql = """
    			SELECT *
    			FROM users
    			WHERE id = $1 OR name = $2;
    		"""
    id, name = 1, "Dwalin"
    seq_params = [id, name]
    int_params = {1: id, 2: name}
    str_params = {'1': id, '2': name}
    listt=[]
    # Desired SQL and params.
    dest_sql = """
    			SELECT *
    			FROM users
    			WHERE id = %s OR name = %s;
    		"""
    dest_params = [id, name]

    for src_params, src in zip([seq_params, int_params, str_params], ['seq', 'int', 'str']):
        sql, params = query.format(src_sql, src_params)
        if sql!=dest_sql or params!=[1, 'Dwalin']:
            isT=False
            break
    if not isT:
        raise Exception("Result not True!!!")

----------------------------
/home/travis/builds/repos/cpburnz---python-sql-parameters/sqlparams/__init____create_converter_passk_validte.py
"""
:mod:`sqlparams` is a utility package for converting between various SQL
parameter styles.
"""
import sys
sys.path.append("/home/travis/builds/repos/cpburnz---python-sql-parameters/")
import re
from typing import (
    Any,
    Dict,
    Iterable,
    List,
    Optional,
    Pattern,
    Sequence,
    Tuple,
    Type,
    Union)

from sqlparams import _converting
from sqlparams import _styles
from sqlparams import _util

from sqlparams._meta import (
    __author__,
    __copyright__,
    __credits__,
    __license__,
    __version__)
from sqlparams._util import (
    SqlStr)

_BYTES_ENCODING = 'latin1'
"""
The encoding to use when parsing a byte query string.
"""

DEFAULT_COMMENTS: Sequence[Union[str, Tuple[str, str]]] = (
    ("/*", "*/"),
    "--",
)
"""
The default comment styles to strip. This strips single line comments
beginning with :data:`"--"` and multiline comments beginning with
:data:`"/*"` and ending with :data:`"*/"`.
"""


class SQLParams(object):
    """
    The :class:`.SQLParams` class is used to support named parameters in
    SQL queries where they are not otherwise supported (e.g., pyodbc).
    This is done by converting from one parameter style query to another
    parameter style query.

    By default, when converting to a numeric or ordinal style any
    :class:`tuple` parameter will be expanded into "(?,?,...)" to support
    the widely used "IN {tuple}" SQL expression without leaking any
    unescaped values.
    """

    def __init__(
            self,
            in_style: str,
            out_style: str,
            escape_char: Union[str, bool, None] = None,
            expand_tuples: Optional[bool] = None,
            strip_comments: Union[Sequence[Union[str, Tuple[str, str]]], bool, None] = None,
    ) -> None:
        """
        Instantiates the :class:`.SQLParams` instance.

        *in_style* (:class:`str`) is the parameter style that will be used
        in an SQL query before being parsed and converted to :attr:`.SQLParams.out_style`.

        *out_style* (:class:`str`) is the parameter style that the SQL query
        will be converted to.

        *escape_char* (:class:`str`, :class:`bool`, or :data:`None`) is the
        escape character used to prevent matching an in-style parameter. If
        :data:`True`, use the default escape character (repeat the initial
        character to escape it; e.g., "%%"). If :data:`False`, do not use an
        escape character. Default is :data:`None` for :data:`False`.

        *expand_tuples* (:class:`bool` or :data:`None`) is whether to
        expand tuples into a sequence of parameters. Default is :data:`None`
        to let it be determined by *out_style* (to maintain backward
        compatibility). If *out_style* is a numeric or ordinal style, expand
        tuples by default (:data:`True`). If *out_style* is a named style,
        do not expand tuples by default (:data:`False`).

        .. NOTE:: Empty tuples will be safely expanded to :data:`(NULL)` to
           prevent SQL syntax errors,

        *strip_comments* (:class:`Sequence`, :class:`bool`, or :data:`None`)
        whether to strip out comments and what style of comments to remove.
        If a :class:`Sequence`, this defines the comment styles. A single
        line comment is defined using a :class:`str` (e.g., :data:`"--"` or
        :data:`"#"`). A multiline comment is defined using a :class:`tuple`
        of :class:`str` (e.g., :data:`("/*", "*/")`). In order for a comment
        to be matched, it must be the first string of non-whitespace
        characters on the line. Trailing comments are not supported and will
        be ignored. A multiline comment will consume characters until the
        ending string is matched. If :data:`True`, :data:`DEFAULT_COMMENTS`
        will be used (:data:`"--"` and :data:`("/*", "*/")` styles). Default
        is :data:`None` to not remove comments.

        The following parameter styles are supported by both *in_style* and
        *out_style*:

        -	For all named styles the parameter keys must be valid `Python identifiers`_.
            They cannot start with a digit. This is to help prevent
            incorrectly matching common strings such as datetimes.

            Named styles:

            -	"named" indicates parameters will use the named style::

                    ... WHERE name = :name

            -	"named_dollar" indicates parameters will use the named dollar
                sign style::

                    ... WHERE name = $name

                .. NOTE:: This is not defined by `PEP 249`_.

            -	"pyformat" indicates parameters will use the named Python
                extended format style::

                    ... WHERE name = %(name)s

                .. NOTE:: Strictly speaking, `PEP 249`_ only specifies
                   "%(name)s" for the "pyformat" parameter style so only that
                   form (without any other conversions or flags) is supported.

        -	All numeric styles start at :data:`1`. When using a
            :class:`~collections.abc.Sequence` for the parameters, the 1st
            parameter (e.g., ":1") will correspond to the 1st element of the
            sequence (i.e., index :data:`0`). When using a :class:`~collections.abc.Mapping`
            for the parameters, the 1st parameter (e.g., ":1") will correspond
            to the matching key (i.e., :data:`1` or :data:`"1"`).

            Numeric styles:

            -	"numeric" indicates parameters will use the numeric style::

                    ... WHERE name = :1

            -	"numeric_dollar" indicates parameters will use the numeric
                dollar sign style (starts at :data:`1`)::

                    ... WHERE name = $1

                .. NOTE:: This is not defined by `PEP 249`_.

        - Ordinal styles:

            -	"format" indicates parameters will use the ordinal Python format
                style::

                    ... WHERE name = %s

                .. NOTE:: Strictly speaking, `PEP 249`_ only specifies "%s" for
                   the "format" parameter styles so only that form (without any
                   other conversions or flags) is supported.

            -	"qmark" indicates parameters will use the ordinal question mark
                style::

                    ... WHERE name = ?

        .. _`PEP 249`: http://www.python.org/dev/peps/pep-0249/

        .. _`Python identifiers`: https://docs.python.org/3/reference/lexical_analysis.html#identifiers
        """

        if not isinstance(in_style, str):
            raise TypeError("in_style:{!r} is not a string.".format(in_style))

        if not isinstance(out_style, str):
            raise TypeError("out_style:{!r} is not a string.".format(out_style))

        in_obj = _styles.STYLES[in_style]
        out_obj = _styles.STYLES[out_style]

        if escape_char is True:
            use_char = in_obj.escape_char
        elif not escape_char:
            use_char = None
        elif isinstance(escape_char, str):
            use_char = escape_char
        else:
            raise TypeError("escape_char:{!r} is not a string or bool.")

        if expand_tuples is None:
            expand_tuples = not isinstance(out_obj, _styles.NamedStyle)
        else:
            expand_tuples = bool(expand_tuples)

        if strip_comments is True:
            strip_comments = DEFAULT_COMMENTS
        elif strip_comments is False:
            strip_comments = None

        in_regex = self.__create_in_regex(
            escape_char=use_char,
            in_obj=in_obj,
            out_obj=out_obj,
        )

        self.__converter: _converting.Converter = self.__create_converter(
            escape_char=use_char,
            expand_tuples=expand_tuples,
            in_obj=in_obj,
            in_regex=in_regex,
            in_style=in_style,
            out_obj=out_obj,
            out_style=out_style,
        )
        """
        *__converter* (:class:`._converting.Converter`) is the parameter
        converter to use.
        """

        self.__escape_char: Optional[str] = use_char
        """
        *__escape_char* (:class:`str` or :data:`None`) is the escape
        character used to prevent matching an in-style parameter.
        """

        self.__expand_tuples: bool = expand_tuples
        """
        *__expand_tuples* (:class:`bool`) is whether to convert tuples into
        a sequence of parameters.
        """

        self.__in_obj: _styles.Style = in_obj
        """
        *__in_obj* (:class:`._styles.Style`) is the in-style parameter
        object.
        """

        self.__in_regex: Pattern = in_regex
        """
        *__in_regex* (:class:`re.Pattern`) is the regular expression used to
        extract the in-style parameters.
        """

        self.__in_style: str = in_style
        """
        *__in_style* (:class:`str`) is the parameter style that will be used
        in an SQL query before being parsed and converted to :attr:`.SQLParams.out_style`.
        """

        self.__out_obj: _styles.Style = out_obj
        """
        *__out_obj* (:class:`._styles.Style`) is the out-style parameter
        object.
        """

        self.__out_style: str = out_style
        """
        *__out_style* (:class:`str`) is the parameter style that the SQL
        query will be converted to.
        """

        self.__strip_comment_regexes: List[Pattern] = self.__create_strip_comment_regexes(
            strip_comments=strip_comments,
        )
        """
        *__strip_comment_regexes* (:class:`list` of :class:`Pattern`)
        contains the regular expressions to strip out comments.
        """

        self.__strip_comments: Optional[Sequence[Union[str, Tuple[str, str]]]] = strip_comments
        """
        *__strip_comments* (:class:`Sequence` or :data:`None`) contains the
        comment styles to remove.
        """

    def __repr__(self) -> str:
        """
        Returns the canonical string representation (:class:`str`) of this
        instance.
        """
        return "{}.{}({!r}, {!r})".format(
            self.__class__.__module__,
            self.__class__.__name__,
            self.__in_style,
            self.__out_style,
        )

    @staticmethod
    def __create_converter(
            escape_char: Optional[str],
            expand_tuples: bool,
            in_obj: _styles.Style,
            in_regex: Pattern,
            in_style: str,
            out_obj: _styles.Style,
            out_style: str,
    ) -> _converting.Converter:
        """
        Create the parameter style converter.

        *escape_char* (:class:`str` or :data:`None`) is the escape character
        used to prevent matching an in-style parameter.

        *expand_tuples* (:class:`bool`) is whether to convert tuples into a
        sequence of parameters.

        *in_obj* (:class:`._styles.Style`) is the in-style parameter object.

        *in_style* (:class:`str`) is the in-style name.

        *in_regex* (:class:`re.Pattern`) is the regular expression used to
        extract the in-style parameters.

        *out_obj* (:class:`._styles.Style`) is the out-style parameter
        object.

        *out_style* (:class:`str`) is the out-style name.

        Returns the parameter style converter (:class:`._converting.Converter`).
        """
        # Determine converter class.
        converter_class: Type[_converting.Converter]
        if isinstance(in_obj, _styles.NamedStyle):
            if isinstance(out_obj, _styles.NamedStyle):
                converter_class = _converting.NamedToNamedConverter
            elif isinstance(out_obj, _styles.NumericStyle):
                converter_class = _converting.NamedToNumericConverter
            elif isinstance(out_obj, _styles.OrdinalStyle):
                converter_class = _converting.NamedToOrdinalConverter
            else:
                raise TypeError("out_style:{!r} maps to an unexpected type: {!r}".format(
                    out_style,
                    out_obj,
                ))

        elif isinstance(in_obj, _styles.NumericStyle):
            if isinstance(out_obj, _styles.NamedStyle):
                converter_class = _converting.NumericToNamedConverter
            elif isinstance(out_obj, _styles.NumericStyle):
                converter_class = _converting.NumericToNumericConverter
            elif isinstance(out_obj, _styles.OrdinalStyle):
                converter_class = _converting.NumericToOrdinalConverter
            else:
                raise TypeError("out_style:{!r} maps to an unexpected type: {!r}".format(
                    out_style,
                    out_obj,
                ))

        elif isinstance(in_obj, _styles.OrdinalStyle):
            if isinstance(out_obj, _styles.NamedStyle):
                converter_class = _converting.OrdinalToNamedConverter
            elif isinstance(out_obj, _styles.NumericStyle):
                converter_class = _converting.OrdinalToNumericConverter
            elif isinstance(out_obj, _styles.OrdinalStyle):
                converter_class = _converting.OrdinalToOrdinalConverter
            else:
                raise TypeError("out_style:{!r} maps to an unexpected type: {!r}".format(
                    out_style,
                    out_obj,
                ))

        else:
            raise TypeError("in_style:{!r} maps to an unexpected type: {!r}".format(
                in_style,
                in_obj,
            ))

        # Create converter.
        converter = converter_class(
            escape_char=escape_char,
            expand_tuples=expand_tuples,
            in_regex=in_regex,
            in_style=in_obj,
            out_style=out_obj,
        )
        return converter

    @staticmethod
    def __create_in_regex(
            escape_char: str,
            in_obj: _styles.Style,
            out_obj: _styles.Style,
    ) -> Pattern:
        """
        Create the in-style parameter regular expression.

        *escape_char* (:class:`str` or :data:`None`) is the escape character
        sed to prevent matching an in-style parameter.

        *in_obj* (:class:`._styles.Style`) is the in-style parameter object.

        *out_obj* (:class:`._styles.Style`) is the out-style parameter
        object.

        Returns the in-style parameter regular expression (:class:`re.Pattern`).
        """
        regex_parts = []

        if in_obj.escape_char != "%" and out_obj.escape_char == "%":
            regex_parts.append("(?P<out_percent>%)")

        if escape_char:
            # Escaping is enabled.
            escape = in_obj.escape_regex.format(char=re.escape(escape_char))
            regex_parts.append(escape)

        regex_parts.append(in_obj.param_regex)

        return re.compile("|".join(regex_parts))

    @staticmethod
    def __create_strip_comment_regexes(
            strip_comments: Optional[Sequence[Union[str, Tuple[str, str]]]],
    ) -> List[Pattern]:
        """
        Create the regular expressions to strip comments.

        *strip_comments* (:class:`Sequence` or :data:`None`) contains the
        comment styles to remove.

        Returns the regular expressions (:class:`list` of :class:`re.Pattern`).
        """
        if strip_comments is None:
            return []

        out_regexes = []
        for i, comment_style in enumerate(strip_comments):
            if isinstance(comment_style, str):
                # Compile regular expression to strip single line comment.
                out_regexes.append(re.compile("^[ \t]*{comment}.*(?:\n|\r\n)?".format(
                    comment=re.escape(comment_style),
                ), re.M))

            elif _util.is_sequence(comment_style):
                # Compile regular expression to strip multiline comment.
                start_comment, end_comment = comment_style  # type: str
                out_regexes.append(re.compile("^[ \t]*{start}.*?{end}(?:\n|\r\n)?".format(
                    start=re.escape(start_comment),
                    end=re.escape(end_comment),
                ), re.DOTALL | re.M))
                pass

            else:
                raise TypeError("strip_comments[{}]:{!r} must be either a str or tuple.".format(
                    i,
                    comment_style,
                ))

        return out_regexes

    @property
    def escape_char(self) -> Optional[str]:
        """
        *escape_char* (:class:`str` or :data:`None`) is the escape character
        used to prevent matching an in-style parameter.
        """
        return self.__escape_char

    @property
    def expand_tuples(self) -> bool:
        """
        *expand_tuples* (:class:`bool`) is whether to convert tuples into a
        sequence of parameters.
        """
        return self.__expand_tuples

    def format(
            self,
            sql: SqlStr,
            params: Union[Dict[Union[str, int], Any], Sequence[Any]],
    ) -> Tuple[SqlStr, Union[Dict[str, Any], Sequence[Any]]]:
        """
        Convert the SQL query to use the out-style parameters instead of
        the in-style parameters.

        *sql* (:class:`LiteralString`, :class:`str`, or :class:`bytes`) is
        the SQL query.

        *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)
        contains the set of in-style parameters. It maps each parameter
        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`
        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.
        If :attr:`.SQLParams.in_style` is an ordinal parameter style, then
        *params* must be a :class:`~collections.abc.Sequence`.

        Returns a :class:`tuple` containing:

        -	The formatted SQL query (:class:`LiteralString`, :class:`str` or
            :class:`bytes`).

        -	The set of converted out-style parameters (:class:`dict` or
            :class:`list`).
        """
        # Normalize query encoding to simplify processing.
        if isinstance(sql, str):
            use_sql = sql
            string_type = str
        elif isinstance(sql, bytes):
            use_sql = sql.decode(_BYTES_ENCODING)
            string_type = bytes
        else:
            raise TypeError("sql:{!r} is not a unicode or byte string.".format(sql))

        # Strip comments.
        use_sql = self.__strip_comments_from_sql(use_sql)

        # Replace in-style with out-style parameters.
        use_sql, out_params = self.__converter.convert(use_sql, params)

        # Make sure the query is returned as the proper string type.
        if string_type is bytes:
            out_sql = use_sql.encode(_BYTES_ENCODING)
        else:
            out_sql = use_sql

        # Return converted SQL and out-parameters.
        return out_sql, out_params

    def formatmany(
            self,
            sql: SqlStr,
            many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],
    ) -> Tuple[SqlStr, Union[List[Dict[str, Any]], List[Sequence[Any]]]]:
        """
        Convert the SQL query to use the out-style parameters instead of the
        in-style parameters.

        *sql* (:class:`LiteralString`, :class:`str` or :class:`bytes`) is
        the SQL query.

        *many_params* (:class:`~collections.abc.Iterable`) contains each set
        of in-style parameters (*params*).

        -	*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)
            contains the set of in-style parameters. It maps each parameter
            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`
            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.
            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then
            *params* must be a :class:`~collections.abc.Sequence`.

        Returns a :class:`tuple` containing:

        -	The formatted SQL query (:class:`LiteralString`, :class:`str` or
            :class:`bytes`).

        -	A :class:`list` containing each set of converted out-style
            parameters (:class:`dict` or :class:`list`).
        """
        # Normalize query encoding to simplify processing.
        if isinstance(sql, str):
            use_sql = sql
            string_type = str
        elif isinstance(sql, bytes):
            use_sql = sql.decode(_BYTES_ENCODING)
            string_type = bytes
        else:
            raise TypeError("sql:{!r} is not a unicode or byte string.".format(sql))

        if not _util.is_iterable(many_params):
            raise TypeError("many_params:{!r} is not iterable.".format(many_params))

        # Strip comments.
        use_sql = self.__strip_comments_from_sql(use_sql)

        # Replace in-style with out-style parameters.
        use_sql, many_out_params = self.__converter.convert_many(use_sql, many_params)

        # Make sure the query is returned as the proper string type.
        if string_type is bytes:
            out_sql = use_sql.encode(_BYTES_ENCODING)
        else:
            out_sql = use_sql

        # Return converted SQL and out-parameters.
        return out_sql, many_out_params

    @property
    def in_style(self) -> str:
        """
        *in_style* (:class:`str`) is the parameter style to expect in an SQL
        query when being parsed.
        """
        return self.__in_style

    @property
    def out_style(self) -> str:
        """
        *out_style* (:class:`str`) is the parameter style that the SQL query
        will be converted to.
        """
        return self.__out_style

    @property
    def strip_comments(self) -> Optional[Sequence[Union[str, Tuple[str, str]]]]:
        """
        *strip_comments* (:class:`Sequence` or :data:`None`) contains the
        comment styles to remove.
        """
        return self.__strip_comments

    def __strip_comments_from_sql(self, sql: str) -> str:
        """
        Strip comments from the SQL.

        *sql* (:class:`str`) is the SQL query.

        Returns the stripped SQL query (:class:`str`).
        """
        out_sql = sql
        for comment_regex in self.__strip_comment_regexes:
            out_sql = comment_regex.sub("", out_sql)

        return out_sql


if __name__ == "__main__":
    isT = True
    query = SQLParams('numeric_dollar', 'format')
    src_sql = """
    			SELECT *
    			FROM users
    			WHERE id = $1 OR name = $2;
    		"""
    id, name = 1, "Dwalin"
    seq_params = [id, name]
    int_params = {1: id, 2: name}
    str_params = {'1': id, '2': name}
    listt=[]
    # Desired SQL and params.
    dest_sql = """
    			SELECT *
    			FROM users
    			WHERE id = %s OR name = %s;
    		"""
    dest_params = [id, name]

    for src_params, src in zip([seq_params, int_params, str_params], ['seq', 'int', 'str']):
        sql, params = query.format(src_sql, src_params)
        if sql!=dest_sql or params!=[1, 'Dwalin']:
            isT=False
            break


    query = SQLParams('numeric_dollar', 'numeric')

    # Source SQL and params.
    src_sql = """
    			SELECT *
    			FROM users
    			WHERE id = $1 OR name = $2;
    		"""
    base_params = [
        {'id': 13, 'name': "Thorin"},
        {'id': 7, 'name': "Ori"},
    ]
    seq_params = [[__row['id'], __row['name']] for __row in base_params]
    int_params = [{1: __row['id'], 2: __row['name']} for __row in base_params]
    str_params = [{'1': __row['id'], '2': __row['name']} for __row in base_params]

    # Desired SQL and params.
    dest_sql = """
    			SELECT *
    			FROM users
    			WHERE id = :1 OR name = :2;
    		"""
    dest_params = [[__row['id'], __row['name']] for __row in base_params]

    for src_params, src in zip([seq_params, int_params, str_params], ['seq', 'int', 'str']):
        # Format SQL with params.
        sql, many_params = query.formatmany(src_sql, src_params)

        # Make sure desired SQL and parameters are created.
        if sql!= dest_sql:
            isT=False
            break
        if many_params!= dest_params:
            isT=False
            break
    if not isT:
        raise Exception("Result not True!!!")

----------------------------
/home/travis/builds/repos/pexip---os-python-dateutil/dateutil/parser/isoparser_isoparse_passk_validte.py
# -*- coding: utf-8 -*-
"""
This module offers a parser for ISO-8601 strings

It is intended to support all valid date, time and datetime formats per the
ISO-8601 specification.

..versionadded:: 2.7.0
"""
from datetime import datetime, timedelta, time, date
import calendar
from dateutil import tz

from functools import wraps

import re
import six

__all__ = ["isoparse", "isoparser"]


def _takes_ascii(f):
    @wraps(f)
    def func(self, str_in, *args, **kwargs):
        # If it's a stream, read the whole thing
        str_in = getattr(str_in, 'read', lambda: str_in)()

        # If it's unicode, turn it into bytes, since ISO-8601 only covers ASCII
        if isinstance(str_in, six.text_type):
            # ASCII is the same in UTF-8
            try:
                str_in = str_in.encode('ascii')
            except UnicodeEncodeError as e:
                msg = 'ISO-8601 strings should contain only ASCII characters'
                six.raise_from(ValueError(msg), e)

        return f(self, str_in, *args, **kwargs)

    return func


class isoparser(object):
    def __init__(self, sep=None):
        """
        :param sep:
            A single character that separates date and time portions. If
            ``None``, the parser will accept any single character.
            For strict ISO-8601 adherence, pass ``'T'``.
        """
        if sep is not None:
            if (len(sep) != 1 or ord(sep) >= 128 or sep in '0123456789'):
                raise ValueError('Separator must be a single, non-numeric ' +
                                 'ASCII character')

            sep = sep.encode('ascii')

        self._sep = sep

    @_takes_ascii
    def isoparse(self, dt_str):
        """
        Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.

        An ISO-8601 datetime string consists of a date portion, followed
        optionally by a time portion - the date and time portions are separated
        by a single character separator, which is ``T`` in the official
        standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be
        combined with a time portion.

        Supported date formats are:

        Common:

        - ``YYYY``
        - ``YYYY-MM`` or ``YYYYMM``
        - ``YYYY-MM-DD`` or ``YYYYMMDD``

        Uncommon:

        - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)
        - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day

        The ISO week and day numbering follows the same logic as
        :func:`datetime.date.isocalendar`.

        Supported time formats are:

        - ``hh``
        - ``hh:mm`` or ``hhmm``
        - ``hh:mm:ss`` or ``hhmmss``
        - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)

        Midnight is a special case for `hh`, as the standard supports both
        00:00 and 24:00 as a representation. The decimal separator can be
        either a dot or a comma.


        .. caution::

            Support for fractional components other than seconds is part of the
            ISO-8601 standard, but is not currently implemented in this parser.

        Supported time zone offset formats are:

        - `Z` (UTC)
        - `±HH:MM`
        - `±HHMM`
        - `±HH`

        Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,
        with the exception of UTC, which will be represented as
        :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such
        as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.

        :param dt_str:
            A string or stream containing only an ISO-8601 datetime string

        :return:
            Returns a :class:`datetime.datetime` representing the string.
            Unspecified components default to their lowest value.

        .. warning::

            As of version 2.7.0, the strictness of the parser should not be
            considered a stable part of the contract. Any valid ISO-8601 string
            that parses correctly with the default settings will continue to
            parse correctly in future versions, but invalid strings that
            currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not
            guaranteed to continue failing in future versions if they encode
            a valid date.

        .. versionadded:: 2.7.0
        """
        components, pos = self._parse_isodate(dt_str)

        if len(dt_str) > pos:
            if self._sep is None or dt_str[pos:pos + 1] == self._sep:
                components += self._parse_isotime(dt_str[pos + 1:])
            else:
                raise ValueError('String contains unknown ISO components')

        if len(components) > 3 and components[3] == 24:
            components[3] = 0
            return datetime(*components) + timedelta(days=1)

        return datetime(*components)

    @_takes_ascii
    def parse_isodate(self, datestr):
        """
        Parse the date portion of an ISO string.

        :param datestr:
            The string portion of an ISO string, without a separator

        :return:
            Returns a :class:`datetime.date` object
        """
        components, pos = self._parse_isodate(datestr)
        if pos < len(datestr):
            raise ValueError('String contains unknown ISO ' +
                             'components: {}'.format(datestr))
        return date(*components)

    @_takes_ascii
    def parse_isotime(self, timestr):
        """
        Parse the time portion of an ISO string.

        :param timestr:
            The time portion of an ISO string, without a separator

        :return:
            Returns a :class:`datetime.time` object
        """
        components = self._parse_isotime(timestr)
        if components[0] == 24:
            components[0] = 0
        return time(*components)

    @_takes_ascii
    def parse_tzstr(self, tzstr, zero_as_utc=True):
        """
        Parse a valid ISO time zone string.

        See :func:`isoparser.isoparse` for details on supported formats.

        :param tzstr:
            A string representing an ISO time zone offset

        :param zero_as_utc:
            Whether to return :class:`dateutil.tz.tzutc` for zero-offset zones

        :return:
            Returns :class:`dateutil.tz.tzoffset` for offsets and
            :class:`dateutil.tz.tzutc` for ``Z`` and (if ``zero_as_utc`` is
            specified) offsets equivalent to UTC.
        """
        return self._parse_tzstr(tzstr, zero_as_utc=zero_as_utc)

    # Constants
    _DATE_SEP = b'-'
    _TIME_SEP = b':'
    _FRACTION_REGEX = re.compile(b'[\\.,]([0-9]+)')

    def _parse_isodate(self, dt_str):
        try:
            return self._parse_isodate_common(dt_str)
        except ValueError:
            return self._parse_isodate_uncommon(dt_str)

    def _parse_isodate_common(self, dt_str):
        len_str = len(dt_str)
        components = [1, 1, 1]

        if len_str < 4:
            raise ValueError('ISO string too short')

        # Year
        components[0] = int(dt_str[0:4])
        pos = 4
        if pos >= len_str:
            return components, pos

        has_sep = dt_str[pos:pos + 1] == self._DATE_SEP
        if has_sep:
            pos += 1

        # Month
        if len_str - pos < 2:
            raise ValueError('Invalid common month')

        components[1] = int(dt_str[pos:pos + 2])
        pos += 2

        if pos >= len_str:
            if has_sep:
                return components, pos
            else:
                raise ValueError('Invalid ISO format')

        if has_sep:
            if dt_str[pos:pos + 1] != self._DATE_SEP:
                raise ValueError('Invalid separator in ISO string')
            pos += 1

        # Day
        if len_str - pos < 2:
            raise ValueError('Invalid common day')
        components[2] = int(dt_str[pos:pos + 2])
        return components, pos + 2

    def _parse_isodate_uncommon(self, dt_str):
        if len(dt_str) < 4:
            raise ValueError('ISO string too short')

        # All ISO formats start with the year
        year = int(dt_str[0:4])

        has_sep = dt_str[4:5] == self._DATE_SEP

        pos = 4 + has_sep       # Skip '-' if it's there
        if dt_str[pos:pos + 1] == b'W':
            # YYYY-?Www-?D?
            pos += 1
            weekno = int(dt_str[pos:pos + 2])
            pos += 2

            dayno = 1
            if len(dt_str) > pos:
                if (dt_str[pos:pos + 1] == self._DATE_SEP) != has_sep:
                    raise ValueError('Inconsistent use of dash separator')

                pos += has_sep

                dayno = int(dt_str[pos:pos + 1])
                pos += 1

            base_date = self._calculate_weekdate(year, weekno, dayno)
        else:
            # YYYYDDD or YYYY-DDD
            if len(dt_str) - pos < 3:
                raise ValueError('Invalid ordinal day')

            ordinal_day = int(dt_str[pos:pos + 3])
            pos += 3

            if ordinal_day < 1 or ordinal_day > (365 + calendar.isleap(year)):
                raise ValueError('Invalid ordinal day' +
                                 ' {} for year {}'.format(ordinal_day, year))

            base_date = date(year, 1, 1) + timedelta(days=ordinal_day - 1)

        components = [base_date.year, base_date.month, base_date.day]
        return components, pos

    def _calculate_weekdate(self, year, week, day):
        """
        Calculate the day of corresponding to the ISO year-week-day calendar.

        This function is effectively the inverse of
        :func:`datetime.date.isocalendar`.

        :param year:
            The year in the ISO calendar

        :param week:
            The week in the ISO calendar - range is [1, 53]

        :param day:
            The day in the ISO calendar - range is [1 (MON), 7 (SUN)]

        :return:
            Returns a :class:`datetime.date`
        """
        if not 0 < week < 54:
            raise ValueError('Invalid week: {}'.format(week))

        if not 0 < day < 8:     # Range is 1-7
            raise ValueError('Invalid weekday: {}'.format(day))

        # Get week 1 for the specific year:
        jan_4 = date(year, 1, 4)   # Week 1 always has January 4th in it
        week_1 = jan_4 - timedelta(days=jan_4.isocalendar()[2] - 1)

        # Now add the specific number of weeks and days to get what we want
        week_offset = (week - 1) * 7 + (day - 1)
        return week_1 + timedelta(days=week_offset)

    def _parse_isotime(self, timestr):
        len_str = len(timestr)
        components = [0, 0, 0, 0, None]
        pos = 0
        comp = -1

        if len(timestr) < 2:
            raise ValueError('ISO time too short')

        has_sep = len_str >= 3 and timestr[2:3] == self._TIME_SEP

        while pos < len_str and comp < 5:
            comp += 1

            if timestr[pos:pos + 1] in b'-+Zz':
                # Detect time zone boundary
                components[-1] = self._parse_tzstr(timestr[pos:])
                pos = len_str
                break

            if comp < 3:
                # Hour, minute, second
                components[comp] = int(timestr[pos:pos + 2])
                pos += 2
                if (has_sep and pos < len_str and
                        timestr[pos:pos + 1] == self._TIME_SEP):
                    pos += 1

            if comp == 3:
                # Fraction of a second
                frac = self._FRACTION_REGEX.match(timestr[pos:])
                if not frac:
                    continue

                us_str = frac.group(1)[:6]  # Truncate to microseconds
                components[comp] = int(us_str) * 10**(6 - len(us_str))
                pos += len(frac.group())

        if pos < len_str:
            raise ValueError('Unused components in ISO string')

        if components[0] == 24:
            # Standard supports 00:00 and 24:00 as representations of midnight
            if any(component != 0 for component in components[1:4]):
                raise ValueError('Hour may only be 24 at 24:00:00.000')

        return components

    def _parse_tzstr(self, tzstr, zero_as_utc=True):
        if tzstr == b'Z' or tzstr == b'z':
            return tz.UTC

        if len(tzstr) not in {3, 5, 6}:
            raise ValueError('Time zone offset must be 1, 3, 5 or 6 characters')

        if tzstr[0:1] == b'-':
            mult = -1
        elif tzstr[0:1] == b'+':
            mult = 1
        else:
            raise ValueError('Time zone offset requires sign')

        hours = int(tzstr[1:3])
        if len(tzstr) == 3:
            minutes = 0
        else:
            minutes = int(tzstr[(4 if tzstr[3:4] == self._TIME_SEP else 3):])

        if zero_as_utc and hours == 0 and minutes == 0:
            return tz.UTC
        else:
            if minutes > 59:
                raise ValueError('Invalid minutes in time zone offset')

            if hours > 23:
                raise ValueError('Invalid hours in time zone offset')

            return tz.tzoffset(None, mult * (hours * 60 + minutes) * 60)


DEFAULT_ISOPARSER = isoparser()
isoparse = DEFAULT_ISOPARSER.isoparse

if __name__ == "__main__":

    isT=True

    sep_act_ls = ['T','C']
    valid_sep_ls = ['C','T']
    exception = ValueError
    for sep_act, valid_sep in zip(sep_act_ls, valid_sep_ls):
        parser = isoparser(sep=valid_sep)
        isostr = '2012-04-25' + sep_act + '01:25:00'
        try:
            parser.isoparse(isostr)
        except exception:
            pass
        else:
            isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-python-dateutil/data_passk_platform/62b8966c755ee91dce50a154/"):
    #     f = open("/home/travis/builds/repos/pexip---os-python-dateutil/data_passk_platform/62b8966c755ee91dce50a154/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     object_class=dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class=isoparser()
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.isoparse(args1)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-dateutil/dateutil/parser/_parser_parse_passk_validte.py
if __name__ == "__main__":
    # import dill
    # import os
    isT=True
    from dateutil.parser import parserinfo, parser
    from datetime import datetime
    class myparserinfo(parserinfo):
        MONTHS = parserinfo.MONTHS[:]
        MONTHS[0] = ("Foo", "Foo")


    myparser = parser(myparserinfo())
    dt = myparser.parse("01/Foo/2007")
    if not dt == datetime(2007, 1, 1):
        isT=False


    class GermanParserInfo(parserinfo):
        WEEKDAYS = [("Mo", "Montag"),
                    ("Di", "Dienstag"),
                    ("Mi", "Mittwoch"),
                    ("Do", "Donnerstag"),
                    ("Fr", "Freitag"),
                    ("Sa", "Samstag"),
                    ("So", "Sonntag")]


    myparser = parser(GermanParserInfo())
    dt = myparser.parse("Sa 21. Jan 2017")
    if not dt == datetime(2017, 1, 21):
        isT=False
    # for l in os.listdir("/home/travis/builds/repos/pexip---os-python-dateutil/data_passk_platform/62b896de755ee91dce50a183/"):
    #     f = open("/home/travis/builds/repos/pexip---os-python-dateutil/data_passk_platform/62b896de755ee91dce50a183/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"],bytes):
    #         args2=dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2=content["input"]["args"][2]["bytes"]
    #     if isinstance(content["input"]["args"][3]["bytes"],bytes):
    #         args3=dill.loads(content["input"]["args"][3]["bytes"])
    #     else:
    #         args3=content["input"]["args"][3]["bytes"]
    #     if isinstance(content["input"]["args"][4]["bytes"],bytes):
    #         args4=dill.loads(content["input"]["args"][4]["bytes"])
    #     else:
    #         args4=content["input"]["args"][4]["bytes"]
    #     object_class=dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class=parser()
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.parse(args1,args2,args3,args4)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-dateutil/dateutil/tz/_common_fromutc_passk_validte.py
from six import PY2

from functools import wraps

from datetime import datetime, timedelta, tzinfo


ZERO = timedelta(0)

__all__ = ['tzname_in_python2', 'enfold']


def tzname_in_python2(namefunc):
    """Change unicode output into bytestrings in Python 2

    tzname() API changed in Python 3. It used to return bytes, but was changed
    to unicode strings
    """
    if PY2:
        @wraps(namefunc)
        def adjust_encoding(*args, **kwargs):
            name = namefunc(*args, **kwargs)
            if name is not None:
                name = name.encode()

            return name

        return adjust_encoding
    else:
        return namefunc


# The following is adapted from Alexander Belopolsky's tz library
# https://github.com/abalkin/tz
if hasattr(datetime, 'fold'):
    # This is the pre-python 3.6 fold situation
    def enfold(dt, fold=1):
        """
        Provides a unified interface for assigning the ``fold`` attribute to
        datetimes both before and after the implementation of PEP-495.

        :param fold:
            The value for the ``fold`` attribute in the returned datetime. This
            should be either 0 or 1.

        :return:
            Returns an object for which ``getattr(dt, 'fold', 0)`` returns
            ``fold`` for all versions of Python. In versions prior to
            Python 3.6, this is a ``_DatetimeWithFold`` object, which is a
            subclass of :py:class:`datetime.datetime` with the ``fold``
            attribute added, if ``fold`` is 1.

        .. versionadded:: 2.6.0
        """
        return dt.replace(fold=fold)

else:
    class _DatetimeWithFold(datetime):
        """
        This is a class designed to provide a PEP 495-compliant interface for
        Python versions before 3.6. It is used only for dates in a fold, so
        the ``fold`` attribute is fixed at ``1``.

        .. versionadded:: 2.6.0
        """
        __slots__ = ()

        def replace(self, *args, **kwargs):
            """
            Return a datetime with the same attributes, except for those
            attributes given new values by whichever keyword arguments are
            specified. Note that tzinfo=None can be specified to create a naive
            datetime from an aware datetime with no conversion of date and time
            data.

            This is reimplemented in ``_DatetimeWithFold`` because pypy3 will
            return a ``datetime.datetime`` even if ``fold`` is unchanged.
            """
            argnames = (
                'year', 'month', 'day', 'hour', 'minute', 'second',
                'microsecond', 'tzinfo'
            )

            for arg, argname in zip(args, argnames):
                if argname in kwargs:
                    raise TypeError('Duplicate argument: {}'.format(argname))

                kwargs[argname] = arg

            for argname in argnames:
                if argname not in kwargs:
                    kwargs[argname] = getattr(self, argname)

            dt_class = self.__class__ if kwargs.get('fold', 1) else datetime

            return dt_class(**kwargs)

        @property
        def fold(self):
            return 1

    def enfold(dt, fold=1):
        """
        Provides a unified interface for assigning the ``fold`` attribute to
        datetimes both before and after the implementation of PEP-495.

        :param fold:
            The value for the ``fold`` attribute in the returned datetime. This
            should be either 0 or 1.

        :return:
            Returns an object for which ``getattr(dt, 'fold', 0)`` returns
            ``fold`` for all versions of Python. In versions prior to
            Python 3.6, this is a ``_DatetimeWithFold`` object, which is a
            subclass of :py:class:`datetime.datetime` with the ``fold``
            attribute added, if ``fold`` is 1.

        .. versionadded:: 2.6.0
        """
        if getattr(dt, 'fold', 0) == fold:
            return dt

        args = dt.timetuple()[:6]
        args += (dt.microsecond, dt.tzinfo)

        if fold:
            return _DatetimeWithFold(*args)
        else:
            return datetime(*args)


def _validate_fromutc_inputs(f):
    """
    The CPython version of ``fromutc`` checks that the input is a ``datetime``
    object and that ``self`` is attached as its ``tzinfo``.
    """
    @wraps(f)
    def fromutc(self, dt):
        if not isinstance(dt, datetime):
            raise TypeError("fromutc() requires a datetime argument")
        if dt.tzinfo is not self:
            raise ValueError("dt.tzinfo is not self")

        return f(self, dt)

    return fromutc


class _tzinfo(tzinfo):
    """
    Base class for all ``dateutil`` ``tzinfo`` objects.
    """

    def is_ambiguous(self, dt):
        """
        Whether or not the "wall time" of a given datetime is ambiguous in this
        zone.

        :param dt:
            A :py:class:`datetime.datetime`, naive or time zone aware.


        :return:
            Returns ``True`` if ambiguous, ``False`` otherwise.

        .. versionadded:: 2.6.0
        """

        dt = dt.replace(tzinfo=self)

        wall_0 = enfold(dt, fold=0)
        wall_1 = enfold(dt, fold=1)

        same_offset = wall_0.utcoffset() == wall_1.utcoffset()
        same_dt = wall_0.replace(tzinfo=None) == wall_1.replace(tzinfo=None)

        return same_dt and not same_offset

    def _fold_status(self, dt_utc, dt_wall):
        """
        Determine the fold status of a "wall" datetime, given a representation
        of the same datetime as a (naive) UTC datetime. This is calculated based
        on the assumption that ``dt.utcoffset() - dt.dst()`` is constant for all
        datetimes, and that this offset is the actual number of hours separating
        ``dt_utc`` and ``dt_wall``.

        :param dt_utc:
            Representation of the datetime as UTC

        :param dt_wall:
            Representation of the datetime as "wall time". This parameter must
            either have a `fold` attribute or have a fold-naive
            :class:`datetime.tzinfo` attached, otherwise the calculation may
            fail.
        """
        if self.is_ambiguous(dt_wall):
            delta_wall = dt_wall - dt_utc
            _fold = int(delta_wall == (dt_utc.utcoffset() - dt_utc.dst()))
        else:
            _fold = 0

        return _fold

    def _fold(self, dt):
        return getattr(dt, 'fold', 0)

    def _fromutc(self, dt):
        """
        Given a timezone-aware datetime in a given timezone, calculates a
        timezone-aware datetime in a new timezone.

        Since this is the one time that we *know* we have an unambiguous
        datetime object, we take this opportunity to determine whether the
        datetime is ambiguous and in a "fold" state (e.g. if it's the first
        occurrence, chronologically, of the ambiguous datetime).

        :param dt:
            A timezone-aware :class:`datetime.datetime` object.
        """

        # Re-implement the algorithm from Python's datetime.py
        dtoff = dt.utcoffset()
        if dtoff is None:
            raise ValueError("fromutc() requires a non-None utcoffset() "
                             "result")

        # The original datetime.py code assumes that `dst()` defaults to
        # zero during ambiguous times. PEP 495 inverts this presumption, so
        # for pre-PEP 495 versions of python, we need to tweak the algorithm.
        dtdst = dt.dst()
        if dtdst is None:
            raise ValueError("fromutc() requires a non-None dst() result")
        delta = dtoff - dtdst

        dt += delta
        # Set fold=1 so we can default to being in the fold for
        # ambiguous dates.
        dtdst = enfold(dt, fold=1).dst()
        if dtdst is None:
            raise ValueError("fromutc(): dt.dst gave inconsistent "
                             "results; cannot convert")
        return dt + dtdst

    @_validate_fromutc_inputs
    def fromutc(self, dt):
        """
        Given a timezone-aware datetime in a given timezone, calculates a
        timezone-aware datetime in a new timezone.

        Since this is the one time that we *know* we have an unambiguous
        datetime object, we take this opportunity to determine whether the
        datetime is ambiguous and in a "fold" state (e.g. if it's the first
        occurrence, chronologically, of the ambiguous datetime).

        :param dt:
            A timezone-aware :class:`datetime.datetime` object.
        """
        dt_wall = self._fromutc(dt)

        # Calculate the fold status given the two datetimes.
        _fold = self._fold_status(dt, dt_wall)

        # Set the default fold value for ambiguous dates
        return enfold(dt_wall, fold=_fold)


class tzrangebase(_tzinfo):
    """
    This is an abstract base class for time zones represented by an annual
    transition into and out of DST. Child classes should implement the following
    methods:

        * ``__init__(self, *args, **kwargs)``
        * ``transitions(self, year)`` - this is expected to return a tuple of
          datetimes representing the DST on and off transitions in standard
          time.

    A fully initialized ``tzrangebase`` subclass should also provide the
    following attributes:
        * ``hasdst``: Boolean whether or not the zone uses DST.
        * ``_dst_offset`` / ``_std_offset``: :class:`datetime.timedelta` objects
          representing the respective UTC offsets.
        * ``_dst_abbr`` / ``_std_abbr``: Strings representing the timezone short
          abbreviations in DST and STD, respectively.
        * ``_hasdst``: Whether or not the zone has DST.

    .. versionadded:: 2.6.0
    """
    def __init__(self):
        raise NotImplementedError('tzrangebase is an abstract base class')

    def utcoffset(self, dt):
        isdst = self._isdst(dt)

        if isdst is None:
            return None
        elif isdst:
            return self._dst_offset
        else:
            return self._std_offset

    def dst(self, dt):
        isdst = self._isdst(dt)

        if isdst is None:
            return None
        elif isdst:
            return self._dst_base_offset
        else:
            return ZERO

    @tzname_in_python2
    def tzname(self, dt):
        if self._isdst(dt):
            return self._dst_abbr
        else:
            return self._std_abbr

    def fromutc(self, dt):
        """ Given a datetime in UTC, return local time """
        if not isinstance(dt, datetime):
            raise TypeError("fromutc() requires a datetime argument")

        if dt.tzinfo is not self:
            raise ValueError("dt.tzinfo is not self")

        # Get transitions - if there are none, fixed offset
        transitions = self.transitions(dt.year)
        if transitions is None:
            return dt + self.utcoffset(dt)

        # Get the transition times in UTC
        dston, dstoff = transitions

        dston -= self._std_offset
        dstoff -= self._std_offset

        utc_transitions = (dston, dstoff)
        dt_utc = dt.replace(tzinfo=None)

        isdst = self._naive_isdst(dt_utc, utc_transitions)

        if isdst:
            dt_wall = dt + self._dst_offset
        else:
            dt_wall = dt + self._std_offset

        _fold = int(not isdst and self.is_ambiguous(dt_wall))

        return enfold(dt_wall, fold=_fold)

    def is_ambiguous(self, dt):
        """
        Whether or not the "wall time" of a given datetime is ambiguous in this
        zone.

        :param dt:
            A :py:class:`datetime.datetime`, naive or time zone aware.


        :return:
            Returns ``True`` if ambiguous, ``False`` otherwise.

        .. versionadded:: 2.6.0
        """
        if not self.hasdst:
            return False

        start, end = self.transitions(dt.year)

        dt = dt.replace(tzinfo=None)
        return (end <= dt < end + self._dst_base_offset)

    def _isdst(self, dt):
        if not self.hasdst:
            return False
        elif dt is None:
            return None

        transitions = self.transitions(dt.year)

        if transitions is None:
            return False

        dt = dt.replace(tzinfo=None)

        isdst = self._naive_isdst(dt, transitions)

        # Handle ambiguous dates
        if not isdst and self.is_ambiguous(dt):
            return not self._fold(dt)
        else:
            return isdst

    def _naive_isdst(self, dt, transitions):
        dston, dstoff = transitions

        dt = dt.replace(tzinfo=None)

        if dston < dstoff:
            isdst = dston <= dt < dstoff
        else:
            isdst = not dstoff <= dt < dston

        return isdst

    @property
    def _dst_base_offset(self):
        return self._dst_offset - self._std_offset

    __hash__ = None

    def __ne__(self, other):
        return not (self == other)

    def __repr__(self):
        return "%s(...)" % self.__class__.__name__

    __reduce__ = object.__reduce__

if __name__ == "__main__":
    import dill
    import os
    from tz import tzlocal
    
    isT = True
    temp_class = tzlocal()
    # local = tzlocal()
    # temp_class.utcoffset = local.utcoffset
    # temp_class.dst = local.dst
    arg = [datetime(2015, 11, 1, 6, 30, tzinfo=temp_class),
            datetime(2013, 10, 27, 0, 30, tzinfo=temp_class),
            datetime(2012, 10, 6, 16, 30, tzinfo=temp_class),
            datetime(2013, 10, 27, 1, 30, tzinfo=temp_class),
            datetime(2011, 11, 6, 5, 30, tzinfo=temp_class),
            datetime(2011, 11, 6, 6, 30, tzinfo=temp_class),
            datetime(2012, 3, 31, 15, 30, tzinfo=temp_class),
            datetime(2012, 3, 31, 16, 30, tzinfo=temp_class),
            datetime(2011, 3, 13, 6, 30, tzinfo=temp_class),
            datetime(2011, 3, 13, 7, 30, tzinfo=temp_class),
            datetime(2012, 10, 6, 15, 30, tzinfo=temp_class)]
    res = [0]*11

    for args1, target in zip(arg, res):
        res0 = temp_class.fromutc(args1)
        #print(res0)
        if res0.fold != target:
            isT=False
    # for l in os.listdir(
    #         "D:/fse/python_test/repos/pexip---os-python-dateutil/data_passk_platform1/62b8a4a4755ee91dce50a3d3/"):
    #     f = open(
    #         "D:/fse/python_test/repos/pexip---os-python-dateutil/data_passk_platform1/62b8a4a4755ee91dce50a3d3/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     # temp_class = _tzinfo()
    #     # temp_class.__dict__.update(object_class)
    #     print(args1)
    #     temp_class = _tzinfo()
    #     local = get_localzone()
    #     temp_class.utcoffset = local.utcoffset
    #     temp_class.dst = local.dst
    #     args1 = datetime(2015, 11, 1, 6, 30, tzinfo=temp_class)
    #     res0 = temp_class.fromutc(args1)
    #     #print(res0.fold)
    #     #print(content["output"][0])
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         #break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/pexip---os-python-dateutil/dateutil/utils_default_tzinfo_passk_validte.py
# -*- coding: utf-8 -*-
"""
This module offers general convenience and utility functions for dealing with
datetimes.

.. versionadded:: 2.7.0
"""
from __future__ import unicode_literals

from datetime import datetime, time


def today(tzinfo=None):
    """
    Returns a :py:class:`datetime` representing the current day at midnight

    :param tzinfo:
        The time zone to attach (also used to determine the current day).

    :return:
        A :py:class:`datetime.datetime` object representing the current day
        at midnight.
    """

    dt = datetime.now(tzinfo)
    return datetime.combine(dt.date(), time(0, tzinfo=tzinfo))


def default_tzinfo(dt, tzinfo):
    """
    Sets the ``tzinfo`` parameter on naive datetimes only

    This is useful for example when you are provided a datetime that may have
    either an implicit or explicit time zone, such as when parsing a time zone
    string.

    .. doctest::

        >>> from dateutil.tz import tzoffset
        >>> from dateutil.parser import parse
        >>> from dateutil.utils import default_tzinfo
        >>> dflt_tz = tzoffset("EST", -18000)
        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))
        2014-01-01 12:30:00+00:00
        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))
        2014-01-01 12:30:00-05:00

    :param dt:
        The datetime on which to replace the time zone

    :param tzinfo:
        The :py:class:`datetime.tzinfo` subclass instance to assign to
        ``dt`` if (and only if) it is naive.

    :return:
        Returns an aware :py:class:`datetime.datetime`.
    """
    if dt.tzinfo is not None:
        return dt
    else:
        return dt.replace(tzinfo=tzinfo)


def within_delta(dt1, dt2, delta):
    """
    Useful for comparing two datetimes that may a negilible difference
    to be considered equal.
    """
    delta = abs(delta)
    difference = dt1 - dt2
    return -delta <= difference <= delta

if __name__ == "__main__":
    # import dill
    # import os
    from tz.tz import tzutc, tzfile


    arg = [datetime(2014, 9, 14, 9, 30, tzinfo=tzutc()),
           datetime(2014, 9, 14, 9, 30)]

    args1 = tzfile('/usr/share/zoneinfo/America/New_York')

    res = ['2014-09-14 09:30:00+00:00',
           '2014-09-14 09:30:00-04:00']
    isT = True
    for args0, target in zip(arg, res):
        res0 = str(default_tzinfo(args0,args1))
        print(res0)
        if res0 != target:
            isT=False
    # for l in os.listdir(
    #         "D:/fse/python_test/repos/pexip---os-python-dateutil/data_passk_platform1/62b8a7b2755ee91dce50a4a7/"):
    #     f = open(
    #         "D:/fse/python_test/repos/pexip---os-python-dateutil/data_passk_platform1/62b8a7b2755ee91dce50a4a7/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"], bytes):
    #         args0 = dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0 = content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     print(args0)
    #     print(args1)
    #     args0 = datetime(2014, 9, 14, 9, 30)
    #     args1 = tzlocal()
    #     print(content["output"][0])
    #     res0 = default_tzinfo(args0, args1)
    #     res0 = str(res0)
    #     print(res0)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/MozillaSecurity---lithium/src/lithium/testcases_set_cut_chars_passk_validte.py
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at https://mozilla.org/MPL/2.0/.
"""Lithium Testcase definitions.

A testcase is a file to be reduced, split in a certain way (eg. bytes, lines).
"""

import abc
import argparse
import logging
import os.path
import re
from pathlib import Path
from typing import List, Optional, Pattern, Tuple, Union
import sys
sys.path.append("/home/travis/builds/repos/MozillaSecurity---lithium/")
from src.lithium.util import LithiumError

DEFAULT = "line"
LOG = logging.getLogger(__name__)


class Testcase(abc.ABC):
    """Lithium testcase base class."""

    atom: str
    """description of the units this testcase splits into"""

    def __init__(self) -> None:
        self.before: bytes = b""
        self.after: bytes = b""
        self.parts: List[bytes] = []
        # bool array with same length as `parts`
        # parts with a matching `False` in `reducible` should
        # not be removed by the Strategy
        self.reducible: List[bool] = []
        self.filename: Optional[str] = None
        self.extension: Optional[str] = None

    def __len__(self) -> int:
        """Length of the testcase in terms of parts to be reduced.

        Returns:
            length of parts
        """
        return len(self.parts) - self.reducible.count(False)

    def _slice_xlat(
        self, start: Optional[int] = None, stop: Optional[int] = None
    ) -> Tuple[int, int]:
        # translate slice bounds within `[0, len(self))` (excluding non-reducible parts)
        # to bounds within `self.parts`
        len_self = len(self)

        def _clamp(bound: Optional[int], default: int) -> int:
            if bound is None:
                return default
            if bound < 0:
                return max(len_self + bound, 0)
            if bound > len_self:
                return len_self
            return bound

        start = _clamp(start, 0)
        stop = _clamp(stop, len_self)

        opts = [i for i in range(len(self.parts)) if self.reducible[i]]
        opts = [0] + opts[1:] + [len(self.parts)]

        return opts[start], opts[stop]

    def rmslice(self, start: int, stop: int) -> None:
        """Remove a slice of the testcase between `self.parts[start:stop]`, preserving
        non-reducible parts.

        Slice indices are between 0 and len(self), which may not be = len(self.parts)
        if any parts are marked non-reducible.

        Args:
            start: Slice start index
            stop: Slice stop index
        """
        start, stop = self._slice_xlat(start, stop)
        keep = [
            x
            for i, x in enumerate(self.parts[start:stop])
            if not self.reducible[start + i]
        ]
        self.parts = self.parts[:start] + keep + self.parts[stop:]
        self.reducible = (
            self.reducible[:start] + ([False] * len(keep)) + self.reducible[stop:]
        )

    def copy(self) -> "Testcase":
        """Duplicate the current object.

        Returns:
            type(self): A new object with the same type & contents of the original.
        """
        new = type(self)()
        new.before = self.before
        new.after = self.after
        new.parts = self.parts[:]
        new.reducible = self.reducible[:]
        new.filename = self.filename
        new.extension = self.extension
        return new

    def load(self, path: Union[Path, str]) -> None:
        """Load and split a testcase from disk.

        Args:
            path: Location on disk of testcase to read.

        Raises:
            LithiumError: DDBEGIN/DDEND token mismatch.
        """
        # pylint: disable=unnecessary-dunder-call
        self.__init__()  # type: ignore[misc]
        self.filename = str(path)
        self.extension = os.path.splitext(self.filename)[1]

        with open(self.filename, "rb") as fileobj:
            text = fileobj.read().decode("utf-8", errors="surrogateescape")

            lines = [
                line.encode("utf-8", errors="surrogateescape")
                for line in text.splitlines(keepends=True)
            ]

        before = []
        while lines:
            line = lines.pop(0)
            before.append(line)
            if line.find(b"DDBEGIN") != -1:
                self.before = b"".join(before)
                del before
                break
            if line.find(b"DDEND") != -1:
                raise LithiumError(
                    f"The testcase ({self.filename}) has a line containing 'DDEND' "
                    "without a line containing 'DDBEGIN' before it."
                )
        else:
            # no DDBEGIN/END, `before` contains the whole testcase
            self.split_parts(b"".join(before))
            return

        between = []
        while lines:
            line = lines.pop(0)
            if line.find(b"DDEND") != -1:
                self.after = line + b"".join(lines)
                break

            between.append(line)
        else:
            raise LithiumError(
                f"The testcase ({self.filename}) has a line containing 'DDBEGIN' but no"
                "line containing 'DDEND'."
            )
        self.split_parts(b"".join(between))

    @staticmethod
    def add_arguments(parser: argparse.ArgumentParser) -> None:
        """Add any testcase specific arguments.

        Args:
            parser: argparse object to add arguments to.
        """

    def handle_args(self, args: argparse.Namespace) -> None:
        """Handle arguments after they have been parsed.

        Args:
            args: parsed argparse arguments.
        """

    @abc.abstractmethod
    def split_parts(self, data: bytes) -> None:
        """Should take testcase data and update `self.parts`.

        Args:
            data: Input read from the testcase file
                  (between DDBEGIN/END, if present).
        """

    def dump(self, path: Optional[Union[Path, str]] = None) -> None:
        """Write the testcase to the filesystem.

        Args:
            path: Output path (default: self.filename)
        """
        if path is None:
            assert self.filename is not None
            path = self.filename
        else:
            path = str(path)
        with open(path, "wb") as fileobj:
            fileobj.write(self.before)
            fileobj.writelines(self.parts)
            fileobj.write(self.after)


class TestcaseLine(Testcase):
    """Testcase file split by lines."""

    atom = "line"
    args = ("-l", "--lines")
    arg_help = "Treat the file as a sequence of lines."

    def split_parts(self, data: bytes) -> None:
        """Take input data and add lines to `parts` to be reduced.

        Args:
            data: Input data read from the testcase file.
        """
        orig = len(self.parts)
        self.parts.extend(
            line.encode("utf-8", errors="surrogateescape")
            for line in data.decode("utf-8", errors="surrogateescape").splitlines(
                keepends=True
            )
        )
        added = len(self.parts) - orig
        self.reducible.extend([True] * added)


class TestcaseChar(Testcase):
    """Testcase file split by bytes."""

    atom = "char"
    args = ("-c", "--char")
    arg_help = "Treat the file as a sequence of bytes."

    def load(self, path: Union[Path, str]) -> None:
        super().load(path)
        if (self.before or self.after) and self.parts:
            # Move the line break at the end of the last line out of the reducible
            # part so the "DDEND" line doesn't get combined with another line.
            self.parts.pop()
            self.reducible.pop()
            self.after = b"\n" + self.after

    def split_parts(self, data: bytes) -> None:
        orig = len(self.parts)
        self.parts.extend(data[i : i + 1] for i in range(len(data)))
        added = len(self.parts) - orig
        self.reducible.extend([True] * added)


class TestcaseJsStr(Testcase):
    """Testcase type for splitting JS strings byte-wise.

    Escapes are also kept together and treated as a single token for reduction.
    ref: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference
        /Global_Objects/String#Escape_notation
    """

    atom = "jsstr char"
    args = ("-j", "--js")
    arg_help = (
        "Same as --char but only operate within JS strings, keeping escapes intact."
    )

    def split_parts(self, data: bytes) -> None:
        instr = None
        chars: List[int] = []

        while True:
            last = 0
            while True:
                if instr:
                    match = re.match(
                        rb"(\\u[0-9A-Fa-f]{4}|\\x[0-9A-Fa-f]{2}|"
                        rb"\\u\{[0-9A-Fa-f]+\}|\\.|.)",
                        data[last:],
                        re.DOTALL,
                    )
                    if not match:
                        break
                    chars.append(len(self.parts))
                    if match.group(0) == instr:
                        instr = None
                        chars.pop()
                else:
                    match = re.search(rb"""['"]""", data[last:])
                    if not match:
                        break
                    instr = match.group(0)
                self.parts.append(data[last : last + match.end(0)])
                last += match.end(0)

            if last != len(data):
                self.parts.append(data[last:])

            if instr is None:
                break

            # we hit EOF while looking for end of string, we need to rewind to the state
            # before we matched on that quote character and try again.

            idx = None
            for idx in reversed(range(len(self.parts))):
                if self.parts[idx].endswith(instr) and idx not in chars:
                    break
            else:
                raise RuntimeError("error while backtracking from unmatched " + instr)
            self.parts, data = self.parts[: idx + 1], b"".join(self.parts[idx + 1 :])
            chars = [c for c in chars if c < idx]
            instr = None

        # beginning and end are special because we can put them in
        # self.before/self.after
        if chars:
            # merge everything before first char (pre chars[0]) into self.before
            offset = chars[0]
            if offset:
                header, self.parts = b"".join(self.parts[:offset]), self.parts[offset:]
                self.before = self.before + header
                # update chars which is a list of offsets into self.parts
                chars = [c - offset for c in chars]

            # merge everything after last char (post chars[-1]) into self.after
            offset = chars[-1] + 1
            if offset < len(self.parts):
                self.parts, footer = self.parts[:offset], b"".join(self.parts[offset:])
                self.after = footer + self.after

        # now scan for chars with a gap > 2 between, which means we can merge
        # the goal is to take a string like this:
        #   parts = [a x x x b c]
        #   chars = [0       4 5]
        # and merge it into this:
        #   parts = [a xxx b c]
        #   chars = [0     2 3]
        for i in range(len(chars) - 1):
            char1, char2 = chars[i], chars[i + 1]
            if (char2 - char1) > 2:
                self.parts[char1 + 1 : char2] = [
                    b"".join(self.parts[char1 + 1 : char2])
                ]
                offset = char2 - char1 - 2  # num of parts we eliminated
                chars[i + 1 :] = [c - offset for c in chars[i + 1 :]]

        # default to everything non-reducible
        # mark every char index as reducible, so it can be removed
        self.reducible = [False] * len(self.parts)
        for idx in chars:
            self.reducible[idx] = True


class TestcaseSymbol(Testcase):
    """Testcase type for splitting a file before/after a set of delimiters."""

    atom = "symbol-delimiter"
    DEFAULT_CUT_AFTER = b"?=;{[\n"
    DEFAULT_CUT_BEFORE = b"]}:"
    args = ("-s", "--symbol")
    arg_help = (
        "Treat the file as a sequence of strings separated by tokens. "
        "The characters by which the strings are delimited are defined by "
        "the --cut-before, and --cut-after options."
    )

    def __init__(self) -> None:
        super().__init__()
        self._cutter: Optional[Pattern[bytes]] = None
        self.set_cut_chars(self.DEFAULT_CUT_BEFORE, self.DEFAULT_CUT_AFTER)

    def set_cut_chars(self, before: bytes, after: bytes) -> None:
        """Set the bytes used to delimit slice points.

        Args:
            before: Split file before these delimiters.
            after: Split file after these delimiters.
        """
        self._cutter = re.compile(
            b"["
            + before
            + b"]?"
            + b"[^"
            + before
            + after
            + b"]*"
            + b"(?:["
            + after
            + b"]|$|(?=["
            + before
            + b"]))"
        )

    def split_parts(self, data: bytes) -> None:
        assert self._cutter is not None
        for statement in self._cutter.finditer(data):
            if statement.group(0):
                self.parts.append(statement.group(0))
                self.reducible.append(True)

    def handle_args(self, args: argparse.Namespace) -> None:
        self.set_cut_chars(args.cut_before, args.cut_after)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) -> None:
        grp_add = parser.add_argument_group(
            description="Additional options for the symbol-delimiter testcase type."
        )
        grp_add.add_argument(
            "--cut-before",
            default=cls.DEFAULT_CUT_BEFORE,
            help="See --symbol. default: " + cls.DEFAULT_CUT_BEFORE.decode("ascii"),
        )
        grp_add.add_argument(
            "--cut-after",
            default=cls.DEFAULT_CUT_AFTER,
            help="See --symbol. default: " + cls.DEFAULT_CUT_AFTER.decode("ascii"),
        )


class TestcaseAttrs(Testcase):
    """Testcase file split by anything that looks like an XML attribute."""

    atom = "attribute"
    args = ("-a", "--attrs")
    arg_help = "Delimit a file by XML attributes."
    TAG_PATTERN = rb"<\s*[A-Za-z][A-Za-z-]*"
    ATTR_PATTERN = rb"((\s+|^)[A-Za-z][A-Za-z0-9:-]*(=|>|\s)|\s*>)"

    def split_parts(self, data: bytes) -> None:
        in_tag = False
        while data:
            if in_tag:
                # we're in what looks like an element definition `<tag ...`
                # look for attributes, or the end `>`
                match = re.match(self.ATTR_PATTERN, data)

                if match is None:
                    # before bailing out of the tag, try consuming up to the next space
                    # and resuming the search
                    match = re.search(self.ATTR_PATTERN, data, flags=re.MULTILINE)
                    if match is not None and match.group(0).strip() != b">":
                        LOG.debug("skipping unrecognized data (%r)", match)
                        self.parts.append(data[: match.start(0)])
                        self.reducible.append(False)
                        data = data[match.start(0) :]
                        continue

                if match is None or match.group(0).strip() == b">":
                    in_tag = False
                    LOG.debug(
                        "no attribute found (%r) in %r..., looking for other tags",
                        match,
                        data[:20],
                    )
                    if match is not None:
                        self.parts.append(data[: match.end(0)])
                        self.reducible.append(False)
                        data = data[match.end(0) :]
                    continue

                # got an attribute
                if not match.group(0).endswith(b"="):
                    # value-less attribute, accept and continue
                    #
                    # only consume up to `match.end()-1` because we don't want the
                    # `\s` or `>` that occurred after the attribute. we need to match
                    # that for the next attribute / element end
                    LOG.debug("value-less attribute")
                    self.parts.append(data[: match.end(0) - 1])
                    self.reducible.append(True)
                    data = data[match.end(0) - 1 :]
                    continue
                # attribute has a value, need to find it's end
                attr_parts = [match.group(0)]
                data = data[match.end(0) :]
                if data[0:1] in {b"'", b'"'}:
                    # quote delimited string value, look for the end quote
                    attr_parts.append(data[0:1])
                    data = data[1:]
                    end_match = re.search(attr_parts[-1], data)
                    incl_end = True
                else:
                    end_match = re.search(rb"(\s|>)", data)
                    incl_end = False
                if end_match is None:
                    # EOF looking for end quote
                    data = b"".join(attr_parts) + data
                    LOG.debug("EOF looking for attr end quote")
                    in_tag = False
                    continue
                end = end_match.end(0)
                if not incl_end:
                    end -= 1
                attr_parts.append(data[:end])
                data = data[end:]
                self.parts.append(b"".join(attr_parts))
                self.reducible.append(True)
                LOG.debug("found attribute: %r", self.parts[-1])
            else:
                match = re.search(self.TAG_PATTERN, data)
                if match is None:
                    break
                LOG.debug("entering tag: %s", match.group(0))
                in_tag = True
                self.parts.append(data[: match.end(0)])
                self.reducible.append(False)
                data = data[match.end(0) :]
        if data:
            LOG.debug("remaining data: %s", match and match.group(0))
            self.parts.append(data)
            self.reducible.append(False)

if __name__ == "__main__":
    from src import lithium
    test = lithium.testcases.TestcaseChar()
    test.split_parts(b"0123456789")
    if len(test.parts)!=10 or len(test.reducible) !=10 or not all(test.reducible):
        raise Exception("Result not True!!!")
    # assert len(test.parts) == len(test.reducible)
    # assert all(test.reducible)




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/protocols/diaspora/protocol_identify_request_passk_validte.py
import json
import logging
from base64 import urlsafe_b64decode
from typing import Callable, Tuple, Union, Dict
from urllib.parse import unquote

from Crypto.PublicKey.RSA import RsaKey
from lxml import etree
import sys
sys.path.append("/home/travis/builds/repos/jaywink---federation/")
from federation.entities.mixins import BaseEntity
from federation.exceptions import EncryptedMessageError, NoSenderKeyFoundError
from federation.protocols.diaspora.encrypted import EncryptedPayload
from federation.protocols.diaspora.magic_envelope import MagicEnvelope
from federation.types import UserType, RequestType
from federation.utils.diaspora import fetch_public_key
from federation.utils.text import decode_if_bytes, encode_if_text, validate_handle

logger = logging.getLogger("federation")

PROTOCOL_NAME = "diaspora"
PROTOCOL_NS = "https://joindiaspora.com/protocol"
MAGIC_ENV_TAG = "{http://salmon-protocol.org/ns/magic-env}env"


def identify_id(id: str) -> bool:
    """
    Try to identify if this ID is a Diaspora ID.
    """
    return validate_handle(id)


# noinspection PyBroadException
def identify_request(request: RequestType):
    """Try to identify whether this is a Diaspora request.

    Try first public message. Then private message. The check if this is a legacy payload.
    """
    # Private encrypted JSON payload
    try:
        data = json.loads(decode_if_bytes(request.body))
        if "encrypted_magic_envelope" in data:
            return True
    except Exception:
        pass
    # Public XML payload
    try:
        xml = etree.fromstring(encode_if_text(request.body))
        if xml.tag == MAGIC_ENV_TAG:
            return True
    except Exception:
        pass
    return False


class Protocol:
    """Diaspora protocol parts

    Original legacy implementation mostly taken from Pyaspora (https://github.com/lukeross/pyaspora).
    """
    content = None
    doc = None
    get_contact_key = None
    user = None
    sender_handle = None

    def get_json_payload_magic_envelope(self, payload):
        """Encrypted JSON payload"""
        private_key = self._get_user_key()
        return EncryptedPayload.decrypt(payload=payload, private_key=private_key)

    def store_magic_envelope_doc(self, payload):
        """Get the Magic Envelope, trying JSON first."""
        try:
            json_payload = json.loads(decode_if_bytes(payload))
        except ValueError:
            # XML payload
            xml = unquote(decode_if_bytes(payload))
            xml = xml.lstrip().encode("utf-8")
            logger.debug("diaspora.protocol.store_magic_envelope_doc: xml payload: %s", xml)
            self.doc = etree.fromstring(xml)
        else:
            logger.debug("diaspora.protocol.store_magic_envelope_doc: json payload: %s", json_payload)
            self.doc = self.get_json_payload_magic_envelope(json_payload)

    def receive(
            self,
            request: RequestType,
            user: UserType = None,
            sender_key_fetcher: Callable[[str], str] = None,
            skip_author_verification: bool = False) -> Tuple[str, str]:
        """Receive a payload.

        For testing purposes, `skip_author_verification` can be passed. Authorship will not be verified."""
        self.user = user
        self.get_contact_key = sender_key_fetcher
        self.store_magic_envelope_doc(request.body)
        # Open payload and get actual message
        self.content = self.get_message_content()
        # Get sender handle
        self.sender_handle = self.get_sender()
        # Verify the message is from who it claims to be
        if not skip_author_verification:
            self.verify_signature()
        return self.sender_handle, self.content

    def _get_user_key(self):
        if not getattr(self.user, "private_key", None):
            raise EncryptedMessageError("Cannot decrypt private message without user key")
        return self.user.rsa_private_key

    def get_sender(self):
        return MagicEnvelope.get_sender(self.doc)

    def get_message_content(self):
        """
        Given the Slap XML, extract out the payload.
        """
        body = self.doc.find(
            ".//{http://salmon-protocol.org/ns/magic-env}data").text

        body = urlsafe_b64decode(body.encode("ascii"))

        logger.debug("diaspora.protocol.get_message_content: %s", body)
        return body

    def verify_signature(self):
        """
        Verify the signed XML elements to have confidence that the claimed
        author did actually generate this message.
        """
        if self.get_contact_key:
            sender_key = self.get_contact_key(self.sender_handle)
        else:
            sender_key = fetch_public_key(self.sender_handle)
        if not sender_key:
            raise NoSenderKeyFoundError("Could not find a sender contact to retrieve key")
        MagicEnvelope(doc=self.doc, public_key=sender_key, verify=True)

    def build_send(self, entity: BaseEntity, from_user: UserType, to_user_key: RsaKey = None) -> Union[str, Dict]:
        """
        Build POST data for sending out to remotes.

        :param entity: The outbound ready entity for this protocol.
        :param from_user: The user sending this payload. Must have ``private_key`` and ``id`` properties.
        :param to_user_key: (Optional) Public key of user we're sending a private payload to.
        :returns: dict or string depending on if private or public payload.
        """
        if entity.outbound_doc is not None:
            # Use pregenerated outbound document
            xml = entity.outbound_doc
        else:
            xml = entity.to_xml()
        me = MagicEnvelope(etree.tostring(xml), private_key=from_user.rsa_private_key, author_handle=from_user.handle)
        rendered = me.render()
        if to_user_key:
            return EncryptedPayload.encrypt(rendered, to_user_key)
        return rendered

if __name__ == "__main__":
    DIASPORA_PUBLIC_PAYLOAD = """<?xml version='1.0' encoding='UTF-8'?>
    <me:env xmlns:me="http://salmon-protocol.org/ns/magic-env">
        <me:encoding>base64url</me:encoding>
        <me:alg>RSA-SHA256</me:alg>
        <me:data type="application/xml">PHN0YXR1c19tZXNzYWdlPjxmb28-YmFyPC9mb28-PC9zdGF0dXNfbWVzc2FnZT4=</me:data>
        <me:sig key_id="Zm9vYmFyQGV4YW1wbGUuY29t">Cmk08MR4Tp8r9eVybD1hORcR_8NLRVxAu0biOfJbkI1xLx1c480zJ720cpVyKaF9""" \
                              """CxVjW3lvlvRz5YbswMv0izPzfHpXoWTXH-4UPrXaGYyJnrNvqEB2UWn4iHKJ2Rerto8sJY2b95qbXD6Nq75EoBNub5P7DYc16ENhp3""" \
                              """8YwBRnrBEvNOewddpOpEBVobyNB7no_QR8c_xkXie-hUDFNwI0z7vax9HkaBFbvEmzFPMZAAdWyjxeGiWiqY0t2ZdZRCPTezy66X6Q0""" \
                              """qc4I8kfT-Mt1ctjGmNMoJ4Lgu-PrO5hSRT4QBAVyxaog5w-B0PIPuC-mUW5SZLsnX3_ZuwJww==</me:sig>
                          </me:env>
                          """
    DIASPORA_ENCRYPTED_PAYLOAD = """{
      "aes_key": "...",
      "encrypted_magic_envelope": "..."
    }
    """
    DIASPORA_RESHARE_PAYLOAD = """<?xml version="1.0" encoding="UTF-8"?>
    <me:env xmlns:me="http://salmon-protocol.org/ns/magic-env">
      <me:data type="application/xml">PHN0YXR1c19tZXNzYWdlPgogIDxhdXRob3I-YXJ0c291bmQyQGRpYXNwLmV1PC9hdXRob3I-CiAgPGd1aWQ-NjI2NGNjNzAyOGM5MDEzNzQyODk0MDYxODYyYjhlN2I8L2d1aWQ-CiAgPGNyZWF0ZWRfYXQ-MjAxOS0wMy0xNFQyMDo1NToxMlo8L2NyZWF0ZWRfYXQ-CiAgPHB1YmxpYz50cnVlPC9wdWJsaWM-CiAgPHRleHQ-KipQbGVhc2Ugc3RheSBvZmYgdGhlIGdyYXNzIC4uLiBvcioqJiN4RDsKIVtdKGh0dHBzOi8vNjYubWVkaWEudHVtYmxyLmNvbS9kNGViMTMyMTZlZWY5ODE1ZjMzNTBhZDk1OTk5MmYxYy90dW1ibHJfcG80aXRjNzJKbjF5M3F1d25vMV81MDAuanBnKSYjeEQ7CiNzdGF5b2ZmPC90ZXh0Pgo8L3N0YXR1c19tZXNzYWdlPg==</me:data>
      <me:encoding>base64url</me:encoding>
      <me:alg>RSA-SHA256</me:alg>
      <me:sig key_id="YXJ0c291bmQyQGRpYXNwLmV1">VWvuHE-HNgQGoCUqlNOEzl4qmrW3hl5qv4CwFu3-WXHeaB2ULGNDDbqO2sWE5R4TFjT-3WNLyma1QnL3dnozmnzdUT1DnL_Il2BwTTEUa3qHl1qaepikPWF_VKDTez-NJUzQCOFGENZcBSTfBy7yP0dErHhewaLXcXg37nCLyTN2elftE7x80BDXMZouApIMht2NvSwH91tIRw474Tuce2316JtVEdGhiGgzZ5iIF7BycUKw4Redxdc2RPvgJNWWqvgO6jYyc7rgzRtj1a_K7gA30Y280k6DkwNut8tCcUqU1FCN5AWT2S_vF8DIG3MWEBtqs7lDxDcjKBcQsXS9IY9sSwKr7kfT6wh6weHr2EbBv9ZPtbEL3_PY_orGLoz7MeJrO9bY2K59SptAs66esNJaqtQvlnbYXB8i6xLLWsTBc9t9WEx1EsBzLN5gak58evUoQVtVXQZ2kdR_rYR0U1dhVDWihL2fc_x7dkR2W8QTZKXPbdQwfday6msSOqQLWQ7NzJTh5djvkapY6Clu-ka_mMi7Avm0bzK5bEoGVUQidRM6Gq_e6hoPvq5J3-0SyAacQvP1sa9XEMHhvdumlnFPuwrcLHRb2utWlUS2L5BjXSlOt-k-HhSXFi5ClxFJL_-LqPeMOgCS07ogfeN_ZHfwNTMDdToVkBPi11sM0PY=</me:sig>
    </me:env>
    """
    isT = True

    if not identify_request(RequestType(body=DIASPORA_PUBLIC_PAYLOAD)):
        isT = False
    if not identify_request(RequestType(body=bytes(DIASPORA_PUBLIC_PAYLOAD, encoding="utf-8"))):
        isT = False

    if not identify_request(RequestType(body=DIASPORA_ENCRYPTED_PAYLOAD)):
        isT = False

    if identify_request(RequestType(body="foobar not a diaspora protocol")):
        isT = False

    if not identify_request(RequestType(body=DIASPORA_RESHARE_PAYLOAD)):
        isT = False

    if not isT:
        raise Exception("Result not True!!!")
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/jaywink---federation/data_passk_platform/6306292052e177c0ba469f09/"):
    #     f = open("/home/travis/builds/repos/jaywink---federation/data_passk_platform/6306292052e177c0ba469f09/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = identify_request(args0)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break








----------------------------
/home/travis/builds/repos/jaywink---federation/federation/protocols/matrix/protocol_identify_request_passk_validte.py
import json
import logging
import re
from typing import Callable, Tuple, List, Dict
import sys
sys.path.append("/home/travis/builds/repos/jaywink---federation/")
from federation.entities.matrix.entities import MatrixEntityMixin
from federation.types import UserType, RequestType
from federation.utils.text import decode_if_bytes

logger = logging.getLogger('federation')

PROTOCOL_NAME = "activitypub"


def identify_id(identifier: str) -> bool:
    """
    Try to identify whether this is a Matrix identifier.

    TODO fix, not entirely correct..
    """
    return re.match(r'^[@#!].*:.*$', identifier, flags=re.IGNORECASE) is not None


def identify_request(request: RequestType) -> bool:
    """
    Try to identify whether this is a Matrix request
    """
    # noinspection PyBroadException
    try:
        data = json.loads(decode_if_bytes(request.body))
        if "events" in data:
            return True
    except Exception:
        pass
    return False


class Protocol:
    actor = None
    get_contact_key = None
    payload = None
    request = None
    user = None

    # noinspection PyUnusedLocal
    @staticmethod
    def build_send(entity: MatrixEntityMixin, *args, **kwargs) -> List[Dict]:
        """
        Build POST data for sending out to the homeserver.

        :param entity: The outbound ready entity for this protocol.
        :returns: list of payloads
        """
        return entity.payloads()

    def extract_actor(self):
        # TODO TBD
        pass

    def receive(
            self,
            request: RequestType,
            user: UserType = None,
            sender_key_fetcher: Callable[[str], str] = None,
            skip_author_verification: bool = False) -> Tuple[str, dict]:
        """
        Receive a request.

        Matrix appservices will deliver 1+ events at a time.
        """
        # TODO TBD
        return self.actor, self.payload

if __name__ == "__main__":
    isT = True

    if not identify_request(RequestType(body=json.dumps('{"events": []}'))):
        isT = False
    if not identify_request(RequestType(body=json.dumps('{"events": []}').encode('utf-8'))):
        isT = False

    if identify_request(RequestType(body='foo')):
        isT = False
    if identify_request(RequestType(body='<xml></<xml>')):
        isT = False
    if identify_request(RequestType(body=b'<xml></<xml>')):
        isT = False
    if identify_request(RequestType(body=json.dumps('{"@context": "foo"}'))):
        isT = False
    if identify_request(RequestType(body=json.dumps('{"@context": "foo"}').encode('utf-8'))):
        isT = False
    if not isT:
        raise Exception("Result not True!!!")
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/jaywink---federation/data_passk_platform/6306292152e177c0ba469f0d/"):
    #     f = open("/home/travis/builds/repos/jaywink---federation/data_passk_platform/6306292152e177c0ba469f0d/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = identify_request(args0)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break





----------------------------
/home/travis/builds/repos/jaywink---federation/federation/entities/diaspora/utils_format_dt_passk_validte.py
from dateutil.tz import tzlocal, tzutc
from lxml import etree


def ensure_timezone(dt, tz=None):
    """
    Make sure the datetime <dt> has a timezone set, using timezone <tz> if it
    doesn't. <tz> defaults to the local timezone.
    """
    if dt.tzinfo is None:
        return dt.replace(tzinfo=tz or tzlocal())
    else:
        return dt


def format_dt(dt):
    """
    Format a datetime in the way that D* nodes expect.
    """
    return ensure_timezone(dt).astimezone(tzutc()).strftime(
        '%Y-%m-%dT%H:%M:%SZ'
    )
    def struct_to_xml(node, struct):
        """
        Turn a list of dicts into XML nodes with tag names taken from the dict
        keys and element text taken from dict values. This is a list of dicts
        so that the XML nodes can be ordered in the XML output.
        """
        for obj in struct:
            for k, v in obj.items():
                etree.SubElement(node, k).text = v


    def get_full_xml_representation(entity, private_key):
        """Get full XML representation of an entity.

        This contains the <XML><post>..</post></XML> wrapper.

        Accepts either a Base entity or a Diaspora entity.

        Author `private_key` must be given so that certain entities can be signed.
        """
        from federation.entities.diaspora.mappers import get_outbound_entity
        diaspora_entity = get_outbound_entity(entity, private_key)
        xml = diaspora_entity.to_xml()
        return "<XML><post>%s</post></XML>" % etree.tostring(xml).decode("utf-8")


    def add_element_to_doc(doc, tag, value):
        """Set text value of an etree.Element of tag, appending a new element with given tag if it doesn't exist."""
        element = doc.find(".//%s" % tag)
        if element is None:
            element = etree.SubElement(doc, tag)
        element.text = value


    if __name__ == "__main__":
        # import dill
        # import os
        # isT=True
        # for l in os.listdir("/home/travis/builds/repos/jaywink---federation/data_passk_platform/6306292252e177c0ba469f11/"):
        #     f = open("/home/travis/builds/repos/jaywink---federation/data_passk_platform/6306292252e177c0ba469f11/"+l, "rb")
        #     content = dill.load(f)
        #     f.close()
        #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
        #         args0=dill.loads(content["input"]["args"][0]["bytes"])
        #     else:
        #         args0=content["input"]["args"][0]["bytes"]
        #     res0 = format_dt(args0)
        #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
        #         isT=False
        #         break

        isT = True
        dt = arrow.get(datetime.datetime(2017, 1, 28, 3, 2, 3), "Europe/Helsinki").datetime
        if(format_dt(dt) != "2017-01-28T01:02:03Z"):
            isT = False
        if not isT:
            raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/utils/text_find_tags_passk_validte.py
import re
from typing import Set, Tuple
from urllib.parse import urlparse

import bleach
from bleach import callbacks

ILLEGAL_TAG_CHARS = "!#$%^&*+.,@£/()=?`'\\{[]}~;:\"’”—\xa0"


def decode_if_bytes(text):
    try:
        return text.decode("utf-8")
    except AttributeError:
        return text


def encode_if_text(text):
    try:
        return bytes(text, encoding="utf-8")
    except TypeError:
        return text


def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:
    """Find tags in text.

    Tries to ignore tags inside code blocks.

    Optionally, if passed a "replacer", will also replace the tag word with the result
    of the replacer function called with the tag word.

    Returns a set of tags and the original or replaced text.
    """
    found_tags = set()
    # <br> and <p> tags cause issues in us finding words - add some spacing around them
    new_text = text.replace("<br>", " <br> ").replace("<p>", " <p> ").replace("</p>", " </p> ")
    lines = new_text.splitlines(keepends=True)
    final_lines = []
    code_block = False
    final_text = None
    # Check each line separately
    for line in lines:
        final_words = []
        if line[0:3] == "```":
            code_block = not code_block
        if line.find("#") == -1 or line[0:4] == "    " or code_block:
            # Just add the whole line
            final_lines.append(line)
            continue
        # Check each word separately
        words = line.split(" ")
        for word in words:
            if word.find('#') > -1:
                candidate = word.strip().strip("([]),.!?:*_%/")
                if candidate.find('<') > -1 or candidate.find('>') > -1:
                    # Strip html
                    candidate = bleach.clean(word, strip=True)
                # Now split with slashes
                candidates = candidate.split("/")
                to_replace = []
                for candidate in candidates:
                    if candidate.startswith("#"):
                        candidate = candidate.strip("#")
                        if test_tag(candidate.lower()):
                            found_tags.add(candidate.lower())
                            to_replace.append(candidate)
                if replacer:
                    tag_word = word
                    try:
                        for counter, replacee in enumerate(to_replace, 1):
                            tag_word = tag_word.replace("#%s" % replacee, replacer(replacee))
                    except Exception:
                        pass
                    final_words.append(tag_word)
                else:
                    final_words.append(word)
            else:
                final_words.append(word)
        final_lines.append(" ".join(final_words))
    if replacer:
        final_text = "".join(final_lines)
    if final_text:
        final_text = final_text.replace(" <br> ", "<br>").replace(" <p> ", "<p>").replace(" </p> ", "</p>")
    return found_tags, final_text or text


def get_path_from_url(url: str) -> str:
    """
    Return only the path part of an URL.
    """
    parsed = urlparse(url)
    return parsed.path


def process_text_links(text):
    """Process links in text, adding some attributes and linkifying textual links."""
    link_callbacks = [callbacks.nofollow, callbacks.target_blank]

    def link_attributes(attrs, new=False):
        """Run standard callbacks except for internal links."""
        href_key = (None, "href")
        if attrs.get(href_key).startswith("/"):
            return attrs

        # Run the standard callbacks
        for callback in link_callbacks:
            attrs = callback(attrs, new)
        return attrs

    return bleach.linkify(
        text,
        callbacks=[link_attributes],
        parse_email=False,
        skip_tags=["code"],
    )


def test_tag(tag: str) -> bool:
    """Test a word whether it could be accepted as a tag."""
    if not tag:
        return False
    for char in ILLEGAL_TAG_CHARS:
        if char in tag:
            return False
    return True


def validate_handle(handle):
    """
    Very basic handle validation as per
    https://diaspora.github.io/diaspora_federation/federation/types.html#diaspora-id
    """
    return re.match(r"[a-z0-9\-_.]+@[^@/]+\.[^@/]+", handle, flags=re.IGNORECASE) is not None


def with_slash(url):
    if url.endswith('/'):
        return url
    return f"{url}/"

if __name__ == "__main__":
    def _replacer(text):
        return f"#{text}/{text.lower()}"
    
    isT = True
    source = "#starting and #MixED with some #line\nendings also tags can\n#start on new line"
    tags, text = find_tags(source)
    if not tags == {"starting", "mixed", "line", "start"}:
        isT= False
    if not text == source:
        isT = False
    tags, text = find_tags(source, replacer=_replacer)
    if not text == "#starting/starting and #MixED/mixed with some #line/line\nendings also tags can\n" \
                   "#start/start on new line":
        isT = False
    
    source = "foo\n```\n#code\n```\n#notcode\n\n    #alsocode\n"
    tags, text = find_tags(source)
    if not tags == {"notcode"}:
        isT = False
    if not text == source:
        isT = False
    tags, text = find_tags(source, replacer=_replacer)
    if not text == "foo\n```\n#code\n```\n#notcode/notcode\n\n    #alsocode\n":
        isT = False

    source = "#parenthesis) #exp! #list] *#doh* _#bah_ #gah% #foo/#bar"
    tags, text = find_tags(source)
    if not tags == {"parenthesis", "exp", "list", "doh", "bah", "gah", "foo", "bar"}:
        isT = False
    if not text == source:
        isT = False
    tags, text = find_tags(source, replacer=_replacer)
    if not text == "#parenthesis/parenthesis) #exp/exp! #list/list] *#doh/doh* _#bah/bah_ #gah/gah% " \
                   "#foo/foo/#bar/bar":
        isT = False
    source = "#post **Foobar** #tag #OtherTag #third\n#fourth"
    tags, text = find_tags(source)
    if not tags == {"third", "fourth", "post", "othertag", "tag"}:
        isT = False
    if not text == source:
        isT = False
    tags, text = find_tags(source, replacer=_replacer)
    if not text == "#post/post **Foobar** #tag/tag #OtherTag/othertag #third/third\n#fourth/fourth":
        isT = False
    source = "<p>#starting and <span>#MixED</span> however not <#>this</#> or <#/>that"
    tags, text = find_tags(source)
    if not tags == {"starting", "mixed"}:
        isT = False
    if not text == source:
        isT = False
    tags, text = find_tags(source, replacer=_replacer)
    if not text == "<p>#starting/starting and <span>#MixED/mixed</span> however not <#>this</#> or <#/>that":
        isT = False
    source = "#foo) #bar] #hoo, #hee."
    tags, text = find_tags(source)
    if not tags == {"foo", "bar", "hoo", "hee"}:
        isT = False
    if not text == source:
        isT = False
    tags, text = find_tags(source, replacer=_replacer)
    if not text == "#foo/foo) #bar/bar] #hoo/hoo, #hee/hee.":
        isT = False
    source = "(#foo [#bar"
    tags, text = find_tags(source)
    if not tags == {"foo", "bar"}:
        isT = False
    if not text == source:
        isT = False
    tags, text = find_tags(source, replacer=_replacer)
    if not text == "(#foo/foo [#bar/bar":
        isT = False

    source = "#a!a #a#a #a$a #a%a #a^a #a&a #a*a #a+a #a.a #a,a #a@a #a£a #a(a #a)a #a=a " \
             "#a?a #a`a #a'a #a\\a #a{a #a[a #a]a #a}a #a~a #a;a #a:a #a\"a #a’a #a”a #\xa0cd"
    tags, text = find_tags(source)
    if not tags == set():
        isT = False
    if not text == source:
        isT = False
    tags, text = find_tags(source, replacer=_replacer)
    if not text == source:
        isT = False
    source = '<p>First line</p><p>#foobar #barfoo</p>'
    tags, text = find_tags(source)
    if not tags == {"foobar", "barfoo"}:
        isT = False
    if not text == source:
        isT = False
    tags, text = find_tags(source, replacer=_replacer)
    if not text == '<p>First line</p><p>#foobar/foobar #barfoo/barfoo</p>':
        isT = False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/jaywink---federation/data_passk_platform/6306292352e177c0ba469f1d/"):
    #     f = open("/home/travis/builds/repos/jaywink---federation/data_passk_platform/6306292352e177c0ba469f1d/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     res0 = find_tags(args0,args1)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/utils/text_process_text_links_passk_validte.py
import re
from typing import Set, Tuple
from urllib.parse import urlparse

import bleach
from bleach import callbacks

ILLEGAL_TAG_CHARS = "!#$%^&*+.,@£/()=?`'\\{[]}~;:\"’”—\xa0"


def decode_if_bytes(text):
    try:
        return text.decode("utf-8")
    except AttributeError:
        return text


def encode_if_text(text):
    try:
        return bytes(text, encoding="utf-8")
    except TypeError:
        return text


def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:
    """Find tags in text.

    Tries to ignore tags inside code blocks.

    Optionally, if passed a "replacer", will also replace the tag word with the result
    of the replacer function called with the tag word.

    Returns a set of tags and the original or replaced text.
    """
    found_tags = set()
    # <br> and <p> tags cause issues in us finding words - add some spacing around them
    new_text = text.replace("<br>", " <br> ").replace("<p>", " <p> ").replace("</p>", " </p> ")
    lines = new_text.splitlines(keepends=True)
    final_lines = []
    code_block = False
    final_text = None
    # Check each line separately
    for line in lines:
        final_words = []
        if line[0:3] == "```":
            code_block = not code_block
        if line.find("#") == -1 or line[0:4] == "    " or code_block:
            # Just add the whole line
            final_lines.append(line)
            continue
        # Check each word separately
        words = line.split(" ")
        for word in words:
            if word.find('#') > -1:
                candidate = word.strip().strip("([]),.!?:*_%/")
                if candidate.find('<') > -1 or candidate.find('>') > -1:
                    # Strip html
                    candidate = bleach.clean(word, strip=True)
                # Now split with slashes
                candidates = candidate.split("/")
                to_replace = []
                for candidate in candidates:
                    if candidate.startswith("#"):
                        candidate = candidate.strip("#")
                        if test_tag(candidate.lower()):
                            found_tags.add(candidate.lower())
                            to_replace.append(candidate)
                if replacer:
                    tag_word = word
                    try:
                        for counter, replacee in enumerate(to_replace, 1):
                            tag_word = tag_word.replace("#%s" % replacee, replacer(replacee))
                    except Exception:
                        pass
                    final_words.append(tag_word)
                else:
                    final_words.append(word)
            else:
                final_words.append(word)
        final_lines.append(" ".join(final_words))
    if replacer:
        final_text = "".join(final_lines)
    if final_text:
        final_text = final_text.replace(" <br> ", "<br>").replace(" <p> ", "<p>").replace(" </p> ", "</p>")
    return found_tags, final_text or text


def get_path_from_url(url: str) -> str:
    """
    Return only the path part of an URL.
    """
    parsed = urlparse(url)
    return parsed.path


def process_text_links(text):
    """Process links in text, adding some attributes and linkifying textual links."""
    link_callbacks = [callbacks.nofollow, callbacks.target_blank]

    def link_attributes(attrs, new=False):
        """Run standard callbacks except for internal links."""
        href_key = (None, "href")
        if attrs.get(href_key).startswith("/"):
            return attrs

        # Run the standard callbacks
        for callback in link_callbacks:
            attrs = callback(attrs, new)
        return attrs

    return bleach.linkify(
        text,
        callbacks=[link_attributes],
        parse_email=False,
        skip_tags=["code"],
    )


def test_tag(tag: str) -> bool:
    """Test a word whether it could be accepted as a tag."""
    if not tag:
        return False
    for char in ILLEGAL_TAG_CHARS:
        if char in tag:
            return False
    return True


def validate_handle(handle):
    """
    Very basic handle validation as per
    https://diaspora.github.io/diaspora_federation/federation/types.html#diaspora-id
    """
    return re.match(r"[a-z0-9\-_.]+@[^@/]+\.[^@/]+", handle, flags=re.IGNORECASE) is not None


def with_slash(url):
    if url.endswith('/'):
        return url
    return f"{url}/"

if __name__ == "__main__":
    isT = True

    if not process_text_links('https://example.org example.org\nhttp://example.org') == \
           '<a href="https://example.org" rel="nofollow" target="_blank">https://example.org</a> ' \
           '<a href="http://example.org" rel="nofollow" target="_blank">example.org</a>\n' \
           '<a href="http://example.org" rel="nofollow" target="_blank">http://example.org</a>':
        isT=False



    if not process_text_links('<a href="https://example.org">https://example.org</a>') == \
           '<a href="https://example.org" rel="nofollow" target="_blank">https://example.org</a>':
        isT = False



    if not process_text_links('<code>https://example.org</code><code>\nhttps://example.org\n</code>') == \
           '<code>https://example.org</code><code>\nhttps://example.org\n</code>':
        isT = False


    if not process_text_links('foo@example.org') == 'foo@example.org':
        isT = False


    if not process_text_links('<a href="/streams/tag/foobar">#foobar</a>') == \
           '<a href="/streams/tag/foobar">#foobar</a>':
        isT = False


    if not process_text_links(
        '<p><span class="h-card"><a class="u-url mention" href="https://dev.jasonrobinson.me/u/jaywink/">'
        '@<span>jaywink</span></a></span> boom</p>') == \
           '<p><span class="h-card"><a class="u-url mention" href="https://dev.jasonrobinson.me/u/jaywink/" ' \
           'rel="nofollow" target="_blank">@<span>jaywink</span></a></span> boom</p>':
        isT = False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/jaywink---federation/data_passk_platform/6306292352e177c0ba469f1e/"):
    #     f = open("/home/travis/builds/repos/jaywink---federation/data_passk_platform/6306292352e177c0ba469f1e/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = process_text_links(args0)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/utils/network_fetch_content_type_passk_validte.py
import calendar
import datetime
import logging
import re
import socket
from typing import Optional, Dict
from urllib.parse import quote
from uuid import uuid4

import requests
from requests_cache import CachedSession, DO_NOT_CACHE
from requests.exceptions import RequestException, HTTPError, SSLError
from requests.exceptions import ConnectionError
from requests.structures import CaseInsensitiveDict
import sys
sys.path.append("/home/travis/builds/repos/jaywink---federation/")
from federation import __version__
from federation.utils.django import get_requests_cache_backend

logger = logging.getLogger("federation")

USER_AGENT = "python/federation/%s" % __version__

session = CachedSession('fed_cache', backend=get_requests_cache_backend('fed_cache'))
EXPIRATION = datetime.timedelta(hours=6)

def fetch_content_type(url: str) -> Optional[str]:
    """
    Fetch the HEAD of the remote url to determine the content type.
    """
    try:
        response = session.head(url, headers={'user-agent': USER_AGENT}, timeout=10)
    except RequestException as ex:
        logger.warning("fetch_content_type - %s when fetching url %s", ex, url)
    else:
        return response.headers.get('Content-Type')


def fetch_document(url=None, host=None, path="/", timeout=10, raise_ssl_errors=True, extra_headers=None, cache=True, **kwargs):
    """Helper method to fetch remote document.

    Must be given either the ``url`` or ``host``.
    If ``url`` is given, only that will be tried without falling back to http from https.
    If ``host`` given, `path` will be added to it. Will fall back to http on non-success status code.

    :arg url: Full url to fetch, including protocol
    :arg host: Domain part only without path or protocol
    :arg path: Path without domain (defaults to "/")
    :arg timeout: Seconds to wait for response (defaults to 10)
    :arg raise_ssl_errors: Pass False if you want to try HTTP even for sites with SSL errors (default True)
    :arg extra_headers: Optional extra headers dictionary to add to requests
    :arg kwargs holds extra args passed to requests.get
    :returns: Tuple of document (str or None), status code (int or None) and error (an exception class instance or None)
    :raises ValueError: If neither url nor host are given as parameters
    """
    if not url and not host:
        raise ValueError("Need url or host.")

    logger.debug("fetch_document: url=%s, host=%s, path=%s, timeout=%s, raise_ssl_errors=%s",
                 url, host, path, timeout, raise_ssl_errors)
    headers = {'user-agent': USER_AGENT}
    if extra_headers:
        headers.update(extra_headers)
    if url:
        # Use url since it was given
        logger.debug("fetch_document: trying %s", url)
        try:
            response = session.get(url, timeout=timeout, headers=headers, 
                    expire_after=EXPIRATION if cache else DO_NOT_CACHE, **kwargs)
            logger.debug("fetch_document: found document, code %s", response.status_code)
            response.raise_for_status()
            if not response.encoding: response.encoding = 'utf-8'
            return response.text, response.status_code, None
        except RequestException as ex:
            logger.debug("fetch_document: exception %s", ex)
            return None, None, ex
    # Build url with some little sanitizing
    host_string = host.replace("http://", "").replace("https://", "").strip("/")
    path_string = path if path.startswith("/") else "/%s" % path
    url = "https://%s%s" % (host_string, path_string)
    logger.debug("fetch_document: trying %s", url)
    try:
        response = session.get(url, timeout=timeout, headers=headers)
        logger.debug("fetch_document: found document, code %s", response.status_code)
        response.raise_for_status()
        return response.text, response.status_code, None
    except (HTTPError, SSLError, ConnectionError) as ex:
        if isinstance(ex, SSLError) and raise_ssl_errors:
            logger.debug("fetch_document: exception %s", ex)
            return None, None, ex
        # Try http then
        url = url.replace("https://", "http://")
        logger.debug("fetch_document: trying %s", url)
        try:
            response = session.get(url, timeout=timeout, headers=headers)
            logger.debug("fetch_document: found document, code %s", response.status_code)
            response.raise_for_status()
            return response.text, response.status_code, None
        except RequestException as ex:
            logger.debug("fetch_document: exception %s", ex)
            return None, None, ex
    except RequestException as ex:
        logger.debug("fetch_document: exception %s", ex)
        return None, None, ex


def fetch_host_ip(host: str) -> str:
    """
    Fetch ip by host
    """
    try:
        ip = socket.gethostbyname(host)
    except socket.gaierror:
        return ''

    return ip


def fetch_file(url: str, timeout: int = 30, extra_headers: Dict = None) -> str:
    """
    Download a file with a temporary name and return the name.
    """
    headers = {'user-agent': USER_AGENT}
    if extra_headers:
        headers.update(extra_headers)
    response = session.get(url, timeout=timeout, headers=headers, stream=True)
    response.raise_for_status()
    name = f"/tmp/{str(uuid4())}"
    with open(name, "wb") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    return name


def parse_http_date(date):
    """
    Parse a date format as specified by HTTP RFC7231 section 7.1.1.1.

    The three formats allowed by the RFC are accepted, even if only the first
    one is still in widespread use.

    Return an integer expressed in seconds since the epoch, in UTC.

    Implementation copied from Django.
    https://github.com/django/django/blob/master/django/utils/http.py#L157
    License: BSD 3-clause
    """
    MONTHS = 'jan feb mar apr may jun jul aug sep oct nov dec'.split()
    __D = r'(?P<day>\d{2})'
    __D2 = r'(?P<day>[ \d]\d)'
    __M = r'(?P<mon>\w{3})'
    __Y = r'(?P<year>\d{4})'
    __Y2 = r'(?P<year>\d{2})'
    __T = r'(?P<hour>\d{2}):(?P<min>\d{2}):(?P<sec>\d{2})'
    RFC1123_DATE = re.compile(r'^\w{3}, %s %s %s %s GMT$' % (__D, __M, __Y, __T))
    RFC850_DATE = re.compile(r'^\w{6,9}, %s-%s-%s %s GMT$' % (__D, __M, __Y2, __T))
    ASCTIME_DATE = re.compile(r'^\w{3} %s %s %s %s$' % (__M, __D2, __T, __Y))
    # email.utils.parsedate() does the job for RFC1123 dates; unfortunately
    # RFC7231 makes it mandatory to support RFC850 dates too. So we roll
    # our own RFC-compliant parsing.
    for regex in RFC1123_DATE, RFC850_DATE, ASCTIME_DATE:
        m = regex.match(date)
        if m is not None:
            break
    else:
        raise ValueError("%r is not in a valid HTTP date format" % date)
    try:
        year = int(m.group('year'))
        if year < 100:
            if year < 70:
                year += 2000
            else:
                year += 1900
        month = MONTHS.index(m.group('mon').lower()) + 1
        day = int(m.group('day'))
        hour = int(m.group('hour'))
        min = int(m.group('min'))
        sec = int(m.group('sec'))
        result = datetime.datetime(year, month, day, hour, min, sec)
        return calendar.timegm(result.utctimetuple())
    except Exception as exc:
        raise ValueError("%r is not a valid date" % date) from exc


def send_document(url, data, timeout=10, method="post", *args, **kwargs):
    """Helper method to send a document via POST.

    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.

    :arg url: Full url to send to, including protocol
    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body
    :arg timeout: Seconds to wait for response (defaults to 10)
    :arg method: Method to use, defaults to post
    :returns: Tuple of status code (int or None) and error (exception class instance or None)
    """
    logger.debug("send_document: url=%s, data=%s, timeout=%s, method=%s", url, data, timeout, method)
    if not method:
        method = "post"
    headers = CaseInsensitiveDict({
        'User-Agent': USER_AGENT,
    })
    if "headers" in kwargs:
        # Update from kwargs
        headers.update(kwargs.get("headers"))
    kwargs.update({
        "data": data, "timeout": timeout, "headers": headers
    })
    request_func = getattr(requests, method)
    try:
        response = request_func(url, *args, **kwargs)
        logger.debug("send_document: response status code %s", response.status_code)
        return response.status_code, None
    # TODO support rate limit 429 code
    except RequestException as ex:
        logger.debug("send_document: exception %s", ex)
        return None, ex


def try_retrieve_webfinger_document(handle: str) -> Optional[str]:
    """
    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.
    """
    try:
        host = handle.split("@")[1]
    except (AttributeError, IndexError):
        logger.warning("retrieve_webfinger_document: invalid handle given: %s", handle)
        return None
    document, code, exception = fetch_document(
        host=host, path="/.well-known/webfinger?resource=acct:%s" % quote(handle),
    )
    if exception:
        logger.debug("retrieve_webfinger_document: failed to fetch webfinger document: %s, %s", code, exception)
    return document

if __name__ == "__main__":
    # import dill
    # import os
    isT=True
    ist1=fetch_content_type("https://github.com/windrunner414/seqgen")=="text/html; charset=utf-8"
    ist2=fetch_content_type("https://www.baidu.com")=="text/html"
    if not ist1 or not ist2:
        isT=False
    # if not fetch_content_type({'base64': 'gASVDAAAAAAAAACMCHVybGxhcmdllC4=', 'subType': '00'}) \
    #     == {'base64': 'gAROLg==', 'subType': '00'}:
    #     isT = False
    # if not fetch_content_type({'base64': 'gASVHwAAAAAAAACMG2h0dHBzOi8vc3RlcGhlbnMtc21pdGguY29tL5Qu', 'subType': '00'}) \
    #     == {'base64': 'gAROLg==', 'subType': '00'}:
    #     isT = False

    # for l in os.listdir("D:/fse/python_test/repos/jaywink---federation/data_passk_platform/6306292652e177c0ba469f34/")[1:]:
    #     f = open("D:/fse/python_test/repos/jaywink---federation/data_passk_platform/6306292652e177c0ba469f34/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = fetch_content_type(args0)
    #     print(args0)
    #     print(res0)
    #     print(content['output'][0])
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/utils/text_test_tag_passk_validte.py
import re
from typing import Set, Tuple
from urllib.parse import urlparse

import bleach
from bleach import callbacks

ILLEGAL_TAG_CHARS = "!#$%^&*+.,@£/()=?`'\\{[]}~;:\"’”—\xa0"


def decode_if_bytes(text):
    try:
        return text.decode("utf-8")
    except AttributeError:
        return text


def encode_if_text(text):
    try:
        return bytes(text, encoding="utf-8")
    except TypeError:
        return text


def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:
    """Find tags in text.

    Tries to ignore tags inside code blocks.

    Optionally, if passed a "replacer", will also replace the tag word with the result
    of the replacer function called with the tag word.

    Returns a set of tags and the original or replaced text.
    """
    found_tags = set()
    # <br> and <p> tags cause issues in us finding words - add some spacing around them
    new_text = text.replace("<br>", " <br> ").replace("<p>", " <p> ").replace("</p>", " </p> ")
    lines = new_text.splitlines(keepends=True)
    final_lines = []
    code_block = False
    final_text = None
    # Check each line separately
    for line in lines:
        final_words = []
        if line[0:3] == "```":
            code_block = not code_block
        if line.find("#") == -1 or line[0:4] == "    " or code_block:
            # Just add the whole line
            final_lines.append(line)
            continue
        # Check each word separately
        words = line.split(" ")
        for word in words:
            if word.find('#') > -1:
                candidate = word.strip().strip("([]),.!?:*_%/")
                if candidate.find('<') > -1 or candidate.find('>') > -1:
                    # Strip html
                    candidate = bleach.clean(word, strip=True)
                # Now split with slashes
                candidates = candidate.split("/")
                to_replace = []
                for candidate in candidates:
                    if candidate.startswith("#"):
                        candidate = candidate.strip("#")
                        if test_tag(candidate.lower()):
                            found_tags.add(candidate.lower())
                            to_replace.append(candidate)
                if replacer:
                    tag_word = word
                    try:
                        for counter, replacee in enumerate(to_replace, 1):
                            tag_word = tag_word.replace("#%s" % replacee, replacer(replacee))
                    except Exception:
                        pass
                    final_words.append(tag_word)
                else:
                    final_words.append(word)
            else:
                final_words.append(word)
        final_lines.append(" ".join(final_words))
    if replacer:
        final_text = "".join(final_lines)
    if final_text:
        final_text = final_text.replace(" <br> ", "<br>").replace(" <p> ", "<p>").replace(" </p> ", "</p>")
    return found_tags, final_text or text


def get_path_from_url(url: str) -> str:
    """
    Return only the path part of an URL.
    """
    parsed = urlparse(url)
    return parsed.path


def process_text_links(text):
    """Process links in text, adding some attributes and linkifying textual links."""
    link_callbacks = [callbacks.nofollow, callbacks.target_blank]

    def link_attributes(attrs, new=False):
        """Run standard callbacks except for internal links."""
        href_key = (None, "href")
        if attrs.get(href_key).startswith("/"):
            return attrs

        # Run the standard callbacks
        for callback in link_callbacks:
            attrs = callback(attrs, new)
        return attrs

    return bleach.linkify(
        text,
        callbacks=[link_attributes],
        parse_email=False,
        skip_tags=["code"],
    )


def test_tag(tag: str) -> bool:
    """Test a word whether it could be accepted as a tag."""
    if not tag:
        return False
    for char in ILLEGAL_TAG_CHARS:
        if char in tag:
            return False
    return True


def validate_handle(handle):
    """
    Very basic handle validation as per
    https://diaspora.github.io/diaspora_federation/federation/types.html#diaspora-id
    """
    return re.match(r"[a-z0-9\-_.]+@[^@/]+\.[^@/]+", handle, flags=re.IGNORECASE) is not None


def with_slash(url):
    if url.endswith('/'):
        return url
    return f"{url}/"

if __name__ == "__main__":
    isT = True
    if not test_tag({'base64': 'gASVDAAAAAAAAACMCHRhZ3RocmVllC4=', 'subType': '00'}):
        isT = False
    if not test_tag({'base64': 'gASVDQAAAAAAAACMCXNuYWtlY2FzZZQu', 'subType': '00'}):
        isT = False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("D:/fse/python_test/repos/jaywink---federation/data_passk_platform/6306292a52e177c0ba469f41/"):
    #     f = open("D:/fse/python_test/repos/jaywink---federation/data_passk_platform/6306292a52e177c0ba469f41/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = test_tag(args0)
    #     print(args0)
    #     print(res0)
    #     print(content['output'][0])
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/entities/diaspora/mappers_xml_children_as_dict_passk_validte.py
import logging
from datetime import datetime
from typing import Callable, List

# noinspection PyPackageRequirements
from Crypto.PublicKey.RSA import RsaKey
from lxml import etree
import sys
sys.path.append("/home/travis/builds/repos/jaywink---federation/")
from federation.entities.base import Comment, Follow, Post, Profile, Reaction, Retraction, Share
from federation.entities.diaspora.entities import (
    DiasporaComment, DiasporaContact, DiasporaLike, DiasporaPost,
    DiasporaProfile, DiasporaReshare, DiasporaRetraction,
    DiasporaImage)
from federation.entities.diaspora.mixins import DiasporaRelayableMixin
from federation.entities.mixins import BaseEntity
from federation.protocols.diaspora.signatures import get_element_child_info
from federation.types import UserType, ReceiverVariant
from federation.utils.diaspora import retrieve_and_parse_profile

logger = logging.getLogger("federation")

MAPPINGS = {
    "status_message": DiasporaPost,
    "comment": DiasporaComment,
    "photo": DiasporaImage,
    "like": DiasporaLike,
    "profile": DiasporaProfile,
    "retraction": DiasporaRetraction,
    "contact": DiasporaContact,
    "reshare": DiasporaReshare,
}

TAGS = [
    # Order is important. Any top level tags should be before possibly child tags
    "reshare", "status_message", "comment", "like", "request", "profile", "retraction", "photo", "contact",
]

BOOLEAN_KEYS = (
    "public",
    "nsfw",
    "following",
    "sharing",
)

DATETIME_KEYS = (
    "created_at",
)

INTEGER_KEYS = (
    "height",
    "width",
)


def xml_children_as_dict(node):
    """Turn the children of node <xml> into a dict, keyed by tag name.

    This is only a shallow conversation - child nodes are not recursively processed.
    """
    return dict((e.tag, e.text) for e in node)


def check_sender_and_entity_handle_match(sender_handle, entity_handle):
    """Ensure that sender and entity handles match.

    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might
    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers
    AND the object. We must ensure they're the same.
    """
    if sender_handle != entity_handle:
        logger.warning("sender_handle and entity_handle don't match, aborting! sender_handle: %s, entity_handle: %s",
                       sender_handle, entity_handle)
        return False
    return True


def element_to_objects(
        element: etree.ElementTree, sender: str, sender_key_fetcher: Callable[[str], str] = None, user: UserType = None,
) -> List:
    """Transform an Element to a list of entities recursively.

    Possible child entities are added to each entity ``_children`` list.

    Optional parameter ``sender_key_fetcher`` can be a function to fetch sender public key.
    If not given, key will always be fetched over the network. The function should take sender as the only parameter.
    """
    entities = []
    cls = MAPPINGS.get(element.tag)
    if not cls:
        return []

    attrs = xml_children_as_dict(element)
    transformed = transform_attributes(attrs, cls)
    if hasattr(cls, "fill_extra_attributes"):
        transformed = cls.fill_extra_attributes(transformed)
    entity = cls(**transformed)
    # Add protocol name
    entity._source_protocol = "diaspora"
    # Save element object to entity for possible later use
    entity._source_object = etree.tostring(element)

    # Save receivers on the entity
    if user:
        # Single receiver
        entity._receivers = [UserType(id=user.id, receiver_variant=ReceiverVariant.ACTOR)]
    else:
        # Followers
        entity._receivers = [UserType(id=sender, receiver_variant=ReceiverVariant.FOLLOWERS)]

    if issubclass(cls, DiasporaRelayableMixin):
        # If relayable, fetch sender key for validation
        entity._xml_tags = get_element_child_info(element, "tag")
        if sender_key_fetcher:
            entity._sender_key = sender_key_fetcher(entity.actor_id)
        else:
            profile = retrieve_and_parse_profile(entity.handle)
            if profile:
                entity._sender_key = profile.public_key
    else:
        # If not relayable, ensure handles match
        if not check_sender_and_entity_handle_match(sender, entity.handle):
            return []
    try:
        entity.validate()
    except ValueError as ex:
        logger.error("Failed to validate entity %s: %s", entity, ex, extra={
            "attrs": attrs,
            "transformed": transformed,
        })
        return []

    # Extract mentions
    if hasattr(entity, "extract_mentions"):
        entity.extract_mentions()

    # Do child elements
    for child in element:
        # noinspection PyProtectedMember
        entity._children.extend(element_to_objects(child, sender, user=user))
    # Add to entities list
    entities.append(entity)
    return entities


def message_to_objects(
        message: str, sender: str, sender_key_fetcher:Callable[[str], str]=None, user: UserType =None,
) -> List:
    """Takes in a message extracted by a protocol and maps it to entities.

    :param message: XML payload
    :type message: str
    :param sender: Payload sender id
    :type message: str
    :param sender_key_fetcher: Function to fetch sender public key. If not given, key will always be fetched
        over network. The function should take sender handle as the only parameter.
    :param user: Optional receiving user object. If given, should have a `handle`.
    :returns: list of entities
    """
    doc = etree.fromstring(message)
    if doc.tag in TAGS:
        return element_to_objects(doc, sender, sender_key_fetcher, user)
    return []


def transform_attributes(attrs, cls):
    """Transform some attribute keys.

    :param attrs: Properties from the XML
    :type attrs: dict
    :param cls: Class of the entity
    :type cls: class
    """
    transformed = {}
    for key, value in attrs.items():
        if value is None:
            value = ""
        if key == "text":
            transformed["raw_content"] = value
        elif key == "activitypub_id":
            transformed["id"] = value
        elif key == "author":
            if cls == DiasporaProfile:
                # Diaspora Profile XML message contains no GUID. We need the guid. Fetch it.
                profile = retrieve_and_parse_profile(value)
                transformed['id'] = value
                transformed["guid"] = profile.guid
            else:
                transformed["actor_id"] = value
            transformed["handle"] = value
        elif key == 'guid':
            if cls != DiasporaProfile:
                transformed["id"] = value
                transformed["guid"] = value
        elif key in ("root_author", "recipient"):
            transformed["target_id"] = value
            transformed["target_handle"] = value
        elif key in ("target_guid", "root_guid", "parent_guid"):
            transformed["target_id"] = value
            transformed["target_guid"] = value
        elif key == "thread_parent_guid":
            transformed["root_target_id"] = value
            transformed["root_target_guid"] = value
        elif key in ("first_name", "last_name"):
            values = [attrs.get('first_name'), attrs.get('last_name')]
            values = [v for v in values if v]
            transformed["name"] = " ".join(values)
        elif key == "image_url":
            if "image_urls" not in transformed:
                transformed["image_urls"] = {}
            transformed["image_urls"]["large"] = value
        elif key == "image_url_small":
            if "image_urls" not in transformed:
                transformed["image_urls"] = {}
            transformed["image_urls"]["small"] = value
        elif key == "image_url_medium":
            if "image_urls" not in transformed:
                transformed["image_urls"] = {}
            transformed["image_urls"]["medium"] = value
        elif key == "tag_string":
            if value:
                transformed["tag_list"] = value.replace("#", "").split(" ")
        elif key == "bio":
            transformed["raw_content"] = value
        elif key == "searchable":
            transformed["public"] = True if value == "true" else False
        elif key in ["target_type"] and cls == DiasporaRetraction:
            transformed["entity_type"] = DiasporaRetraction.entity_type_from_remote(value)
        elif key == "remote_photo_path":
            transformed["url"] = f"{value}{attrs.get('remote_photo_name')}"
        elif key == "author_signature":
            transformed["signature"] = value
        elif key in BOOLEAN_KEYS:
            transformed[key] = True if value == "true" else False
        elif key in DATETIME_KEYS:
            transformed[key] = datetime.strptime(value, "%Y-%m-%dT%H:%M:%SZ")
        elif key in INTEGER_KEYS:
            transformed[key] = int(value)
        else:
            transformed[key] = value
    return transformed


def get_outbound_entity(entity: BaseEntity, private_key: RsaKey):
    """Get the correct outbound entity for this protocol.

    We might have to look at entity values to decide the correct outbound entity.
    If we cannot find one, we should raise as conversion cannot be guaranteed to the given protocol.

    Private key of author is needed to be passed for signing the outbound entity.

    :arg entity: An entity instance which can be of a base or protocol entity class.
    :arg private_key: Private key of sender as an RSA object
    :returns: Protocol specific entity class instance.
    :raises ValueError: If conversion cannot be done.
    """
    if getattr(entity, "outbound_doc", None):
        # If the entity already has an outbound doc, just return the entity as is
        return entity
    outbound = None
    cls = entity.__class__
    if cls in [DiasporaPost, DiasporaImage, DiasporaComment, DiasporaLike, DiasporaProfile, DiasporaRetraction,
               DiasporaContact, DiasporaReshare]:
        # Already fine
        outbound = entity
    elif cls == Post:
        outbound = DiasporaPost.from_base(entity)
    elif cls == Comment:
        outbound = DiasporaComment.from_base(entity)
    elif cls == Reaction:
        if entity.reaction == "like":
            outbound = DiasporaLike.from_base(entity)
    elif cls == Follow:
        outbound = DiasporaContact.from_base(entity)
    elif cls == Profile:
        outbound = DiasporaProfile.from_base(entity)
    elif cls == Retraction:
        outbound = DiasporaRetraction.from_base(entity)
    elif cls == Share:
        outbound = DiasporaReshare.from_base(entity)
    if not outbound:
        raise ValueError("Don't know how to convert this base entity to Diaspora protocol entities.")
    if isinstance(outbound, DiasporaRelayableMixin) and not outbound.signature:
        # Sign by author if not signed yet. We don't want to overwrite any existing signature in the case
        # that this is being sent by the parent author
        outbound.sign(private_key)
        # If missing, also add same signature to `parent_author_signature`. This is required at the moment
        # in all situations but is apparently being removed.
        # TODO: remove this once Diaspora removes the extra signature
        outbound.parent_signature = outbound.signature
    if hasattr(outbound, "pre_send"):
        outbound.pre_send()
    # Validate the entity
    outbound.validate(direction="outbound")
    return outbound

if __name__ == "__main__":
    f=open("/home/travis/builds/repos/jaywink---federation/federation/entities/diaspora/pom.xml",'r',encoding="utf-8")
    content=f.read()
    f.close()

    isT=xml_children_as_dict(etree.fromstring(content))=={'modelVersion': '4.0.0', 'groupId': 'org.example', 'artifactId': 'ISSTA2022', 'version': '1.0-SNAPSHOT', 'properties': '\n        ', 'dependencies': '\n        ', 'build': '\n        '}
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("D:/fse/python_test/repos/jaywink---federation/data_passk_platform/6306298b52e177c0ba469fdc/"):
    #     f = open("D:/fse/python_test/repos/jaywink---federation/data_passk_platform/6306298b52e177c0ba469fdc/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = xml_children_as_dict(args0)
    #     print(res0)
    #     print(content["output"][0])
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/entities/diaspora/mappers_check_sender_and_entity_handle_match_passk_validte.py
import logging
from datetime import datetime
from typing import Callable, List

# noinspection PyPackageRequirements
from Crypto.PublicKey.RSA import RsaKey
from lxml import etree
import sys
sys.path.append("/home/travis/builds/repos/jaywink---federation/")
from federation.entities.base import Comment, Follow, Post, Profile, Reaction, Retraction, Share
from federation.entities.diaspora.entities import (
    DiasporaComment, DiasporaContact, DiasporaLike, DiasporaPost,
    DiasporaProfile, DiasporaReshare, DiasporaRetraction,
    DiasporaImage)
from federation.entities.diaspora.mixins import DiasporaRelayableMixin
from federation.entities.mixins import BaseEntity
from federation.protocols.diaspora.signatures import get_element_child_info
from federation.types import UserType, ReceiverVariant
from federation.utils.diaspora import retrieve_and_parse_profile

logger = logging.getLogger("federation")

MAPPINGS = {
    "status_message": DiasporaPost,
    "comment": DiasporaComment,
    "photo": DiasporaImage,
    "like": DiasporaLike,
    "profile": DiasporaProfile,
    "retraction": DiasporaRetraction,
    "contact": DiasporaContact,
    "reshare": DiasporaReshare,
}

TAGS = [
    # Order is important. Any top level tags should be before possibly child tags
    "reshare", "status_message", "comment", "like", "request", "profile", "retraction", "photo", "contact",
]

BOOLEAN_KEYS = (
    "public",
    "nsfw",
    "following",
    "sharing",
)

DATETIME_KEYS = (
    "created_at",
)

INTEGER_KEYS = (
    "height",
    "width",
)


def xml_children_as_dict(node):
    """Turn the children of node <xml> into a dict, keyed by tag name.

    This is only a shallow conversation - child nodes are not recursively processed.
    """
    return dict((e.tag, e.text) for e in node)


def check_sender_and_entity_handle_match(sender_handle, entity_handle):
    """Ensure that sender and entity handles match.

    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might
    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers
    AND the object. We must ensure they're the same.
    """
    if sender_handle != entity_handle:
        logger.warning("sender_handle and entity_handle don't match, aborting! sender_handle: %s, entity_handle: %s",
                       sender_handle, entity_handle)
        return False
    return True


def element_to_objects(
        element: etree.ElementTree, sender: str, sender_key_fetcher: Callable[[str], str] = None, user: UserType = None,
) -> List:
    """Transform an Element to a list of entities recursively.

    Possible child entities are added to each entity ``_children`` list.

    Optional parameter ``sender_key_fetcher`` can be a function to fetch sender public key.
    If not given, key will always be fetched over the network. The function should take sender as the only parameter.
    """
    entities = []
    cls = MAPPINGS.get(element.tag)
    if not cls:
        return []

    attrs = xml_children_as_dict(element)
    transformed = transform_attributes(attrs, cls)
    if hasattr(cls, "fill_extra_attributes"):
        transformed = cls.fill_extra_attributes(transformed)
    entity = cls(**transformed)
    # Add protocol name
    entity._source_protocol = "diaspora"
    # Save element object to entity for possible later use
    entity._source_object = etree.tostring(element)

    # Save receivers on the entity
    if user:
        # Single receiver
        entity._receivers = [UserType(id=user.id, receiver_variant=ReceiverVariant.ACTOR)]
    else:
        # Followers
        entity._receivers = [UserType(id=sender, receiver_variant=ReceiverVariant.FOLLOWERS)]

    if issubclass(cls, DiasporaRelayableMixin):
        # If relayable, fetch sender key for validation
        entity._xml_tags = get_element_child_info(element, "tag")
        if sender_key_fetcher:
            entity._sender_key = sender_key_fetcher(entity.actor_id)
        else:
            profile = retrieve_and_parse_profile(entity.handle)
            if profile:
                entity._sender_key = profile.public_key
    else:
        # If not relayable, ensure handles match
        if not check_sender_and_entity_handle_match(sender, entity.handle):
            return []
    try:
        entity.validate()
    except ValueError as ex:
        logger.error("Failed to validate entity %s: %s", entity, ex, extra={
            "attrs": attrs,
            "transformed": transformed,
        })
        return []

    # Extract mentions
    if hasattr(entity, "extract_mentions"):
        entity.extract_mentions()

    # Do child elements
    for child in element:
        # noinspection PyProtectedMember
        entity._children.extend(element_to_objects(child, sender, user=user))
    # Add to entities list
    entities.append(entity)
    return entities


def message_to_objects(
        message: str, sender: str, sender_key_fetcher:Callable[[str], str]=None, user: UserType =None,
) -> List:
    """Takes in a message extracted by a protocol and maps it to entities.

    :param message: XML payload
    :type message: str
    :param sender: Payload sender id
    :type message: str
    :param sender_key_fetcher: Function to fetch sender public key. If not given, key will always be fetched
        over network. The function should take sender handle as the only parameter.
    :param user: Optional receiving user object. If given, should have a `handle`.
    :returns: list of entities
    """
    doc = etree.fromstring(message)
    if doc.tag in TAGS:
        return element_to_objects(doc, sender, sender_key_fetcher, user)
    return []


def transform_attributes(attrs, cls):
    """Transform some attribute keys.

    :param attrs: Properties from the XML
    :type attrs: dict
    :param cls: Class of the entity
    :type cls: class
    """
    transformed = {}
    for key, value in attrs.items():
        if value is None:
            value = ""
        if key == "text":
            transformed["raw_content"] = value
        elif key == "activitypub_id":
            transformed["id"] = value
        elif key == "author":
            if cls == DiasporaProfile:
                # Diaspora Profile XML message contains no GUID. We need the guid. Fetch it.
                profile = retrieve_and_parse_profile(value)
                transformed['id'] = value
                transformed["guid"] = profile.guid
            else:
                transformed["actor_id"] = value
            transformed["handle"] = value
        elif key == 'guid':
            if cls != DiasporaProfile:
                transformed["id"] = value
                transformed["guid"] = value
        elif key in ("root_author", "recipient"):
            transformed["target_id"] = value
            transformed["target_handle"] = value
        elif key in ("target_guid", "root_guid", "parent_guid"):
            transformed["target_id"] = value
            transformed["target_guid"] = value
        elif key == "thread_parent_guid":
            transformed["root_target_id"] = value
            transformed["root_target_guid"] = value
        elif key in ("first_name", "last_name"):
            values = [attrs.get('first_name'), attrs.get('last_name')]
            values = [v for v in values if v]
            transformed["name"] = " ".join(values)
        elif key == "image_url":
            if "image_urls" not in transformed:
                transformed["image_urls"] = {}
            transformed["image_urls"]["large"] = value
        elif key == "image_url_small":
            if "image_urls" not in transformed:
                transformed["image_urls"] = {}
            transformed["image_urls"]["small"] = value
        elif key == "image_url_medium":
            if "image_urls" not in transformed:
                transformed["image_urls"] = {}
            transformed["image_urls"]["medium"] = value
        elif key == "tag_string":
            if value:
                transformed["tag_list"] = value.replace("#", "").split(" ")
        elif key == "bio":
            transformed["raw_content"] = value
        elif key == "searchable":
            transformed["public"] = True if value == "true" else False
        elif key in ["target_type"] and cls == DiasporaRetraction:
            transformed["entity_type"] = DiasporaRetraction.entity_type_from_remote(value)
        elif key == "remote_photo_path":
            transformed["url"] = f"{value}{attrs.get('remote_photo_name')}"
        elif key == "author_signature":
            transformed["signature"] = value
        elif key in BOOLEAN_KEYS:
            transformed[key] = True if value == "true" else False
        elif key in DATETIME_KEYS:
            transformed[key] = datetime.strptime(value, "%Y-%m-%dT%H:%M:%SZ")
        elif key in INTEGER_KEYS:
            transformed[key] = int(value)
        else:
            transformed[key] = value
    return transformed


def get_outbound_entity(entity: BaseEntity, private_key: RsaKey):
    """Get the correct outbound entity for this protocol.

    We might have to look at entity values to decide the correct outbound entity.
    If we cannot find one, we should raise as conversion cannot be guaranteed to the given protocol.

    Private key of author is needed to be passed for signing the outbound entity.

    :arg entity: An entity instance which can be of a base or protocol entity class.
    :arg private_key: Private key of sender as an RSA object
    :returns: Protocol specific entity class instance.
    :raises ValueError: If conversion cannot be done.
    """
    if getattr(entity, "outbound_doc", None):
        # If the entity already has an outbound doc, just return the entity as is
        return entity
    outbound = None
    cls = entity.__class__
    if cls in [DiasporaPost, DiasporaImage, DiasporaComment, DiasporaLike, DiasporaProfile, DiasporaRetraction,
               DiasporaContact, DiasporaReshare]:
        # Already fine
        outbound = entity
    elif cls == Post:
        outbound = DiasporaPost.from_base(entity)
    elif cls == Comment:
        outbound = DiasporaComment.from_base(entity)
    elif cls == Reaction:
        if entity.reaction == "like":
            outbound = DiasporaLike.from_base(entity)
    elif cls == Follow:
        outbound = DiasporaContact.from_base(entity)
    elif cls == Profile:
        outbound = DiasporaProfile.from_base(entity)
    elif cls == Retraction:
        outbound = DiasporaRetraction.from_base(entity)
    elif cls == Share:
        outbound = DiasporaReshare.from_base(entity)
    if not outbound:
        raise ValueError("Don't know how to convert this base entity to Diaspora protocol entities.")
    if isinstance(outbound, DiasporaRelayableMixin) and not outbound.signature:
        # Sign by author if not signed yet. We don't want to overwrite any existing signature in the case
        # that this is being sent by the parent author
        outbound.sign(private_key)
        # If missing, also add same signature to `parent_author_signature`. This is required at the moment
        # in all situations but is apparently being removed.
        # TODO: remove this once Diaspora removes the extra signature
        outbound.parent_signature = outbound.signature
    if hasattr(outbound, "pre_send"):
        outbound.pre_send()
    # Validate the entity
    outbound.validate(direction="outbound")
    return outbound

if __name__ == "__main__":
    isT = True
    if check_sender_and_entity_handle_match("foo", "bar"):
        isT = False
    if not check_sender_and_entity_handle_match("foo", "foo"):
        isT = False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/jaywink---federation/data_passk_platform/6306299052e177c0ba469fe8/"):
    #     f = open("/home/travis/builds/repos/jaywink---federation/data_passk_platform/6306299052e177c0ba469fe8/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     res0 = check_sender_and_entity_handle_match(args0,args1)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/hostmeta/generators_get_nodeinfo_well_known_document_passk_validte.py
import json
import os
import warnings
from base64 import b64encode
from string import Template
from typing import Dict

from jsonschema import validate
from jsonschema.exceptions import ValidationError
from xrd import XRD, Link, Element


def generate_host_meta(template=None, *args, **kwargs):
    """Generate a host-meta XRD document.

    Template specific key-value pairs need to be passed as ``kwargs``, see classes.

    :arg template: Ready template to fill with args, for example "diaspora" (optional)
    :returns: Rendered XRD document (str)
    """
    if template == "diaspora":
        hostmeta = DiasporaHostMeta(*args, **kwargs)
    else:
        hostmeta = BaseHostMeta(*args, **kwargs)
    return hostmeta.render()


def generate_legacy_webfinger(template=None, *args, **kwargs):
    """Generate a legacy webfinger XRD document.

    Template specific key-value pairs need to be passed as ``kwargs``, see classes.

    :arg template: Ready template to fill with args, for example "diaspora" (optional)
    :returns: Rendered XRD document (str)
    """
    if template == "diaspora":
        webfinger = DiasporaWebFinger(*args, **kwargs)
    else:
        webfinger = BaseLegacyWebFinger(*args, **kwargs)
    return webfinger.render()


def generate_nodeinfo2_document(**kwargs):
    """
    Generate a NodeInfo2 document.

    Pass in a dictionary as per NodeInfo2 1.0 schema:
    https://github.com/jaywink/nodeinfo2/blob/master/schemas/1.0/schema.json

    Minimum required schema:
        {server:
          baseUrl
          name
          software
          version
        }
        openRegistrations

    Protocols default will match what this library supports, ie "diaspora" currently.

    :return: dict
    :raises: KeyError on missing required items
    """
    return {
        "version": "1.0",
        "server": {
            "baseUrl": kwargs['server']['baseUrl'],
            "name": kwargs['server']['name'],
            "software": kwargs['server']['software'],
            "version": kwargs['server']['version'],
        },
        "organization": {
            "name": kwargs.get('organization', {}).get('name', None),
            "contact": kwargs.get('organization', {}).get('contact', None),
            "account": kwargs.get('organization', {}).get('account', None),
        },
        "protocols": kwargs.get('protocols', ["diaspora"]),
        "relay": kwargs.get('relay', ''),
        "services": {
            "inbound": kwargs.get('service', {}).get('inbound', []),
            "outbound": kwargs.get('service', {}).get('outbound', []),
        },
        "openRegistrations": kwargs['openRegistrations'],
        "usage": {
            "users": {
                "total": kwargs.get('usage', {}).get('users', {}).get('total'),
                "activeHalfyear": kwargs.get('usage', {}).get('users', {}).get('activeHalfyear'),
                "activeMonth": kwargs.get('usage', {}).get('users', {}).get('activeMonth'),
                "activeWeek": kwargs.get('usage', {}).get('users', {}).get('activeWeek'),
            },
            "localPosts": kwargs.get('usage', {}).get('localPosts'),
            "localComments": kwargs.get('usage', {}).get('localComments'),
        }
    }


def generate_hcard(template=None, **kwargs):
    """Generate a hCard document.

    Template specific key-value pairs need to be passed as ``kwargs``, see classes.

    :arg template: Ready template to fill with args, for example "diaspora" (optional)
    :returns: HTML document (str)
    """
    if template == "diaspora":
        hcard = DiasporaHCard(**kwargs)
    else:
        raise NotImplementedError()
    return hcard.render()


class BaseHostMeta:
    def __init__(self, *args, **kwargs):
        self.xrd = XRD()

    def render(self):
        return self.xrd.to_xml().toprettyxml(indent="  ", encoding="UTF-8")


class DiasporaHostMeta(BaseHostMeta):
    """Diaspora host-meta.

    Required keyword args:

    * webfinger_host (str)
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        link = Link(
            rel='lrdd',
            type_='application/xrd+xml',
            template='%s/webfinger?q={uri}' % kwargs["webfinger_host"]
        )
        self.xrd.links.append(link)


class BaseLegacyWebFinger(BaseHostMeta):
    """Legacy XRD WebFinger.

    See: https://code.google.com/p/webfinger/wiki/WebFingerProtocol
    """
    def __init__(self, address, *args, **kwargs):
        super().__init__(*args, **kwargs)
        subject = Element("Subject", "acct:%s" % address)
        self.xrd.elements.append(subject)


class DiasporaWebFinger(BaseLegacyWebFinger):
    """Diaspora version of legacy WebFinger.

    Required keyword args:

    * handle (str)        - eg user@domain.tld
    * host (str)          - eg https://domain.tld
    * guid (str)          - guid of user
    * public_key (str)    - public key
    """
    def __init__(self, handle, host, guid, public_key, *args, **kwargs):
        super().__init__(handle, *args, **kwargs)
        self.xrd.elements.append(Element("Alias", "%s/people/%s" % (
            host, guid
        )))
        username = handle.split("@")[0]
        self.xrd.links.append(Link(
            rel="http://microformats.org/profile/hcard",
            type_="text/html",
            href="%s/hcard/users/%s" %(
                host, guid
            )
        ))
        self.xrd.links.append(Link(
            rel="http://joindiaspora.com/seed_location",
            type_="text/html",
            href=host
        ))
        self.xrd.links.append(Link(
            rel="http://joindiaspora.com/guid",
            type_="text/html",
            href=guid
        ))
        self.xrd.links.append(Link(
            rel="http://webfinger.net/rel/profile-page",
            type_="text/html",
            href="%s/u/%s" % (
                host, username
            )
        ))
        self.xrd.links.append(Link(
            rel="http://schemas.google.com/g/2010#updates-from",
            type_="application/atom+xml",
            href="%s/public/%s.atom" % (
                host, username
            )
        ))
        # Base64 the key
        # See https://wiki.diasporafoundation.org/Federation_Protocol_Overview#Diaspora_Public_Key
        try:
            base64_key = b64encode(bytes(public_key, encoding="UTF-8")).decode("ascii")
        except TypeError:
            # Python 2
            base64_key = b64encode(public_key).decode("ascii")
        self.xrd.links.append(Link(
            rel="diaspora-public-key",
            type_="RSA",
            href=base64_key
        ))


class DiasporaHCard:
    """Diaspora hCard document.

    Must receive the `required` attributes as keyword arguments to init.
    """

    required = [
        "hostname", "fullname", "firstname", "lastname", "photo300", "photo100", "photo50", "searchable", "guid", "public_key", "username",
    ]

    def __init__(self, **kwargs):
        self.kwargs = kwargs
        template_path = os.path.join(os.path.dirname(__file__), "templates", "hcard_diaspora.html")
        with open(template_path) as f:
            self.template = Template(f.read())

    def render(self):
        required = self.required[:]
        for key, value in self.kwargs.items():
            required.remove(key)
            assert value is not None
            assert isinstance(value, str)
        assert len(required) == 0
        return self.template.substitute(self.kwargs)


class SocialRelayWellKnown:
    """A `.well-known/social-relay` document in JSON.

    For apps wanting to announce their preferences towards relay applications.

    See WIP spec: https://wiki.diasporafoundation.org/Relay_servers_for_public_posts

    Schema see `schemas/social-relay-well-known.json`

    :arg subscribe: bool
    :arg tags: tuple, optional
    :arg scope: Should be either "all" or "tags", default is "all" if not given
    """
    def __init__(self, subscribe, tags=(), scope="all", *args, **kwargs):
        self.doc = {
            "subscribe": subscribe,
            "scope": scope,
            "tags": list(tags),
        }

    def render(self):
        self.validate_doc()
        return json.dumps(self.doc)

    def validate_doc(self):
        schema_path = os.path.join(os.path.dirname(__file__), "schemas", "social-relay-well-known.json")
        with open(schema_path) as f:
            schema = json.load(f)
        validate(self.doc, schema)


class NodeInfo:
    """Generate a NodeInfo document.

    See spec: http://nodeinfo.diaspora.software

    NodeInfo is unnecessarely restrictive in field values. We wont be supporting such strictness, though
    we will raise a warning unless validation is skipped with `skip_validate=True`.

    For strictness, `raise_on_validate=True` will cause a `ValidationError` to be raised.

    See schema document `federation/hostmeta/schemas/nodeinfo-1.0.json` for how to instantiate this class.
    """

    def __init__(self, software, protocols, services, open_registrations, usage, metadata, skip_validate=False,
                 raise_on_validate=False):
        self.doc = {
            "version": "1.0",
            "software": software,
            "protocols": protocols,
            "services": services,
            "openRegistrations": open_registrations,
            "usage": usage,
            "metadata": metadata,
        }
        self.skip_validate = skip_validate
        self.raise_on_validate = raise_on_validate

    def render(self):
        if not self.skip_validate:
            self.validate_doc()
        return json.dumps(self.doc)

    def validate_doc(self):
        schema_path = os.path.join(os.path.dirname(__file__), "schemas", "nodeinfo-1.0.json")
        with open(schema_path) as f:
            schema = json.load(f)
        try:
            validate(self.doc, schema)
        except ValidationError:
            if self.raise_on_validate:
                raise
            warnings.warn("NodeInfo document generated does not validate against NodeInfo 1.0 specification.")


# The default NodeInfo document path
NODEINFO_DOCUMENT_PATH = "/nodeinfo/1.0"


def get_nodeinfo_well_known_document(url, document_path=None):
    """Generate a NodeInfo .well-known document.

    See spec: http://nodeinfo.diaspora.software

    :arg url: The full base url with protocol, ie https://example.com
    :arg document_path: Custom NodeInfo document path if supplied (optional)
    :returns: dict
    """
    return {
        "links": [
            {
                "rel": "http://nodeinfo.diaspora.software/ns/schema/1.0",
                "href": "{url}{path}".format(
                    url=url, path=document_path or NODEINFO_DOCUMENT_PATH
                )
            }
        ]
    }


class MatrixClientWellKnown:
    """
    Matrix Client well-known as per https://matrix.org/docs/spec/client_server/r0.6.1#server-discovery
    """
    def __init__(self, homeserver_base_url: str, identity_server_base_url: str = None, other_keys: Dict = None):
        self.homeserver_base_url = homeserver_base_url
        self.identity_server_base_url = identity_server_base_url
        self.other_keys = other_keys

    def render(self):
        doc = {
            "m.homeserver": {
                "base_url": self.homeserver_base_url,
            }
        }
        if self.identity_server_base_url:
            doc["m.identity_server"] = {
                "base_url": self.identity_server_base_url,
            }
        if self.other_keys:
            doc.update(self.other_keys)
        return doc


class MatrixServerWellKnown:
    """
    Matrix Server well-known as per https://matrix.org/docs/spec/server_server/r0.1.4#server-discovery
    """
    def __init__(self, homeserver_domain_with_port: str):
        self.homeserver_domain_with_port = homeserver_domain_with_port

    def render(self):
        return {
            "m.server": self.homeserver_domain_with_port,
        }


class RFC7033Webfinger:
    """
    RFC 7033 webfinger - see https://tools.ietf.org/html/rfc7033

    A Django view is also available, see the child ``django`` module for view and url configuration.

    :param id: Profile ActivityPub ID in URL format
    :param handle: Profile Diaspora handle
    :param guid: Profile Diaspora guid
    :param base_url: The base URL of the server (protocol://domain.tld)
    :param profile_path: Profile path for the user (for example `/profile/johndoe/`)
    :param hcard_path: (Optional) hCard path, defaults to ``/hcard/users/``.
    :param atom_path: (Optional) atom feed path
    :returns: dict
    """
    def __init__(
            self, id: str, handle: str, guid: str, base_url: str, profile_path: str, hcard_path: str="/hcard/users/",
            atom_path: str=None, search_path: str=None,
    ):
        self.id = id
        self.handle = handle
        self.guid = guid
        self.base_url = base_url
        self.hcard_path = hcard_path
        self.profile_path = profile_path
        self.atom_path = atom_path
        self.search_path = search_path

    def render(self):
        webfinger = {
            "subject": "acct:%s" % self.handle,
            "aliases": [
                f"{self.base_url}{self.profile_path}",
                self.id,
            ],
            "links": [
                {
                    "rel": "http://microformats.org/profile/hcard",
                    "type": "text/html",
                    "href": "%s%s%s" % (self.base_url, self.hcard_path, self.guid),
                },
                {
                    "rel": "http://joindiaspora.com/seed_location",
                    "type": "text/html",
                    "href": self.base_url,
                },
                {
                    "rel": "http://webfinger.net/rel/profile-page",
                    "type": "text/html",
                    "href": "%s%s" % (self.base_url, self.profile_path),
                },
                {
                    "rel": "salmon",
                    "href": "%s/receive/users/%s" % (self.base_url, self.guid),
                },
            ],
        }

        webfinger["links"].append({
            "rel": "self",
            "href": self.id,
            "type": "application/activity+json",
        })

        if self.atom_path:
            webfinger['links'].append(
                {
                    "rel": "http://schemas.google.com/g/2010#updates-from",
                    "type": "application/atom+xml",
                    "href": "%s%s" % (self.base_url, self.atom_path),
                }
            )
        if self.search_path:
            webfinger['links'].append(
                {
                    "rel": "http://ostatus.org/schema/1.0/subscribe",
                    "template": "%s%s{uri}" % (self.base_url, self.search_path),
                },
            )
        return webfinger

if __name__ == "__main__":
    isT = True
    wellknown = get_nodeinfo_well_known_document("https://example.com")
    if not wellknown["links"][0]["rel"] == "http://nodeinfo.diaspora.software/ns/schema/1.0" or \
       not wellknown["links"][0]["href"] == "https://example.com/nodeinfo/1.0":
        isT = False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/jaywink---federation/data_passk_platform/630629b952e177c0ba46a043/"):
    #     f = open("/home/travis/builds/repos/jaywink---federation/data_passk_platform/630629b952e177c0ba46a043/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     res0 = get_nodeinfo_well_known_document(args0,args1)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/protocols/diaspora/signatures_verify_relayable_signature_passk_validte.py
from base64 import b64decode, b64encode

from Crypto.Hash import SHA256
from Crypto.PublicKey import RSA
from Crypto.PublicKey.RSA import RsaKey
from Crypto.Signature import PKCS1_v1_5


def get_element_child_info(doc, attr):
    """Get information from child elements of this elementas a list since order is important.

    Don't include signature tags.

    :param doc: XML element
    :param attr: Attribute to get from the elements, for example "tag" or "text".
    """
    props = []
    for child in doc:
        if child.tag not in ["author_signature", "parent_author_signature"]:
            props.append(getattr(child, attr))
    return props


def _create_signature_hash(doc):
    props = get_element_child_info(doc, "text")
    content = ";".join(props)
    return SHA256.new(content.encode("utf-8"))


def verify_relayable_signature(public_key, doc, signature):
    """
    Verify the signed XML elements to have confidence that the claimed
    author did actually generate this message.
    """
    sig_hash = _create_signature_hash(doc)
    cipher = PKCS1_v1_5.new(RSA.importKey(public_key))
    return cipher.verify(sig_hash, b64decode(signature))


def create_relayable_signature(private_key: RsaKey, doc):
    sig_hash = _create_signature_hash(doc)
    cipher = PKCS1_v1_5.new(private_key)
    return b64encode(cipher.sign(sig_hash)).decode("ascii")

if __name__ == "__main__":
    isT = True
    from lxml import etree

    XML = "<comment><guid>0dd40d800db1013514416c626dd55703</guid><parent_guid>69ab2b83-aa69-4456-ad0a-dd669" \
          "7f54714</parent_guid><text>Woop Woop</text><diaspora_handle>jaywink@iliketoast.net</diaspora_handle></comment>"
    PUBKEY = "-----BEGIN PUBLIC KEY-----\nMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAuCfU1G5X+3O6vPdSz6QY\nSFbgdbv3KPv" \
             "xHi8tRmlyOLdLt5i1eqsy2WCW1iYNijiCL7OfbrvymBQxe3GA9S64\nVuavwzQ8nO7nzpNMqxY5tBXsBM1lECCHDOvm5dzINXWT9Sg7P1" \
             "8iIxE/2wQEgMUL\nAeVbJtAriXM4zydL7c91agFMJu1aHp0lxzoH8I13xzUetGMutR1tbcfWvoQvPAoU\n89uAz5j/DFMhWrkVEKGeWt1" \
             "YtHMmJqpYqR6961GDlwRuUsOBsLgLLVohzlBsTBSn\n3580o2E6G3DEaX0Az9WB9ylhNeV/L/PP3c5htpEyoPZSy1pgtut6TRYQwC8wns" \
             "qO\nbVIbFBkrKoaRDyVCnpMuKdDNLZqOOfhzas+SWRAby6D8VsXpPi/DpeS9XkX0o/uH\nJ9N49GuYMSUGC8gKtaddD13pUqS/9rpSvLD" \
             "rrDQe5Lhuyusgd28wgEAPCTmM3pEt\nQnlxEeEmFMIn3OBLbEDw5TFE7iED0z7a4dAkqqz8KCGEt12e1Kz7ujuOVMxJxzk6\nNtwt40Sq" \
             "EOPcdsGHAA+hqzJnXUihXfmtmFkropaCxM2f+Ha0bOQdDDui5crcV3sX\njShmcqN6YqFzmoPK0XM9P1qC+lfL2Mz6bHC5p9M8/FtcM46" \
             "hCj1TF/tl8zaZxtHP\nOrMuFJy4j4yAsyVy3ddO69ECAwEAAQ==\n-----END PUBLIC KEY-----\n"

    SIGNATURE = "A/vVRxM3V1ceEH1JrnPOaIZGM3gMjw/fnT9TgUh3poI4q9eH95AIoig+3eTA8XFuGvuo0tivxci4e0NJ1VLVkl/aqp8rvBNrRI1RQk" \
                "n2WVF6zk15Gq6KSia/wyzyiJHGxNGM8oFY4qPfNp6K+8ydUti22J11tVBEvQn+7FPAoloF2Xz1waK48ZZCFs8Rxzj+4jlz1PmuXCnT" \
                "j7v7GYS1Rb6sdFz4nBSuVk5X8tGOSXIRYxPgmtsDRMRrvDeEK+v3OY6VnT8dLTckS0qCwTRUULub1CGwkz/2mReZk/M1W4EbUnugF5" \
                "ptslmFqYDYJZM8PA/g89EKVpkx2gaFbsC4KXocWnxHNiue18rrFQ5hMnDuDRiRybLnQkxXbE/HDuLdnognt2S5wRshPoZmhe95v3qq" \
                "/5nH/GX1D7VmxEEIG9fX+XX+Vh9kzO9bLbwoJZwm50zXxCvrLlye/2JU5Vd2Hbm4aMuAyRAZiLS/EQcBlsts4DaFu4txe60HbXSh6n" \
                "qNofGkusuzZnCd0VObOpXizrI8xNQzZpjJEB5QqE2gbCC2YZNdOS0eBGXw42dAXa/QV3jZXGES7DdQlqPqqT3YjcMFLiRrWQR8cl4h" \
                "JIBRpV5piGyLmMMKYrWu7hQSrdRAEL3K6mNZZU6/yoG879LjtQbVwaFGPeT29B4zBE97FIo="
    XML2 = "<comment><guid>d728fe501584013514526c626dd55703</guid><parent_guid>d641bd35-8142-414e-a12d-f956cc2c1bb9" \
           "</parent_guid><text>What about the mystical problem with &#x1F44D; (pt2 with more logging)</text>" \
           "<diaspora_handle>jaywink@iliketoast.net</diaspora_handle></comment>"
    SIGNATURE2 = "Xla/AlirMihx72hehGMgpKILRUA2ZkEhFgVc65sl80iN+F62yQdSikGyUQVL+LaGNUgmzgK0zEahamfaMFep/9HE2FWuXlTCM+ZXx" \
                 "OhGWUnjkGW9vi41/Turm7ALzaJoFm1f3Iv4nh1sRD1jySzlZvYwrq4LwmgZ8r0M+Q6xUSIIJfgS8Zjmp43strKo28vKT+DmUKu9Fg" \
                 "jZWjW3S8WPPJFO0UqA0b1UQspmNLZOVxsNpa0OCM1pofJvT09n6xG+byV30Bed27Kw+D3fzfYq5xvohyeCyliTq8LHnOykecki3Y2" \
                 "Pvl1qsxxBehlwc/WH8yIUiwC2Du6zY61tN3LGgMAoIFl40Roo1z/I7YfOy4ZCukOGqqyiLdjoXxIVQqqsPtKsrVXS+A9OQ+sVESgw" \
                 "f8jeEIw/KXLVB/aEyrZJXQR1pBfqkOTCSnAfZVBSjJyxhanS/8iGmnRV5zz3auYMLR9aA8QHjV/VZOj0Bxhuba9VIzJlY9XoUt5Vs" \
                 "h3uILJM3uVJzSjlZV+Jw3O+NdQFnZyh7m1+eJUMQJ8i0Sr3sMLsdb9me/I0HueXCa5eBHAoTtAyQgS4uN4NMhvpqrB/lQCx7pqnkt" \
                 "xiCO/bUEZONQjWrvJT+EfD+I0UMFtPFiGDzJ0yi0Ah7LxSTGEGPFZHH5RgsJA8lJwGMCUtc9Cpy8A="
    doc = etree.XML(XML)
    assert verify_relayable_signature(PUBKEY, doc, SIGNATURE)


    doc = etree.XML(XML2)
    assert verify_relayable_signature(PUBKEY, doc, SIGNATURE2)
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/jaywink---federation/data_passk_platform/630629d052e177c0ba46a0a1/"):
    #     f = open("/home/travis/builds/repos/jaywink---federation/data_passk_platform/630629d052e177c0ba46a0a1/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"],bytes):
    #         args2=dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2=content["input"]["args"][2]["bytes"]
    #     res0 = verify_relayable_signature(args0,args1,args2)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/utils/diaspora_parse_diaspora_webfinger_passk_validte.py
import json
import logging
import xml
from typing import Callable, Dict
from urllib.parse import quote

from lxml import html
from xrd import XRD
import sys
sys.path.append("/home/travis/builds/repos/jaywink---federation/")
from federation.inbound import handle_receive
from federation.types import RequestType
from federation.utils.network import fetch_document, try_retrieve_webfinger_document
from federation.utils.text import validate_handle

logger = logging.getLogger("federation")


def fetch_public_key(handle):
    """Fetch public key over the network.

    :param handle: Remote handle to retrieve public key for.
    :return: Public key in str format from parsed profile.
    """
    profile = retrieve_and_parse_profile(handle)
    return profile.public_key


def parse_diaspora_webfinger(document: str) -> Dict:
    """
    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).

    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html
    """
    webfinger = {
        "hcard_url": None,
    }
    # noinspection PyBroadException
    try:
        doc = json.loads(document)
        for link in doc["links"]:
            if link["rel"] == "http://microformats.org/profile/hcard":
                webfinger["hcard_url"] = link["href"]
                break
        else:
            logger.warning("parse_diaspora_webfinger: found JSON webfinger but it has no hcard href")
            raise ValueError
    except Exception:
        try:
            xrd = XRD.parse_xrd(document)
            webfinger["hcard_url"] = xrd.find_link(rels="http://microformats.org/profile/hcard").href
        except (xml.parsers.expat.ExpatError, TypeError):
            logger.warning("parse_diaspora_webfinger: found XML webfinger but it fails to parse")
            pass
    return webfinger


def retrieve_diaspora_hcard(handle):
    """
    Retrieve a remote Diaspora hCard document.

    :arg handle: Remote handle to retrieve
    :return: str (HTML document)
    """
    webfinger = retrieve_and_parse_diaspora_webfinger(handle)
    document, code, exception = fetch_document(webfinger.get("hcard_url"))
    if exception:
        return None
    return document


def retrieve_and_parse_diaspora_webfinger(handle):
    """
    Retrieve a and parse a remote Diaspora webfinger document.

    :arg handle: Remote handle to retrieve
    :returns: dict
    """
    document = try_retrieve_webfinger_document(handle)
    if document:
        return parse_diaspora_webfinger(document)
    host = handle.split("@")[1]
    hostmeta = retrieve_diaspora_host_meta(host)
    if not hostmeta:
        return None
    url = hostmeta.find_link(rels="lrdd").template.replace("{uri}", quote(handle))
    document, code, exception = fetch_document(url)
    if exception:
        return None
    return parse_diaspora_webfinger(document)


def retrieve_diaspora_host_meta(host):
    """
    Retrieve a remote Diaspora host-meta document.

    :arg host: Host to retrieve from
    :returns: ``XRD`` instance
    """
    document, code, exception = fetch_document(host=host, path="/.well-known/host-meta")
    if exception:
        return None
    xrd = XRD.parse_xrd(document)
    return xrd


def _get_element_text_or_none(document, selector):
    """
    Using a CSS selector, get the element and return the text, or None if no element.

    :arg document: ``HTMLElement`` document
    :arg selector: CSS selector
    :returns: str or None
    """
    element = document.cssselect(selector)
    if element:
        return element[0].text
    return None


def _get_element_attr_or_none(document, selector, attribute):
    """
    Using a CSS selector, get the element and return the given attribute value, or None if no element.

    Args:
        document (HTMLElement) - HTMLElement document
        selector (str) - CSS selector
        attribute (str) - The attribute to get from the element
    """
    element = document.cssselect(selector)
    if element:
        return element[0].get(attribute)
    return None


def parse_profile_from_hcard(hcard: str, handle: str):
    """
    Parse all the fields we can from a hCard document to get a Profile.

    :arg hcard: HTML hcard document (str)
    :arg handle: User handle in username@domain.tld format
    :returns: ``federation.entities.diaspora.entities.DiasporaProfile`` instance
    """
    from federation.entities.diaspora.entities import DiasporaProfile  # Circulars
    doc = html.fromstring(hcard)
    profile = DiasporaProfile(
        name=_get_element_text_or_none(doc, ".fn"),
        image_urls={
            "small": _get_element_attr_or_none(doc, ".entity_photo_small .photo", "src"),
            "medium": _get_element_attr_or_none(doc, ".entity_photo_medium .photo", "src"),
            "large": _get_element_attr_or_none(doc, ".entity_photo .photo", "src"),
        },
        public=True,
        id=handle,
        handle=handle,
        finger=handle,
        guid=_get_element_text_or_none(doc, ".uid"),
        public_key=_get_element_text_or_none(doc, ".key"),
        username=handle.split('@')[0],
        _source_protocol="diaspora",
    )
    return profile


def retrieve_and_parse_content(
        id: str, guid: str, handle: str, entity_type: str, cache: bool=True, 
        sender_key_fetcher: Callable[[str], str]=None):
    """Retrieve remote content and return an Entity class instance.

    This is basically the inverse of receiving an entity. Instead, we fetch it, then call "handle_receive".

    :param sender_key_fetcher: Function to use to fetch sender public key. If not given, network will be used
        to fetch the profile and the key. Function must take handle as only parameter and return a public key.
    :returns: Entity object instance or ``None``
    """
    if not validate_handle(handle):
        return
    _username, domain = handle.split("@")
    url = get_fetch_content_endpoint(domain, entity_type.lower(), guid)
    document, status_code, error = fetch_document(url, cache=cache)
    if status_code == 200:
        request = RequestType(body=document)
        _sender, _protocol, entities = handle_receive(request, sender_key_fetcher=sender_key_fetcher)
        if len(entities) > 1:
            logger.warning("retrieve_and_parse_content - more than one entity parsed from remote even though we"
                           "expected only one! ID %s", guid)
        if entities:
            return entities[0]
        return
    elif status_code == 404:
        logger.warning("retrieve_and_parse_content - remote content %s not found", guid)
        return
    if error:
        raise error
    raise Exception("retrieve_and_parse_content - unknown problem when fetching document: %s, %s, %s" % (
        document, status_code, error,
    ))


def retrieve_and_parse_profile(handle):
    """
    Retrieve the remote user and return a Profile object.

    :arg handle: User handle in username@domain.tld format
    :returns: ``federation.entities.Profile`` instance or None
    """
    hcard = retrieve_diaspora_hcard(handle)
    if not hcard:
        return None
    profile = parse_profile_from_hcard(hcard, handle)
    try:
        profile.validate()
    except ValueError as ex:
        logger.warning("retrieve_and_parse_profile - found profile %s but it didn't validate: %s",
                       profile, ex)
        return None
    return profile


def get_fetch_content_endpoint(domain, entity_type, guid):
    """Get remote fetch content endpoint.

    See: https://diaspora.github.io/diaspora_federation/federation/fetching.html
    """
    return "https://%s/fetch/%s/%s" % (domain, entity_type, guid)


def get_public_endpoint(id: str) -> str:
    """Get remote endpoint for delivering public payloads."""
    _username, domain = id.split("@")
    return "https://%s/receive/public" % domain


def get_private_endpoint(id: str, guid: str) -> str:
    """Get remote endpoint for delivering private payloads."""
    _username, domain = id.split("@")
    return "https://%s/receive/users/%s" % (domain, guid)
DIASPORA_WEBFINGER_JSON = """{
  "subject": "acct:alice@example.org",
  "links": [
    {
      "rel": "http://microformats.org/profile/hcard",
      "type": "text/html",
      "href": "https://example.org/hcard/users/7dba7ca01d64013485eb3131731751e9"
    },
    {
      "rel": "http://joindiaspora.com/seed_location",
      "type": "text/html",
      "href": "https://example.org/"
    }
  ]
}
"""

DIASPORA_HOSTMETA = """<?xml version="1.0" encoding="UTF-8"?>
<XRD xmlns="http://docs.oasis-open.org/ns/xri/xrd-1.0">
  <Link rel="lrdd" type="application/xrd+xml" template="https://example.com/webfinger?q={uri}"/>
</XRD>
"""

DIASPORA_WEBFINGER = """<?xml version="1.0" encoding="UTF-8"?>
<XRD xmlns="http://docs.oasis-open.org/ns/xri/xrd-1.0">
  <Subject>acct:user@server.example</Subject>
  <Alias>https://server.example/people/0123456789abcdef</Alias>
  <Link rel="http://microformats.org/profile/hcard" type="text/html" href="https://server.example/hcard/users/0123456789abcdef"/>
  <Link rel="http://joindiaspora.com/seed_location" type="text/html" href="https://server.example"/>
  <Link rel="http://joindiaspora.com/guid" type="text/html" href="0123456789abcdef"/>
  <Link rel="http://webfinger.net/rel/profile-page" type="text/html" href="https://server.example/u/user"/>
  <Link rel="http://schemas.google.com/g/2010#updates-from" type="application/atom+xml" href="https://server.example/public/user.atom"/>
  <Link rel="diaspora-public-key" type="RSA" href="QUJDREVGPT0="/>
</XRD>
"""
if __name__ == "__main__":
    isT = True

    result = parse_diaspora_webfinger(DIASPORA_WEBFINGER_JSON)
    if not result == {"hcard_url": "https://example.org/hcard/users/7dba7ca01d64013485eb3131731751e9"}:
        isT = False


    result = parse_diaspora_webfinger(DIASPORA_WEBFINGER)
    if not result == {"hcard_url": "https://server.example/hcard/users/0123456789abcdef"}:
        isT = False


    result = parse_diaspora_webfinger("not a valid doc")
    if not result == {"hcard_url": None}:
        isT=False
    # result = parse_diaspora_webfinger(DIASPORA_WEBFINGER_JSON)
    # print(result == {"hcard_url": "https://example.org/hcard/users/7dba7ca01d64013485eb3131731751e9"})
    #
    # result = parse_diaspora_webfinger(DIASPORA_WEBFINGER)
    # print(result == {"hcard_url": "https://server.example/hcard/users/0123456789abcdef"})
    #
    #
    # result = parse_diaspora_webfinger("not a valid doc")
    # print(result == {"hcard_url": None})
    #
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\jaywink---federation/data_passk_platform1/630629e052e177c0ba46a0c4/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\jaywink---federation/data_passk_platform1/630629e052e177c0ba46a0c4/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     print(args0)
    #     res0 = parse_diaspora_webfinger(args0)
    #     print(res0)
        # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
        #     isT=False
        #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/utils/network_try_retrieve_webfinger_document_passk_validte.py
import calendar
import datetime
import logging
import re
import socket
from typing import Optional, Dict
from urllib.parse import quote
from uuid import uuid4

import requests
from requests_cache import CachedSession, DO_NOT_CACHE
from requests.exceptions import RequestException, HTTPError, SSLError
from requests.exceptions import ConnectionError
from requests.structures import CaseInsensitiveDict
import sys
sys.path.append("/home/travis/builds/repos/jaywink---federation/")
from federation import __version__
from federation.utils.django import get_requests_cache_backend

logger = logging.getLogger("federation")

USER_AGENT = "python/federation/%s" % __version__

session = CachedSession('fed_cache', backend=get_requests_cache_backend('fed_cache'))
EXPIRATION = datetime.timedelta(hours=6)

def fetch_content_type(url: str) -> Optional[str]:
    """
    Fetch the HEAD of the remote url to determine the content type.
    """
    try:
        response = session.head(url, headers={'user-agent': USER_AGENT}, timeout=10)
    except RequestException as ex:
        logger.warning("fetch_content_type - %s when fetching url %s", ex, url)
    else:
        return response.headers.get('Content-Type')


def fetch_document(url=None, host=None, path="/", timeout=10, raise_ssl_errors=True, extra_headers=None, cache=True, **kwargs):
    """Helper method to fetch remote document.

    Must be given either the ``url`` or ``host``.
    If ``url`` is given, only that will be tried without falling back to http from https.
    If ``host`` given, `path` will be added to it. Will fall back to http on non-success status code.

    :arg url: Full url to fetch, including protocol
    :arg host: Domain part only without path or protocol
    :arg path: Path without domain (defaults to "/")
    :arg timeout: Seconds to wait for response (defaults to 10)
    :arg raise_ssl_errors: Pass False if you want to try HTTP even for sites with SSL errors (default True)
    :arg extra_headers: Optional extra headers dictionary to add to requests
    :arg kwargs holds extra args passed to requests.get
    :returns: Tuple of document (str or None), status code (int or None) and error (an exception class instance or None)
    :raises ValueError: If neither url nor host are given as parameters
    """
    if not url and not host:
        raise ValueError("Need url or host.")

    logger.debug("fetch_document: url=%s, host=%s, path=%s, timeout=%s, raise_ssl_errors=%s",
                 url, host, path, timeout, raise_ssl_errors)
    headers = {'user-agent': USER_AGENT}
    if extra_headers:
        headers.update(extra_headers)
    if url:
        # Use url since it was given
        logger.debug("fetch_document: trying %s", url)
        try:
            response = session.get(url, timeout=timeout, headers=headers, 
                    expire_after=EXPIRATION if cache else DO_NOT_CACHE, **kwargs)
            logger.debug("fetch_document: found document, code %s", response.status_code)
            response.raise_for_status()
            if not response.encoding: response.encoding = 'utf-8'
            return response.text, response.status_code, None
        except RequestException as ex:
            logger.debug("fetch_document: exception %s", ex)
            return None, None, ex
    # Build url with some little sanitizing
    host_string = host.replace("http://", "").replace("https://", "").strip("/")
    path_string = path if path.startswith("/") else "/%s" % path
    url = "https://%s%s" % (host_string, path_string)
    logger.debug("fetch_document: trying %s", url)
    try:
        response = session.get(url, timeout=timeout, headers=headers)
        logger.debug("fetch_document: found document, code %s", response.status_code)
        response.raise_for_status()
        return response.text, response.status_code, None
    except (HTTPError, SSLError, ConnectionError) as ex:
        if isinstance(ex, SSLError) and raise_ssl_errors:
            logger.debug("fetch_document: exception %s", ex)
            return None, None, ex
        # Try http then
        url = url.replace("https://", "http://")
        logger.debug("fetch_document: trying %s", url)
        try:
            response = session.get(url, timeout=timeout, headers=headers)
            logger.debug("fetch_document: found document, code %s", response.status_code)
            response.raise_for_status()
            return response.text, response.status_code, None
        except RequestException as ex:
            logger.debug("fetch_document: exception %s", ex)
            return None, None, ex
    except RequestException as ex:
        logger.debug("fetch_document: exception %s", ex)
        return None, None, ex


def fetch_host_ip(host: str) -> str:
    """
    Fetch ip by host
    """
    try:
        ip = socket.gethostbyname(host)
    except socket.gaierror:
        return ''

    return ip


def fetch_file(url: str, timeout: int = 30, extra_headers: Dict = None) -> str:
    """
    Download a file with a temporary name and return the name.
    """
    headers = {'user-agent': USER_AGENT}
    if extra_headers:
        headers.update(extra_headers)
    response = session.get(url, timeout=timeout, headers=headers, stream=True)
    response.raise_for_status()
    name = f"/tmp/{str(uuid4())}"
    with open(name, "wb") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    return name


def parse_http_date(date):
    """
    Parse a date format as specified by HTTP RFC7231 section 7.1.1.1.

    The three formats allowed by the RFC are accepted, even if only the first
    one is still in widespread use.

    Return an integer expressed in seconds since the epoch, in UTC.

    Implementation copied from Django.
    https://github.com/django/django/blob/master/django/utils/http.py#L157
    License: BSD 3-clause
    """
    MONTHS = 'jan feb mar apr may jun jul aug sep oct nov dec'.split()
    __D = r'(?P<day>\d{2})'
    __D2 = r'(?P<day>[ \d]\d)'
    __M = r'(?P<mon>\w{3})'
    __Y = r'(?P<year>\d{4})'
    __Y2 = r'(?P<year>\d{2})'
    __T = r'(?P<hour>\d{2}):(?P<min>\d{2}):(?P<sec>\d{2})'
    RFC1123_DATE = re.compile(r'^\w{3}, %s %s %s %s GMT$' % (__D, __M, __Y, __T))
    RFC850_DATE = re.compile(r'^\w{6,9}, %s-%s-%s %s GMT$' % (__D, __M, __Y2, __T))
    ASCTIME_DATE = re.compile(r'^\w{3} %s %s %s %s$' % (__M, __D2, __T, __Y))
    # email.utils.parsedate() does the job for RFC1123 dates; unfortunately
    # RFC7231 makes it mandatory to support RFC850 dates too. So we roll
    # our own RFC-compliant parsing.
    for regex in RFC1123_DATE, RFC850_DATE, ASCTIME_DATE:
        m = regex.match(date)
        if m is not None:
            break
    else:
        raise ValueError("%r is not in a valid HTTP date format" % date)
    try:
        year = int(m.group('year'))
        if year < 100:
            if year < 70:
                year += 2000
            else:
                year += 1900
        month = MONTHS.index(m.group('mon').lower()) + 1
        day = int(m.group('day'))
        hour = int(m.group('hour'))
        min = int(m.group('min'))
        sec = int(m.group('sec'))
        result = datetime.datetime(year, month, day, hour, min, sec)
        return calendar.timegm(result.utctimetuple())
    except Exception as exc:
        raise ValueError("%r is not a valid date" % date) from exc


def send_document(url, data, timeout=10, method="post", *args, **kwargs):
    """Helper method to send a document via POST.

    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.

    :arg url: Full url to send to, including protocol
    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body
    :arg timeout: Seconds to wait for response (defaults to 10)
    :arg method: Method to use, defaults to post
    :returns: Tuple of status code (int or None) and error (exception class instance or None)
    """
    logger.debug("send_document: url=%s, data=%s, timeout=%s, method=%s", url, data, timeout, method)
    if not method:
        method = "post"
    headers = CaseInsensitiveDict({
        'User-Agent': USER_AGENT,
    })
    if "headers" in kwargs:
        # Update from kwargs
        headers.update(kwargs.get("headers"))
    kwargs.update({
        "data": data, "timeout": timeout, "headers": headers
    })
    request_func = getattr(requests, method)
    try:
        response = request_func(url, *args, **kwargs)
        logger.debug("send_document: response status code %s", response.status_code)
        return response.status_code, None
    # TODO support rate limit 429 code
    except RequestException as ex:
        logger.debug("send_document: exception %s", ex)
        return None, ex


def try_retrieve_webfinger_document(handle: str) -> Optional[str]:
    """
    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.
    """
    try:
        host = handle.split("@")[1]
    except (AttributeError, IndexError):
        logger.warning("retrieve_webfinger_document: invalid handle given: %s", handle)
        return None
    document, code, exception = fetch_document(
        host=host, path="/.well-known/webfinger?resource=acct:%s" % quote(handle),
    )
    if exception:
        logger.debug("retrieve_webfinger_document: failed to fetch webfinger document: %s, %s", code, exception)
    return document

if __name__ == "__main__":
    isT=try_retrieve_webfinger_document("dsf@https://github.com/Yelp/elastalert/issues/1927") is None and try_retrieve_webfinger_document("dsf@localhost") is None
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\yh199\Downloads\jaywink---federation\jaywink---federation/data_passk_platform1/630629e152e177c0ba46a0d1/"):
    #     f = open("C:\\Users\yh199\Downloads\jaywink---federation\jaywink---federation/data_passk_platform1/630629e152e177c0ba46a0d1/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = try_retrieve_webfinger_document(args0)
    #     print(args0)
    #     print(res0)
    #     print(content['output'][0])
    # #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    # #         isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/utils/diaspora_retrieve_and_parse_diaspora_webfinger_passk_validte.py
import json
import logging
import xml
from typing import Callable, Dict
from urllib.parse import quote

from lxml import html
from xrd import XRD
import sys
sys.path.append("/home/travis/builds/repos/jaywink---federation/")
from federation.inbound import handle_receive
from federation.types import RequestType
from federation.utils.network import fetch_document, try_retrieve_webfinger_document
from federation.utils.text import validate_handle

logger = logging.getLogger("federation")


def fetch_public_key(handle):
    """Fetch public key over the network.

    :param handle: Remote handle to retrieve public key for.
    :return: Public key in str format from parsed profile.
    """
    profile = retrieve_and_parse_profile(handle)
    return profile.public_key


def parse_diaspora_webfinger(document: str) -> Dict:
    """
    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).

    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html
    """
    webfinger = {
        "hcard_url": None,
    }
    # noinspection PyBroadException
    try:
        doc = json.loads(document)
        for link in doc["links"]:
            if link["rel"] == "http://microformats.org/profile/hcard":
                webfinger["hcard_url"] = link["href"]
                break
        else:
            logger.warning("parse_diaspora_webfinger: found JSON webfinger but it has no hcard href")
            raise ValueError
    except Exception:
        try:
            xrd = XRD.parse_xrd(document)
            webfinger["hcard_url"] = xrd.find_link(rels="http://microformats.org/profile/hcard").href
        except (xml.parsers.expat.ExpatError, TypeError):
            logger.warning("parse_diaspora_webfinger: found XML webfinger but it fails to parse")
            pass
    return webfinger


def retrieve_diaspora_hcard(handle):
    """
    Retrieve a remote Diaspora hCard document.

    :arg handle: Remote handle to retrieve
    :return: str (HTML document)
    """
    webfinger = retrieve_and_parse_diaspora_webfinger(handle)
    document, code, exception = fetch_document(webfinger.get("hcard_url"))
    if exception:
        return None
    return document


def retrieve_and_parse_diaspora_webfinger(handle):
    """
    Retrieve a and parse a remote Diaspora webfinger document.

    :arg handle: Remote handle to retrieve
    :returns: dict
    """
    document = try_retrieve_webfinger_document(handle)
    if document:
        return parse_diaspora_webfinger(document)
    host = handle.split("@")[1]
    hostmeta = retrieve_diaspora_host_meta(host)
    if not hostmeta:
        return None
    url = hostmeta.find_link(rels="lrdd").template.replace("{uri}", quote(handle))
    document, code, exception = fetch_document(url)
    if exception:
        return None
    return parse_diaspora_webfinger(document)


def retrieve_diaspora_host_meta(host):
    """
    Retrieve a remote Diaspora host-meta document.

    :arg host: Host to retrieve from
    :returns: ``XRD`` instance
    """
    document, code, exception = fetch_document(host=host, path="/.well-known/host-meta")
    if exception:
        return None
    xrd = XRD.parse_xrd(document)
    return xrd


def _get_element_text_or_none(document, selector):
    """
    Using a CSS selector, get the element and return the text, or None if no element.

    :arg document: ``HTMLElement`` document
    :arg selector: CSS selector
    :returns: str or None
    """
    element = document.cssselect(selector)
    if element:
        return element[0].text
    return None


def _get_element_attr_or_none(document, selector, attribute):
    """
    Using a CSS selector, get the element and return the given attribute value, or None if no element.

    Args:
        document (HTMLElement) - HTMLElement document
        selector (str) - CSS selector
        attribute (str) - The attribute to get from the element
    """
    element = document.cssselect(selector)
    if element:
        return element[0].get(attribute)
    return None


def parse_profile_from_hcard(hcard: str, handle: str):
    """
    Parse all the fields we can from a hCard document to get a Profile.

    :arg hcard: HTML hcard document (str)
    :arg handle: User handle in username@domain.tld format
    :returns: ``federation.entities.diaspora.entities.DiasporaProfile`` instance
    """
    from federation.entities.diaspora.entities import DiasporaProfile  # Circulars
    doc = html.fromstring(hcard)
    profile = DiasporaProfile(
        name=_get_element_text_or_none(doc, ".fn"),
        image_urls={
            "small": _get_element_attr_or_none(doc, ".entity_photo_small .photo", "src"),
            "medium": _get_element_attr_or_none(doc, ".entity_photo_medium .photo", "src"),
            "large": _get_element_attr_or_none(doc, ".entity_photo .photo", "src"),
        },
        public=True,
        id=handle,
        handle=handle,
        finger=handle,
        guid=_get_element_text_or_none(doc, ".uid"),
        public_key=_get_element_text_or_none(doc, ".key"),
        username=handle.split('@')[0],
        _source_protocol="diaspora",
    )
    return profile


def retrieve_and_parse_content(
        id: str, guid: str, handle: str, entity_type: str, cache: bool=True, 
        sender_key_fetcher: Callable[[str], str]=None):
    """Retrieve remote content and return an Entity class instance.

    This is basically the inverse of receiving an entity. Instead, we fetch it, then call "handle_receive".

    :param sender_key_fetcher: Function to use to fetch sender public key. If not given, network will be used
        to fetch the profile and the key. Function must take handle as only parameter and return a public key.
    :returns: Entity object instance or ``None``
    """
    if not validate_handle(handle):
        return
    _username, domain = handle.split("@")
    url = get_fetch_content_endpoint(domain, entity_type.lower(), guid)
    document, status_code, error = fetch_document(url, cache=cache)
    if status_code == 200:
        request = RequestType(body=document)
        _sender, _protocol, entities = handle_receive(request, sender_key_fetcher=sender_key_fetcher)
        if len(entities) > 1:
            logger.warning("retrieve_and_parse_content - more than one entity parsed from remote even though we"
                           "expected only one! ID %s", guid)
        if entities:
            return entities[0]
        return
    elif status_code == 404:
        logger.warning("retrieve_and_parse_content - remote content %s not found", guid)
        return
    if error:
        raise error
    raise Exception("retrieve_and_parse_content - unknown problem when fetching document: %s, %s, %s" % (
        document, status_code, error,
    ))


def retrieve_and_parse_profile(handle):
    """
    Retrieve the remote user and return a Profile object.

    :arg handle: User handle in username@domain.tld format
    :returns: ``federation.entities.Profile`` instance or None
    """
    hcard = retrieve_diaspora_hcard(handle)
    if not hcard:
        return None
    profile = parse_profile_from_hcard(hcard, handle)
    try:
        profile.validate()
    except ValueError as ex:
        logger.warning("retrieve_and_parse_profile - found profile %s but it didn't validate: %s",
                       profile, ex)
        return None
    return profile


def get_fetch_content_endpoint(domain, entity_type, guid):
    """Get remote fetch content endpoint.

    See: https://diaspora.github.io/diaspora_federation/federation/fetching.html
    """
    return "https://%s/fetch/%s/%s" % (domain, entity_type, guid)


def get_public_endpoint(id: str) -> str:
    """Get remote endpoint for delivering public payloads."""
    _username, domain = id.split("@")
    return "https://%s/receive/public" % domain


def get_private_endpoint(id: str, guid: str) -> str:
    """Get remote endpoint for delivering private payloads."""
    _username, domain = id.split("@")
    return "https://%s/receive/users/%s" % (domain, guid)



if __name__ == "__main__":
    from unittest.mock import patch, call, Mock

    def _is_str(s):
        return isinstance(s, str)


    class BaseHostMeta:
        def __init__(self, *args, **kwargs):
            self.xrd = XRD()

        def render(self):
            return self.xrd.to_xml().toprettyxml(indent="  ", encoding="UTF-8")


    class Title(object):
        def __init__(self, value, xml_lang=None):
            self.value = value
            self.xml_lang = xml_lang

        def __eq__(self, value):
            return str(self) == value

        def __str__(self):
            if self.xml_lang:
                return "%s:%s" % (self.xml_lang, self.value)
            return self.value


    class ListLikeObject(list):
        def __setitem__(self, key, value):
            value = self.item(value)
            super(ListLikeObject, self).__setitem__(key, value)

        def append(self, value):
            value = self.item(value)
            super(ListLikeObject, self).append(value)

        def extend(self, values):
            values = (self.item(value) for value in values)
            super(ListLikeObject, self).extend(values)
    class TitleList(ListLikeObject):
        def item(self, value):
            if _is_str(value):
                return Title(value)
            elif isinstance(value, (list, tuple)):
                return Title(*value)
            elif not isinstance(value, Title):
                raise ValueError('value must be an instance of Title')
            return value

    from xrd import Link
    class DiasporaHostMeta(BaseHostMeta):
        """Diaspora host-meta.

        Required keyword args:

        * webfinger_host (str)
        """

        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            link = Link(
                rel='lrdd',
                type_='application/xrd+xml',
                template='%s/webfinger?q={uri}' % kwargs["webfinger_host"]
            )
            self.xrd.links.append(link)

    isT = True
    retrieve_diaspora_host_meta = Mock(return_value=None)
    retrieve_and_parse_diaspora_webfinger("bob@localhost")
    #print(retrieve_diaspora_host_meta.call_args,retrieve_diaspora_host_meta.call_count)
    if not retrieve_diaspora_host_meta.call_args == call('localhost'):
        isT=False

    try_retrieve_webfinger_document = Mock(return_value=None)
    retrieve_and_parse_diaspora_webfinger("bob@localhost")
    #print(try_retrieve_webfinger_document.call_args, try_retrieve_webfinger_document.call_count)
    if not try_retrieve_webfinger_document.call_args == call("bob@localhost") or \
        not try_retrieve_webfinger_document.call_count == 1:
        isT=False


    try_retrieve_webfinger_document.reset_mock()
    retrieve_diaspora_host_meta.reset_mock()
    fetch_document = Mock(return_value=(None,None,None))
    parse_diaspora_webfinger = Mock(return_value={'hcard_url': None})

    with patch("federation.utils.diaspora_retrieve_and_parse_diaspora_webfinger_passk_validte.XRD.parse_xrd") \
          as mock_xrd:
        retrieve_diaspora_host_meta.return_value = DiasporaHostMeta(
            webfinger_host="https://localhost"
        ).xrd
        mock_xrd.return_value = "document"
        try_retrieve_webfinger_document.return_value=None
        fetch_document.return_value = (None, None, None)
        result = retrieve_and_parse_diaspora_webfinger("bob@localhost")
        try_retrieve_webfinger_document.assert_called_once_with("bob@localhost")
        calls = [
            call("https://localhost/webfinger?q=%s" % quote("bob@localhost")),
        ]
        #print(result)
        #print(fetch_document.call_args,fetch_document.call_count)
        if not calls == fetch_document.call_args_list or not result == {'hcard_url': None}:
            isT=False


    # @patch("federation.utils.diaspora.XRD.parse_xrd")
    # @patch("federation.utils.diaspora.fetch_document", return_value=(None, None, None))
    # @patch("federation.utils.diaspora.parse_diaspora_webfinger", return_value={'hcard_url': None})
    # @patch("federation.utils.diaspora.retrieve_diaspora_host_meta", return_value=None)
    # @patch("federation.utils.diaspora.try_retrieve_webfinger_document", return_value=None)
    # def test_fetch_document_is_called__to_fetch_xml_webfinger(
    # #         self, mock_try, mock_retrieve, mock_parse, mock_fetch, mock_xrd,
    # # ):
    # mock_retrieve.return_value = DiasporaHostMeta(
    #     webfinger_host="https://localhost"
    # ).xrd
    # mock_xrd.return_value = "document"
    # result = retrieve_and_parse_diaspora_webfinger("bob@localhost")
    # mock_try.assert_called_once_with("bob@localhost")
    # calls = [
    #     call("https://localhost/webfinger?q=%s" % quote("bob@localhost")),
    # ]
    # assert calls == mock_fetch.call_args_list
    # assert result == {'hcard_url': None}
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/jaywink---federation/data_passk_platform/630629e152e177c0ba46a0d2/"):
    #     f = open("/home/travis/builds/repos/jaywink---federation/data_passk_platform/630629e152e177c0ba46a0d2/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = retrieve_and_parse_diaspora_webfinger(args0)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/utils/diaspora_retrieve_diaspora_host_meta_passk_validte.py
import json
import logging
import xml
from typing import Callable, Dict
from urllib.parse import quote

from lxml import html
from xrd import XRD
import sys
sys.path.append("/home/travis/builds/repos/jaywink---federation/")
from federation.inbound import handle_receive
from federation.types import RequestType
from federation.utils.network import fetch_document, try_retrieve_webfinger_document
from federation.utils.text import validate_handle

logger = logging.getLogger("federation")


def fetch_public_key(handle):
    """Fetch public key over the network.

    :param handle: Remote handle to retrieve public key for.
    :return: Public key in str format from parsed profile.
    """
    profile = retrieve_and_parse_profile(handle)
    return profile.public_key


def parse_diaspora_webfinger(document: str) -> Dict:
    """
    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).

    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html
    """
    webfinger = {
        "hcard_url": None,
    }
    # noinspection PyBroadException
    try:
        doc = json.loads(document)
        for link in doc["links"]:
            if link["rel"] == "http://microformats.org/profile/hcard":
                webfinger["hcard_url"] = link["href"]
                break
        else:
            logger.warning("parse_diaspora_webfinger: found JSON webfinger but it has no hcard href")
            raise ValueError
    except Exception:
        try:
            xrd = XRD.parse_xrd(document)
            webfinger["hcard_url"] = xrd.find_link(rels="http://microformats.org/profile/hcard").href
        except (xml.parsers.expat.ExpatError, TypeError):
            logger.warning("parse_diaspora_webfinger: found XML webfinger but it fails to parse")
            pass
    return webfinger


def retrieve_diaspora_hcard(handle):
    """
    Retrieve a remote Diaspora hCard document.

    :arg handle: Remote handle to retrieve
    :return: str (HTML document)
    """
    webfinger = retrieve_and_parse_diaspora_webfinger(handle)
    document, code, exception = fetch_document(webfinger.get("hcard_url"))
    if exception:
        return None
    return document


def retrieve_and_parse_diaspora_webfinger(handle):
    """
    Retrieve a and parse a remote Diaspora webfinger document.

    :arg handle: Remote handle to retrieve
    :returns: dict
    """
    document = try_retrieve_webfinger_document(handle)
    if document:
        return parse_diaspora_webfinger(document)
    host = handle.split("@")[1]
    hostmeta = retrieve_diaspora_host_meta(host)
    if not hostmeta:
        return None
    url = hostmeta.find_link(rels="lrdd").template.replace("{uri}", quote(handle))
    document, code, exception = fetch_document(url)
    if exception:
        return None
    return parse_diaspora_webfinger(document)


def retrieve_diaspora_host_meta(host):
    """
    Retrieve a remote Diaspora host-meta document.

    :arg host: Host to retrieve from
    :returns: ``XRD`` instance
    """
    document, code, exception = fetch_document(host=host, path="/.well-known/host-meta")
    if exception:
        return None
    xrd = XRD.parse_xrd(document)
    return xrd


def _get_element_text_or_none(document, selector):
    """
    Using a CSS selector, get the element and return the text, or None if no element.

    :arg document: ``HTMLElement`` document
    :arg selector: CSS selector
    :returns: str or None
    """
    element = document.cssselect(selector)
    if element:
        return element[0].text
    return None


def _get_element_attr_or_none(document, selector, attribute):
    """
    Using a CSS selector, get the element and return the given attribute value, or None if no element.

    Args:
        document (HTMLElement) - HTMLElement document
        selector (str) - CSS selector
        attribute (str) - The attribute to get from the element
    """
    element = document.cssselect(selector)
    if element:
        return element[0].get(attribute)
    return None


def parse_profile_from_hcard(hcard: str, handle: str):
    """
    Parse all the fields we can from a hCard document to get a Profile.

    :arg hcard: HTML hcard document (str)
    :arg handle: User handle in username@domain.tld format
    :returns: ``federation.entities.diaspora.entities.DiasporaProfile`` instance
    """
    from federation.entities.diaspora.entities import DiasporaProfile  # Circulars
    doc = html.fromstring(hcard)
    profile = DiasporaProfile(
        name=_get_element_text_or_none(doc, ".fn"),
        image_urls={
            "small": _get_element_attr_or_none(doc, ".entity_photo_small .photo", "src"),
            "medium": _get_element_attr_or_none(doc, ".entity_photo_medium .photo", "src"),
            "large": _get_element_attr_or_none(doc, ".entity_photo .photo", "src"),
        },
        public=True,
        id=handle,
        handle=handle,
        finger=handle,
        guid=_get_element_text_or_none(doc, ".uid"),
        public_key=_get_element_text_or_none(doc, ".key"),
        username=handle.split('@')[0],
        _source_protocol="diaspora",
    )
    return profile


def retrieve_and_parse_content(
        id: str, guid: str, handle: str, entity_type: str, cache: bool=True, 
        sender_key_fetcher: Callable[[str], str]=None):
    """Retrieve remote content and return an Entity class instance.

    This is basically the inverse of receiving an entity. Instead, we fetch it, then call "handle_receive".

    :param sender_key_fetcher: Function to use to fetch sender public key. If not given, network will be used
        to fetch the profile and the key. Function must take handle as only parameter and return a public key.
    :returns: Entity object instance or ``None``
    """
    if not validate_handle(handle):
        return
    _username, domain = handle.split("@")
    url = get_fetch_content_endpoint(domain, entity_type.lower(), guid)
    document, status_code, error = fetch_document(url, cache=cache)
    if status_code == 200:
        request = RequestType(body=document)
        _sender, _protocol, entities = handle_receive(request, sender_key_fetcher=sender_key_fetcher)
        if len(entities) > 1:
            logger.warning("retrieve_and_parse_content - more than one entity parsed from remote even though we"
                           "expected only one! ID %s", guid)
        if entities:
            return entities[0]
        return
    elif status_code == 404:
        logger.warning("retrieve_and_parse_content - remote content %s not found", guid)
        return
    if error:
        raise error
    raise Exception("retrieve_and_parse_content - unknown problem when fetching document: %s, %s, %s" % (
        document, status_code, error,
    ))


def retrieve_and_parse_profile(handle):
    """
    Retrieve the remote user and return a Profile object.

    :arg handle: User handle in username@domain.tld format
    :returns: ``federation.entities.Profile`` instance or None
    """
    hcard = retrieve_diaspora_hcard(handle)
    if not hcard:
        return None
    profile = parse_profile_from_hcard(hcard, handle)
    try:
        profile.validate()
    except ValueError as ex:
        logger.warning("retrieve_and_parse_profile - found profile %s but it didn't validate: %s",
                       profile, ex)
        return None
    return profile


def get_fetch_content_endpoint(domain, entity_type, guid):
    """Get remote fetch content endpoint.

    See: https://diaspora.github.io/diaspora_federation/federation/fetching.html
    """
    return "https://%s/fetch/%s/%s" % (domain, entity_type, guid)


def get_public_endpoint(id: str) -> str:
    """Get remote endpoint for delivering public payloads."""
    _username, domain = id.split("@")
    return "https://%s/receive/public" % domain


def get_private_endpoint(id: str, guid: str) -> str:
    """Get remote endpoint for delivering private payloads."""
    _username, domain = id.split("@")
    return "https://%s/receive/users/%s" % (domain, guid)

if __name__ == "__main__":
    isT = True
    from unittest.mock import patch, Mock

    fetch_document = Mock()

    with patch("diaspora_retrieve_and_parse_diaspora_webfinger_passk_validte.XRD.parse_xrd") as mock_xrd:
        fetch_document.return_value = "document", None, None
        mock_xrd.return_value = "document"
        document = retrieve_diaspora_host_meta("localhost")
        fetch_document.assert_called_with(host="localhost", path="/.well-known/host-meta")
        if not document == "document":
            isT=False


    fetch_document.reset_mock()
    fetch_document.return_value = None, None, ValueError()
    document = retrieve_diaspora_host_meta("localhost")
    fetch_document.assert_called_with(host="localhost", path="/.well-known/host-meta")
    if not document is None:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/jaywink---federation/data_passk_platform/630629e252e177c0ba46a0d6/"):
    #     f = open("/home/travis/builds/repos/jaywink---federation/data_passk_platform/630629e252e177c0ba46a0d6/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     res0 = retrieve_diaspora_host_meta(args0)
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/jaywink---federation/federation/utils/network_send_document_passk_validte.py
import calendar
import datetime
import logging
import re
import socket
from typing import Optional, Dict
from urllib.parse import quote
from uuid import uuid4

import requests
from requests_cache import CachedSession, DO_NOT_CACHE
from requests.exceptions import RequestException, HTTPError, SSLError
from requests.exceptions import ConnectionError
from requests.structures import CaseInsensitiveDict
import sys
sys.path.append("/home/travis/builds/repos/jaywink---federation/")
from federation import __version__
from federation.utils.django import get_requests_cache_backend

logger = logging.getLogger("federation")

USER_AGENT = "python/federation/%s" % __version__

session = CachedSession('fed_cache', backend=get_requests_cache_backend('fed_cache'))
EXPIRATION = datetime.timedelta(hours=6)

def fetch_content_type(url: str) -> Optional[str]:
    """
    Fetch the HEAD of the remote url to determine the content type.
    """
    try:
        response = session.head(url, headers={'user-agent': USER_AGENT}, timeout=10)
    except RequestException as ex:
        logger.warning("fetch_content_type - %s when fetching url %s", ex, url)
    else:
        return response.headers.get('Content-Type')


def fetch_document(url=None, host=None, path="/", timeout=10, raise_ssl_errors=True, extra_headers=None, cache=True, **kwargs):
    """Helper method to fetch remote document.

    Must be given either the ``url`` or ``host``.
    If ``url`` is given, only that will be tried without falling back to http from https.
    If ``host`` given, `path` will be added to it. Will fall back to http on non-success status code.

    :arg url: Full url to fetch, including protocol
    :arg host: Domain part only without path or protocol
    :arg path: Path without domain (defaults to "/")
    :arg timeout: Seconds to wait for response (defaults to 10)
    :arg raise_ssl_errors: Pass False if you want to try HTTP even for sites with SSL errors (default True)
    :arg extra_headers: Optional extra headers dictionary to add to requests
    :arg kwargs holds extra args passed to requests.get
    :returns: Tuple of document (str or None), status code (int or None) and error (an exception class instance or None)
    :raises ValueError: If neither url nor host are given as parameters
    """
    if not url and not host:
        raise ValueError("Need url or host.")

    logger.debug("fetch_document: url=%s, host=%s, path=%s, timeout=%s, raise_ssl_errors=%s",
                 url, host, path, timeout, raise_ssl_errors)
    headers = {'user-agent': USER_AGENT}
    if extra_headers:
        headers.update(extra_headers)
    if url:
        # Use url since it was given
        logger.debug("fetch_document: trying %s", url)
        try:
            response = session.get(url, timeout=timeout, headers=headers, 
                    expire_after=EXPIRATION if cache else DO_NOT_CACHE, **kwargs)
            logger.debug("fetch_document: found document, code %s", response.status_code)
            response.raise_for_status()
            if not response.encoding: response.encoding = 'utf-8'
            return response.text, response.status_code, None
        except RequestException as ex:
            logger.debug("fetch_document: exception %s", ex)
            return None, None, ex
    # Build url with some little sanitizing
    host_string = host.replace("http://", "").replace("https://", "").strip("/")
    path_string = path if path.startswith("/") else "/%s" % path
    url = "https://%s%s" % (host_string, path_string)
    logger.debug("fetch_document: trying %s", url)
    try:
        response = session.get(url, timeout=timeout, headers=headers)
        logger.debug("fetch_document: found document, code %s", response.status_code)
        response.raise_for_status()
        return response.text, response.status_code, None
    except (HTTPError, SSLError, ConnectionError) as ex:
        if isinstance(ex, SSLError) and raise_ssl_errors:
            logger.debug("fetch_document: exception %s", ex)
            return None, None, ex
        # Try http then
        url = url.replace("https://", "http://")
        logger.debug("fetch_document: trying %s", url)
        try:
            response = session.get(url, timeout=timeout, headers=headers)
            logger.debug("fetch_document: found document, code %s", response.status_code)
            response.raise_for_status()
            return response.text, response.status_code, None
        except RequestException as ex:
            logger.debug("fetch_document: exception %s", ex)
            return None, None, ex
    except RequestException as ex:
        logger.debug("fetch_document: exception %s", ex)
        return None, None, ex


def fetch_host_ip(host: str) -> str:
    """
    Fetch ip by host
    """
    try:
        ip = socket.gethostbyname(host)
    except socket.gaierror:
        return ''

    return ip


def fetch_file(url: str, timeout: int = 30, extra_headers: Dict = None) -> str:
    """
    Download a file with a temporary name and return the name.
    """
    headers = {'user-agent': USER_AGENT}
    if extra_headers:
        headers.update(extra_headers)
    response = session.get(url, timeout=timeout, headers=headers, stream=True)
    response.raise_for_status()
    name = f"/tmp/{str(uuid4())}"
    with open(name, "wb") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    return name


def parse_http_date(date):
    """
    Parse a date format as specified by HTTP RFC7231 section 7.1.1.1.

    The three formats allowed by the RFC are accepted, even if only the first
    one is still in widespread use.

    Return an integer expressed in seconds since the epoch, in UTC.

    Implementation copied from Django.
    https://github.com/django/django/blob/master/django/utils/http.py#L157
    License: BSD 3-clause
    """
    MONTHS = 'jan feb mar apr may jun jul aug sep oct nov dec'.split()
    __D = r'(?P<day>\d{2})'
    __D2 = r'(?P<day>[ \d]\d)'
    __M = r'(?P<mon>\w{3})'
    __Y = r'(?P<year>\d{4})'
    __Y2 = r'(?P<year>\d{2})'
    __T = r'(?P<hour>\d{2}):(?P<min>\d{2}):(?P<sec>\d{2})'
    RFC1123_DATE = re.compile(r'^\w{3}, %s %s %s %s GMT$' % (__D, __M, __Y, __T))
    RFC850_DATE = re.compile(r'^\w{6,9}, %s-%s-%s %s GMT$' % (__D, __M, __Y2, __T))
    ASCTIME_DATE = re.compile(r'^\w{3} %s %s %s %s$' % (__M, __D2, __T, __Y))
    # email.utils.parsedate() does the job for RFC1123 dates; unfortunately
    # RFC7231 makes it mandatory to support RFC850 dates too. So we roll
    # our own RFC-compliant parsing.
    for regex in RFC1123_DATE, RFC850_DATE, ASCTIME_DATE:
        m = regex.match(date)
        if m is not None:
            break
    else:
        raise ValueError("%r is not in a valid HTTP date format" % date)
    try:
        year = int(m.group('year'))
        if year < 100:
            if year < 70:
                year += 2000
            else:
                year += 1900
        month = MONTHS.index(m.group('mon').lower()) + 1
        day = int(m.group('day'))
        hour = int(m.group('hour'))
        min = int(m.group('min'))
        sec = int(m.group('sec'))
        result = datetime.datetime(year, month, day, hour, min, sec)
        return calendar.timegm(result.utctimetuple())
    except Exception as exc:
        raise ValueError("%r is not a valid date" % date) from exc


def send_document(url, data, timeout=10, method="post", *args, **kwargs):
    """Helper method to send a document via POST.

    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.

    :arg url: Full url to send to, including protocol
    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body
    :arg timeout: Seconds to wait for response (defaults to 10)
    :arg method: Method to use, defaults to post
    :returns: Tuple of status code (int or None) and error (exception class instance or None)
    """
    logger.debug("send_document: url=%s, data=%s, timeout=%s, method=%s", url, data, timeout, method)
    if not method:
        method = "post"
    headers = CaseInsensitiveDict({
        'User-Agent': USER_AGENT,
    })
    if "headers" in kwargs:
        # Update from kwargs
        headers.update(kwargs.get("headers"))
    kwargs.update({
        "data": data, "timeout": timeout, "headers": headers
    })
    request_func = getattr(requests, method)
    try:
        response = request_func(url, *args, **kwargs)
        logger.debug("send_document: response status code %s", response.status_code)
        return response.status_code, None
    # TODO support rate limit 429 code
    except RequestException as ex:
        logger.debug("send_document: exception %s", ex)
        return None, ex


def try_retrieve_webfinger_document(handle: str) -> Optional[str]:
    """
    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.
    """
    try:
        host = handle.split("@")[1]
    except (AttributeError, IndexError):
        logger.warning("retrieve_webfinger_document: invalid handle given: %s", handle)
        return None
    document, code, exception = fetch_document(
        host=host, path="/.well-known/webfinger?resource=acct:%s" % quote(handle),
    )
    if exception:
        logger.debug("retrieve_webfinger_document: failed to fetch webfinger document: %s, %s", code, exception)
    return document

if __name__ == "__main__":
    from unittest.mock import patch, Mock

    isT = True
    call_args = {"timeout": 10, "headers": {'user-agent': USER_AGENT}}
    with patch("requests.post", return_value=Mock(status_code=200)) as mock_post:
        code, exc = send_document("http://localhost", {"foo": "bar"})
        mock_post.assert_called_once_with(
            "http://localhost", data={"foo": "bar"}, **call_args
        )
        if not code == 200:
            isT=False
        if not exc == None:
            isT=False

    with patch("requests.post", side_effect=RequestException):
        code, exc = send_document("http://localhost", {"foo": "bar"})
        if not code == None:
            isT=False
        if not exc.__class__ == RequestException:
            isT=False


    # A failure might raise:
    # TypeError: MagicMock object got multiple values for keyword argument 'headers'
    with patch("requests.post", return_value=Mock(status_code=200)) as mock_post:
        send_document("http://localhost", {"foo": "bar"}, **call_args)
        mock_post.assert_called_once_with(
            "http://localhost", data={"foo": "bar"}, **call_args
        )

    with patch("requests.post", return_value=Mock(status_code=200)) as mock_post:
        send_document("http://localhost", {"foo": "bar"}, **call_args)
        mock_post.assert_called_once_with(
            "http://localhost", data={"foo": "bar"}, headers={'user-agent': USER_AGENT}, timeout=10
        )
        mock_post.reset_mock()
        send_document("http://localhost", {"foo": "bar"}, headers={'User-Agent': USER_AGENT})
        mock_post.assert_called_once_with(
            "http://localhost", data={"foo": "bar"}, headers={'User-Agent': USER_AGENT}, timeout=10
        )
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/jaywink---federation/data_passk_platform/630629e752e177c0ba46a0fb/"):
    #     f = open("/home/travis/builds/repos/jaywink---federation/data_passk_platform/630629e752e177c0ba46a0fb/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     if isinstance(content["input"]["args"][1]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1=content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"],bytes):
    #         args2=dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2=content["input"]["args"][2]["bytes"]
    #     if isinstance(content["input"]["args"][3]["bytes"],bytes):
    #         args3=dill.loads(content["input"]["args"][3]["bytes"])
    #     else:
    #         args3=content["input"]["args"][3]["bytes"]
    #     res0 = send_document(args0,args1,args2,args3)
    #     # print(res0)
    #     # print(content["output"][0])
    #     if str(res0[1]).split(":")[0]!=str(content["output"][0]).split(":")[0]:
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/utils/dict_utils_dict_insert_passk_validte.py
"""This module provides helper methods for dict merging and dict insertion. """
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.utils import logger

LOG = logger.LOG


def dict_insert(dic, val, key, *keys):
    """insert a value of a nested key into a dictionary

    to insert value for a nested key, all ancestor keys should be given as
    method's arguments

    example:
      dict_insert({}, 'val', 'key1.key2'.split('.'))

    :param dic: a dictionary object to insert the nested key value into
    :param val: a value to insert to the given dictionary
    :param key: first key in a chain of key that will store the value
    :param keys: sub keys in the keys chain
    """
    if dic is None:
        return

    if not keys:
        if isinstance(dic.get(key, None), dict) and isinstance(val, dict):
            dict_merge(dic[key], val)
        else:
            dic[key] = val
        return

    dict_insert(dic.setdefault(key, {}), val, *keys)


class ConflictResolver(object):
    """Resolves conflicts while merging dicts. """

    @staticmethod
    def none_resolver(first, second, key):
        """Replaces value in first dict only if it is None.

        Appends second value into the first in type is list.
        """

        # tyr to merge lists first
        if isinstance(first[key], list):
            if isinstance(second[key], list):
                first[key].extend(second[key])
            elif second[key] is not None:
                first[key].append(second[key])

        if key not in first or first[key] is None:
            first[key] = second[key]

    @staticmethod
    def greedy_resolver(first, second, key):
        """Replace always first with the value from second """
        first[key] = second[key]

    @staticmethod
    def unique_append_list_resolver(first, second, key):
        """Merges first and second lists """
        if isinstance(first[key], list) and isinstance(second[key], list):
            for item in second[key]:
                if item not in first[key]:
                    first[key].append(item)
        else:
            return ConflictResolver.greedy_resolver(first, second, key)


def dict_merge(first, second,
               conflict_resolver=ConflictResolver.greedy_resolver):
    """Merge `second` dict into `first`.

    :param first: Modified dict
    :param second: Modifier dict
    :param conflict_resolver: Function that resolves a merge between 2 values
        when one of them isn't a dict
    """
    for key in second:
        if key in first:
            if isinstance(first[key], dict) and isinstance(second[key], dict):
                dict_merge(first[key], second[key],
                           conflict_resolver=conflict_resolver)
            else:
                # replace first value with the value from second
                conflict_resolver(first, second, key)
        else:
            try:
                first[key] = second[key]
            except TypeError as e:
                LOG.error("dict_merge(%s, %s) failed on: %s" % (first, second, key))
                raise e

if __name__ == "__main__":
    isT=True
    dict_input={"project":"project1","method_name":"method_name1"}
    dict_insert(dict_input,"project_new","project")
    if dict_input["project"] != "project_new" or dict_input["method_name"] != "method_name1":
        isT=False
    if not isT:
        raise Exception("Result not True!!!")
    dict_input = {"project": "project1", "method_name": "method_name1"}
    dict_insert(dict_input, "class1", "class_name")
    if dict_input["project"] != "project1" or dict_input["method_name"] != "method_name1" or dict_input["class_name"] != "class1":
        isT=False
    if not isT:
        raise Exception("Result not True!!!")
    dict_input = {"project": "project1", "method_name": "method_name1"}
    dict_insert(dict_input, "project_new", "project")
    if dict_input["project"] != "project_new" or dict_input["method_name"] != "method_name1":
        isT=False
    if not isT:
        raise Exception("Result not True!!!")
    dict_input = {"project": {"project_new":"zz"}, "method_name": "method_name1"}
    dict_insert(dict_input, {"project_new":"zz"}, "project")
    if dict_input["project"]["project_new"] != "zz" or dict_input["method_name"] != "method_name1":
        isT=False
    if not isT:
        raise Exception("Result not True!!!")
    dict_input = None
    dict_insert(None, {"project_new": "zz"}, "project")
    if dict_input is not None:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")
    dict_input = {"project": {"project_new": "zz"}, "method_name": "method_name1"}
    dict_insert(dict_input, {"project_new": "zz"}, "project","a")
    if dict_input["project"]["project_new"] != "zz" or dict_input["project"]["a"]["project_new"]  != "zz" or dict_input["method_name"] != "method_name1":
        isT=False
    if not isT:
        raise Exception("Result not True!!!")
    print(isT)



----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/tests/test_complex_types_list_of_file_names_passk_validte.py
import os

# import pytest
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.cli import cli
# from infrared.core.utils import exceptions


# @pytest.fixture
# def list_value_type():
#     """
#     Create a new list value complex type
#     """
#     return cli.ListValue("test", [os.getcwd(), ], 'cmd', None)
#
#
# @pytest.fixture
# def dict_type():
#     """Create a new IniType complex type
#     """
#     return cli.Dict("TestDict", None, None, None)
#
#
# @pytest.fixture
# def nested_dict():
#     """Create a new NestedDict complex type
#     """
#     return cli.NestedDict("TestNestedDict", None, None, None)
#
#
# @pytest.fixture
# def nested_list():
#     """Create a new NestedList complex type
#     """
#     return cli.NestedList("TestNestedList", None, None, None)
#
#
# @pytest.fixture
# def flag_type():
#     """Create a new Flag complex type
#     """
#     return cli.Flag("test", None, None, None)
#
#
# @pytest.mark.parametrize(
#     "test_value,expected", [
#         ("item1,item2", ["item1", "item2"]),
#         ("item1", ["item1", ]),
#         ("item1,item2,item3,", ["item1", "item2", "item3", ''])])
# def test_list_value_resolve(list_value_type, test_value, expected):
#     """
#     Verifies the string value can be resolved to the list.
#     """
#     assert expected == list_value_type.resolve(test_value)
#
#
# @pytest.mark.parametrize("input_value, expected_return", [
#     (['k1=v1'], {'k1': 'v1'}),
#     (['l1.s1.k1=v1'], {'l1': {'s1': {'k1': 'v1'}}}),
#     ([' s1.k1=v1 '], {'s1': {'k1': 'v1'}}),
#     (['s1.k1=v1', 's1.k2=v2', 's2.k3=v3'],
#      {'s1': {'k1': 'v1', 'k2': 'v2'}, 's2': {'k3': 'v3'}}),
#     ('k1=v1', {'k1': 'v1'}),
#     ('s1.k1=v1', {'s1': {'k1': 'v1'}}),
#     (' s1.k1=v1 ', {'s1': {'k1': 'v1'}}),
#     ('s1.k1=v1,s1.k2=v2,s2.k3=v3',
#      {'s1': {'k1': 'v1', 'k2': 'v2'}, 's2': {'k3': 'v3'}}),
#     ('s1.k1=v1, s1.l2.k2=v2, s2.k3=v3',
#      {'s1': {'k1': 'v1', 'l2': {'k2': 'v2'}}, 's2': {'k3': 'v3'}}),
# ])
# def test_nested_dict_resolve(input_value, expected_return, nested_dict):
#     """Verifies the return value of 'resolve' method in 'NestedDict' Complex type
#     """
#     assert nested_dict.resolve(input_value) == expected_return
#
#
# @pytest.mark.parametrize("input_value, expected_return", [
#     (["k1=v1","k2=v2"], [{"k1":"v1"},{"k2":"v2"}])
# ])
# def test_nested_list_resolve(input_value, expected_return, nested_list):
#     """Verifies the return value of 'resolve' method in 'NestedList'
#     Complex type"""
#     assert nested_list.resolve(input_value) == expected_return
#
#
# @pytest.mark.parametrize("input_value, expected_return", [
#     (['k1=v1'], {'k1': 'v1'}),
#     (['l1.s1.k1=v1'], {'l1.s1.k1': 'v1'}),
#     ([' s1.k1=v1 '], {'s1.k1': 'v1'}),
#     (['s1.k1=v1', 's1.k2=v2', 's2.k3=v3'],
#      {'s1.k1': 'v1', 's1.k2': 'v2', 's2.k3': 'v3'}),
#     ('k1=v1', {'k1': 'v1'}),
# ])
# def test_dict_type_resolve(input_value, expected_return, dict_type):
#     """Verifies the return value of 'resolve' method in 'IniType' Complex type
#     """
#     assert dict_type.resolve(input_value) == expected_return
#
#
# @pytest.mark.parametrize("input_value, expected_return", [
#     ('test', True),
# ])
# def test_flag_type_resolve(input_value, expected_return, flag_type):
#     """Verifies the return value of 'resolve' method in 'Flag' Complex type
#     """
#     assert flag_type.resolve(input_value) == expected_return
#
#
# @pytest.fixture(scope="module")
# def file_root_dir(tmpdir_factory):
#     """Prepares the testing dirs for file tests"""
#     root_dir = tmpdir_factory.mktemp('complex_file_dir')
#
#     for file_path in ['file1.yml',
#                       'arg/name/file2',
#                       'defaults/arg/name/file.yml',
#                       'defaults/arg/name/file2',
#                       'vars/arg/name/file1.yml',
#                       'vars/arg/name/file3.yml',
#                       'vars/arg/name/nested/file4.yml']:
#         root_dir.join(file_path).ensure()
#
#     return root_dir
#
#
# @pytest.fixture(scope="module")
# def dir_root_dir(tmpdir_factory):
#     """Prepares the testing dirs for dir tests"""
#     root_dir = tmpdir_factory.mktemp('complex_dir')
#
#     for dir_path in ['dir0/1.file',
#                      'arg/name/dir1/1.file',
#                      'vars/arg/name/dir2/1.file',
#                      'defaults/arg/name/dir3/1.file']:
#         # creating a file will create a dir
#         root_dir.join(dir_path).ensure()
#
#     return root_dir
#
#
# def create_file_type(root_dir, type_class):
#     return type_class("arg-name",
#                       (root_dir.join('vars').strpath,
#                        root_dir.join('defaults').strpath),
#                       None,
#                       None)
#
#
# @pytest.fixture
# def file_type(file_root_dir, request):
#     return create_file_type(file_root_dir, request.param)
#
#
# @pytest.fixture
# def dir_type(dir_root_dir, request):
#     return create_file_type(dir_root_dir, request.param)
#
#
# @pytest.mark.parametrize('file_type', [cli.FileType], indirect=True)
# def test_file_type_resolve(file_root_dir, file_type, monkeypatch):
#     """Verifies the file complex type"""
#     # change cwd to the temp dir
#     monkeypatch.setattr("os.getcwd", lambda: file_root_dir.strpath)
#
#     assert file_type.resolve('file1') == file_root_dir.join(
#         'file1.yml').strpath
#     assert file_type.resolve('file2') == file_root_dir.join(
#         'arg/name/file2').strpath
#     with pytest.raises(exceptions.IRFileNotFoundException):
#         file_type.resolve('file.yml')
#
#
# @pytest.mark.parametrize('file_type', [cli.VarFileType], indirect=True)
# def test_var_file_type_resolve(file_root_dir, file_type, monkeypatch):
#     """Verifies the file complex type"""
#     # change cwd to the temp dir
#     monkeypatch.setattr("os.getcwd", lambda: file_root_dir.strpath)
#
#     assert file_type.resolve('file1') == file_root_dir.join(
#         'file1.yml').strpath
#     assert file_type.resolve(
#         os.path.abspath('file1')) == file_root_dir.join('file1.yml').strpath
#     assert file_type.resolve('file2') == file_root_dir.join(
#         'arg/name/file2').strpath
#     assert file_type.resolve('file.yml') == file_root_dir.join(
#         'defaults/arg/name/file.yml').strpath
#     assert file_type.resolve('file3') == file_root_dir.join(
#         'vars/arg/name/file3.yml').strpath
#     assert file_type.resolve('nested/file4.yml') == file_root_dir.join(
#         'vars/arg/name/nested/file4.yml').strpath
#
#     with pytest.raises(exceptions.IRFileNotFoundException):
#         file_type.resolve('file5')
#
#
# @pytest.mark.parametrize('file_type', [cli.ListFileType], indirect=True)
# def test_list_of_var_files(file_root_dir, file_type, monkeypatch):
#     """Verifies the list of files"""
#     monkeypatch.setattr("os.getcwd", lambda: file_root_dir.strpath)
#
#     assert file_type.resolve('file1') == [
#         file_root_dir.join('file1.yml').strpath]
#     assert file_type.resolve('file1,file2') == [
#         file_root_dir.join('file1.yml').strpath,
#         file_root_dir.join('arg/name/file2').strpath]
#     assert file_type.resolve('file3.yml,vars/arg/name/file3') == [
#         file_root_dir.join('vars/arg/name/file3.yml').strpath,
#         file_root_dir.join('vars/arg/name/file3.yml').strpath]
#
#
# @pytest.mark.parametrize('dir_type', [cli.VarDirType], indirect=True)
# def test_dir_type_resolve(dir_root_dir, dir_type, monkeypatch):
#     """Verifies the file complex type"""
#     # change cwd to the temp dir
#     monkeypatch.setattr("os.getcwd", lambda: dir_root_dir.strpath)
#
#     assert dir_type.resolve('dir0') == dir_root_dir.join(
#         'dir0/').strpath
#     assert dir_type.resolve('dir1') == dir_root_dir.join(
#         'arg/name/dir1/').strpath
#     assert dir_type.resolve('dir2') == dir_root_dir.join(
#         'vars/arg/name/dir2/').strpath
#     assert dir_type.resolve('dir3') == dir_root_dir.join(
#         'defaults/arg/name/dir3/').strpath
#     with pytest.raises(exceptions.IRFileNotFoundException):
#         dir_type.resolve('dir4')


def list_of_file_names(settings_dirs, spec_option):
    """Create a new IniType complex type
    """
    return cli.ListOfFileNames("ListOfFileNames", settings_dirs, None,
                               spec_option)


# def test_list_of_file_names_values_auto_propagation():
#     expected = ["task1", "task2", "task3"]
#     settings_dirs = ["", "", 'tests/example']
#     spec_option = {'lookup_dir': 'post_tasks'}
#
#     complex_action = list_of_file_names(settings_dirs, spec_option)
#     allowed_values = complex_action.get_allowed_values()
#
#     assert expected.sort() == allowed_values.sort()
#
#
# def test_list_of_file_names_resolve():
#     expected = ["task2", "task3"]
#     settings_dirs = ["", "", 'tests/example/']
#     spec_option = {'lookup_dir': 'post_tasks'}
#     value = "task2,task3"
#
#     complex_action = list_of_file_names(settings_dirs, spec_option)
#     values = complex_action.resolve(value)
#     print(values)
#
#     assert expected.sort() == values.sort()

if __name__ == "__main__":
    isT=True

    obj=list_of_file_names(['', '', 'tests/example'],{'lookup_dir': 'post_tasks'})
    print(obj.settings_dirs)
    print(obj.spec_option)
    print(obj.files_path)
    if obj.settings_dirs!=['', '', 'tests/example'] or obj.spec_option!={'lookup_dir': 'post_tasks'} or obj.files_path!="tests/example/post_tasks":
        isT=False

    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/services/__init___ansible_config_manager_passk_validte.py
"""Service locator for the IR services

Stores and resolves all the dependencies for the services.
"""
import os
import sys
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.services import ansible_config
from infrared.core.services import execution_logger
from infrared.core.services import plugins
from infrared.core.services import workspaces
from infrared.core.utils import logger

LOG = logger.LOG


class ServiceName(object):
    """Holds the supported services names. """
    WORKSPACE_MANAGER = "workspace_manager"
    PLUGINS_MANAGER = "plugins_manager"
    ANSIBLE_CONFIG_MANAGER = "ansible_config_manager"
    EXECUTION_LOGGER_MANAGER = "execution_logger_manager"


class CoreSettings(object):
    """Holds the main settings for the infrared. """

    def __init__(self, workspaces_base_folder=None,
                 plugins_conf_file=None,
                 install_plugin_at_start=True,
                 plugins_base_folder=None):
        """Constructor.

        :param workspaces_base_folder: folder where the
               workspace will be stored
        :param plugins_conf_file: location of the plugins.ini file with the
               list of all plugins and types.
        :param install_plugin_at_start: specifies whether all the plugins
               should be installed on ir start. Skip installation may be
               required for unit tests, for example.
        """

        self.infrared_home = os.path.abspath(os.environ.get(
            "IR_HOME", os.path.join(os.path.expanduser("~"), '.infrared')))

        # todo(obaranov) replace .workspaces to workspaces and .plugins.ini to
        # todo(obaranov) plugins.ini once IR is packaged as pip
        self.plugins_conf_file = plugins_conf_file or os.path.join(
            self.infrared_home, '.plugins.ini')
        self.workspaces_base_folder = workspaces_base_folder or os.path.join(
            self.infrared_home, '.workspaces')
        self.install_plugin_at_start = install_plugin_at_start
        self.plugins_base_folder = plugins_base_folder or os.path.join(
            self.infrared_home, 'plugins')


class CoreServices(object):
    """Holds and configures all the required for core services. """

    _SERVICES = {}

    @classmethod
    def setup(cls, core_settings=None):
        """Creates configuration from file or from defaults.

        :param core_settings: the instance of the CoreSettings class with the
        desired settings. If None is provided then the default settings
        will be used.
        """

        if core_settings is None:
            core_settings = CoreSettings()

        # create workspace manager
        if ServiceName.WORKSPACE_MANAGER not in cls._SERVICES:
            cls.register_service(ServiceName.WORKSPACE_MANAGER,
                                 workspaces.WorkspaceManager(
                                     core_settings.workspaces_base_folder))

        # create plugins manager
        if ServiceName.PLUGINS_MANAGER not in cls._SERVICES:
            # A temporary WH to skip all plugins installation on first InfraRed
            # command if the command is 'infrared plugin add'.
            # Should be removed together with auto plugins installation
            # mechanism.
            skip_plugins_install = {'plugin', 'add'}.issubset(sys.argv)
            cls.register_service(
                ServiceName.PLUGINS_MANAGER, plugins.InfraredPluginManager(
                    plugins_conf=core_settings.plugins_conf_file,
                    install_plugins=(core_settings.install_plugin_at_start and
                                     not skip_plugins_install),
                    plugins_dir=core_settings.plugins_base_folder))

        # create ansible config manager
        if ServiceName.ANSIBLE_CONFIG_MANAGER not in cls._SERVICES:
            cls.register_service(ServiceName.ANSIBLE_CONFIG_MANAGER,
                                 ansible_config.AnsibleConfigManager(
                                     core_settings.infrared_home))

        # create execution logger manager
        if ServiceName.EXECUTION_LOGGER_MANAGER not in cls._SERVICES:
            # get ansible manager
            ansible_manager = CoreServices.ansible_config_manager()
            # build log file path
            log_file = \
                os.path.join(core_settings.infrared_home, 'ir-commands.log')
            cls.register_service(ServiceName.EXECUTION_LOGGER_MANAGER,
                                 execution_logger.ExecutionLoggerManager(
                                     ansible_manager.ansible_config_path,
                                     log_file=log_file))

    @classmethod
    def register_service(cls, service_name, service):
        """Protect the _SERVICES dict"""
        CoreServices._SERVICES[service_name] = service

    @classmethod
    def _get_service(cls, name):
        if name not in cls._SERVICES:
            cls.setup()
        return cls._SERVICES[name]

    @classmethod
    def workspace_manager(cls):
        """Gets the workspace manager. """
        return cls._get_service(ServiceName.WORKSPACE_MANAGER)

    @classmethod
    def plugins_manager(cls):
        """Gets the plugin manager. """
        return cls._get_service(ServiceName.PLUGINS_MANAGER)

    @classmethod
    def ansible_config_manager(cls):
        """Gets the ansible config manager. """
        return cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)

    @classmethod
    def execution_logger_manager(cls):
        """Gets the execution logger manager. """
        return cls._get_service(ServiceName.EXECUTION_LOGGER_MANAGER)

if __name__ == "__main__":
    CoreServices.register_service("huawei", "service_huawei")
    CoreServices.register_service("apple", "apple!!!")
    CoreServices.register_service("xiaomi", "xiaomi!!!")
    CoreServices.register_service("workspace_manager", "world!!")
    CoreServices.register_service("plugins_manager", "best world!!")
    CoreServices.register_service("ansible_config_manager", "best univers!!")
    isT = CoreServices.ansible_config_manager() == "best univers!!"
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/services/__init___workspace_manager_passk_validte.py
"""Service locator for the IR services

Stores and resolves all the dependencies for the services.
"""
import os
import sys
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.services import ansible_config
from infrared.core.services import execution_logger
from infrared.core.services import plugins
from infrared.core.services import workspaces
from infrared.core.utils import logger

LOG = logger.LOG


class ServiceName(object):
    """Holds the supported services names. """
    WORKSPACE_MANAGER = "workspace_manager"
    PLUGINS_MANAGER = "plugins_manager"
    ANSIBLE_CONFIG_MANAGER = "ansible_config_manager"
    EXECUTION_LOGGER_MANAGER = "execution_logger_manager"


class CoreSettings(object):
    """Holds the main settings for the infrared. """

    def __init__(self, workspaces_base_folder=None,
                 plugins_conf_file=None,
                 install_plugin_at_start=True,
                 plugins_base_folder=None):
        """Constructor.

        :param workspaces_base_folder: folder where the
               workspace will be stored
        :param plugins_conf_file: location of the plugins.ini file with the
               list of all plugins and types.
        :param install_plugin_at_start: specifies whether all the plugins
               should be installed on ir start. Skip installation may be
               required for unit tests, for example.
        """

        self.infrared_home = os.path.abspath(os.environ.get(
            "IR_HOME", os.path.join(os.path.expanduser("~"), '.infrared')))

        # todo(obaranov) replace .workspaces to workspaces and .plugins.ini to
        # todo(obaranov) plugins.ini once IR is packaged as pip
        self.plugins_conf_file = plugins_conf_file or os.path.join(
            self.infrared_home, '.plugins.ini')
        self.workspaces_base_folder = workspaces_base_folder or os.path.join(
            self.infrared_home, '.workspaces')
        self.install_plugin_at_start = install_plugin_at_start
        self.plugins_base_folder = plugins_base_folder or os.path.join(
            self.infrared_home, 'plugins')


class CoreServices(object):
    """Holds and configures all the required for core services. """

    _SERVICES = {}

    @classmethod
    def setup(cls, core_settings=None):
        """Creates configuration from file or from defaults.

        :param core_settings: the instance of the CoreSettings class with the
        desired settings. If None is provided then the default settings
        will be used.
        """

        if core_settings is None:
            core_settings = CoreSettings()

        # create workspace manager
        if ServiceName.WORKSPACE_MANAGER not in cls._SERVICES:
            cls.register_service(ServiceName.WORKSPACE_MANAGER,
                                 workspaces.WorkspaceManager(
                                     core_settings.workspaces_base_folder))

        # create plugins manager
        if ServiceName.PLUGINS_MANAGER not in cls._SERVICES:
            # A temporary WH to skip all plugins installation on first InfraRed
            # command if the command is 'infrared plugin add'.
            # Should be removed together with auto plugins installation
            # mechanism.
            skip_plugins_install = {'plugin', 'add'}.issubset(sys.argv)
            cls.register_service(
                ServiceName.PLUGINS_MANAGER, plugins.InfraredPluginManager(
                    plugins_conf=core_settings.plugins_conf_file,
                    install_plugins=(core_settings.install_plugin_at_start and
                                     not skip_plugins_install),
                    plugins_dir=core_settings.plugins_base_folder))

        # create ansible config manager
        if ServiceName.ANSIBLE_CONFIG_MANAGER not in cls._SERVICES:
            cls.register_service(ServiceName.ANSIBLE_CONFIG_MANAGER,
                                 ansible_config.AnsibleConfigManager(
                                     core_settings.infrared_home))

        # create execution logger manager
        if ServiceName.EXECUTION_LOGGER_MANAGER not in cls._SERVICES:
            # get ansible manager
            ansible_manager = CoreServices.ansible_config_manager()
            # build log file path
            log_file = \
                os.path.join(core_settings.infrared_home, 'ir-commands.log')
            cls.register_service(ServiceName.EXECUTION_LOGGER_MANAGER,
                                 execution_logger.ExecutionLoggerManager(
                                     ansible_manager.ansible_config_path,
                                     log_file=log_file))

    @classmethod
    def register_service(cls, service_name, service):
        """Protect the _SERVICES dict"""
        CoreServices._SERVICES[service_name] = service

    @classmethod
    def _get_service(cls, name):
        if name not in cls._SERVICES:
            cls.setup()
        return cls._SERVICES[name]

    @classmethod
    def workspace_manager(cls):
        """Gets the workspace manager. """
        return cls._get_service(ServiceName.WORKSPACE_MANAGER)

    @classmethod
    def plugins_manager(cls):
        """Gets the plugin manager. """
        return cls._get_service(ServiceName.PLUGINS_MANAGER)

    @classmethod
    def ansible_config_manager(cls):
        """Gets the ansible config manager. """
        return cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)

    @classmethod
    def execution_logger_manager(cls):
        """Gets the execution logger manager. """
        return cls._get_service(ServiceName.EXECUTION_LOGGER_MANAGER)

if __name__ == "__main__":
    CoreServices.register_service("huawei","service_huawei")
    CoreServices.register_service("apple", "apple!!!")
    CoreServices.register_service("xiaomi", "xiaomi!!!")
    CoreServices.register_service("workspace_manager","world!!")
    isT=CoreServices.workspace_manager()=="world!!"
    # isT = True
    # for l in os.listdir(
    #         "/home/travis/builds/repos/redhat-openstack---infrared/data_passk_platform/6306091b73426c38ae68acd9/"):
    #     f = open(
    #         "/home/travis/builds/repos/redhat-openstack---infrared/data_passk_platform/6306091b73426c38ae68acd9/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     # temp_class=CoreServices()
    #     # temp_class.__dict__.update(object_class)
    #     res0 = object_class.workspace_manager()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/services/__init___plugins_manager_passk_validte.py
"""Service locator for the IR services

Stores and resolves all the dependencies for the services.
"""
import os
import sys
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.services import ansible_config
from infrared.core.services import execution_logger
from infrared.core.services import plugins
from infrared.core.services import workspaces
from infrared.core.utils import logger

LOG = logger.LOG


class ServiceName(object):
    """Holds the supported services names. """
    WORKSPACE_MANAGER = "workspace_manager"
    PLUGINS_MANAGER = "plugins_manager"
    ANSIBLE_CONFIG_MANAGER = "ansible_config_manager"
    EXECUTION_LOGGER_MANAGER = "execution_logger_manager"


class CoreSettings(object):
    """Holds the main settings for the infrared. """

    def __init__(self, workspaces_base_folder=None,
                 plugins_conf_file=None,
                 install_plugin_at_start=True,
                 plugins_base_folder=None):
        """Constructor.

        :param workspaces_base_folder: folder where the
               workspace will be stored
        :param plugins_conf_file: location of the plugins.ini file with the
               list of all plugins and types.
        :param install_plugin_at_start: specifies whether all the plugins
               should be installed on ir start. Skip installation may be
               required for unit tests, for example.
        """

        self.infrared_home = os.path.abspath(os.environ.get(
            "IR_HOME", os.path.join(os.path.expanduser("~"), '.infrared')))

        # todo(obaranov) replace .workspaces to workspaces and .plugins.ini to
        # todo(obaranov) plugins.ini once IR is packaged as pip
        self.plugins_conf_file = plugins_conf_file or os.path.join(
            self.infrared_home, '.plugins.ini')
        self.workspaces_base_folder = workspaces_base_folder or os.path.join(
            self.infrared_home, '.workspaces')
        self.install_plugin_at_start = install_plugin_at_start
        self.plugins_base_folder = plugins_base_folder or os.path.join(
            self.infrared_home, 'plugins')


class CoreServices(object):
    """Holds and configures all the required for core services. """

    _SERVICES = {}

    @classmethod
    def setup(cls, core_settings=None):
        """Creates configuration from file or from defaults.

        :param core_settings: the instance of the CoreSettings class with the
        desired settings. If None is provided then the default settings
        will be used.
        """

        if core_settings is None:
            core_settings = CoreSettings()

        # create workspace manager
        if ServiceName.WORKSPACE_MANAGER not in cls._SERVICES:
            cls.register_service(ServiceName.WORKSPACE_MANAGER,
                                 workspaces.WorkspaceManager(
                                     core_settings.workspaces_base_folder))

        # create plugins manager
        if ServiceName.PLUGINS_MANAGER not in cls._SERVICES:
            # A temporary WH to skip all plugins installation on first InfraRed
            # command if the command is 'infrared plugin add'.
            # Should be removed together with auto plugins installation
            # mechanism.
            skip_plugins_install = {'plugin', 'add'}.issubset(sys.argv)
            cls.register_service(
                ServiceName.PLUGINS_MANAGER, plugins.InfraredPluginManager(
                    plugins_conf=core_settings.plugins_conf_file,
                    install_plugins=(core_settings.install_plugin_at_start and
                                     not skip_plugins_install),
                    plugins_dir=core_settings.plugins_base_folder))

        # create ansible config manager
        if ServiceName.ANSIBLE_CONFIG_MANAGER not in cls._SERVICES:
            cls.register_service(ServiceName.ANSIBLE_CONFIG_MANAGER,
                                 ansible_config.AnsibleConfigManager(
                                     core_settings.infrared_home))

        # create execution logger manager
        if ServiceName.EXECUTION_LOGGER_MANAGER not in cls._SERVICES:
            # get ansible manager
            ansible_manager = CoreServices.ansible_config_manager()
            # build log file path
            log_file = \
                os.path.join(core_settings.infrared_home, 'ir-commands.log')
            cls.register_service(ServiceName.EXECUTION_LOGGER_MANAGER,
                                 execution_logger.ExecutionLoggerManager(
                                     ansible_manager.ansible_config_path,
                                     log_file=log_file))

    @classmethod
    def register_service(cls, service_name, service):
        """Protect the _SERVICES dict"""
        CoreServices._SERVICES[service_name] = service

    @classmethod
    def _get_service(cls, name):
        if name not in cls._SERVICES:
            cls.setup()
        return cls._SERVICES[name]

    @classmethod
    def workspace_manager(cls):
        """Gets the workspace manager. """
        return cls._get_service(ServiceName.WORKSPACE_MANAGER)

    @classmethod
    def plugins_manager(cls):
        """Gets the plugin manager. """
        return cls._get_service(ServiceName.PLUGINS_MANAGER)

    @classmethod
    def ansible_config_manager(cls):
        """Gets the ansible config manager. """
        return cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)

    @classmethod
    def execution_logger_manager(cls):
        """Gets the execution logger manager. """
        return cls._get_service(ServiceName.EXECUTION_LOGGER_MANAGER)

if __name__ == "__main__":
    CoreServices.register_service("huawei", "service_huawei")
    CoreServices.register_service("apple", "apple!!!")
    CoreServices.register_service("xiaomi", "xiaomi!!!")
    CoreServices.register_service("workspace_manager", "world!!")
    CoreServices.register_service("plugins_manager", "best world!!")
    isT = CoreServices.plugins_manager() == "best world!!"
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/utils/validators_validate_from_content_passk_validte.py
import jsonschema
import os
from six.moves import configparser
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.utils.exceptions import IRValidatorException
from infrared.core.utils.logger import LOG as logger


class Validator(object):

    @classmethod
    def validate_from_file(cls, yaml_file=None):
        """Loads & validates that a YAML file has all required fields

        :param yaml_file: Path to YAML file
        :raise IRValidatorException: when mandatory data is missing in file
        :return: Dictionary with data loaded from a YAML file
        """
        if yaml_file is None:
            raise IRValidatorException(
                "YAML file is missing")

        if not os.path.isfile(yaml_file):
            raise IRValidatorException(
                "The YAML file doesn't exist: {}".format(yaml_file))

        with open(yaml_file) as fp:
            spec_dict = cls.validate_from_content(fp.read())

        return spec_dict

    @classmethod
    def validate_from_content(cls, file_content=None):
        """validates that YAML content has all required fields

        :param file_content: content of the YAML file
        :raise IRValidatorException: when mandatory data is missing in file
        :return: Dictionary with data loaded from a YAML file
        """
        raise NotImplementedError


class SpecValidator(Validator):
    """Class for validating a plugin spec.

    It checks that a plugin spec (YAML) has all required fields.
    """
    CONFIG_PART_SCHEMA = {
        "type": "object",
        "properties": {
            "plugin_type": {"type": "string", "minLength": 1},
            "entry_point": {"type": "string", "minLength": 1},
            "roles_path": {"type": "string", "minLength": 1},
        },
        "additionalProperties": False,
        "required": ["plugin_type"]
    }

    SUBPARSER_PART_SCHEMA = {
        "type": "object",
        "minProperties": 1,
        "maxProperties": 1,
        "patternProperties": {
            "^(?!(?:all)$).+$": {
                "type": "object",
            }
        },
        "additionalProperties": False
    }

    SCHEMA_WITH_CONFIG = {
        "type": "object",
        "properties": {
            "description": {"type": "string", "minLength": 1},
            "config": CONFIG_PART_SCHEMA,
            "subparsers": SUBPARSER_PART_SCHEMA
        },
        "additionalProperties": False,
        "required": ["config", "subparsers"]
    }

    SCHEMA_WITHOUT_CONFIG = {
        "type": "object",
        "properties": {
            "plugin_type": {"type": "string", "minLength": 1},
            "entry_point": {"type": "string", "minLength": 1},
            "roles_path": {"type": "string", "minLength": 1},
            "description": {"type": "string", "minLength": 1},
            "subparsers": SUBPARSER_PART_SCHEMA
        },
        "additionalProperties": False,
        "required": ["plugin_type", "subparsers"]
    }

    @classmethod
    def validate_from_content(cls, spec_content=None):
        """validates that spec (YAML) content has all required fields

        :param spec_content: content of spec file
        :raise IRValidatorException: when mandatory data
        is missing in spec file
        :return: Dictionary with data loaded from a spec (YAML) file
        """
        if spec_content is None:
            raise IRValidatorException(
                "Plugin spec content is missing")

        spec_dict = yaml.safe_load(spec_content)

        if not isinstance(spec_dict, dict):
            raise IRValidatorException(
                "Spec file is empty or corrupted: {}".format(spec_content))

        # check if new spec file structure
        try:
            if "config" in spec_dict:
                jsonschema.validate(spec_dict,
                                    cls.SCHEMA_WITH_CONFIG)
            else:
                jsonschema.validate(spec_dict,
                                    cls.SCHEMA_WITHOUT_CONFIG)

        except jsonschema.exceptions.ValidationError as error:
            raise IRValidatorException(
                "{} in file:\n{}".format(error.message, spec_content))

        subparsers_key = "subparsers"
        if ("description" not in spec_dict and "description"
                not in list(spec_dict[subparsers_key].values())[0]):
            raise IRValidatorException(
                "Required key 'description' is missing for supbarser '{}' in "
                "spec file: {}".format(
                    list(spec_dict[subparsers_key].keys())[0], spec_content))

        return spec_dict


class RegistryValidator(Validator):
    SCHEMA_REGISTRY = {
        "type": "object",
        "patternProperties": {
            "^.+$": {
                "type": "object",
                "properties": {
                    "src": {"type": "string", "minLength": 1},
                    "src_path": {"type": "string", "minLength": 1},
                    "rev": {"type": "string", "minLength": 1},
                    "desc": {"type": "string", "minLength": 1},
                    "type": {"type": "string", "minLength": 1},
                },
                "additionalProperties": False,
                "required": ["src", "desc", "type"]
            }
        },
        "additionalProperties": False,
    }

    @classmethod
    def validate_from_content(cls, file_content=None):
        """validates that Registry YAML content has all required fields

        :param file_content: content of the Registry YAML file
        :raise IRValidatorException: when mandatory data is missing in Registry
        :return: Dictionary with data loaded from a Registry YAML file
        """
        if file_content is None:
            raise IRValidatorException(
                "Registry YAML content is missing")

        registry_dict = yaml.safe_load(file_content)

        if not isinstance(registry_dict, dict):
            raise IRValidatorException(
                "Registry file is empty or corrupted: {}".format(file_content))

        try:
            # validate schema
            jsonschema.validate(registry_dict,
                                cls.SCHEMA_REGISTRY)

        except jsonschema.exceptions.ValidationError as error:
            raise IRValidatorException(
                "{} in file:\n{}".format(error.message, file_content))

        return registry_dict


class AnsibleConfigValidator(Validator):
    ANSIBLE_CONFIG_OPTIONS = {
        'defaults': {
            'host_key_checking': {
                'type': 'bool',
                'comparison': 'eq',
                'expected_value': False,
                'critical': True
            },
            'forks': {
                'type': 'int',
                'comparison': 'gt',
                'expected_value': 500,
                'critical': False
            },
            'timeout': {
                'type': 'int',
                'comparison': 'gt',
                'expected_value': 30,
                'critical': False
            }
        }
    }

    @classmethod
    def validate_from_file(cls, yaml_file=None):
        config = configparser.RawConfigParser()
        config.read(yaml_file)
        config_dict = cls._convert_config_to_dict(config)

        for section, option_details in cls.ANSIBLE_CONFIG_OPTIONS.items():
            for opt_name, opt_params in option_details.items():
                try:
                    config_value = config_dict[section][opt_name]
                    cls._validate_config_option(yaml_file,
                                                opt_name,
                                                opt_params['type'],
                                                opt_params['comparison'],
                                                opt_params['expected_value'],
                                                config_value,
                                                opt_params['critical'])
                except KeyError:
                    cls._handle_missing_value(yaml_file, section, opt_name,
                                              opt_params['expected_value'],
                                              opt_params['critical'])

    @classmethod
    def validate_from_content(cls, file_content=None):
        pass

    @classmethod
    def _validate_config_option(cls, yaml_file, opt_name, opt_type,
                                comparison, exp_value, cur_value, critical):
        if opt_type == 'int':
            cur_value = int(cur_value)
        if opt_type == 'bool':
            if cur_value == 'True':
                cur_value = True
            else:
                cur_value = False

        if comparison == 'eq':
            if cur_value != exp_value:
                cls._handle_wrong_value(yaml_file, opt_name, exp_value,
                                        cur_value, critical)

        if comparison == 'gt':
            if cur_value < exp_value:
                cls._handle_wrong_value(yaml_file, opt_name, exp_value,
                                        cur_value, critical)

    @classmethod
    def _handle_wrong_value(cls, yaml_file, option_name, exp_value,
                            cur_value, critical):
        msg = "There is an issue with Ansible configuration in " \
              "{}. Expected value for the option '{}' is '{}', " \
              "current value is '{}'".format(yaml_file, option_name,
                                             exp_value, cur_value)
        if critical:
            raise IRValidatorException(msg)
        else:
            logger.warn(msg)

    @classmethod
    def _handle_missing_value(cls, yaml_file, section, option_name,
                              exp_value, critical):
        msg = "There is an issue with Ansible configuration in" \
              " {}. Option '{}' with value of '{}' not found in" \
              " section '{}'".format(yaml_file, option_name,
                                     exp_value, section)
        if critical:
            raise IRValidatorException(msg)
        else:
            logger.warn(msg)

    @staticmethod
    def _convert_config_to_dict(config):
        config_dict = {}
        for section in config.sections():
            if section not in config_dict:
                config_dict[section] = {}

            for option in config.options(section):
                option_value = config.get(section, option)
                try:
                    option_value = int(option_value)
                except ValueError:
                    pass

                config_dict[section][option] = option_value

        return config_dict

if __name__ == "__main__":
    import os
    f=open("/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/utils/plugin.spec",'r',encoding="utf-8")
    content=f.read()
    f.close()
    out=SpecValidator.validate_from_content(content)
    isT=out=={'config': {'plugin_type': 'provision', 'entry_point': 'main.yml'}, 'subparsers': {'example': {'description': 'Example provisioner plugin', 'include_groups': ['Ansible options', 'Inventory', 'Common options', 'Answers file'], 'groups': [{'title': 'Group A', 'options': {'foo-bar': {'type': 'Value', 'help': 'foo.bar option', 'default': 'default string'}, 'flag': {'type': 'Flag', 'help': 'flag option'}, 'dictionary-val': {'type': 'KeyValueList', 'help': 'dictionary-val option'}}}, {'title': 'Group B', 'options': {'iniopt': {'type': 'IniType', 'help': "Help for '--iniopt'", 'action': 'append'}, 'nestedlist-app': {'type': 'NestedList', 'help': "Help for '--nestedlist-app'", 'action': 'append'}, 'nestedlist': {'type': 'NestedList', 'help': "Help for '--nestedlist'"}}}, {'title': 'Group C', 'options': {'uni-dep': {'type': 'Value', 'help': 'Help for --uni-dep', 'required_when': 'req-arg-a == yes'}, 'multi-dep': {'type': 'Value', 'help': 'Help for --multi-dep', 'required_when': ['req-arg-a == yes', 'req-arg-b == yes']}, 'uni-neg-dep': {'type': 'Value', 'help': 'Help for --uni-neg', 'required_when': 'uni-dep != uni-val'}, 'uni-int': {'type': 'Value', 'help': 'Help for --uni-neg', 'required_when': 'version > 10'}, 'req-arg-a': {'type': 'Bool', 'help': 'Help for --req-arg-a'}, 'req-arg-b': {'type': 'Bool', 'help': 'Help for --req-arg-b'}, 'either-dep': {'type': 'Value', 'help': 'Help for --either-dep', 'required_when': ['req-arg-c == yes or req-arg-d == yes']}, 'req-arg-c': {'type': 'Bool', 'help': 'Help for --req-arg-c'}, 'req-arg-d': {'type': 'Bool', 'help': 'Help for --req-arg-d'}, 'version': {'type': 'int', 'help': 'Help for --version'}}}, {'title': 'Group D', 'options': {'deprecated-way': {'type': 'Value', 'help': 'Deprecated way to do it'}, 'new-way': {'deprecates': 'deprecated-way', 'type': 'Value', 'help': 'New way to do it'}}}, {'title': 'Group E', 'options': {'tasks': {'type': 'ListOfFileNames', 'help': 'This is example for option which is with type "ListOfFileNames" and has\nauto propagation of "Allowed Values" in help. When we ask for --help it\nwill look in plugin folder for directory name as \'lookup_dir\' value, and\nwill add all file names to "Allowed Values"\n', 'lookup_dir': 'post_tasks'}}}, {'title': 'Group F', 'options': {'value-minmax-str': {'type': 'Value', 'help': 'str type value'}, 'value-minmax-int': {'type': 'int', 'help': 'value with a minimum and maximum values', 'minimum': 100, 'maximum': 200}, 'value-minmax-float': {'type': 'float', 'help': 'value with a minimum and maximum values', 'minimum': 0.5, 'maximum': 1.5}, 'value-min-zero': {'type': 'int', 'help': 'value with a minimum zero value', 'minimum': 0}, 'value-max-zero': {'type': 'int', 'help': 'value with a maximum zero value', 'maximum': 0}}}, {'title': 'Group G', 'options': {'value-len': {'type': 'Value', 'help': 'value with length', 'length': 4}}}]}}}


    # for l in os.listdir(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos\\redhat-openstack---infrared/data_passk_platform1/6306091c73426c38ae68acdc/"):
    #     f = open(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos\\redhat-openstack---infrared/data_passk_platform1/6306091c73426c38ae68acdc/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     # temp_class = Validator()
    #     # temp_class.__dict__.update(object_class)
    #     print(args1)
    #     faa=open("temp.txt",'w',encoding="utf-8")
    #     faa.write(args1)
    #     faa.close()
    #     # res0 = validate_from_content(args1)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/utils/validators_validate_from_file_passk_validte.py
import jsonschema
import os
from six.moves import configparser
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.utils.exceptions import IRValidatorException
from infrared.core.utils.logger import LOG as logger


class Validator(object):

    @classmethod
    def validate_from_file(cls, yaml_file=None):
        """Loads & validates that a YAML file has all required fields

        :param yaml_file: Path to YAML file
        :raise IRValidatorException: when mandatory data is missing in file
        :return: Dictionary with data loaded from a YAML file
        """
        if yaml_file is None:
            raise IRValidatorException(
                "YAML file is missing")

        if not os.path.isfile(yaml_file):
            raise IRValidatorException(
                "The YAML file doesn't exist: {}".format(yaml_file))

        with open(yaml_file) as fp:
            spec_dict = cls.validate_from_content(fp.read())

        return spec_dict

    @classmethod
    def validate_from_content(cls, file_content=None):
        """validates that YAML content has all required fields

        :param file_content: content of the YAML file
        :raise IRValidatorException: when mandatory data is missing in file
        :return: Dictionary with data loaded from a YAML file
        """
        raise NotImplementedError


class SpecValidator(Validator):
    """Class for validating a plugin spec.

    It checks that a plugin spec (YAML) has all required fields.
    """
    CONFIG_PART_SCHEMA = {
        "type": "object",
        "properties": {
            "plugin_type": {"type": "string", "minLength": 1},
            "entry_point": {"type": "string", "minLength": 1},
            "roles_path": {"type": "string", "minLength": 1},
        },
        "additionalProperties": False,
        "required": ["plugin_type"]
    }

    SUBPARSER_PART_SCHEMA = {
        "type": "object",
        "minProperties": 1,
        "maxProperties": 1,
        "patternProperties": {
            "^(?!(?:all)$).+$": {
                "type": "object",
            }
        },
        "additionalProperties": False
    }

    SCHEMA_WITH_CONFIG = {
        "type": "object",
        "properties": {
            "description": {"type": "string", "minLength": 1},
            "config": CONFIG_PART_SCHEMA,
            "subparsers": SUBPARSER_PART_SCHEMA
        },
        "additionalProperties": False,
        "required": ["config", "subparsers"]
    }

    SCHEMA_WITHOUT_CONFIG = {
        "type": "object",
        "properties": {
            "plugin_type": {"type": "string", "minLength": 1},
            "entry_point": {"type": "string", "minLength": 1},
            "roles_path": {"type": "string", "minLength": 1},
            "description": {"type": "string", "minLength": 1},
            "subparsers": SUBPARSER_PART_SCHEMA
        },
        "additionalProperties": False,
        "required": ["plugin_type", "subparsers"]
    }

    @classmethod
    def validate_from_content(cls, spec_content=None):
        """validates that spec (YAML) content has all required fields

        :param spec_content: content of spec file
        :raise IRValidatorException: when mandatory data
        is missing in spec file
        :return: Dictionary with data loaded from a spec (YAML) file
        """
        if spec_content is None:
            raise IRValidatorException(
                "Plugin spec content is missing")

        spec_dict = yaml.safe_load(spec_content)

        if not isinstance(spec_dict, dict):
            raise IRValidatorException(
                "Spec file is empty or corrupted: {}".format(spec_content))

        # check if new spec file structure
        try:
            if "config" in spec_dict:
                jsonschema.validate(spec_dict,
                                    cls.SCHEMA_WITH_CONFIG)
            else:
                jsonschema.validate(spec_dict,
                                    cls.SCHEMA_WITHOUT_CONFIG)

        except jsonschema.exceptions.ValidationError as error:
            raise IRValidatorException(
                "{} in file:\n{}".format(error.message, spec_content))

        subparsers_key = "subparsers"
        if ("description" not in spec_dict and "description"
                not in list(spec_dict[subparsers_key].values())[0]):
            raise IRValidatorException(
                "Required key 'description' is missing for supbarser '{}' in "
                "spec file: {}".format(
                    list(spec_dict[subparsers_key].keys())[0], spec_content))

        return spec_dict


class RegistryValidator(Validator):
    SCHEMA_REGISTRY = {
        "type": "object",
        "patternProperties": {
            "^.+$": {
                "type": "object",
                "properties": {
                    "src": {"type": "string", "minLength": 1},
                    "src_path": {"type": "string", "minLength": 1},
                    "rev": {"type": "string", "minLength": 1},
                    "desc": {"type": "string", "minLength": 1},
                    "type": {"type": "string", "minLength": 1},
                },
                "additionalProperties": False,
                "required": ["src", "desc", "type"]
            }
        },
        "additionalProperties": False,
    }

    @classmethod
    def validate_from_content(cls, file_content=None):
        """validates that Registry YAML content has all required fields

        :param file_content: content of the Registry YAML file
        :raise IRValidatorException: when mandatory data is missing in Registry
        :return: Dictionary with data loaded from a Registry YAML file
        """
        if file_content is None:
            raise IRValidatorException(
                "Registry YAML content is missing")

        registry_dict = yaml.safe_load(file_content)

        if not isinstance(registry_dict, dict):
            raise IRValidatorException(
                "Registry file is empty or corrupted: {}".format(file_content))

        try:
            # validate schema
            jsonschema.validate(registry_dict,
                                cls.SCHEMA_REGISTRY)

        except jsonschema.exceptions.ValidationError as error:
            raise IRValidatorException(
                "{} in file:\n{}".format(error.message, file_content))

        return registry_dict


class AnsibleConfigValidator(Validator):
    ANSIBLE_CONFIG_OPTIONS = {
        'defaults': {
            'host_key_checking': {
                'type': 'bool',
                'comparison': 'eq',
                'expected_value': False,
                'critical': True
            },
            'forks': {
                'type': 'int',
                'comparison': 'gt',
                'expected_value': 500,
                'critical': False
            },
            'timeout': {
                'type': 'int',
                'comparison': 'gt',
                'expected_value': 30,
                'critical': False
            }
        }
    }

    @classmethod
    def validate_from_file(cls, yaml_file=None):
        config = configparser.RawConfigParser()
        config.read(yaml_file)
        config_dict = cls._convert_config_to_dict(config)

        for section, option_details in cls.ANSIBLE_CONFIG_OPTIONS.items():
            for opt_name, opt_params in option_details.items():
                try:
                    config_value = config_dict[section][opt_name]
                    cls._validate_config_option(yaml_file,
                                                opt_name,
                                                opt_params['type'],
                                                opt_params['comparison'],
                                                opt_params['expected_value'],
                                                config_value,
                                                opt_params['critical'])
                except KeyError:
                    cls._handle_missing_value(yaml_file, section, opt_name,
                                              opt_params['expected_value'],
                                              opt_params['critical'])

    @classmethod
    def validate_from_content(cls, file_content=None):
        pass

    @classmethod
    def _validate_config_option(cls, yaml_file, opt_name, opt_type,
                                comparison, exp_value, cur_value, critical):
        if opt_type == 'int':
            cur_value = int(cur_value)
        if opt_type == 'bool':
            if cur_value == 'True':
                cur_value = True
            else:
                cur_value = False

        if comparison == 'eq':
            if cur_value != exp_value:
                cls._handle_wrong_value(yaml_file, opt_name, exp_value,
                                        cur_value, critical)

        if comparison == 'gt':
            if cur_value < exp_value:
                cls._handle_wrong_value(yaml_file, opt_name, exp_value,
                                        cur_value, critical)

    @classmethod
    def _handle_wrong_value(cls, yaml_file, option_name, exp_value,
                            cur_value, critical):
        msg = "There is an issue with Ansible configuration in " \
              "{}. Expected value for the option '{}' is '{}', " \
              "current value is '{}'".format(yaml_file, option_name,
                                             exp_value, cur_value)
        if critical:
            raise IRValidatorException(msg)
        else:
            logger.warn(msg)

    @classmethod
    def _handle_missing_value(cls, yaml_file, section, option_name,
                              exp_value, critical):
        msg = "There is an issue with Ansible configuration in" \
              " {}. Option '{}' with value of '{}' not found in" \
              " section '{}'".format(yaml_file, option_name,
                                     exp_value, section)
        if critical:
            raise IRValidatorException(msg)
        else:
            logger.warn(msg)

    @staticmethod
    def _convert_config_to_dict(config):
        config_dict = {}
        for section in config.sections():
            if section not in config_dict:
                config_dict[section] = {}

            for option in config.options(section):
                option_value = config.get(section, option)
                try:
                    option_value = int(option_value)
                except ValueError:
                    pass

                config_dict[section][option] = option_value

        return config_dict

if __name__ == "__main__":
    import os
    out=SpecValidator.validate_from_file("/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/utils/plugin.spec")
    isT=out=={'config': {'plugin_type': 'provision', 'entry_point': 'main.yml'}, 'subparsers': {'example': {'description': 'Example provisioner plugin', 'include_groups': ['Ansible options', 'Inventory', 'Common options', 'Answers file'], 'groups': [{'title': 'Group A', 'options': {'foo-bar': {'type': 'Value', 'help': 'foo.bar option', 'default': 'default string'}, 'flag': {'type': 'Flag', 'help': 'flag option'}, 'dictionary-val': {'type': 'KeyValueList', 'help': 'dictionary-val option'}}}, {'title': 'Group B', 'options': {'iniopt': {'type': 'IniType', 'help': "Help for '--iniopt'", 'action': 'append'}, 'nestedlist-app': {'type': 'NestedList', 'help': "Help for '--nestedlist-app'", 'action': 'append'}, 'nestedlist': {'type': 'NestedList', 'help': "Help for '--nestedlist'"}}}, {'title': 'Group C', 'options': {'uni-dep': {'type': 'Value', 'help': 'Help for --uni-dep', 'required_when': 'req-arg-a == yes'}, 'multi-dep': {'type': 'Value', 'help': 'Help for --multi-dep', 'required_when': ['req-arg-a == yes', 'req-arg-b == yes']}, 'uni-neg-dep': {'type': 'Value', 'help': 'Help for --uni-neg', 'required_when': 'uni-dep != uni-val'}, 'uni-int': {'type': 'Value', 'help': 'Help for --uni-neg', 'required_when': 'version > 10'}, 'req-arg-a': {'type': 'Bool', 'help': 'Help for --req-arg-a'}, 'req-arg-b': {'type': 'Bool', 'help': 'Help for --req-arg-b'}, 'either-dep': {'type': 'Value', 'help': 'Help for --either-dep', 'required_when': ['req-arg-c == yes or req-arg-d == yes']}, 'req-arg-c': {'type': 'Bool', 'help': 'Help for --req-arg-c'}, 'req-arg-d': {'type': 'Bool', 'help': 'Help for --req-arg-d'}, 'version': {'type': 'int', 'help': 'Help for --version'}}}, {'title': 'Group D', 'options': {'deprecated-way': {'type': 'Value', 'help': 'Deprecated way to do it'}, 'new-way': {'deprecates': 'deprecated-way', 'type': 'Value', 'help': 'New way to do it'}}}, {'title': 'Group E', 'options': {'tasks': {'type': 'ListOfFileNames', 'help': 'This is example for option which is with type "ListOfFileNames" and has\nauto propagation of "Allowed Values" in help. When we ask for --help it\nwill look in plugin folder for directory name as \'lookup_dir\' value, and\nwill add all file names to "Allowed Values"\n', 'lookup_dir': 'post_tasks'}}}, {'title': 'Group F', 'options': {'value-minmax-str': {'type': 'Value', 'help': 'str type value'}, 'value-minmax-int': {'type': 'int', 'help': 'value with a minimum and maximum values', 'minimum': 100, 'maximum': 200}, 'value-minmax-float': {'type': 'float', 'help': 'value with a minimum and maximum values', 'minimum': 0.5, 'maximum': 1.5}, 'value-min-zero': {'type': 'int', 'help': 'value with a minimum zero value', 'minimum': 0}, 'value-max-zero': {'type': 'int', 'help': 'value with a maximum zero value', 'maximum': 0}}}, {'title': 'Group G', 'options': {'value-len': {'type': 'Value', 'help': 'value with length', 'length': 4}}}]}}}


    # for l in os.listdir(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos\\redhat-openstack---infrared/data_passk_platform1/6306091c73426c38ae68acdc/"):
    #     f = open(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos\\redhat-openstack---infrared/data_passk_platform1/6306091c73426c38ae68acdc/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     # temp_class = Validator()
    #     # temp_class.__dict__.update(object_class)
    #     print(args1)
    #     faa=open("temp.txt",'w',encoding="utf-8")
    #     faa.write(args1)
    #     faa.close()
    #     # res0 = validate_from_content(args1)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/helper__include_groups_passk_validte.py
from copy import deepcopy
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.utils.exceptions import SpecParserException

OptionState = dict(
    UNRECOGNIZED='unrecognized',
    IS_SET='is set',
    NOT_SET='is no set'
)


class SpecDictHelper(object):
    """Controls the spec dicts and provides useful methods to get spec info."""

    def __init__(self, spec_dict):
        self.spec_dict = spec_dict
        # make structure of the dict flat
        # 1. handle include_groups directive in main parser
        parser_dict = self.spec_dict
        self._include_groups(parser_dict)
        # 2. Include groups for all subparsers
        for subparser_name, subparser_dict in parser_dict.get(
                'subparsers', {}).items():
            self._include_groups(subparser_dict)

    def iterate_parsers(self):
        """Iterates over the main parsers and subparsers. """

        for subparser_name, subparser_dict in self.spec_dict.get(
                'subparsers', {}).items():
            yield dict(name=subparser_name, **subparser_dict)

    def iterate_option_specs(self):
        """Iterates over all the option specs.

        Returns pair of parser and option on every iteration.
        """
        for parser in self.iterate_parsers():
            for spec_option in self._get_all_options_spec(parser):
                yield parser, spec_option

    @staticmethod
    def _get_all_options_spec(parser_dict):
        """Gets all the options specification as the list of dicts. """
        result = []
        for group in parser_dict.get('groups', []):
            for option_name, option_dict in group.get('options', {}).items():
                result.append(dict(name=option_name, **option_dict))

        for option_name, option_dict in parser_dict.get('options', {}).items():
            result.append(dict(name=option_name, **option_dict))

        return result

    def get_parser_option_specs(self, command_name):
        """Gets all the options for the specified command

        :param command_name: the command name (main, virsh, ospd, etc...)
        :return: the list of all command options
        """
        options = []
        for parser in self.iterate_parsers():
            if parser['name'] == command_name:
                options = self._get_all_options_spec(parser)
                break
        return options

    def get_option_spec(self, command_name, argument_name):
        """Gets the specification for the specified option name. """

        options = self.get_parser_option_specs(command_name)
        return next((opt for opt in options
                     if opt['name'] == argument_name), {})

    def get_option_state(self, command_name, option_name, args):
        """Gets the option state.

        :param command_name: The command name
        :param option_name: The option name to analyze
        :param args: The received arguments.
        """
        option_spec = self.get_option_spec(command_name, option_name)

        if not option_spec:
            res = OptionState['UNRECOGNIZED']

        elif option_name not in args.get(command_name, {}):
            res = OptionState['NOT_SET']
        else:
            option_value = args[command_name][option_name]
            if option_spec.get('action', '') in ['store_true'] \
                    and option_value is False:
                res = OptionState['NOT_SET']
            else:
                res = OptionState['IS_SET']

        return res

    def _include_groups(self, parser_dict):
        """Resolves the include dict directive in the spec files. """
        for group in parser_dict.get('include_groups', []):
            # ensure we have that group
            grp_dict = next(
                (grp for grp in self.spec_dict.get('shared_groups', [])
                 if grp['title'] == group),
                None)
            if grp_dict is None:
                raise SpecParserException(
                    "Unable to include group '{}' in '{}' parser. "
                    "Group was not found!".format(
                        group,
                        parser_dict['name']))

            for option in grp_dict.get('options', {}).values():
                option['is_shared_group_option'] = True

            parser_groups_list = parser_dict.get('groups', [])
            parser_groups_list.append(deepcopy(grp_dict))
            parser_dict['groups'] = parser_groups_list

if __name__ == "__main__":
    isT = True


    init_para={"shared_groups": [{"title":"bbb","options": {"a":{"aa":"aaa"},"b":{"bb":"bbb"}}}]}
    dict_input={"include_groups":["bbb"]}
    temp_class = SpecDictHelper(init_para)
    temp_class._include_groups(dict_input)
    if dict_input["include_groups"][0]!="bbb" or dict_input["groups"][0]["title"]!="bbb" or dict_input["groups"][0]["options"]["a"]["is_shared_group_option"]==False:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")
    init_para = {"shared_groups": [{"title": "bbb"}]}
    dict_input = {"include_groups": ["bbb"]}
    temp_class = SpecDictHelper(init_para)
    temp_class._include_groups(dict_input)
    if dict_input["include_groups"][0]!="bbb" or dict_input["groups"][0]["title"]!="bbb":
        isT=False
    if not isT:
        raise Exception("Result not True!!!")
    # print(dict_input)



----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/inspector_get_spec_defaults_passk_validte.py
import collections
import os
from six.moves import configparser
from string import Template
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
from infrared.core.inspector import helper
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = helper.SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get(name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """

        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))

                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == helper.OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)

        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)

            for option in expected_options:
                name = option['name']

                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }

                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == helper.OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value

if __name__ == "__main__":
    isT = True
    temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
                                 'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                 'mock_key': 'expected_value'}, "", "", "")
    temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
        {"options": {"JAVA_HOME": {"type": "int"}, "key2": {"value2": 2}, "key3": {"value3": 3}}},
        {"options": {"JAVA_HOME": {"type": "int"}, "key22": {"value22": 22}, "key33": {"deprecates": 33}}}]}}}
    try:
        isT=temp_class.get_spec_defaults().get("foo-bar").get("JAVA_HOME")==os.environ.get("JAVA_HOME")
    except:
        isT=False
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos/redhat-openstack---infrared\\data_passk_platform/6306092373426c38ae68acfa/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos/redhat-openstack---infrared\\data_passk_platform/6306092373426c38ae68acfa/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     # object_class=dill.loads(content["input"]["args"][0]["bytes"])
    #     # temp_class=SpecParser("", {}, "", "", "")
    #     # temp_class.__dict__.update(object_class)
    #     res0 = temp_class.get_spec_defaults()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/inspector_get_deprecated_args_passk_validte.py
import collections
import os
from six.moves import configparser
from string import Template
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
from infrared.core.inspector import helper
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = helper.SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get('IR_' + name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """

        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))

                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == helper.OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)

        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)

            for option in expected_options:
                name = option['name']

                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }

                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == helper.OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value

if __name__ == "__main__":
    isT = True
    temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
                                 'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                 'mock_key': 'expected_value'}, "", "", "")
    temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
        {"options": {"foo-bar": {"type": "int"}, "key2": {"value2": 2}, "key3": {"value3": 3}}},
        {"options": {"foo-bar": {"type": "int"}, "key22": {"value22": 22}, "key33": {"deprecates": 33}}}]}}}
    res0 = temp_class.get_deprecated_args()
    if 33 not in res0.keys():
        isT=False
        if res0.get(33)!="key33":
            isT=False
    if len(res0)!=1:
        isT=False
    # print(res0)
    temp_class1 = SpecParser("", {'foo-bar': 'from_answers_file',
                                 'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                 'mock_key': 'expected_value'}, "", "", "")
    temp_class1.spec_helper.spec_dict = {'subparsers': {"deprecates": {"one": 1, "tow": 2, "deprecates": [
        {"deprecates": {"deprecates": {"type": "int"}, "key2": {"value2": 2}, "key3": {"value3": 3}}},
        {"options": {"foo-bar": {"deprecates": "int"}, "key22": {"value22": 22}, "key33": {"deprecates": 33}}}]}}}
    res1 = temp_class1.get_deprecated_args()
    if len(res1)!=0:
        isT=False
    # print(res1)
    # for l in os.listdir(
    #         "/home/travis/builds/repos/redhat-openstack---infrared/data_passk_platform/6306092973426c38ae68ad01/"):
    #     f = open(
    #         "/home/travis/builds/repos/redhat-openstack---infrared/data_passk_platform/6306092973426c38ae68ad01/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = SpecParser("", {}, "", "", "")
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.get_deprecated_args()
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/inspector_validate_arg_deprecation_passk_validte.py
import collections
import os
from six.moves import configparser
from string import Template
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
from infrared.core.inspector import helper
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = helper.SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get('IR_' + name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """
        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))
                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == helper.OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)

        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)

            for option in expected_options:
                name = option['name']

                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }

                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == helper.OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value

if __name__ == "__main__":
    isT = True
    temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
                                 'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                 'mock_key': 'expected_value'}, "", "", "")
    temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
        {"options": {"JAVA_HOME": {"type": "int"}, "key2": {"value2": 2}, "key3": {"value3": 3}}},
        {"options": {"JAVA_HOME": {"type": "int"}, "key22": {"value22": 22}, "abca": {"deprecates": "abc"}}}]}}}
    result = collections.defaultdict(dict)
    result["example"]={}
    temp_class.validate_arg_deprecation(result,{"example":{"abca":"aaa"},"example1":{"abc":"aaa"},})
    ist1=len(result)==2
    ist2="example" in result.keys() and "example1" in result.keys()
    ist3=result["example"]=={} and result["example1"]=={}
    if not ist1 or not ist2 or not ist3:
        isT=False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/helper_get_parser_option_specs_passk_validte.py
from copy import deepcopy
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.utils.exceptions import SpecParserException

OptionState = dict(
    UNRECOGNIZED='unrecognized',
    IS_SET='is set',
    NOT_SET='is no set'
)


class SpecDictHelper(object):
    """Controls the spec dicts and provides useful methods to get spec info."""

    def __init__(self, spec_dict):
        self.spec_dict = spec_dict
        # make structure of the dict flat
        # 1. handle include_groups directive in main parser
        parser_dict = self.spec_dict
        self._include_groups(parser_dict)
        # 2. Include groups for all subparsers
        for subparser_name, subparser_dict in parser_dict.get(
                'subparsers', {}).items():
            self._include_groups(subparser_dict)

    def iterate_parsers(self):
        """Iterates over the main parsers and subparsers. """

        for subparser_name, subparser_dict in self.spec_dict.get(
                'subparsers', {}).items():
            yield dict(name=subparser_name, **subparser_dict)

    def iterate_option_specs(self):
        """Iterates over all the option specs.

        Returns pair of parser and option on every iteration.
        """
        for parser in self.iterate_parsers():
            for spec_option in self._get_all_options_spec(parser):
                yield parser, spec_option

    @staticmethod
    def _get_all_options_spec(parser_dict):
        """Gets all the options specification as the list of dicts. """
        result = []
        for group in parser_dict.get('groups', []):
            for option_name, option_dict in group.get('options', {}).items():
                result.append(dict(name=option_name, **option_dict))

        for option_name, option_dict in parser_dict.get('options', {}).items():
            result.append(dict(name=option_name, **option_dict))

        return result

    def get_parser_option_specs(self, command_name):
        """Gets all the options for the specified command

        :param command_name: the command name (main, virsh, ospd, etc...)
        :return: the list of all command options
        """
        options = []
        for parser in self.iterate_parsers():
            if parser['name'] == command_name:
                options = self._get_all_options_spec(parser)
                break
        return options

    def get_option_spec(self, command_name, argument_name):
        """Gets the specification for the specified option name. """

        options = self.get_parser_option_specs(command_name)
        return next((opt for opt in options
                     if opt['name'] == argument_name), {})

    def get_option_state(self, command_name, option_name, args):
        """Gets the option state.

        :param command_name: The command name
        :param option_name: The option name to analyze
        :param args: The received arguments.
        """
        option_spec = self.get_option_spec(command_name, option_name)

        if not option_spec:
            res = OptionState['UNRECOGNIZED']

        elif option_name not in args.get(command_name, {}):
            res = OptionState['NOT_SET']
        else:
            option_value = args[command_name][option_name]
            if option_spec.get('action', '') in ['store_true'] \
                    and option_value is False:
                res = OptionState['NOT_SET']
            else:
                res = OptionState['IS_SET']

        return res

    def _include_groups(self, parser_dict):
        """Resolves the include dict directive in the spec files. """
        for group in parser_dict.get('include_groups', []):
            # ensure we have that group
            grp_dict = next(
                (grp for grp in self.spec_dict.get('shared_groups', [])
                 if grp['title'] == group),
                None)
            if grp_dict is None:
                raise SpecParserException(
                    "Unable to include group '{}' in '{}' parser. "
                    "Group was not found!".format(
                        group,
                        parser_dict['name']))

            for option in grp_dict.get('options', {}).values():
                option['is_shared_group_option'] = True

            parser_groups_list = parser_dict.get('groups', [])
            parser_groups_list.append(deepcopy(grp_dict))
            parser_dict['groups'] = parser_groups_list

import collections
import os
from six.moves import configparser
from string import Template
import yaml

from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
# from infrared.core.inspector import helper
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get('IR_' + name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """

        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))

                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)

        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)

            for option in expected_options:
                name = option['name']

                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }

                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value


if __name__ == "__main__":
    isT = True
    try:
        temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
                                     'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                     'mock_key': 'expected_value'}, "", "", "")
        temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
            {"options": {"foo-bar": {"type": "int"}, "key2": {"value2": 2}, "key3": {"value3": 3}}},
            {"options": {"foo-bar": {"type": "int"}, "key22": {"value22": 22}, "key33": {"value33": 33}}}]}}}
        input1 = {'foo-bar': "111"}
        input2 = {'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3'}
        temp_class._convert_non_cli_args("foo-bar", input1)
        temp_class._convert_non_cli_args("example", input2)
        if not isinstance(input1["foo-bar"], int):
            isT = False
        if input2["iniopt"] != 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3':
            isT = False
    except:
        isT = False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/helper_get_option_spec_passk_validte.py
from copy import deepcopy
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.utils.exceptions import SpecParserException

OptionState = dict(
    UNRECOGNIZED='unrecognized',
    IS_SET='is set',
    NOT_SET='is no set'
)


class SpecDictHelper(object):
    """Controls the spec dicts and provides useful methods to get spec info."""

    def __init__(self, spec_dict):
        self.spec_dict = spec_dict
        # make structure of the dict flat
        # 1. handle include_groups directive in main parser
        parser_dict = self.spec_dict
        self._include_groups(parser_dict)
        # 2. Include groups for all subparsers
        for subparser_name, subparser_dict in parser_dict.get(
                'subparsers', {}).items():
            self._include_groups(subparser_dict)

    def iterate_parsers(self):
        """Iterates over the main parsers and subparsers. """

        for subparser_name, subparser_dict in self.spec_dict.get(
                'subparsers', {}).items():
            yield dict(name=subparser_name, **subparser_dict)

    def iterate_option_specs(self):
        """Iterates over all the option specs.

        Returns pair of parser and option on every iteration.
        """
        for parser in self.iterate_parsers():
            for spec_option in self._get_all_options_spec(parser):
                yield parser, spec_option

    @staticmethod
    def _get_all_options_spec(parser_dict):
        """Gets all the options specification as the list of dicts. """
        result = []
        for group in parser_dict.get('groups', []):
            for option_name, option_dict in group.get('options', {}).items():
                result.append(dict(name=option_name, **option_dict))

        for option_name, option_dict in parser_dict.get('options', {}).items():
            result.append(dict(name=option_name, **option_dict))

        return result

    def get_parser_option_specs(self, command_name):
        """Gets all the options for the specified command

        :param command_name: the command name (main, virsh, ospd, etc...)
        :return: the list of all command options
        """
        options = []
        for parser in self.iterate_parsers():
            if parser['name'] == command_name:
                options = self._get_all_options_spec(parser)
                break
        return options

    def get_option_spec(self, command_name, argument_name):
        """Gets the specification for the specified option name. """

        options = self.get_parser_option_specs(command_name)
        return next((opt for opt in options
                     if opt['name'] == argument_name), {})

    def get_option_state(self, command_name, option_name, args):
        """Gets the option state.

        :param command_name: The command name
        :param option_name: The option name to analyze
        :param args: The received arguments.
        """
        option_spec = self.get_option_spec(command_name, option_name)

        if not option_spec:
            res = OptionState['UNRECOGNIZED']

        elif option_name not in args.get(command_name, {}):
            res = OptionState['NOT_SET']
        else:
            option_value = args[command_name][option_name]
            if option_spec.get('action', '') in ['store_true'] \
                    and option_value is False:
                res = OptionState['NOT_SET']
            else:
                res = OptionState['IS_SET']

        return res

    def _include_groups(self, parser_dict):
        """Resolves the include dict directive in the spec files. """
        for group in parser_dict.get('include_groups', []):
            # ensure we have that group
            grp_dict = next(
                (grp for grp in self.spec_dict.get('shared_groups', [])
                 if grp['title'] == group),
                None)
            if grp_dict is None:
                raise SpecParserException(
                    "Unable to include group '{}' in '{}' parser. "
                    "Group was not found!".format(
                        group,
                        parser_dict['name']))

            for option in grp_dict.get('options', {}).values():
                option['is_shared_group_option'] = True

            parser_groups_list = parser_dict.get('groups', [])
            parser_groups_list.append(deepcopy(grp_dict))
            parser_dict['groups'] = parser_groups_list

import collections
import os
from six.moves import configparser
from string import Template
import yaml

from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
# from infrared.core.inspector import helper
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get('IR_' + name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """

        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))

                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)

        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)

            for option in expected_options:
                name = option['name']

                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }

                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value


if __name__ == "__main__":
    isT = True
    try:
        temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
                                     'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                     'mock_key': 'expected_value'}, "", "", "")
        temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
            {"options": {"foo-bar": {"type": "int"}, "key2": {"value2": 2}, "key3": {"value3": 3}}},
            {"options": {"foo-bar": {"type": "int"}, "key22": {"value22": 22}, "key33": {"value33": 33}}}]}}}
        input1 = {'foo-bar': "111"}
        input2 = {'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3'}
        temp_class._convert_non_cli_args("foo-bar", input1)
        temp_class._convert_non_cli_args("example", input2)
        if not isinstance(input1["foo-bar"], int):
            isT = False
        if input2["iniopt"] != 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3':
            isT = False
    except:
        isT = False
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/inspector_get_silent_args_passk_validte.py
import collections
import os
from six.moves import configparser
from string import Template
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
import tests
from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
from infrared.core.inspector import helper
from infrared.core.services import plugins
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = helper.SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get('IR_' + name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """

        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))

                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == helper.OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)

        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)

            for option in expected_options:
                name = option['name']

                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }

                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == helper.OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value

if __name__ == "__main__":
    import os


    args1 = {'example': {'foo-bar': 'default string', 'verbose': 0, 'dry-run': False},
             'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
                 {"options": {"foo-bar": {"type": "int"}, "silent": {"silent": [2,3,4]}, "key3": {"value3": 3}}},
                 {"options": {"foo-bar": {"type": "int"}, "silent": {"value22": 22}, "key33": {"deprecates": 33}}}]}},
             "foo-bar": {"silent": "bca"}}
    args2 = {'example': {'foo-bar': 'fail', 'verbose': 0, 'dry-run': False}}
    args3 = {'example': {'foo-bar': 'default string', 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val'],
                         'output': '/tmp/pytest-of-travis/pytest-2/test_extra_vars_input_args2_ex0/tmp/dry_output.yml'}}
    isT = True
    temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
                                 'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                 'mock_key': 'expected_value'}, "", "", "")
    temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
        {"options": {"foo-bar": {"type": "int"}, "silent": {"silent": [2,3,4]}, "key3": {"value3": 3}}},
        {"options": {"foo-bar": {"type": "int"}, "silent": {"value22": 22}, "key33": {"deprecates": 33}}}]}}}
    ist1=temp_class.get_silent_args(args1)==[2,3,4]
    ist2=temp_class.get_silent_args(args2)==[]
    ist3=temp_class.get_silent_args(args3)==[]

    if not ist1 or not ist2 or not ist3:
        isT=False

    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/inspector_validate_requires_args_passk_validte.py
import collections
import os
from six.moves import configparser
from string import Template
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
from infrared.core.inspector import helper
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = helper.SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get('IR_' + name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """

        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))

                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == helper.OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)
        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)
            for option in expected_options:
                name = option['name']
                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }

                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == helper.OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value

if __name__ == "__main__":
    args1 = {'example': {'foo-bar': 'default string', 'verbose': 0, 'dry-run': False},
             'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
                 {"options": {"foo-bar": {"type": "int"}, "silent": {"silent": [2, 3, 4]}, "key3": {"value3": 3}}},
                 {"options": {"foo-bar": {"type": "int"}, "silent": {"value22": 22}, "key33": {"deprecates": 33}}}]}},
             "foo-bar": {"silent": "bca"}}
    args2 = {'example': {'foo-bar': 'fail', 'verbose': 0, 'dry-run': False}}
    args3 = {'example': {'foo-bar': 'default string', 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val'],
                         'output': '/tmp/pytest-of-travis/pytest-2/test_extra_vars_input_args2_ex0/tmp/dry_output.yml'}}
    isT = True
    temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
                                 'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                 'mock_key': 'expected_value'}, "", "", "")
    temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
        {"options": {"foo-bar": {"required": True}, "silent": {"silent": [2, 3, 4]}, "key3": {"value3": 3}}},
        {"options": {"foo-bar": {"type": "int"}, "silent": {"value22": 22}, "key33": {"deprecates": 33}}}]}}}
    isT=True
    try:
        temp_class.validate_requires_args(args1)
        isT=False
    except Exception as e:
        if str(e)!="{'foo-bar': ['foo-bar']}":
            isT=False

    try:
        temp_class.validate_requires_args(args2)
    except Exception as e:
        isT=False

    try:
        temp_class.validate_requires_args(args3)
    except Exception as e:
        isT=False

    if not isT:
        raise Exception("Result not True!!!")



----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/inspector__get_conditionally_required_args_passk_validte.py
import collections
import os
from six.moves import configparser
from string import Template
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
from infrared.core.inspector import helper
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = helper.SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get('IR_' + name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """

        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))

                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == helper.OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)

        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)

            for option in expected_options:
                name = option['name']

                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }

                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == helper.OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value

if __name__ == "__main__":
    isT = True

    temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
                                'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                'mock_key': 'expected_value'}, "", "", "")
    temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
        {"options": {"foo-bar": {"type": "int"}, "key2": {"value2": 2}, "key3": {"value3": 3}}},
        {"options": {"foo-bar": {"type": "int"}, "key22": {"value22": 22}, "key33": {"value33": 33}}}]}}}
    # fff = open("aaa.json", 'w')
    #
    # fff.write(str(dill.dumps(temp_class)))
    # fff.close()
    args2=[{'name': 'foo-bar', 'type': 'Value', 'help': "foo.bar option\nDefault value: 'default string'.",
               'default': 'default string'}, {'name': 'flag', 'type': 'Flag', 'help': 'flag option'}, {
                  'name': 'dictionary-val', 'type': 'KeyValueList', 'help': 'dictionary-val option'}, {'name': 'iniopt',
                                                                                                       'type': 'IniType',
                                                                                                       'help': "Help for '--iniopt'",
                                                                                                       'action': 'append'}, {
                  'name': 'nestedlist-app', 'type': 'NestedList', 'help': "Help for '--nestedlist-app'",
                  'action': 'append'}, {'name': 'nestedlist', 'type': 'NestedList',
                                        'help': "Help for '--nestedlist'"}, {'name': 'uni-dep', 'type': 'Value',
                                                                             'help': 'Help for --uni-dep',
                                                                             'required_when': 'req-arg-a == yes'}, {
                  'name': 'multi-dep', 'type': 'Value', 'help': 'Help for --multi-dep',
                  'required_when': ['req-arg-a == yes', 'req-arg-b == yes']}, {'name': 'uni-neg-dep', 'type': 'Value',
                                                                               'help': 'Help for --uni-neg',
                                                                               'required_when': 'uni-dep != uni-val'}, {
                  'name': 'uni-int', 'type': 'Value', 'help': 'Help for --uni-neg', 'required_when': 'version > 10'}, {
                  'name': 'req-arg-a', 'type': 'Bool', 'help': 'Help for --req-arg-a'}, {'name': 'req-arg-b',
                                                                                         'type': 'Bool',
                                                                                         'help': 'Help for --req-arg-b'}, {
                  'name': 'either-dep', 'type': 'Value', 'help': 'Help for --either-dep',
                  'required_when': ['req-arg-c == yes or req-arg-d == yes']}, {'name': 'req-arg-c', 'type': 'Bool',
                                                                               'help': 'Help for --req-arg-c'}, {
                  'name': 'req-arg-d', 'type': 'Bool', 'help': 'Help for --req-arg-d'}, {'name': 'version',
                                                                                         'type': 'int',
                                                                                         'help': 'Help for --version'}, {
                  'name': 'deprecated-way', 'type': 'Value', 'help': 'Deprecated way to do it'}, {'name': 'new-way',
                                                                                                  'deprecates': 'deprecated-way',
                                                                                                  'type': 'Value',
                                                                                                  'help': 'New way to do it'}, {
                  'name': 'tasks', 'type': 'ListOfFileNames',
                  'help': 'This is example for option which is with type "ListOfFileNames" and has\nauto propagation of "Allowed Values" in help. When we ask for --help it\nwill look in plugin folder for directory name as \'lookup_dir\' value, and\nwill add all file names to "Allowed Values"\n\nAllowed values: [\'task1\', \'task2\', \'task3\'].',
                  'lookup_dir': 'post_tasks'}, {'name': 'value-minmax-str', 'type': 'Value',
                                                'help': 'str type value'}, {'name': 'value-minmax-int', 'type': 'int',
                                                                            'help': "value with a minimum and maximum values\nMinimum value: '100'.\nMaximum value: '200'.",
                                                                            'minimum': 100, 'maximum': 200}, {
                  'name': 'value-minmax-float', 'type': 'float',
                  'help': "value with a minimum and maximum values\nMinimum value: '0.5'.\nMaximum value: '1.5'.",
                  'minimum': 0.5, 'maximum': 1.5}, {'name': 'value-min-zero', 'type': 'int',
                                                    'help': "value with a minimum zero value\nMinimum value: '0'.",
                                                    'minimum': 0}, {'name': 'value-max-zero', 'type': 'int',
                                                                    'help': "value with a maximum zero value\nMaximum value: '0'.",
                                                                    'maximum': 0}, {'name': 'value-len',
                                                                                    'type': 'Value',
                                                                                    'help': "value with length\nMaximum variable length: '4'.",
                                                                                    'length': 4}, {'name': 'verbose',
                                                                                                   'action': 'count',
                                                                                                   'default': 0,
                                                                                                   'help': "Control Ansible verbosity level\nDefault value: '0'.",
                                                                                                   'short': 'v',
                                                                                                   'is_shared_group_option': True}, {
                  'name': 'ansible-args',
                  'help': 'Extra variables for ansible - playbook tool \nShould be specified as a list of ansible-playbook options, separated with ";". \nFor example, --ansible-args="tags=tagging,overcloud;forks=500"',
                  'type': 'AdditionalArgs', 'is_shared_group_option': True}, {'name': 'inventory',
                                                                              'help': 'Inventory file',
                                                                              'type': 'Inventory',
                                                                              'is_shared_group_option': True}, {
                  'name': 'dry-run', 'action': 'store_true',
                  'help': 'Only generate settings, skip the playbook execution stage',
                  'is_shared_group_option': True}, {'name': 'extra-vars', 'action': 'append',
                                                    'help': 'Extra variables to be merged last', 'short': 'e',
                                                    'type': 'str', 'is_shared_group_option': True}, {'name': 'output',
                                                                                                     'help': 'File to dump the generated settings into (default: stdout)',
                                                                                                     'short': 'o',
                                                                                                     'type': 'str',
                                                                                                     'is_shared_group_option': True}, {
                  'name': 'from-file', 'action': 'read-answers', 'help': 'reads arguments from file.',
                  'is_shared_group_option': True}, {'name': 'generate-answers-file', 'action': 'generate-answers',
                                                    'help': 'generate configuration file with default values',
                                                    'is_shared_group_option': True}]
    args3={'example': {'foo-bar': 'default string', 'verbose': 0, 'dry-run': False}}
    args22 = [{'name': 'foo-bar', 'type': 'Value', 'help': "foo.bar option\nDefault value: 'default string'.",
              'default': 'default string'}, {'name': 'flag', 'type': 'Flag', 'help': 'flag option'}, {
                 'name': 'dictionary-val', 'type': 'KeyValueList', 'help': 'dictionary-val option'}, {'name': 'iniopt',
                                                                                                      'type': 'IniType',
                                                                                                      'help': "Help for '--iniopt'",
                                                                                                      'action': 'append'},
             {
                 'name': 'nestedlist-app', 'type': 'NestedList', 'help': "Help for '--nestedlist-app'",
                 'action': 'append'}, {'name': 'nestedlist', 'type': 'NestedList',
                                       'help': "Help for '--nestedlist'"}, {'name': 'uni-dep', 'type': 'Value',
                                                                            'help': 'Help for --uni-dep',
                                                                            'required_when1': 'req-arg-a == yes'}, {
                 'name': 'multi-dep', 'type': 'Value', 'help': 'Help for --multi-dep',
                 'required_when1': ['req-arg-a == yes', 'req-arg-b == yes']}, {'name': 'uni-neg-dep', 'type': 'Value',
                                                                              'help': 'Help for --uni-neg',
                                                                              'required_when1': 'uni-dep != uni-val'}, {
                 'name': 'uni-int', 'type': 'Value', 'help': 'Help for --uni-neg', 'required_when1': 'version > 10'}, {
                 'name': 'req-arg-a', 'type': 'Bool', 'help': 'Help for --req-arg-a'}, {'name': 'req-arg-b',
                                                                                        'type': 'Bool',
                                                                                        'help': 'Help for --req-arg-b'},
             {
                 'name': 'either-dep', 'type': 'Value', 'help': 'Help for --either-dep',
                 'required_when1': ['req-arg-c == yes or req-arg-d == yes']}, {'name': 'req-arg-c', 'type': 'Bool',
                                                                              'help': 'Help for --req-arg-c'}, {
                 'name': 'req-arg-d', 'type': 'Bool', 'help': 'Help for --req-arg-d'}, {'name': 'version',
                                                                                        'type': 'int',
                                                                                        'help': 'Help for --version'}, {
                 'name': 'deprecated-way', 'type': 'Value', 'help': 'Deprecated way to do it'}, {'name': 'new-way',
                                                                                                 'deprecates': 'deprecated-way',
                                                                                                 'type': 'Value',
                                                                                                 'help': 'New way to do it'},
             {
                 'name': 'tasks', 'type': 'ListOfFileNames',
                 'help': 'This is example for option which is with type "ListOfFileNames" and has\nauto propagation of "Allowed Values" in help. When we ask for --help it\nwill look in plugin folder for directory name as \'lookup_dir\' value, and\nwill add all file names to "Allowed Values"\n\nAllowed values: [\'task1\', \'task2\', \'task3\'].',
                 'lookup_dir': 'post_tasks'}, {'name': 'value-minmax-str', 'type': 'Value',
                                               'help': 'str type value'}, {'name': 'value-minmax-int', 'type': 'int',
                                                                           'help': "value with a minimum and maximum values\nMinimum value: '100'.\nMaximum value: '200'.",
                                                                           'minimum': 100, 'maximum': 200}, {
                 'name': 'value-minmax-float', 'type': 'float',
                 'help': "value with a minimum and maximum values\nMinimum value: '0.5'.\nMaximum value: '1.5'.",
                 'minimum': 0.5, 'maximum': 1.5}, {'name': 'value-min-zero', 'type': 'int',
                                                   'help': "value with a minimum zero value\nMinimum value: '0'.",
                                                   'minimum': 0}, {'name': 'value-max-zero', 'type': 'int',
                                                                   'help': "value with a maximum zero value\nMaximum value: '0'.",
                                                                   'maximum': 0}, {'name': 'value-len',
                                                                                   'type': 'Value',
                                                                                   'help': "value with length\nMaximum variable length: '4'.",
                                                                                   'length': 4}, {'name': 'verbose',
                                                                                                  'action': 'count',
                                                                                                  'default': 0,
                                                                                                  'help': "Control Ansible verbosity level\nDefault value: '0'.",
                                                                                                  'short': 'v',
                                                                                                  'is_shared_group_option': True},
             {
                 'name': 'ansible-args',
                 'help': 'Extra variables for ansible - playbook tool \nShould be specified as a list of ansible-playbook options, separated with ";". \nFor example, --ansible-args="tags=tagging,overcloud;forks=500"',
                 'type': 'AdditionalArgs', 'is_shared_group_option': True}, {'name': 'inventory',
                                                                             'help': 'Inventory file',
                                                                             'type': 'Inventory',
                                                                             'is_shared_group_option': True}, {
                 'name': 'dry-run', 'action': 'store_true',
                 'help': 'Only generate settings, skip the playbook execution stage',
                 'is_shared_group_option': True}, {'name': 'extra-vars', 'action': 'append',
                                                   'help': 'Extra variables to be merged last', 'short': 'e',
                                                   'type': 'str', 'is_shared_group_option': True}, {'name': 'output',
                                                                                                    'help': 'File to dump the generated settings into (default: stdout)',
                                                                                                    'short': 'o',
                                                                                                    'type': 'str',
                                                                                                    'is_shared_group_option': True},
             {
                 'name': 'from-file', 'action': 'read-answers', 'help': 'reads arguments from file.',
                 'is_shared_group_option': True}, {'name': 'generate-answers-file', 'action': 'generate-answers',
                                                   'help': 'generate configuration file with default values',
                                                   'is_shared_group_option': True}]
    ist1=temp_class._get_conditionally_required_args("example", args2, args3)==[]
    ist2=temp_class._get_conditionally_required_args("example", args22, args3)==[]
    if not ist1 or not ist2:
        isT=False
    # fff = open("aaa.json", 'r')
    # content=dill.loads(fff.read().encode())
    # fff.close()
    # print(content)
    # input1 = {'foo-bar': "111"}
    # print(content._convert_non_cli_args("example", input1))
    # try:
    #     temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
    #                                  'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
    #                                  'mock_key': 'expected_value'}, "", "", "")
    #     temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
    #         {"options": {"foo-bar": {"type": "int"}, "key2": {"value2": 2}, "key3": {"value3": 3}}},
    #         {"options": {"foo-bar": {"type": "int"}, "key22": {"value22": 22}, "key33": {"value33": 33}}}]}}}
    #     fff=open("aaa.json",'wb')
    #
    #     fff.write( dill.dumps(temp_class))
    #     fff.close()
    #     input1 = {'foo-bar': "111"}
    #     input2 = {'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3'}
    #     temp_class._get_conditionally_required_args("example", input1)
    #     temp_class._get_conditionally_required_args("example", input2)
    #     if not isinstance(input1["foo-bar"], int):
    #         isT = False
    #     if input2["iniopt"] != 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3':
    #         isT = False
    # except:
    #     isT = False
    # import os

    # isT = True
    # for l in os.listdir(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos/redhat-openstack---infrared/data_passk_platform1/6306092d73426c38ae68ad08/"):
    #     f = open(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos/redhat-openstack---infrared/data_passk_platform1/6306092d73426c38ae68ad08/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     if isinstance(content["input"]["args"][3]["bytes"], bytes):
    #         args3 = dill.loads(content["input"]["args"][3]["bytes"])
    #     else:
    #         args3 = content["input"]["args"][3]["bytes"]
    #     # print(content["input"]["args"][0]["bytes"])
    #     # print(dill.dumps(content["input"]["args"][0]["bytes"]["$binary"]["base64"]))
    #     print(args1, args2, args3)
    #     object_class = content["input"]["args"][0]["bytes"]
    #     temp_class = SpecParser("",{},"","","")
    #     # temp_class.__dict__.update(dill.dumps(content["input"]["args"][0]["bytes"]["$binary"]["base64"]))
    #     # print(temp_class.vars)
    #     # print(content["input"]["args"][0]["bytes"])
    #     # res0 = object_class._get_conditionally_required_args(args1, args2, args3)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/inspector_validate_length_args_passk_validte.py
import collections
import os
from six.moves import configparser
from string import Template
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
from infrared.core.inspector import helper
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = helper.SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get('IR_' + name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """

        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))

                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == helper.OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)

        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)

            for option in expected_options:
                name = option['name']

                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }

                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == helper.OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value

if __name__ == "__main__":
    isT = True
    import os

    temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
                                 'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                 'mock_key': 'expected_value'}, "", "", "")
    temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
        {"options": {"JAVA_HOME": {"type": "int"}, "key2": {"length": 2}, "key3": {"value3": 3}}},
        {"options": {"JAVA_HOME": {"type": "int"}, "key22": {"length": 22}, "abca": {"length": 10}}}]}}}
    input1 = {'foo-bar': {'abca': 'default string', 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val='],
                          'output': '/tmp/pytest-of-travis/pytest-2/test_extra_vars_input_args1_ex0/tmp/dry_output.yml'}}
    input2 = {'foo-bar': {'key2': 'string', 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val='],
                          'output': '/tmp/pytest-of-travis/pytest-2/test_extra_vars_input_args1_ex0/tmp/dry_output.yml'}}
    input3 = {'foo-bar': {'key22': 'ag', 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val='],
                          'output': 'sdf'}}
    # input4 = {'foo-bar': {'foo-bar': 'default string', 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val='],
    #                       'output': '/tmp/pytest-of-travis/pytest-2/test_extra_vars_input_args1_ex0/tmp/dry_output.yml'}}
    # temp_class.validate_length_args(input1)
    try:
        temp_class.validate_length_args(input1)
        isT = False
    except Exception as e:
        if str(e) != str([('abca', 'default string', 10)]):
            isT = False
    try:
        temp_class.validate_length_args(input2)
        isT = False
    except Exception as e:
        if str(e) != str([('key2', 'string', 2)]):
            isT = False
    try:
        temp_class.validate_length_args(input3)
    except:
        isT = False

    if not isT:
        raise Exception("Result not True!!!")



----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/inspector_validate_choices_args_passk_validte.py
import collections
import os
from six.moves import configparser
from string import Template
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
from infrared.core.inspector import helper
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = helper.SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get('IR_' + name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """

        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))

                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == helper.OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)

        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)

            for option in expected_options:
                name = option['name']

                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }

                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == helper.OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value

if __name__ == "__main__":
    isT=True
    import os
    temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
                                 'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                 'mock_key': 'expected_value'}, "", "", "")
    temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
        {"options": {"JAVA_HOME": {"type": "int"}, "key2": {"choices": 2}, "key3": {"value3": 3}}},
        {"options": {"JAVA_HOME": {"type": "int"}, "key22": {"choices": 22}, "abca": {"choices": "abc"}}}]}}}
    input1={'foo-bar': {'abca': 'default string', 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val='],
                 'output': '/tmp/pytest-of-travis/pytest-2/test_extra_vars_input_args1_ex0/tmp/dry_output.yml'}}
    input2 = {'foo-bar': {'key2': 'default string', 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val='],
                          'output': '/tmp/pytest-of-travis/pytest-2/test_extra_vars_input_args1_ex0/tmp/dry_output.yml'}}
    input3 = {'foo-bar': {'key22': 'default string', 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val='],
                          'output': '/tmp/pytest-of-travis/pytest-2/test_extra_vars_input_args1_ex0/tmp/dry_output.yml'}}
    input4 = {'foo-bar': {'foo-bar': 'default string', 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val='],
                          'output': '/tmp/pytest-of-travis/pytest-2/test_extra_vars_input_args1_ex0/tmp/dry_output.yml'}}
    try:
        temp_class.validate_choices_args(input1)
        isT = False
    except Exception as e:
        if str(e)!=str([('abca', 'default string', 'abc')]):
            isT=False
    try:
        temp_class.validate_choices_args(input2)
        isT = False
    except Exception as e:
        if str(e)!="argument of type 'int' is not iterable":
            isT=False
    try:
        temp_class.validate_choices_args(input3)
        isT = False
    except Exception as e:
        if str(e)!="argument of type 'int' is not iterable":
            isT=False
    try:
        temp_class.validate_choices_args(input4)
    except:
        isT = False

    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/inspector_validate_min_max_args_passk_validte.py
import collections
import os
from six.moves import configparser
from string import Template
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
from infrared.core.inspector import helper
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = helper.SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get('IR_' + name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """

        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))

                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == helper.OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)

        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)

            for option in expected_options:
                name = option['name']

                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }
                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == helper.OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value

if __name__ == "__main__":
    isT = True
    import os

    temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
                                 'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                 'mock_key': 'expected_value'}, "", "", "")
    temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
        {"options": {"JAVA_HOME": {"maximum": 10,"minimum":5}, "key2": {"maximum": 7,"minimum":5}, "key3": {"maximum": 9,"minimum":4}}},
        {"options": {"JAVA_HOME": {"maximum": 7,"minimum":3}, "key22": {"maximum": 6,"minimum":1}, "abca": {"maximum": 8,"minimum":3}}}]}}}
    input1 = {'foo-bar': {'abca': 6, 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val='],
                          'output': '/tmp/pytest-of-travis/pytest-2/test_extra_vars_input_args1_ex0/tmp/dry_output.yml'}}
    input2 = {'foo-bar': {'key2': 1, 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val='],
                          'output': '/tmp/pytest-of-travis/pytest-2/test_extra_vars_input_args1_ex0/tmp/dry_output.yml'}}
    input3 = {'foo-bar': {'key22': 100, 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val='],
                          'output': 'sdf'}}
    # input4 = {'foo-bar': {'foo-bar': 'default string', 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val='],
    #                       'output': '/tmp/pytest-of-travis/pytest-2/test_extra_vars_input_args1_ex0/tmp/dry_output.yml'}}
    # temp_class.validate_min_max_args(input1)
    try:
        temp_class.validate_min_max_args(input1)
    except Exception as e:
        isT = False
    try:
        temp_class.validate_min_max_args(input2)
        isT = False
    except Exception as e:
        if str(e)!="[('key2', 'minimum', 5, 1)]":
            isT = False
    try:
        temp_class.validate_min_max_args(input3)
        isT = False
    except Exception as e:
        if str(e) != "[('key22', 'maximum', 6, 100)]":
            isT = False

    # try:
    #     temp_class.validate_length_args(input2)
    #     isT = False
    # except Exception as e:
    #     if str(e) != str([('key2', 'string', 2)]):
    #         isT = False
    # try:
    #     temp_class.validate_length_args(input3)
    # except:
    #     isT = False
    #
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/inspector_create_complex_argumet_type_passk_validte.py
import collections
import os
from six.moves import configparser
from string import Template
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
from infrared.core.inspector import helper
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = helper.SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get('IR_' + name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """

        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))

                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == helper.OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)

        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)

            for option in expected_options:
                name = option['name']

                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }

                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == helper.OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value

if __name__ == "__main__":
    import os

    isT = True
    temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
                                 'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                 'mock_key': 'expected_value'}, "", "", "")
    temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
        {"options": {"foo-bar": {"type": "int"}, "key2": {"value2": 2}, "key3": {"value3": 3}}},
        {"options": {"foo-bar": {"type": "int"}, "key22": {"value22": 22}, "key33": {"value33": 33}}}]}}}
    args1='example'
    args2='Value'
    args3='foo-bar'
    args4={'name': 'foo-bar', 'type': 'Value', 'help': "foo.bar option\nDefault value: 'default string'.",
     'default': 'default string'}

    ist1=temp_class.create_complex_argumet_type(args1,args2,args3,args4).arg_name=="foo-bar"
    ist2=temp_class.create_complex_argumet_type(args1, args2, args3, args4).spec_option=={'name': 'foo-bar', 'type': 'Value', 'help': "foo.bar option\nDefault value: 'default string'.", 'default': 'default string'}
    if not ist1 or not ist2:
        isT=False
    # for l in os.listdir(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos/redhat-openstack---infrared\\data_passk_platform1/6306092e73426c38ae68ad0d/"):
    #     f = open(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos/redhat-openstack---infrared\\data_passk_platform1/6306092e73426c38ae68ad0d/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     if isinstance(content["input"]["args"][3]["bytes"], bytes):
    #         args3 = dill.loads(content["input"]["args"][3]["bytes"])
    #     else:
    #         args3 = content["input"]["args"][3]["bytes"]
    #     if isinstance(content["input"]["args"][4]["bytes"], bytes):
    #         args4 = dill.loads(content["input"]["args"][4]["bytes"])
    #     else:
    #         args4 = content["input"]["args"][4]["bytes"]
    #     print(args1, args2, args3, args4)
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     # temp_class = SpecParser("", {}, "", "", "")
    #     # temp_class.__dict__.update(object_class)
    #     # res0 = temp_class.create_complex_argumet_type(args1, args2, args3, args4)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/inspector_get_nested_custom_and_control_args_passk_validte.py
import collections
import os
from six.moves import configparser
from string import Template
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
from infrared.core.inspector import helper
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = helper.SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get('IR_' + name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """

        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))

                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == helper.OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)

        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)

            for option in expected_options:
                name = option['name']

                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }

                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == helper.OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value

if __name__ == "__main__":

    args1={'example': {'foo-bar': 'default string', 'verbose': 0, 'dry-run': False},'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
        {"options": {"foo-bar": {"type": "int"}, "key2": {"value2": 2}, "key3": {"value3": 3}}},
        {"options": {"foo-bar": {"type": "int"}, "key22": {"value22": 22}, "key33": {"deprecates": 33}}}]}},"foo-bar":{"abc":"bca"}}
    args2={'example': {'foo-bar': 'fail', 'verbose': 0, 'dry-run': False}}
    args3={'example': {'foo-bar': 'default string', 'verbose': 0, 'dry-run': True, 'extra-vars': ['key=val'],
                 'output': '/tmp/pytest-of-travis/pytest-2/test_extra_vars_input_args2_ex0/tmp/dry_output.yml'}}
    isT = True
    temp_class = SpecParser("", {'foo-bar': 'from_answers_file',
                                 'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3',
                                 'mock_key': 'expected_value'}, "", "", "")
    temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one": 1, "tow": 2, "groups": [
        {"options": {"foo-bar": {"type": "int"}, "key2": {"value2": 2}, "key3": {"value3": 3}}},
        {"options": {"foo-bar": {"type": "int"}, "key22": {"value22": 22}, "key33": {"deprecates": 33}}}]}}}
    res1 = temp_class.get_nested_custom_and_control_args(args1)[0]=={'abc': 'bca'} and temp_class.get_nested_custom_and_control_args(args1)[1]=={} and temp_class.get_nested_custom_and_control_args(args1)[2]== {}
    res2 = temp_class.get_nested_custom_and_control_args(args2)[0]=={} and temp_class.get_nested_custom_and_control_args(args2)[1]=={} and temp_class.get_nested_custom_and_control_args(args2)[2]== {}
    res3 = temp_class.get_nested_custom_and_control_args(args3)[0]=={} and temp_class.get_nested_custom_and_control_args(args3)[1]=={} and temp_class.get_nested_custom_and_control_args(args3)[2]== {}
    if not res1 or not res2 or not res3:
        isT=False
    # for l in os.listdir(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos/redhat-openstack---infrared\\data_passk_platform1/6306092e73426c38ae68ad0f/"):
    #     f = open(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos/redhat-openstack---infrared\\data_passk_platform1/6306092e73426c38ae68ad0f/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     import json
    #
    #     # print(eval(args1))
    #     # try:
    #     #
    #     #     # print(eval(args1))
    #     #     res0 = temp_class.get_nested_custom_and_control_args(eval(args1))
    #     #     print(args1)
    #     #     print(res0)
    #     # except:
    #     #     pass
    #     # object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     # temp_class = SpecParser("",{},"","","")
    #     # temp_class.__dict__.update(object_class)
    #     # res0 = temp_class.get_nested_custom_and_control_args(args1)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/settings_merge_extra_vars_passk_validte.py
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions


class VarsDictManager(object):

    @staticmethod
    def generate_settings(entry_point,
                          nested_args,
                          delimiter='-'):
        """Unifies all input into a single dict of Ansible extra-vars

        :param entry_point: All input will be nested under this key
        :param nested_args: dict. these values will be nested
            example:
                {
                    foo-bar: value1,
                    foo2: value2
                    foo-another-bar: value3
                }
        :param delimiter: character to split keys by.
        :return: dict. nest input with keys splitted by delimiter

        >>> VarsDictManager.generate_settings(
        ... 'entry_point', {'foo-bar': 'value1',
        ...                 'foo2': 'value2',
        ...                 'foo-another-bar': 'value3'})
        {'entry_point': {'foo': {'bar': 'value1', 'another':\
 {'bar': 'value3'}}, 'foo2': 'value2'}}
        """
        vars_dict = {entry_point: {}}
        try:
            for _name, argument in nested_args.items():
                dict_utils.dict_insert(vars_dict[entry_point],
                                       argument,
                                       *_name.split(delimiter))

        # handle errors here and provide more output for user if required
        except exceptions.IRKeyNotFoundException as key_exception:
            if key_exception and key_exception.key.startswith("private."):
                raise exceptions.IRPrivateSettingsMissingException(
                    key_exception.key)
            else:
                raise
        return vars_dict

    @staticmethod
    def merge_extra_vars(vars_dict, extra_vars=None):
        """Extend ``vars_dict`` with ``extra-vars``

        :param vars_dict: Dictionary to merge extra-vars into
        :param extra_vars: List of extra-vars
        """
        for extra_var in extra_vars or []:
            # print(extra_var)
            if extra_var.startswith('@'):
                with open(extra_var[1:]) as f_obj:
                    loaded_yml = yaml.safe_load(f_obj)

                dict_utils.dict_merge(
                    vars_dict,
                    loaded_yml,
                    conflict_resolver=dict_utils.ConflictResolver.
                    unique_append_list_resolver)

            else:
                if '=' not in extra_var:
                    raise exceptions.IRExtraVarsException(extra_var)
                key, value = extra_var.split("=", 1)
                if value.startswith('@'):
                    with open(value[1:]) as f_obj:
                        loaded_yml = yaml.safe_load(f_obj)

                    tmp_dict = {}
                    dict_utils.dict_insert(tmp_dict, loaded_yml, *key.split("."))

                    dict_utils.dict_merge(
                        vars_dict,
                        tmp_dict,
                        conflict_resolver=dict_utils.ConflictResolver.
                        unique_append_list_resolver)

                else:
                    dict_utils.dict_insert(vars_dict, value, *key.split("."))

if __name__ == "__main__":
    input1={'dict_var': {'test': {'bar': 'default string'}}}
    # input2={'provision': {'foo': {'bar': 'fail'}}}
    # input3 = {'@provision': {'foo': {'@bar': 'fail'}}}
    VarsDictManager.merge_extra_vars(input1,["@some_var_file.yml"])
    # print(VarsDictManager.merge_extra_vars(input2))
    # print(VarsDictManager.merge_extra_vars(input3))
    isT=input1=={'dict_var': {'test': 'var', 'some_list': ['a', 'b', 'c']}, 'another_dict_var': {'with': 'completely', 'different': {'set': ['of', 'values']}}}

    # print(input2)
    # print(input3)
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("C:\\Users\yh199\Downloads\\repos1\\repos\\redhat-openstack---infrared/data_passk_platform1/6306092e73426c38ae68ad11/"):
    #     f = open("C:\\Users\yh199\Downloads\\repos1\\repos\\redhat-openstack---infrared/data_passk_platform1/6306092e73426c38ae68ad11/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args1=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args1=content["input"]["args"][0]["bytes"]
    #     print(args1)
    #     # temp_class=VarsDictManager()
    #     # res0 = temp_class.merge_extra_vars(args1)
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/execute_ansible_playbook_passk_validte.py
from datetime import datetime
from distutils.util import strtobool
import errno
import json
import os
import re
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
import tempfile

from infrared.core.services.workspaces import Workspace
from infrared.core.utils import logger
import yaml

LOG = logger.LOG


class NoAnsiFile(object):

    re_ansi = re.compile(r'\x1b[^m]*m')

    def __init__(self, fd):
        self.fd = fd

    def write(self, data):
        no_ansi_data = self.re_ansi.sub('', data)
        self.fd.write(no_ansi_data)

    def close(self):
        self.fd.close()

    def flush(self):
        self.fd.flush()


class IRStdFd(object):
    pass


class IRStdoutFd(IRStdFd):

    def __init__(self, print_stdout=True):
        self.print_stdout = print_stdout
        self.org_stdout = sys.stdout
        sys.stdout = self

    def write(self, data):
        if self.print_stdout:
            sys.__stdout__.write(data)
            sys.__stdout__.flush()
        for fd in IRSTDFDManager.fds:
            if not isinstance(fd, IRStdFd):
                fd.write(data)
                fd.flush()

    @staticmethod
    def flush():
        sys.__stdout__.flush()

    @staticmethod
    def close():
        sys.stdout = sys.__stdout__

    @staticmethod
    def fileno():
        return sys.__stdout__.fileno()


class IRStderrFd(IRStdFd):

    def __init__(self, print_stderr=True):
        self.print_stderr = print_stderr
        self.org_stderr = sys.stderr
        sys.stderr = self

    def write(self, data):
        if self.print_stderr:
            sys.__stderr__.write(data)
            sys.__stderr__.flush()
        for fd in IRSTDFDManager.fds:
            if not isinstance(fd, IRStdFd):
                fd.write(data)
                fd.flush()

    @staticmethod
    def flush():
        sys.__stderr__.flush()

    @staticmethod
    def close():
        sys.stderr = sys.__stderr__


class IRSTDFDManager(object):

    fds = set()

    def __init__(self, stdout=True, stderr=True, *fds):

        self.stdout = stdout
        self.stderr = stderr

        for fd in fds:
            self.add(fd)

        self.add(IRStdoutFd(print_stdout=self.stdout))
        self.add(IRStderrFd(print_stderr=self.stderr))

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.close()

    def write(self, data):
        for fd in self.fds:
            fd.write(data)
            fd.flush()

    def flush(self):
        for fd in self.fds:
            fd.flush()

    def close(self):
        for fd in self.fds:
            fd.close()

    def add(self, fd):
        self.fds.add(fd)


def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,
                     extra_vars=None, ansible_args=None):
    """Wraps the 'ansible-playbook' CLI.

     :param ir_workspace: An Infrared Workspace object represents the active
     workspace
     :param ir_plugin: An InfraredPlugin object of the current plugin
     :param playbook_path: the playbook to invoke
     :param verbose: Ansible verbosity level
     :param extra_vars: dict. Passed to Ansible as extra-vars
     :param ansible_args: dict of ansible-playbook arguments to plumb down
         directly to Ansible.
    """
    ansible_args = ansible_args or []
    LOG.debug("Additional ansible args: {}".format(ansible_args))

    # hack for verbosity
    # from ansible.utils.display import Display
    # display = Display(verbosity=verbose)
    # import __main__ as main
    # setattr(main, "display", display)

    # TODO(yfried): Use proper ansible API instead of emulating CLI
    cli_args = ['execute',
                playbook_path,
                '--inventory', ir_workspace.inventory]

    # infrared should not change ansible verbosity unless user specifies that
    if verbose:
        cli_args.append('-' + 'v' * int(verbose))

    cli_args.extend(ansible_args)

    results = _run_playbook(cli_args, vars_dict=extra_vars or {},
                            ir_workspace=ir_workspace, ir_plugin=ir_plugin)

    if results:
        LOG.error('Playbook "%s" failed!' % playbook_path)
    return results


def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):
    """Runs ansible cli with vars dict

    :param vars_dict: dict, Will be passed as Ansible extra-vars
    :param cli_args: the list  of command line arguments
    :param ir_workspace: An Infrared Workspace object represents the active
     workspace
    :param ir_plugin: An InfraredPlugin object of the current plugin
    :return: ansible results
    """

    # TODO(yfried): use ansible vars object instead of tmpfile
    # NOTE(oanufrii): !!!this import should be exactly here!!!
    #                 Ansible uses 'display' singleton from '__main__' and
    #                 gets it on module level. While we monkeypatching our
    #                 '__main__' in 'ansible_playbook' function import of
    #                 PlaybookCLI shoul be after that, to get patched
    #                 '__main__'. Otherwise ansible gets unpatched '__main__'
    #                 and creates new 'display' object with default (0)
    #                 verbosity.
    # NOTE(afazekas): GlobalCLIArgs gets value only once per invocation, but
    # since it has singleton decorator, so it would remember to old arguments in different tests
    # removing the singleton decorator
    try:
        from ansible.utils import context_objects
        context_objects.GlobalCLIArgs = context_objects.CLIArgs
    except ImportError:
        # older version
        pass

    # from ansible.cli.playbook import PlaybookCLI
    from ansible.errors import AnsibleOptionsError
    from ansible.errors import AnsibleParserError

    with tempfile.NamedTemporaryFile(
            mode='w+', prefix="ir-settings-", delete=True) as tmp:
        tmp.write(yaml.safe_dump(vars_dict, default_flow_style=False))
        # make sure created file is readable.
        tmp.flush()
        cli_args.extend(['--extra-vars', "@" + tmp.name])

        if not bool(strtobool(os.environ.get('IR_NO_EXTRAS', 'no'))):
            ir_extras = {
                'infrared': {
                    'python': {
                        'executable': sys.executable,
                        'version': {
                            'full': sys.version.split()[0],
                            'major': sys.version_info.major,
                            'minor': sys.version_info.minor,
                            'micro': sys.version_info.micro,
                        }
                    }
                }
            }
            cli_args.extend(['--extra-vars', str(ir_extras)])

        # cli = PlaybookCLI(cli_args)
        LOG.debug('Starting ansible cli with args: {}'.format(cli_args[1:]))
        try:
            # cli.parse()

            stdout = not bool(
                strtobool(os.environ.get('IR_ANSIBLE_NO_STDOUT', 'no')))
            stderr = not bool(
                strtobool(os.environ.get('IR_ANSIBLE_NO_STDERR', 'no')))

            ansible_outputs_dir = \
                os.path.join(ir_workspace.path, 'ansible_outputs')
            ansible_vars_dir = \
                os.path.join(ir_workspace.path, 'ansible_vars')

            timestamp = datetime.utcnow().strftime("%Y-%m-%d_%H-%M-%S.%f")
            filename_template = \
                "ir_{timestamp}_{plugin_name}{postfix}.{file_ext}"

            for _dir in (ansible_outputs_dir, ansible_vars_dir):
                try:
                    os.makedirs(_dir)
                except OSError as e:
                    if e.errno != errno.EEXIST:
                        raise

            if bool(strtobool(os.environ.get('IR_GEN_VARS_JSON', 'no'))):
                filename = filename_template.format(
                    timestamp=timestamp,
                    plugin_name=ir_plugin.name,
                    postfix='',
                    file_ext='json'
                )
                vars_file = os.path.join(ansible_vars_dir, filename)
                with open(vars_file, 'w') as fp:
                    json.dump(vars_dict, fp, indent=4, sort_keys=True)

            with IRSTDFDManager(stdout=stdout, stderr=stderr) as fd_manager:

                if bool(strtobool(os.environ.get(
                        'IR_ANSIBLE_LOG_OUTPUT', 'no'))):
                    filename = filename_template.format(
                        timestamp=timestamp,
                        plugin_name=ir_plugin.name,
                        postfix='',
                        file_ext='log'
                    )
                    log_file = os.path.join(ansible_outputs_dir, filename)
                    fd_manager.add(open(log_file, 'w'))

                if bool(strtobool(os.environ.get(
                        'IR_ANSIBLE_LOG_OUTPUT_NO_ANSI', 'no'))):
                    filename = filename_template.format(
                        timestamp=timestamp,
                        plugin_name=ir_plugin.name,
                        postfix='_no_ansi',
                        file_ext='log'
                    )
                    log_file = os.path.join(ansible_outputs_dir, filename)
                    fd_manager.add(NoAnsiFile(open(log_file, 'w')))

                # Return the result:
                # 0: Success
                # 1: "Error"
                # 2: Host failed
                # 3: Unreachable
                # 4: Parser Error
                # 5: Options error

                # return cli.run()

        except (AnsibleParserError, AnsibleOptionsError) as error:
            LOG.error('{}: {}'.format(type(error), error))
            raise error

if __name__ == "__main__":
    args1 = Workspace("example", "/home/travis/builds/repos/redhat-openstack---infrared/")
    args2="example"
    args3='/home/travis/builds/repos/redhat-openstack---infrared/tests/example/main.yml'
    args4=0
    args5={'provision': {'foo': {'bar': 'default string'}}}
    args6 =None

    out=ansible_playbook(args1,args2,args3,args4,args5,args6)
    isT=out==None
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/execute__run_playbook_passk_validte.py
from datetime import datetime
from distutils.util import strtobool
import errno
import json
import os
import re
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
import tempfile

from infrared.core.services.workspaces import Workspace
from infrared.core.utils import logger
import yaml

LOG = logger.LOG


class NoAnsiFile(object):
    re_ansi = re.compile(r'\x1b[^m]*m')

    def __init__(self, fd):
        self.fd = fd

    def write(self, data):
        no_ansi_data = self.re_ansi.sub('', data)
        self.fd.write(no_ansi_data)

    def close(self):
        self.fd.close()

    def flush(self):
        self.fd.flush()


class IRStdFd(object):
    pass


class IRStdoutFd(IRStdFd):

    def __init__(self, print_stdout=True):
        self.print_stdout = print_stdout
        self.org_stdout = sys.stdout
        sys.stdout = self

    def write(self, data):
        if self.print_stdout:
            sys.__stdout__.write(data)
            sys.__stdout__.flush()
        for fd in IRSTDFDManager.fds:
            if not isinstance(fd, IRStdFd):
                fd.write(data)
                fd.flush()

    @staticmethod
    def flush():
        sys.__stdout__.flush()

    @staticmethod
    def close():
        sys.stdout = sys.__stdout__

    @staticmethod
    def fileno():
        return sys.__stdout__.fileno()


class IRStderrFd(IRStdFd):

    def __init__(self, print_stderr=True):
        self.print_stderr = print_stderr
        self.org_stderr = sys.stderr
        sys.stderr = self

    def write(self, data):
        if self.print_stderr:
            sys.__stderr__.write(data)
            sys.__stderr__.flush()
        for fd in IRSTDFDManager.fds:
            if not isinstance(fd, IRStdFd):
                fd.write(data)
                fd.flush()

    @staticmethod
    def flush():
        sys.__stderr__.flush()

    @staticmethod
    def close():
        sys.stderr = sys.__stderr__


class IRSTDFDManager(object):
    fds = set()

    def __init__(self, stdout=True, stderr=True, *fds):

        self.stdout = stdout
        self.stderr = stderr

        for fd in fds:
            self.add(fd)

        self.add(IRStdoutFd(print_stdout=self.stdout))
        self.add(IRStderrFd(print_stderr=self.stderr))

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.close()

    def write(self, data):
        for fd in self.fds:
            fd.write(data)
            fd.flush()

    def flush(self):
        for fd in self.fds:
            fd.flush()

    def close(self):
        for fd in self.fds:
            fd.close()

    def add(self, fd):
        self.fds.add(fd)


def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,
                     extra_vars=None, ansible_args=None):
    """Wraps the 'ansible-playbook' CLI.

     :param ir_workspace: An Infrared Workspace object represents the active
     workspace
     :param ir_plugin: An InfraredPlugin object of the current plugin
     :param playbook_path: the playbook to invoke
     :param verbose: Ansible verbosity level
     :param extra_vars: dict. Passed to Ansible as extra-vars
     :param ansible_args: dict of ansible-playbook arguments to plumb down
         directly to Ansible.
    """
    ansible_args = ansible_args or []
    LOG.debug("Additional ansible args: {}".format(ansible_args))

    # hack for verbosity
    from ansible.utils.display import Display
    display = Display(verbosity=verbose)
    import __main__ as main
    setattr(main, "display", display)

    # TODO(yfried): Use proper ansible API instead of emulating CLI
    cli_args = ['execute',
                playbook_path,
                '--inventory', ir_workspace.inventory]

    # infrared should not change ansible verbosity unless user specifies that
    if verbose:
        cli_args.append('-' + 'v' * int(verbose))

    cli_args.extend(ansible_args)

    results = _run_playbook(cli_args, vars_dict=extra_vars or {},
                            ir_workspace=ir_workspace, ir_plugin=ir_plugin)

    if results:
        LOG.error('Playbook "%s" failed!' % playbook_path)
    return results


def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):
    """Runs ansible cli with vars dict

    :param vars_dict: dict, Will be passed as Ansible extra-vars
    :param cli_args: the list  of command line arguments
    :param ir_workspace: An Infrared Workspace object represents the active
     workspace
    :param ir_plugin: An InfraredPlugin object of the current plugin
    :return: ansible results
    """

    # TODO(yfried): use ansible vars object instead of tmpfile
    # NOTE(oanufrii): !!!this import should be exactly here!!!
    #                 Ansible uses 'display' singleton from '__main__' and
    #                 gets it on module level. While we monkeypatching our
    #                 '__main__' in 'ansible_playbook' function import of
    #                 PlaybookCLI shoul be after that, to get patched
    #                 '__main__'. Otherwise ansible gets unpatched '__main__'
    #                 and creates new 'display' object with default (0)
    #                 verbosity.
    # NOTE(afazekas): GlobalCLIArgs gets value only once per invocation, but
    # since it has singleton decorator, so it would remember to old arguments in different tests
    # removing the singleton decorator
    try:
        from ansible.utils import context_objects
        context_objects.GlobalCLIArgs = context_objects.CLIArgs
    except ImportError:
        # older version
        pass

    # from ansible.cli.playbook import PlaybookCLI
    from ansible.errors import AnsibleOptionsError
    from ansible.errors import AnsibleParserError

    with tempfile.NamedTemporaryFile(
            mode='w+', prefix="ir-settings-", delete=True) as tmp:
        tmp.write(yaml.safe_dump(vars_dict, default_flow_style=False))
        # make sure created file is readable.
        tmp.flush()
        cli_args.extend(['--extra-vars', "@" + tmp.name])

        if not bool(strtobool(os.environ.get('IR_NO_EXTRAS', 'no'))):
            ir_extras = {
                'infrared': {
                    'python': {
                        'executable': sys.executable,
                        'version': {
                            'full': sys.version.split()[0],
                            'major': sys.version_info.major,
                            'minor': sys.version_info.minor,
                            'micro': sys.version_info.micro,
                        }
                    }
                }
            }
            cli_args.extend(['--extra-vars', str(ir_extras)])

        # cli = PlaybookCLI(cli_args)
        LOG.debug('Starting ansible cli with args: {}'.format(cli_args[1:]))
        try:
            # cli.parse()

            stdout = not bool(
                strtobool(os.environ.get('IR_ANSIBLE_NO_STDOUT', 'no')))
            stderr = not bool(
                strtobool(os.environ.get('IR_ANSIBLE_NO_STDERR', 'no')))

            ansible_outputs_dir = \
                os.path.join(ir_workspace.path, 'ansible_outputs')
            ansible_vars_dir = \
                os.path.join(ir_workspace.path, 'ansible_vars')

            timestamp = datetime.utcnow().strftime("%Y-%m-%d_%H-%M-%S.%f")
            filename_template = \
                "ir_{timestamp}_{plugin_name}{postfix}.{file_ext}"

            for _dir in (ansible_outputs_dir, ansible_vars_dir):
                try:
                    os.makedirs(_dir)
                except OSError as e:
                    if e.errno != errno.EEXIST:
                        raise

            if bool(strtobool(os.environ.get('IR_GEN_VARS_JSON', 'no'))):
                filename = filename_template.format(
                    timestamp=timestamp,
                    plugin_name=ir_plugin.name,
                    postfix='',
                    file_ext='json'
                )
                vars_file = os.path.join(ansible_vars_dir, filename)
                with open(vars_file, 'w') as fp:
                    json.dump(vars_dict, fp, indent=4, sort_keys=True)

            with IRSTDFDManager(stdout=stdout, stderr=stderr) as fd_manager:

                if bool(strtobool(os.environ.get(
                        'IR_ANSIBLE_LOG_OUTPUT', 'no'))):
                    filename = filename_template.format(
                        timestamp=timestamp,
                        plugin_name=ir_plugin.name,
                        postfix='',
                        file_ext='log'
                    )
                    log_file = os.path.join(ansible_outputs_dir, filename)
                    fd_manager.add(open(log_file, 'w'))

                if bool(strtobool(os.environ.get(
                        'IR_ANSIBLE_LOG_OUTPUT_NO_ANSI', 'no'))):
                    filename = filename_template.format(
                        timestamp=timestamp,
                        plugin_name=ir_plugin.name,
                        postfix='_no_ansi',
                        file_ext='log'
                    )
                    log_file = os.path.join(ansible_outputs_dir, filename)
                    fd_manager.add(NoAnsiFile(open(log_file, 'w')))

                # Return the result:
                # 0: Success
                # 1: "Error"
                # 2: Host failed
                # 3: Unreachable
                # 4: Parser Error
                # 5: Options error

                # return cli.run()

        except (AnsibleParserError, AnsibleOptionsError) as error:
            LOG.error('{}: {}'.format(type(error), error))
            raise error


if __name__ == "__main__":
    args1 = ['execute', '/home/travis/builds/repos/redhat-openstack---infrared/tests/example/main.yml', '--inventory',
             '/tmp/pytest-of-travis/pytest-2/pmtest0/test_workspace/hosts']
    args2 = {'provision': {'foo': {'bar': 'default string'}, 'dictionary': {
        'val': {'option1': 'value1', 'option2': 'value2'}}}}
    # args3 = Workspace("example", "/home/travis/builds/repos/redhat-openstack---infrared/example")
    args3 = Workspace("example", "/home/travis/builds/repos/redhat-openstack---infrared/")

    args4 = "example"
    _run_playbook(args1, args2, args3, args4)
    output=['execute', '/home/travis/builds/repos/redhat-openstack---infrared/tests/example/main.yml', '--inventory', '/tmp/pytest-of-travis/pytest-2/pmtest0/test_workspace/hosts', '--extra-vars', '@/tmp/ir-settings-39c8d6bz', '--extra-vars', "{'infrared': {'python': {'executable': '/usr/local/bin/python', 'version': {'full': '3.10.12', 'major': 3, 'minor': 10, 'micro': 12}}}}"]
    for l in args1:
        if l.startswith("@"):
            args1.remove(l)
    for l in output:
        if l.startswith("@"):
            output.remove(l)
    isT=str(args1)==str(output)
    if not isT:
        raise Exception("Result not True!!!")

----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/inspector/inspector__convert_non_cli_args_passk_validte.py
import collections
import os
from six.moves import configparser
from string import Template
import yaml
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.cli.cli import CliParser
from infrared.core.cli.cli import COMPLEX_TYPES
from infrared.core.inspector import helper
from infrared.core.utils import dict_utils
from infrared.core.utils import exceptions
from infrared.core.utils import logger

LOG = logger.LOG


class SpecParser(object):
    """Parses input arguments from different sources (cli, answers file). """

    @classmethod
    def from_plugin(cls, subparser, plugin, base_groups):
        """Reads spec & vars from plugin and constructs the parser instance

        :param subparser: argparse.subparser to extend
        :param plugin: InfraredPlugin object
        :param base_groups: dict, included groups
        :return: SpecParser object based on given plugin spec & vars
        """

        spec_dict = base_groups or {}
        with open(plugin.spec) as stream:
            spec = yaml.safe_load(stream) or {}
            dict_utils.dict_merge(
                base_groups,
                spec,
                dict_utils.ConflictResolver.unique_append_list_resolver)

        # The "try-excpet" block here is for adding spec file path if it
        # includes an unsupported option type
        try:
            return SpecParser(subparser, spec_dict, plugin.vars_dir,
                              plugin.defaults_dir, plugin.path)
        except exceptions.IRUnsupportedSpecOptionType as ex:
            ex.message += ' in file: {}'.format(plugin.spec)
            raise ex

    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,
                 plugin_path):
        """Constructor.

        :param subparser: argparse.subparser to extend
        :param spec_dict: dict with CLI description
        :param vars_dir: Path to plugin's vars dir
        :param defaults_dir: Path to plugin's defaults dir
        """
        self.vars = vars_dir
        self.defaults = defaults_dir
        self.plugin_path = plugin_path
        self.spec_helper = helper.SpecDictHelper(spec_dict)

        # create parser
        self.parser = CliParser.create_parser(self, subparser)

    def add_shared_groups(self, list_of_groups):
        """Adds the user defined shared groups

        :param list_of_groups: list, of group dicts
        """
        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])
        shared_groups.expand(list_of_groups)
        self.spec_helper.spec_dict['shared_groups'] = shared_groups

    def _get_defaults(self, default_getter_func):
        """Resolve arguments' values from cli or answers file.

        :param default_getter_func: callable. will be called for all the
            available options in spec file.
        """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            default_value = default_getter_func(option)
            if default_value is not None:
                sub = parser['name']
                result[sub][option['name']] = default_value

        return result

    def get_spec_defaults(self):
        """Resolve arguments' values from spec and other sources. """

        def spec_default_getter(option):
            """Getter function to retrieve the default value from spec.

            :param option: argument name
            """

            # first try to get environment variable with IR_ prefix
            default_value = SpecParser.get_env_option(option['name'])
            if default_value is not None:
                LOG.info(
                    "[environ] Loading '{0}' default value"
                    " '{1}' from the environment variable".format(
                        option['name'], default_value))
            elif option.get('default', None) is not None:
                default_value = option['default']
            elif option.get('action', None) in ['store_true']:
                default_value = False
            return default_value

        return self._get_defaults(spec_default_getter)

    @staticmethod
    def get_env_option(name):
        """Try get """
        return os.environ.get('IR_' + name.upper().replace('-', '_'))

    def get_deprecated_args(self):
        """Returning dict with options which deprecate others. """

        result = collections.defaultdict(dict)
        for parser, option in self.spec_helper.iterate_option_specs():
            if option.get('deprecates') is not None:
                result[option.get('deprecates')] = option.get('name')

        return result

    @staticmethod
    def parse_env_variable_from_file(value):
        if isinstance(value, str):
            t = Template(value)
            try:
                value = t.substitute(os.environ)
            except KeyError as undefined_var:
                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)
        return value

    def get_answers_file_args(self, cli_args):
        """Resolve arguments' values from answers INI file. """

        file_result = {}
        args_to_remove = []
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            file_result[parser_name] = file_result.get(parser_name, {})
            if option_spec and option_spec.get(
                    'action', '') == 'read-answers':
                # Iterate over arguments supplied by file
                for parsed_arg in parser_dict[arg_name]:
                    # Supplied arguments' value can be a list
                    if isinstance(parser_dict[arg_name][parsed_arg], list):
                        i = 0
                        # Iterrate over argument values list
                        for parsed_value in parser_dict[arg_name][parsed_arg]:
                            parser_dict[arg_name][parsed_arg][i] = \
                                SpecParser.parse_env_variable_from_file(parsed_value)
                            i += 1
                    else:
                        parser_dict[arg_name][parsed_arg] = \
                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])
                # we have config option. saving it.
                self._convert_non_cli_args(
                    parser_name, parser_dict[arg_name])
                dict_utils.dict_merge(
                    file_result[parser_name],
                    parser_dict[arg_name])
                # remove from cli args
                args_to_remove.append((parser_name, arg_name))

        # remove parser dict outside loop to avoid iteration dict modification
        for parser_name, arg_name in args_to_remove:
            for spec_parser in self.spec_helper.iterate_parsers():
                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:
                    parser_dict = cli_args[spec_parser['name']]
                    parser_dict.pop(arg_name)
                    break

        return file_result

    def generate_answers_file(self, cli_args, spec_defaults):
        """Generates answers INI file

        :param cli_args: list, cli arguments.
        :param spec_defaults: the default values.
        """

        def put_option(config, parser_name, option_name, value):
            for opt_help in option.get('help', '').split('\n'):
                help_opt = '# ' + opt_help

                # add help comment
                if config.has_option(parser_name, help_opt):
                    config.remove_option(parser_name, help_opt)
                config.set(
                    parser_name, help_opt)

            if config.has_option(parser_name, option_name):
                value = config.get(parser_name, option_name)
                config.remove_option(parser_name, option_name)

            config.set(
                parser_name,
                option_name,
                str(value))

        file_generated = False

        # load generate answers file for all the parsers
        for (parser_name, parser_dict, arg_name, arg_value,
             option_spec) in self._iterate_received_arguments(cli_args):
            if option_spec and option_spec.get(
                    'action', '') == 'generate-answers':
                options_to_save = \
                    self.spec_helper.get_parser_option_specs(parser_name)
                out_answers = configparser.ConfigParser(allow_no_value=True)

                if not out_answers.has_section(parser_name):
                    out_answers.add_section(parser_name)

                for option in options_to_save:
                    opt_name = option['name']
                    if opt_name in parser_dict:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            parser_dict[opt_name])
                    elif opt_name in spec_defaults[parser_name]:
                        put_option(
                            out_answers,
                            parser_name,
                            opt_name,
                            spec_defaults[parser_name][opt_name])
                    elif option.get('required', False):
                        put_option(
                            out_answers,
                            parser_name,
                            '# ' + opt_name,
                            "Required argument. "
                            "Edit with one of the allowed values OR "
                            "override with "
                            "CLI: --{}=<option>".format(opt_name))

                # write to file
                with open(arg_value, 'w') as answers_file:
                    out_answers.write(answers_file)
                file_generated = True

        return file_generated

    def resolve_custom_types(self, args):
        """Transforms the arguments with custom types

        :param args: the list of received arguments.
        """
        for parser_name, parser_dict in args.items():
            spec_complex_options = [opt for opt in
                                    self.spec_helper.get_parser_option_specs(
                                        parser_name) if
                                    opt.get('type', None) in COMPLEX_TYPES]
            for spec_option in spec_complex_options:
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # we have custom type to resolve
                    type_name = spec_option['type']
                    option_value = parser_dict[option_name]
                    action = self.create_complex_argumet_type(
                        parser_name,
                        type_name,
                        option_name,
                        spec_option)

                    # resolving value
                    parser_dict[option_name] = action.resolve(option_value)

    def create_complex_argumet_type(self, subcommand, type_name, option_name,
                                    spec_option):
        """Build the complex argument type

        :param subcommand: the command name
        :param type_name: the complex type name
        :param option_name: the option name
        :param spec_option: option's specifications
        :return: the complex type instance
        """
        complex_action = COMPLEX_TYPES.get(
            type_name, None)
        if complex_action is None:
            raise exceptions.SpecParserException(
                "Unknown complex type: {}".format(type_name))
        return complex_action(
            option_name,
            (self.vars, self.defaults, self.plugin_path),
            subcommand,
            spec_option)

    def parse_args(self, arg_parser, args=None):
        """Parses all the arguments (cli, answers file)

        :return: None, if ``--generate-answers-file`` in arg_arg_parser
        :return: (dict, dict):
            * command arguments dict (arguments to control the IR logic)
            * nested arguments dict (arguments to pass to the playbooks)
        """

        spec_defaults = self.get_spec_defaults()
        cli_args = CliParser.parse_cli_input(arg_parser, args)

        file_args = self.get_answers_file_args(cli_args)

        # generate answers file and exit
        if self.generate_answers_file(cli_args, spec_defaults):
            LOG.warning("Answers file generated. Exiting.")

        # print warnings when something was overridden from non-cli source.
        self.validate_arg_sources(cli_args, file_args,
                                  spec_defaults)

        # print warnings for deprecated
        self.validate_arg_deprecation(cli_args, file_args)

        # now filter defaults to have only parser defined in cli
        defaults = dict((key, spec_defaults[key])
                        for key in cli_args.keys() if
                        key in spec_defaults)

        # copy cli args with the same name to all parser groups
        self._merge_duplicated_cli_args(cli_args)
        self._merge_duplicated_cli_args(file_args)

        dict_utils.dict_merge(defaults, file_args)
        dict_utils.dict_merge(defaults, cli_args)
        self.validate_requires_args(defaults)
        self.validate_length_args(defaults)
        self.validate_choices_args(defaults)
        self.validate_min_max_args(defaults)

        # now resolve complex types.
        self.resolve_custom_types(defaults)
        nested, control, custom = \
            self.get_nested_custom_and_control_args(defaults)
        return nested, control, custom

    def validate_arg_deprecation(self, cli_args, answer_file_args):
        """Validates and prints the deprecated arguments.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        """

        for deprecated, deprecates in self.get_deprecated_args().items():
            for input_args in (answer_file_args.items(), cli_args.items()):
                for command, command_dict in input_args:
                    if deprecated in command_dict:
                        if deprecates in command_dict:
                            raise exceptions.IRDeprecationException(
                                "[{}] Argument '{}' deprecates '{}',"
                                " please use only the new one.".format(
                                    command, deprecated, deprecates))

                        if deprecated in answer_file_args[command]:
                            answer_file_args[command][deprecates] = \
                                answer_file_args[command][deprecated]

                        if deprecated in cli_args[command]:
                            cli_args[command][deprecates] = \
                                cli_args[command][deprecated]

                        LOG.warning(
                            "[{}] Argument '{}' was deprecated,"
                            " please use '{}'.".format(
                                command, deprecated, deprecates))

    @staticmethod
    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):
        """Validates and prints the arguments' source.

        :param cli_args: the dict of arguments from cli
        :param answer_file_args:  the dict of arguments from files
        :param spec_defaults:  the default values from spec files
        """

        def show_diff(diff, command_name, cmd_dict, source_name):
            if diff:
                for arg_name in diff:
                    value = cmd_dict[arg_name]
                    LOG.info(
                        "[{}] Argument '{}' was set to"
                        " '{}' from the {} source.".format(
                            command_name, arg_name, value, source_name))

        for command, command_dict in cli_args.items():
            file_dict = answer_file_args.get(command, {})
            file_diff = set(file_dict.keys()) - set(command_dict.keys())
            show_diff(file_diff, command, file_dict, 'answers file')

            def_dict = spec_defaults.get(command, {})
            default_diff = set(def_dict.keys()) - set(
                command_dict.keys()) - file_diff
            show_diff(default_diff, command, def_dict, 'spec defaults')

    def _get_conditionally_required_args(self, command_name, options_spec,
                                         args):
        """List arguments with ``required_when`` condition matched.

        :param command_name: the command name.
        :param options_spec:  the list of command spec options.
        :param args: the received input arguments
        :return: list, list of argument names with matched ``required_when``
            condition
        """
        opts_names = [option_spec['name'] for option_spec in options_spec]
        missing_args = []
        for option_spec in options_spec:
            option_results = []
            if option_spec and 'required_when' in option_spec:
                req_when_args = [option_spec['required_when']] \
                    if not type(option_spec['required_when']) is list \
                    else option_spec['required_when']

                # validate conditions
                for req_when_arg in req_when_args:
                    splited_args_list = req_when_arg.split()
                    for idx, req_arg in enumerate(splited_args_list):
                        if req_arg in opts_names:
                            splited_args_list[idx] = \
                                args.get(command_name, {}).get(req_arg.strip())
                        if splited_args_list[idx] is None:
                            option_results.append(False)
                            break
                        splited_args_list[idx] = str(splited_args_list[idx])
                        if (splited_args_list[idx] not in ['and', 'or'] and
                            not any(
                                (c in '<>=') for c in splited_args_list[idx])):
                            splited_args_list[idx] = "'{0}'".format(
                                yaml.safe_load(splited_args_list[idx]))
                    else:
                        option_results.append(
                            eval(' '.join(splited_args_list)))
                if all(option_results) and \
                        self.spec_helper.get_option_state(
                            command_name,
                            option_spec['name'],
                            args) == helper.OptionState['NOT_SET']:
                    missing_args.append(option_spec['name'])
        return missing_args

    def validate_requires_args(self, args):
        """Check if all the required arguments have been provided. """

        silent_args = self.get_silent_args(args)

        def validate_parser(parser_name, expected_options, parser_args):
            """Helper method to resolve dict_merge. """

            result = collections.defaultdict(list)
            condition_req_args = self._get_conditionally_required_args(
                parser_name, expected_options, args)

            for option in expected_options:
                name = option['name']

                # check required options.
                if (option.get('required', False) and
                    name not in parser_args or
                    option['name'] in condition_req_args) and \
                        name not in silent_args:
                    result[parser_name].append(name)

            return result

        res = {}
        for command_data in self.spec_helper.iterate_parsers():
            cmd_name = command_data['name']
            if cmd_name in args:
                dict_utils.dict_merge(
                    res,
                    validate_parser(
                        cmd_name,
                        self.spec_helper.get_parser_option_specs(cmd_name),
                        args[cmd_name]))

        missing_args = dict((cmd_name, args)
                            for cmd_name, args in res.items() if len(args) > 0)
        if missing_args:
            raise exceptions.IRRequiredArgsMissingException(missing_args)

    def validate_length_args(self, args):
        """Check if value of arguments is not longer than length specified.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'length' not in spec_option:
                    # skip options that does not contain length
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve length
                    length = spec_option['length']
                    option_value = parser_dict[option_name]
                    if len(option_value) > int(length):
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            length
                        ))
        if invalid_options:
            # raise exception with all arguments that exceed length
            raise exceptions.IRInvalidLengthException(invalid_options)

    def validate_choices_args(self, args):
        """Check if value of choice arguments is one of the available choices.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if 'choices' not in spec_option:
                    # skip options that does not contain choices
                    continue
                option_name = spec_option['name']
                if option_name in parser_dict:
                    # resolve choices
                    choices = spec_option['choices']
                    option_value = parser_dict[option_name]
                    if option_value not in choices:
                        # found invalid option, append to list of invalid opts
                        invalid_options.append((
                            option_name,
                            option_value,
                            choices
                        ))
        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidChoiceException(invalid_options)

    def validate_min_max_args(self, args):
        """Check if value of arguments is between minimum and maximum values.

        :param args: The received arguments.
        """
        invalid_options = []
        for parser_name, parser_dict in args.items():
            for spec_option in \
                    self.spec_helper.get_parser_option_specs(parser_name):
                if all([key not in spec_option
                        for key in ('maximum', 'minimum')]):
                    # skip options that does not contain minimum or maximum
                    continue
                option_name = spec_option['name']

                if option_name in parser_dict:
                    option_value = parser_dict[option_name]
                    min_value = spec_option.get('minimum')
                    max_value = spec_option.get('maximum')
                    # handle empty values in spec files which load as None
                    min_value = '' if 'minimum' in spec_option \
                                      and min_value is None else min_value
                    max_value = '' if 'maximum' in spec_option \
                                      and max_value is None else max_value

                    values = {
                        "value": option_value,
                        "maximum": max_value,
                        "minimum": min_value
                    }

                    # make sure that values are numbers
                    is_all_values_numbers = True
                    for name, num in values.items():
                        if num is not None \
                                and (isinstance(num, bool) or
                                     not isinstance(num, (int, float))):
                            invalid_options.append((
                                option_name,
                                name,
                                "number",
                                type(num).__name__
                            ))
                            is_all_values_numbers = False

                    if not is_all_values_numbers:
                        # don't continue to min max checks since some of the
                        # values are not numbers
                        continue

                    # check bigger than minimum
                    if min_value is not None and option_value < min_value:
                        invalid_options.append((
                            option_name,
                            "minimum",
                            min_value,
                            option_value
                        ))
                    # check smaller than maximum
                    if max_value is not None and option_value > max_value:
                        invalid_options.append((
                            option_name,
                            "maximum",
                            max_value,
                            option_value
                        ))

        if invalid_options:
            # raise exception with all arguments that contains invalid choices
            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)

    def get_silent_args(self, args):
        """list of silenced argument

        :param args: The received arguments.
        :return: list, slienced argument names
        """
        silent_args_names = []
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if arg_spec and 'silent' in arg_spec and \
                    self.spec_helper.get_option_state(
                        parser_name,
                        arg_name,
                        args) == helper.OptionState['IS_SET']:
                silent_args_names.extend(arg_spec['silent'])

        return list(set(silent_args_names))

    def get_nested_custom_and_control_args(self, args):
        """Split input arguments to control nested and custom.

        Controls arguments: control the IR behavior. These arguments
            will not be put into the spec yml file
        Nested arguments: are used by the Ansible playbooks and will be put
            into the spec yml file.
        Custom arguments: Custom ansible variables to be used instead of the
            normal nested usage.

        :param args: the collected list of args.
        :return: (dict, dict): flat dicts (control_args, nested_args)
        """
        # returns flat dicts
        nested = {}
        control_args = {}
        custom_args = {}
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(args):
            if all([arg_spec, arg_spec.get('type', None),
                    arg_spec.get('type', None) in
                    [ctype_name for ctype_name, klass in
                     COMPLEX_TYPES.items() if klass.is_nested]
                    ]) or ('is_shared_group_option' not in arg_spec):
                if arg_name in nested:
                    LOG.warning(
                        "Duplicated nested argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, nested[arg_name]))
                elif arg_name in custom_args:
                    LOG.warning(
                        "Duplicated custom argument found:'{}'. "
                        "Using old value: '{}'".format(
                            arg_name, custom_args[arg_name]))
                else:
                    if "ansible_variable" in arg_spec:
                        custom_args[arg_spec["ansible_variable"]] = arg_value
                    else:
                        nested[arg_name] = arg_value
            else:
                if arg_name in control_args:
                    LOG.warning(
                        "Duplicated control argument found: '{}'. Using "
                        "old value: '{}'".format(
                            arg_name, control_args[arg_name]))
                else:
                    control_args[arg_name] = arg_value

        return nested, control_args, custom_args

    def _iterate_received_arguments(self, args):
        """Iterator helper method over all the received arguments

        :return: yields tuple:
            (spec name, spec dict,
             argument name, argument value, argument spec)
        """
        for spec_parser in self.spec_helper.iterate_parsers():
            if spec_parser['name'] in args:
                parser_dict = args[spec_parser['name']]
                for arg_name, arg_val in parser_dict.items():
                    arg_spec = self.spec_helper.get_option_spec(
                        spec_parser['name'], arg_name)
                    yield (spec_parser['name'], parser_dict,
                           arg_name, arg_val, arg_spec)

    def _convert_non_cli_args(self, parser_name, values_dict):
        """Casts arguments to correct types by modifying values_dict param.

        By default all the values are strings.

        :param parser_name: The command name, e.g. main, virsh, ospd, etc
        :param values_dict: The dict of with arguments
       """
        for opt_name, opt_value in values_dict.items():
            file_option_spec = self.spec_helper.get_option_spec(
                parser_name, opt_name)
            if file_option_spec.get('type', None) in ['int', ] or \
                    file_option_spec.get('action', None) in ['count', ]:
                values_dict[opt_name] = int(opt_value)

    def _merge_duplicated_cli_args(self, cli_args):
        """Merge duplicated arguments to all the parsers

        This is need to handle control args, shared among several parsers.
        for example, verbose, inventory
        """
        for (parser_name, parser_dict, arg_name, arg_value,
             arg_spec) in self._iterate_received_arguments(cli_args):
            for parser_name2, parser_dict2 in cli_args.items():
                if all([parser_name2, parser_name != parser_name2,
                        arg_name not in parser_dict2]):
                    if self.spec_helper.get_option_spec(parser_name2,
                                                        arg_name):
                        parser_dict2[arg_name] = arg_value

if __name__ == "__main__":
    isT = True
    try:
        temp_class = SpecParser("", {'foo-bar': 'from_answers_file','iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3','mock_key': 'expected_value'}, "", "", "")
        temp_class.spec_helper.spec_dict = {'subparsers': {"foo-bar": {"one":1,"tow":2,"groups":[{"options":{"foo-bar":{"type":"int"},"key2":{"value2":2},"key3":{"value3":3}}},{"options":{"foo-bar":{"type":"int"},"key22":{"value22":22},"key33":{"value33":33}}}]}}}
        input1= {'foo-bar': "111"}
        input2 = {'iniopt': 'sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3'}
        temp_class._convert_non_cli_args("foo-bar", input1)
        temp_class._convert_non_cli_args("example", input2)
        if not isinstance(input1["foo-bar"],int):
            isT=False
        if input2["iniopt"]!='sec1.opt1=f_val1,sec1.opt2=f_val2,sec2.opt1=f_val3':
            isT=False
    except:
        isT=False
    # for l in os.listdir(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos/redhat-openstack---infrared/data_passk_platform1/63060ada73426c38ae68ad31/"):
    #     f = open(
    #         "C:\\Users\yh199\Downloads\\repos1\\repos/redhat-openstack---infrared/data_passk_platform1/63060ada73426c38ae68ad31/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     # f1 = open(
    #     #     "C:\\Users\yh199\Downloads\\repos1\\repos/redhat-openstack---infrared/data_passk_platform/63060ada73426c38ae68ad31/" + l,
    #     #     "rb")
    #     # content1 = dill.load(f1)
    #     # f1.close()
    #     if isinstance(content["input"]["args"][1]["bytes"], bytes):
    #         args1 = dill.loads(content["input"]["args"][1]["bytes"])
    #     else:
    #         args1 = content["input"]["args"][1]["bytes"]
    #     if isinstance(content["input"]["args"][2]["bytes"], bytes):
    #         args2 = dill.loads(content["input"]["args"][2]["bytes"])
    #     else:
    #         args2 = content["input"]["args"][2]["bytes"]
    #     print(args1,args2)
    #     # object_class = dill.loads(content1["input"]["args"][0]["bytes"])
    #     # temp_class = SpecParser("",{},"","","")
    #     # temp_class.__dict__.update(object_class)
    #     # print(temp_class.vars,temp_class.spec_helper,temp_class.plugin_path)
    #     # res0 = temp_class._convert_non_cli_args(args1, args2)
    #     # # print(res0)
    #     # # print(content["output"][0])
    #     # if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #     #     isT=False
    #     #     break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/tests/test_plugins_get_plugin_spec_flatten_dict_passk_validte.py
from six.moves import configparser
import os
import git
import yaml
import shutil
import sys
import tarfile
import tempfile
import filecmp

import pytest
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.utils.exceptions import IRPluginExistsException, \
    IRUnsupportedPluginType
from infrared.core.utils.exceptions import IRFailedToAddPlugin
from infrared.core.utils.exceptions import IRValidatorException
from infrared.core.utils.exceptions import IRFailedToRemovePlugin
from infrared.core.utils.exceptions import IRFailedToUpdatePlugin
from infrared.core.utils.exceptions import IRUnsupportedSpecOptionType
from infrared.core.utils.dict_utils import dict_insert
import infrared.core.services.plugins
from infrared.core.services.plugins import InfraredPluginManager
from infrared.core.services.plugins import InfraredPlugin
from infrared.core.utils.validators import SpecValidator, RegistryValidator
from infrared.core.services import CoreServices, ServiceName


PLUGIN_SPEC = 'plugin.spec'
SAMPLE_PLUGINS_DIR = 'tests/example/plugins'

SUPPORTED_TYPES_DICT = dict(
    supported_types=dict(
        supported_type1='Tools of supported_type1',
        supported_type2='Tools of supported_type2',
        provision='Provisioning plugins',
        install='Installing plugins',
        test='Testing plugins'
    )
)


@pytest.fixture()
def plugins_conf_fixture(tmpdir):
    """Creates temporary IR

    :param tmpdir: builtin pytest fixtures to create temporary files & dirs
    :return: plugins conf file as a LocalPath object (py.path)
    """

    # Creates temporary plugins conf file
    lp_dir = tmpdir.mkdir('test_tmp_dir')
    lp_file = lp_dir.join('.plugins.ini')

    try:
        yield lp_file
    finally:
        lp_dir.remove()


@pytest.fixture()
def plugin_manager_fixture(plugins_conf_fixture):
    """Creates a PluginManager fixture

    Creates a fixture which returns a PluginManager object based on
    temporary plugins conf with default values(sections - provision, install &
    test)
    :param plugins_conf_fixture: fixture that returns a path of a temporary
    plugins conf
    """

    lp_file = plugins_conf_fixture

    def plugin_manager_helper(plugins_conf_dict=None):

        if plugins_conf_dict is None:
            plugins_conf_dict = {}

        plugins_conf_dict.update(SUPPORTED_TYPES_DICT)

        with lp_file.open(mode='w') as fp:
            config = configparser.ConfigParser()
            for section, section_data in plugins_conf_dict.items():
                config.add_section(section)
                for option, value in section_data.items():
                    config.set(section, option, value)
            config.write(fp)

        CoreServices.register_service(
            ServiceName.PLUGINS_MANAGER, InfraredPluginManager(
                lp_file.strpath,
                os.path.join(lp_file.dirname, "plugins")))
        return CoreServices.plugins_manager()

    yield plugin_manager_helper


@pytest.fixture()
def git_plugin_manager_fixture(tmpdir, plugin_manager_fixture):
    """Yields an IRPluginManager obj configured with git plugin

    Just like plugin_manager_fixture but also create two temporary directories
    that will be used to mimic local and remote git repos of an InfraRed's
    plugin. The IRPluginManager that will be returned, will be configured with
    this InfraRed git plugin.
    :param tmpdir: builtin pytest fixtures to create temporary files & dirs
    :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    """
    plugin_tar_gz = os.path.join(
        os.path.dirname(os.path.realpath(__file__)),
        'example/plugins/git_plugin/git_plugin_repo.tar.gz')

    plugin_repo_dir = tmpdir.mkdir('plugin_repo_dir')
    plugin_install_dir = tmpdir.mkdir('plugin_install_dir')

    t_file = tarfile.open(plugin_tar_gz)
    t_file.extractall(path=str(plugin_repo_dir))

    repo = git.Repo.clone_from(
        url=str(plugin_repo_dir),
        to_path=str(plugin_install_dir))

    repo.git.config('user.name', 'dummy-user')
    repo.git.config('user.email', 'dummy@email.com')

    plugin_spec_dict = get_plugin_spec_flatten_dict(str(plugin_install_dir))

    try:
        plugin_manager = plugin_manager_fixture({
            plugin_spec_dict['type']: {
                plugin_spec_dict['name']: str(plugin_install_dir)}
        })
        yield plugin_manager
    finally:
        plugin_repo_dir.remove()
        plugin_install_dir.remove()


def get_plugin_spec_flatten_dict(plugin_dir):
    """Creates a flat dict from the plugin spec

    :param plugin_dir: A path to the plugin's dir
    :return: A flatten dictionary contains the plugin's properties
    """
    with open(os.path.join(plugin_dir, PLUGIN_SPEC)) as fp:
        spec_yaml = yaml.safe_load(fp)

    plugin_name = list(spec_yaml['subparsers'].keys())[0]

    plugin_description = spec_yaml['description'] \
        if "description" in spec_yaml \
        else spec_yaml['subparsers'][plugin_name]['description']

    plugin_type = spec_yaml["config"]["plugin_type"] \
        if "config" in spec_yaml \
        else spec_yaml["plugin_type"]

    plugin_spec_dict = dict(
        name=plugin_name,
        dir=plugin_dir,
        description=plugin_description,
        type=plugin_type
    )

    return plugin_spec_dict


def plugin_in_conf(plugins_conf, plugin_type, plugin_name):
    """Checks if a plugin exists in a conf file

    :param plugins_conf: A path to the plugins conf file
    :param plugin_type: The plugin's type
    :param plugin_name: The Plugin's name
    :return: True if plugin is in the conf file, otherwise False
    """
    config = configparser.ConfigParser()
    with open(plugins_conf) as fp:
        if (sys.version_info > (3, 2)):
            config.read_file(fp)
        else:
            config.readfp(fp)

    return config.has_option(plugin_type, plugin_name)


def test_add_plugin(plugin_manager_fixture):
    """Tests the ability to add plugins

    :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    """

    plugin_manager = plugin_manager_fixture()

    for plugin_dir, plugins_cnt in (
            ('type1_plugin1', 1),   # Add a plugin
            ('type1_plugin2', 2),   # Add a plugin - same type
            ('type2_plugin1', 3)):  # Add a plugin - different type

        plugin_dict = get_plugin_spec_flatten_dict(
            os.path.join(SAMPLE_PLUGINS_DIR, plugin_dir))

        plugin_manager.add_plugin(plugin_dict['dir'])

        assert plugin_dict['name'] in plugin_manager.PLUGINS_DICT,\
            "Plugin wasn't added to the plugins manager."
        assert plugin_in_conf(
            plugins_conf=plugin_manager.config_file,
            plugin_type=plugin_dict['type'],
            plugin_name=plugin_dict['name']), \
            "Plugin wasn't added to conf file."
        assert len(plugin_manager.PLUGINS_DICT) == plugins_cnt


def test_load_plugin(plugin_manager_fixture):
    """Test that an existing plugin can be loaded and it's properties

    :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    """

    plugin_dir = 'type1_plugin1'
    plugin_dict = get_plugin_spec_flatten_dict(
        os.path.join(os.path.abspath(SAMPLE_PLUGINS_DIR), plugin_dir))

    plugin_manager = plugin_manager_fixture({
        plugin_dict['type']: {
            plugin_dict['name']: plugin_dict['dir']}
    })

    plugin = plugin_manager.get_plugin(plugin_name=plugin_dict['name'])

    assert type(plugin) is InfraredPlugin, "Failed to add a plugin"
    assert plugin.name == plugin_dict['name'], "Wrong plugin name"
    assert plugin.description == plugin_dict['description'], \
        'Wrong plugin description'


def test_entry_point(plugin_manager_fixture):
    """Test that spec file has a valid entry point
     :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    """

    plugin_dir = 'plugin_with_entry_point'
    plugin_dict = get_plugin_spec_flatten_dict(
        os.path.join(os.path.abspath(SAMPLE_PLUGINS_DIR), plugin_dir))

    plugin_manager = plugin_manager_fixture({
        plugin_dict['type']: {
            plugin_dict['name']: plugin_dict['dir']}
    })

    plugin = plugin_manager.get_plugin(plugin_name=plugin_dict['name'])
    assert plugin.playbook == os.path.join(plugin_dict['dir'], "example.yml")


def test_add_plugin_with_same_name(plugin_manager_fixture):
    """Tests that it not possible to add a plugin with a name that already
    exists

    :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    """

    plugin_dir = 'type1_plugin1'
    plugin_dict = get_plugin_spec_flatten_dict(
        os.path.join(SAMPLE_PLUGINS_DIR, plugin_dir))

    plugin_manager = plugin_manager_fixture({
        plugin_dict['type']: {
            plugin_dict['name']: plugin_dict['dir']}
    })

    plugins_cfg_mtime_before_add = os.path.getmtime(plugin_manager.config_file)
    plugins_cnt_before_try = len(plugin_manager.PLUGINS_DICT)

    with pytest.raises(IRPluginExistsException):
        plugin_manager.add_plugin(plugin_dict['dir'])

    assert plugins_cnt_before_try == len(plugin_manager.PLUGINS_DICT)
    assert os.path.getmtime(
        plugin_manager.config_file) == plugins_cfg_mtime_before_add, \
        "Plugins configuration file has been modified."


def test_add_plugin_unsupported_type(plugin_manager_fixture):
    """Test that it's not possible to add a plugin from unsupported type

    :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    """

    plugin_manager = plugin_manager_fixture()

    plugin_dict = get_plugin_spec_flatten_dict(
        os.path.join(SAMPLE_PLUGINS_DIR, 'unsupported_plugin'))

    plugins_cfg_mtime_before_add = os.path.getmtime(plugin_manager.config_file)
    plugins_cnt_before_try = len(plugin_manager.PLUGINS_DICT)

    with pytest.raises(IRUnsupportedPluginType):
        plugin_manager.add_plugin(plugin_dict['dir'])

    assert not plugin_in_conf(
        plugins_conf=plugin_manager.config_file,
        plugin_type=plugin_dict['type'],
        plugin_name=plugin_dict['name']), \
        "Plugin was added to conf file."
    assert plugins_cnt_before_try == len(plugin_manager.PLUGINS_DICT)
    assert os.path.getmtime(
        plugin_manager.config_file) == plugins_cfg_mtime_before_add, \
        "Plugins configuration file has been modified."


def test_remove_plugin(plugin_manager_fixture):
    """ Tests the ability to remove a plugin

    :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    """

    plugins_conf = {}
    for plugin_dir in ('type1_plugin1', 'type1_plugin2', 'type2_plugin1'):
        plugin_dict = get_plugin_spec_flatten_dict(
            os.path.join(os.path.abspath(SAMPLE_PLUGINS_DIR), plugin_dir))
        dict_insert(plugins_conf,
                    plugin_dict['dir'],
                    plugin_dict['type'],
                    plugin_dict['name'],)

    plugin_manager = plugin_manager_fixture(plugins_conf)

    for plugin_dir, plugins_cnt in (
            ('type1_plugin1', 2),
            ('type2_plugin1', 1),
            ('type1_plugin2', 0)):
        plugin_dict = get_plugin_spec_flatten_dict(
            os.path.join(SAMPLE_PLUGINS_DIR, plugin_dir))

        assert plugin_dict['name'] in plugin_manager.PLUGINS_DICT, \
            "Can't remove unexisting plugin"

        plugin_manager.remove_plugin(plugin_dict['name'])

        with pytest.raises(KeyError):
            plugin_manager.get_plugin(plugin_name=plugin_dict['name'])

        assert not plugin_in_conf(
            plugins_conf=plugin_manager.config_file,
            plugin_type=plugin_dict['type'],
            plugin_name=plugin_dict['name']), \
            "Plugin wasn't removed from conf file."
        assert len(plugin_manager.PLUGINS_DICT) == plugins_cnt


def test_remove_unexisting_plugin(plugin_manager_fixture):
    """Tests the behavior of removing unexisting plugin

    Checks that no exception is being raised and no changes in
    InfraredPluginManager dict and configuration file
    :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    """

    plugin_manager = plugin_manager_fixture()

    plugins_cfg_mtime_before_add = os.path.getmtime(plugin_manager.config_file)
    plugins_cnt_before_try = len(plugin_manager.PLUGINS_DICT)

    with pytest.raises(IRFailedToRemovePlugin):
        plugin_manager.remove_plugin('unexisting_plugin')

    assert plugins_cnt_before_try == len(plugin_manager.PLUGINS_DICT)
    assert os.path.getmtime(
        plugin_manager.config_file) == plugins_cfg_mtime_before_add, \
        "Plugins configuration file has been modified."


@pytest.mark.parametrize("input_args, plugins_conf", [
    ("plugin list", None),
    ("plugin add tests/example/plugins/type1_plugin1", None),
    ("plugin remove type1_plugin1", dict(
        supported_type1=dict(
            type1_plugin1='tests/example/plugins/type1_plugin1'))),
    ("plugin add "
     "tests/example/plugins/type1_plugin1 "
     "tests/example/plugins/type1_plugin2", None),
    ("plugin remove type1_plugin1 type1_plugin2", dict(
            supported_type1=dict(
                type1_plugin1='tests/example/plugins/type1_plugin1',
                type1_plugin2='tests/example/plugins/type1_plugin2'))),
])
def test_plugin_cli(plugin_manager_fixture, input_args, plugins_conf):
    """Tests that plugin CLI works

    :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    :param input_args: infrared's testing arguments
    :param plugins_conf: Plugins conf data as a dictionary
    """
    plugin_manager_fixture(plugins_conf)

    from infrared.main import main as ir_main
    rc = ir_main(input_args.split())

    assert rc == 0, \
        "Return code ({}) != 0, cmd='infrared {}'".format(rc, input_args)


def test_add_plugin_no_spec(plugin_manager_fixture):
    """Tests that it's not possible to add plugin without a spec file

    :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    """

    plugin_dir = os.path.join(SAMPLE_PLUGINS_DIR, 'plugin_without_spec')

    plugin_manager = plugin_manager_fixture({})

    plugins_cfg_mtime_before_add = os.path.getmtime(plugin_manager.config_file)
    plugins_cnt_before_try = len(plugin_manager.PLUGINS_DICT)

    with pytest.raises(IRValidatorException):
        plugin_manager.add_plugin(plugin_dir)

    assert plugins_cnt_before_try == len(plugin_manager.PLUGINS_DICT)
    assert os.path.getmtime(
        plugin_manager.config_file) == plugins_cfg_mtime_before_add, \
        "Plugins configuration file has been modified."


@pytest.mark.parametrize("description, plugin_spec", [
    ('no_description', {
        'plugin_type': 'supported_type',
        'subparsers': {
            'sample_plugin1:': {}}}),
    ('no_type', {
        'description': 'some plugin description',
        'subparsers': {
            'sample_plugin1:': {}}}),
    ('no_value', {
        'plugin_type': '',
        'subparsers': {
            'sample_plugin1:': {}}}),
    ('no_subparsers_key', {
        'plugin_type': 'supported_type',
        'description': 'some plugin description'}),
    ('no_subparsers_value', {
        'plugin_type': 'supported_type',
        'description': 'some plugin description',
        'subparsers': ''}),
    ('no_entry_point_value',{
        'plugin_type': 'supported_type',
        'entry_point': '',
        'subparsers': {
            'sample_plugin1:': {}}}),
    ('no_entry_point_value_in_config',{
        'config': {
            "plugin_type": 'supported_type',
            "entry_point": '',
        },
        'subparsers': {
            'sample_plugin1:': {}}}),
    ('no_type_in_config', {
        'config': {
        },
        'description': 'some plugin description',
        'subparsers': {
            'sample_plugin1:': {}}}),
])
def test_add_plugin_corrupted_spec(tmpdir_factory, description, plugin_spec):
    """Tests that it's not possible to add a plugin with invalid spec file

    :param tmpdir_factory: pytest builtin fixture for creating temp dirs
    :param description: test description (adds a description in pytest run)
    :param plugin_spec: dictionary with data for spec file
    :return:
    """

    lp_dir = tmpdir_factory.mktemp('test_tmp_dir')
    lp_file = lp_dir.join('plugin.spec')

    with open(lp_file.strpath, 'w') as fp:
        yaml.dump(plugin_spec, fp, default_flow_style=True)

    try:
        with pytest.raises(IRValidatorException):
            SpecValidator.validate_from_file(lp_file.strpath)
    finally:
        lp_dir.remove()


def test_plugin_with_unsupporetd_option_type_in_spec(plugin_manager_fixture):
    """Tests that the user get a proper error

    :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    """
    plugin_dir = os.path.join(SAMPLE_PLUGINS_DIR,
                              'plugin_with_unsupported_option_type_in_spec')
    plugin_dict = get_plugin_spec_flatten_dict(plugin_dir)

    plugin_manager = plugin_manager_fixture()
    plugin_manager.add_plugin(plugin_dir)

    from infrared.main import main as ir_main
    with pytest.raises(IRUnsupportedSpecOptionType):
        ir_main([plugin_dict['name'], '--help'])


def test_add_plugin_from_git(plugin_manager_fixture, mocker):

    plugin_manager = plugin_manager_fixture()

    mock_git = mocker.patch("infrared.core.services.plugins.git.Repo")
    mock_os = mocker.patch("infrared.core.services.plugins.os")
    mock_os.path.exists.return_value = False
    mock_os.listdir.return_value = ["sample_plugin"]
    mock_tempfile = mocker.patch("infrared.core.services.plugins.tempfile")
    mock_shutil = mocker.patch("infrared.core.services.plugins.shutil")

    plugin_dict = get_plugin_spec_flatten_dict(
        os.path.join(SAMPLE_PLUGINS_DIR, 'type1_plugin1'))
    mock_os.path.join.return_value = os.path.join(plugin_dict["dir"],
                                                  PLUGIN_SPEC)

    # add_plugin call
    plugin_manager.add_plugin(
        "https://sample_github.null/plugin_repo.git", rev="test",
        skip_roles=True)

    mock_tempfile.mkdtemp.assert_called_once()
    mock_git.clone_from.assert_called_with(
        url='https://sample_github.null/plugin_repo.git',
        to_path=mock_os.path.join.return_value,
        kill_after_timeout=300)
    mock_os.join.has_call(SAMPLE_PLUGINS_DIR, mock_os.listdir.return_value[0])
    mock_os.join.has_call(mock_tempfile.mkdtemp.return_value,
                          mock_os.listdir.return_value[0])
    mock_shutil.rmtree.assert_called_with(mock_os.path.join.return_value)


def test_add_plugin_from_git_dirname_from_spec(plugin_manager_fixture, mocker):
    """
    Validate that we take the folder name from the spec plugin name
    instead of the git repo name
    :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    :param mocker: mocker fixture
    """

    def clone_from_side_effect(url, to_path, **kwargs):
        """
        Define a side effect function to override the
        original behaviour of clone_from
        """
        shutil.copytree(src=plugin_dict["dir"], dst=to_path)

    plugin_manager = plugin_manager_fixture()

    mock_git = mocker.patch("infrared.core.services.plugins.git.Repo")
    # use side effect to use copytree instead of original clone
    mock_git.clone_from.side_effect = clone_from_side_effect
    mock_os_path_exists = mocker.patch(
        "infrared.core.services.plugins.os.path.exists")
    # set to false in order to enter the git section
    # in if/else inside add_plugin func
    mock_os_path_exists.return_value = False
    mock_tempfile = mocker.patch("infrared.core.services.plugins.tempfile")
    mock_tempfile.mkdtemp.return_value = tempfile.mkdtemp(prefix="ir-")
    mock_shutil = mocker.patch("infrared.core.services.plugins.shutil")

    plugin_dict = get_plugin_spec_flatten_dict(
        os.path.abspath(os.path.join(SAMPLE_PLUGINS_DIR, 'type1_plugin1')))

    # add_plugin call
    with pytest.raises(IRFailedToAddPlugin):
        plugin_manager.add_plugin(
            "https://sample_github.null/plugin_repo.git")

    mock_shutil.rmtree.assert_called_with(os.path.join(
            mock_tempfile.mkdtemp.return_value, "plugin_repo"))
    # clean tmp folder
    shutil.rmtree(mock_tempfile.mkdtemp.return_value)

    # check it was cloned with the temp name
    mock_git.clone_from.assert_called_with(
        url='https://sample_github.null/plugin_repo.git',
        to_path=os.path.join(
            mock_tempfile.mkdtemp.return_value, "plugin_repo"),
        kill_after_timeout=300)

    # check that it was copied with the plugin name and not repo name
    mock_shutil.copytree.assert_called_with(
        os.path.join(mock_tempfile.mkdtemp.return_value, "plugin_repo"),
        os.path.join(plugin_manager.plugins_dir, plugin_dict["name"]))


def test_add_plugin_from_git_exception(plugin_manager_fixture, mocker):

    plugin_manager = plugin_manager_fixture()

    mock_git = mocker.patch("infrared.core.services.plugins.git")
    mock_git.Repo.clone_from.side_effect = git.exc.GitCommandError(
        "some_git_cmd", 1)
    mock_git.exc.GitCommandError = git.exc.GitCommandError
    mock_tempfile = mocker.patch("infrared.core.services.plugins.tempfile")
    mock_shutil = mocker.patch("infrared.core.services.plugins.shutil")
    mock_os = mocker.patch("infrared.core.services.plugins.os")
    mock_os.path.exists.return_value = False

    # add_plugin call
    with pytest.raises(IRFailedToAddPlugin):
        plugin_manager.add_plugin(
            "https://sample_github.null/plugin_repo.git")

    mock_shutil.rmtree.assert_called_with(mock_tempfile.mkdtemp.return_value)


def validate_plugins_presence_in_conf(
        plugin_manager, plugins_dict, present=True):
    """Validate presence of plugins in the configuration file

    :param plugin_manager: InfraredPluginManager object
    :param plugins_dict:  Dict of plugins
    {plugin_name: plugin_dir_path, ...}
    :param present: Whether all plugins in the dict should be present in the
    plugins configuration file or not.
    """
    assert present in (True, False), \
        "'absent' accept only Boolean values, got: '{}'".format(str(present))

    with open(plugin_manager.config_file) as config_file:
        plugins_cfg = configparser.ConfigParser()
        if (sys.version_info > (3, 2)):
            plugins_cfg.read_file(config_file)
        else:
            plugins_cfg.readfp(config_file)

        for plugin_path in plugins_dict.values():
            plugin = InfraredPlugin(plugin_path['src'])

            if present:
                assert plugins_cfg.has_option(plugin.type, plugin.name), \
                    "Plugin '{}' was suppose to be in the plugins " \
                    "configuration file".format(plugin.name)
            else:
                assert not plugins_cfg.has_option(plugin.type, plugin.name), \
                    "Plugin '{}' wasn't suppose to be in the plugins " \
                    "configuration file".format(plugin.name)


def test_plugin_add_all(plugin_manager_fixture):
    """Tests the add and remove all plugins functioning

    :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    """
    tests_plugins = (
        'provision_plugin1', 'provision_plugin2',
        'install_plugin1', 'install_plugin2',
        'test_plugin1', 'test_plugin2'
    )
    tests_plugins_dir = 'tests/example/plugins/add_remove_all_plugins/'

    plugins_registry = \
        dict((pname, {'src': os.path.join(tests_plugins_dir, pname)})
             for pname in tests_plugins)

    plugin_manager = plugin_manager_fixture()

    # Validates that plugins aren't in configuration file from the beginning
    validate_plugins_presence_in_conf(
        plugin_manager, plugins_registry, present=False)

    # Validates all plugins are in the configuration file
    plugin_manager.add_all_available(plugins_registry=plugins_registry)
    validate_plugins_presence_in_conf(
        plugin_manager, plugins_registry, present=True)

    # Validates all plugins are no longer in the configuration file
    plugin_manager.remove_all()
    validate_plugins_presence_in_conf(
        plugin_manager, plugins_registry, present=False)


def test_git_plugin_update(git_plugin_manager_fixture):
    """Tests the git plugin update functionality

    Tests the following:
      1. Plugin update without new changes
      2. Plugin update to an older commit
      3. No update when there are local changes
      4. Switch back to master after checking out old revision
      5. Switch to revision that does not exists
    :param git_plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object with git plugin installed
    """
    gpm = git_plugin_manager_fixture

    repo = git.Repo(gpm.get_plugin('git_plugin').path)
    commits_list = repo.git.rev_list('HEAD').splitlines()

    assert len(commits_list) > 1, \
        "Can perform the test without at least two commits"

    # Plugin update without new changes
    assert gpm.update_plugin('git_plugin') is None, \
        "Failed to pull changes from remote with up-to-date local branch"

    # Plugin update to an older commit
    gpm.update_plugin(plugin_name='git_plugin', revision=commits_list[-1])
    assert commits_list[-1] == repo.git.rev_parse('HEAD'), \
        "Failed to Update plugin to: {}".format(commits_list[-1])

    # No update when there are local changes
    file_name = os.path.join(repo.working_dir, 'test.txt')
    # create new file and add it to git to create local changes
    with open(file_name, 'w') as f:
        f.write('test')
    repo.git.add([file_name])
    with pytest.raises(IRFailedToUpdatePlugin):
        gpm.update_plugin(plugin_name='git_plugin')
    assert commits_list[-1] == repo.git.rev_parse('HEAD'), \
        "Plugin wasn't supposed to be changed when update failed..."

    # Switch back to master after checking out old revision
    gpm.update_plugin(plugin_name='git_plugin',
                      revision='master', hard_reset=True)
    assert commits_list[0] == repo.git.rev_parse('HEAD'), \
        "Plugin haven't been updated from '{}' to '{}'".format(
            commits_list[-1], commits_list[0])

    # Switch to revision that does not exists
    branch_before = repo.active_branch
    with pytest.raises(IRFailedToUpdatePlugin):
        gpm.update_plugin(plugin_name='git_plugin', revision='not_exists_rev')
    assert branch_before == repo.active_branch, \
        "Plugin's revision wasn't supposed to change"


@pytest.mark.parametrize("description, registry_yaml", [
    ('no_type', {
        'some_plugin_name': {
            'src': '/path/to/plugin',
            'desc': 'some plugin description'
        }
    }),
    ('no_desc', {
        'some_plugin_name': {
            'src': '/path/to/plugin',
            'type': 'supported_type'
        }
    }),
    ('no_src', {
        'some_plugin_name': {
            'desc': 'some plugin description',
            'type': 'supported_type'
        }
    }),
    ('empty_revision', {
        'some_plugin_name': {
            'src': '/path/to/plugin',
            'type': 'supported_type',
            'desc': 'some plugin description',
            'rev': ''
        }
    }),
    ('empty_src_path', {
        'some_plugin_name': {
            'src': '/path/to/plugin',
            'type': 'supported_type',
            'desc': 'some plugin description',
            'src_path': ''
        }
    }),
    ('empty_plugin_key', {
        '': {
            'src': '/path/to/plugin',
            'type': 'supported_type',
            'desc': 'some plugin description',
            'src_path': ''
        }
    }),
    ('additional_not_allowed_param', {
        '': {
            'src': '/path/to/plugin',
            'type': 'supported_type',
            'desc': 'some plugin description',
            'src_path': '/relative/path',
            'rev': 'some_rev',
            'not_allowed_additional_key': 'some_value'
        }
    }),
])
def test_import_plugins_corrupted_registry(tmpdir_factory, description,
                                           registry_yaml):
    """
    Tests that it's not possible to import plugins with invalid registry file
    :param tmpdir_factory: pytest builtin fixture for creating temp dirs
    :param description: test description (adds a description in pytest run)
    :param registry_yaml: dictionary with data for registry file
    :return:
    """

    lp_dir = tmpdir_factory.mktemp('test_tmp_dir')
    lp_file = lp_dir.join('registry.yaml')

    with open(lp_file.strpath, 'w') as fp:
        yaml.dump(registry_yaml, fp, default_flow_style=True)

    try:
        with pytest.raises(IRValidatorException):
            RegistryValidator.validate_from_file(lp_file.strpath)
    finally:
        lp_dir.remove()


def test_import_plugins_from_registry(tmpdir, plugin_manager_fixture):
    """
    Test that plugins import actually imports the plugins specified in the
    registry file supplied
    :param tmpdir: pytest builtin fixture for creating temp dirs
    :param plugin_manager_fixture: Fixture object which yields
    """
    plugin_manager = plugin_manager_fixture()
    plugins_registry = os.path.join(SAMPLE_PLUGINS_DIR, "registry_example.yml")

    with open(plugins_registry) as fp:
        registry_yaml = yaml.safe_load(fp)

    # prepare tmp library folder to hold the dependencies
    tmp_pluginss_dir = str(tmpdir.mkdir("tmp_pluginss_dir"))
    plugin_manager.plugins_dir = tmp_pluginss_dir

    # Validates that plugins aren't in configuration file from the beginning
    validate_plugins_presence_in_conf(
        plugin_manager, registry_yaml, present=False)

    # import all plugins from registry
    plugin_manager.import_plugins(plugins_registry)

    # check that plugins were copied to the plugins directory
    assert os.path.isdir(os.path.join(
        tmp_pluginss_dir, 'type1_plugin1'))
    assert os.path.isdir(os.path.join(
        tmp_pluginss_dir, 'type2_plugin1'))
    assert os.path.isdir(os.path.join(
        tmp_pluginss_dir, 'type1_plugin2'))

    # Validates all plugins are in the configuration file
    validate_plugins_presence_in_conf(
        plugin_manager, registry_yaml, present=True)


def test_add_plugin_with_src_path(plugin_manager_fixture, mocker):
    """
    Validates that add plugin copies the whole directory and only reference
    to the plugin inside the directory
    :param plugin_manager_fixture: Fixture object which yields
    InfraredPluginManger object
    :param mocker: mocker fixture
    """

    def clone_from_side_effect(url, to_path, **kwargs):
        """
        Define a side effect function to override the
        original behaviour of clone_from
        """
        shutil.copytree(src=plugin_src, dst=to_path)
        return to_path

    plugin_manager = plugin_manager_fixture()

    mock_git = mocker.patch("infrared.core.services.plugins.git.Repo")
    # use side effect to use copytree instead of original clone
    mock_git.clone_from.side_effect = clone_from_side_effect

    plugin_src = os.path.abspath(os.path.join(SAMPLE_PLUGINS_DIR,
                                              "plugin_with_src_path"))

    # add_plugin call
    plugin_manager.add_plugin(
        plugin_source="https://sample_github.null/plugin_repo.git",
        plugin_src_path="infrared_plugin")

    plugin = plugin_manager.get_plugin("plugin_with_src_path")
    expected_plugin_path = os.path.join(plugin_manager.plugins_dir,
                                        "plugin_with_src_path")

    expected_plugin_src_path = \
        os.path.join(expected_plugin_path, "infrared_plugin")

    assert expected_plugin_src_path == plugin.path, \
        "Plugin path is not as expected"

    # compare the dirs before and after to make sure we copied it entirely
    dirs_cmp = filecmp.dircmp(plugin_src, expected_plugin_path)
    assert dirs_cmp.right_list == dirs_cmp.left_list, \
        "Plugin directory is does not contain the original files from " \
        "the original plugin source."



if __name__ == "__main__":
    args0 = "/home/travis/builds/repos/redhat-openstack---infrared/tests/example/plugins/help_screen_plugin_with_list_yamls"
    res0 = get_plugin_spec_flatten_dict(args0)
    print(res0)
    if res0["name"] != "list_yamls_plugin" or res0["dir"] != '/home/travis/builds/repos/redhat-openstack---infrared/tests/example/plugins/help_screen_plugin_with_list_yamls' or \
            res0["description"] != "Description for list_yamls_plugin" or res0["type"] != "supported_type1":
        raise Exception("Result not True!!!")
    # import dill
    # import os
    # isT=True
    # for l in os.listdir("/home/travis/builds/repos/redhat-openstack---infrared/data_passk_platform/63060b1a73426c38ae68ad3e/"):
    #     f = open("/home/travis/builds/repos/redhat-openstack---infrared/data_passk_platform/63060b1a73426c38ae68ad3e/"+l, "rb")
    #     content = dill.load(f)
    #     f.close()
    #     if isinstance(content["input"]["args"][0]["bytes"],bytes):
    #         args0=dill.loads(content["input"]["args"][0]["bytes"])
    #     else:
    #         args0=content["input"]["args"][0]["bytes"]
    #     args0="./example/plugins/help_screen_plugin_with_list_yamls"
    #     res0 = get_plugin_spec_flatten_dict(args0)
    #     # print(res0)
    #     if res0["name"]!="list_yamls_plugin" or res0["dir"]!='./example/plugins/help_screen_plugin_with_list_yamls' or res0["description"]!="Description for list_yamls_plugin" or res0["type"]!="supported_type1":
    #         raise Exception("Result not True!!!")





----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/core/services/ansible_config_inject_config_passk_validte.py
from collections import OrderedDict
import os
from six.moves import configparser
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")
from infrared.core.utils import logger
from infrared.core.utils.validators import AnsibleConfigValidator

LOG = logger.LOG


class AnsibleConfigManager(object):

    def __init__(self, infrared_home):
        """Constructor.

        :param ansible_config: A path to the ansible config
        """
        self.ansible_config_path = self._get_ansible_conf_path(infrared_home)
        config_validator = AnsibleConfigValidator()

        if not os.path.isfile(self.ansible_config_path):
            self._create_ansible_config(infrared_home)
        else:
            config_validator.validate_from_file(self.ansible_config_path)

    @staticmethod
    def _get_ansible_conf_path(infrared_home):
        """Get path to Ansible config.

        Check for Ansible config in specific locations and return the first
        located.

        :param infrared_home: infrared's home directory
        :return: the first located Ansible config
        """
        locations_list = [
            os.path.join(os.getcwd(), 'ansible.cfg'),
            os.path.join(infrared_home, 'ansible.cfg'),
            os.path.join(os.path.expanduser('~'), '.ansible.cfg')
        ]

        env_var_path = os.environ.get('ANSIBLE_CONFIG', '')

        if env_var_path != '':
            return env_var_path

        for location in locations_list:
            if os.path.isfile(location):
                return location

        return os.path.join(infrared_home, 'ansible.cfg')

    def _create_ansible_config(self, infrared_home):
        """Create ansible config file """
        infrared_common_path = os.path.realpath(__file__ + '/../../../common')
        default_ansible_settings = dict(
            defaults=OrderedDict([
                ('host_key_checking', 'False'),
                ('forks', 500),
                ('timeout', 30),
                ('force_color', 1),
                ('show_custom_stats', 'True'),
                ('callback_plugins', infrared_common_path + '/callback_plugins'),
                ('filter_plugins', infrared_common_path + '/filter_plugins'),
                ('library', infrared_common_path + '/modules'),
                ('roles', infrared_common_path + '/roles'),
                ('collections_paths', infrared_home + '/.ansible/collections'),
                ('local_tmp', infrared_home + '/.ansible/tmp'),
            ]),
            ssh_connection=OrderedDict([
                ('pipelining', 'True'),
                ('retries', 2),
            ]),
            galaxy=OrderedDict([
                ('cache_dir', infrared_home + '/.ansible/galaxy_cache'),
                ('token_path', infrared_home + '/.ansible/galaxy_token'),
            ]),
        )

        LOG.warning("Ansible conf ('{}') not found, creating it with "
                    "default data".format(self.ansible_config_path))

        # with open(self.ansible_config_path, 'w') as fp:
        #     config = configparser.ConfigParser()
        #
        #     for section, section_data in default_ansible_settings.items():
        #         if not config.has_section(section):
        #             config.add_section(section)
        #         for option, value in section_data.items():
        #             config.set(section, option, str(value))
        #
        #     config.write(fp)

    def inject_config(self):
        """Set the environment variable for config path, if it is undefined."""
        if os.environ.get('ANSIBLE_CONFIG', '') == '':
            os.environ['ANSIBLE_CONFIG'] = self.ansible_config_path

if __name__ == "__main__":
    import os

    core_settings = AnsibleConfigManager("name")
    isT = True
    core_settings.ansible_config_path="abcdefgsdf"
    os.environ.get('ANSIBLE_CONFIG', '')
    core_settings.inject_config()
    ist1=os.environ['ANSIBLE_CONFIG']=="abcdefgsdf"
    os.environ.__setitem__("ANSIBLE_CONFIG","skfjsdf")
    core_settings.inject_config()
    ist2=os.environ['ANSIBLE_CONFIG']=="skfjsdf"
    if not ist1 or not ist2:
        isT=False
    # for l in os.listdir(
    #         "/home/travis/builds/repos/redhat-openstack---infrared/data_passk_platform/63060b1b73426c38ae68ad42/"):
    #     f = open(
    #         "/home/travis/builds/repos/redhat-openstack---infrared/data_passk_platform/63060b1b73426c38ae68ad42/" + l,
    #         "rb")
    #     content = dill.load(f)
    #     f.close()
    #     object_class = dill.loads(content["input"]["args"][0]["bytes"])
    #     temp_class = AnsibleConfigManager(core_settings.infrared_home)
    #     temp_class.__dict__.update(object_class)
    #     res0 = temp_class.inject_config()
    #     # print(res0)
    #     # print(content["output"][0])
    #     if not ( dill.dumps(res0)== dill.dumps(content["output"][0])):
    #         isT=False
    #         break
    if not isT:
        raise Exception("Result not True!!!")




----------------------------
/home/travis/builds/repos/redhat-openstack---infrared/infrared/main_extend_cli_passk_validte.py
from __future__ import print_function

import argcomplete
import json
import os
from pbr import version
import pkg_resources as pkg
import sys
sys.path.append("/home/travis/builds/repos/redhat-openstack---infrared")



def inject_common_paths():
    """Discover the path to the common/ directory provided by infrared core."""
    def override_conf_path(common_path, envvar, specific_dir):
        conf_path = os.environ.get(envvar, '')
        additional_conf_path = os.path.join(common_path, specific_dir)
        if conf_path:
            full_conf_path = ':'.join([additional_conf_path, conf_path])
        else:
            full_conf_path = additional_conf_path
        os.environ[envvar] = full_conf_path

    version_info = version.VersionInfo('infrared')

    common_path = pkg.resource_filename(version_info.package,
                                        'common')
    override_conf_path(common_path, 'ANSIBLE_ROLES_PATH', 'roles')
    override_conf_path(common_path, 'ANSIBLE_FILTER_PLUGINS', 'filter_plugins')
    override_conf_path(common_path, 'ANSIBLE_CALLBACK_PLUGINS',
                       'callback_plugins')
    override_conf_path(common_path, 'ANSIBLE_LIBRARY', 'library')


# This needs to be called here because as soon as an ansible class is loaded
# the code in constants.py is triggered. That code reads the configuration
# settings from all sources (ansible.cfg, environment variables, etc).
# If the first include to ansible modules is moved deeper in the InfraRed
# code (or on demand), then this call can be moved as well in that place.
inject_common_paths()

from infrared import api  # noqa
import infrared.bash_completers as completers  # noqa
from infrared.core.services import CoreServices  # noqa
from infrared.core.services.plugins import PLUGINS_REGISTRY, InfraredPlugin  # noqa
from infrared.core.utils import exceptions  # noqa
from infrared.core.utils import interactive_ssh  # noqa
from infrared.core.utils import logger  # noqa
from infrared.core.utils.print_formats import fancy_table  # noqa

LOG = logger.LOG


class WorkspaceManagerSpec(api.SpecObject):
    """The workspace manager CLI. """

    def __init__(self, name, *args, **kwargs):
        super(WorkspaceManagerSpec, self).__init__(name, **kwargs)
        self.workspace_manager = CoreServices.workspace_manager()

    def extend_cli(self, root_subparsers):
        # print(self.name,self.kwargs)
        workspace_plugin = root_subparsers.add_parser(
            self.name,
            # help=self.kwargs["description"],
            **self.kwargs)
        workspace_subparsers = workspace_plugin.add_subparsers(dest="command0")

        # create
        create_parser = workspace_subparsers.add_parser(
            'create', help='Creates a new workspace')
        create_parser.add_argument("name", help="Workspace name")

        # checkout
        checkout_parser = workspace_subparsers.add_parser(
            'checkout',
            help='Switches workspace to the specified workspace')
        checkout_parser.add_argument(
            "name",
            help="Workspace name").completer = completers.workspace_list
        checkout_parser.add_argument(
            "-c", "--create", action='store_true', dest="checkout_create",
            help="Creates a workspace if not exists and "
                 "switches to it")

        # inventory
        inventory_parser = workspace_subparsers.add_parser(
            'inventory',
            help="prints workspace's inventory file")
        inventory_parser.add_argument(
            "name", help="Workspace name",
            nargs="?").completer = completers.workspace_list

        # list
        wrkspc_list_parser = workspace_subparsers.add_parser(
            'list', help='Lists all the workspaces')
        wrkspc_list_parser.add_argument(
            "--active", action='store_true', dest='print_active',
            help="Prints the active workspace only")

        # delete
        delete_parser = workspace_subparsers.add_parser(
            'delete', help='Deletes workspaces')
        delete_parser.add_argument(
            'name', nargs='+',
            help="Workspace names").completer = completers.workspace_list

        # cleanup
        cleanup_parser = workspace_subparsers.add_parser(
            'cleanup', help='Removes all the files from workspace')
        cleanup_parser.add_argument(
            "name",
            help="Workspace name").completer = completers.workspace_list

        # import settings
        importer_parser = workspace_subparsers.add_parser(
            'import', help='Import deployment configs.')
        importer_parser.add_argument("filename", help="Archive file name or URL.")
        importer_parser.add_argument(
            "-n", "--name", dest="workspacename",
            help="Workspace name to import with. "
            "If not specified - file name will be used.")

        # export settings
        exporter_parser = workspace_subparsers.add_parser(
            'export', help='Export deployment configurations.')
        exporter_parser.add_argument(
            "-n", "--name", dest="workspacename",
            help="Workspace name. If not sepecified - active "
            "workspace will be used.").completer = completers.workspace_list
        exporter_parser.add_argument("-f", "--filename", dest="filename",
                                     help="Archive file name.")

        exporter_parser.add_argument("-K", "--copy-keys", dest="copykeys",
                                     action="store_true",
                                     help="Silently copy ssh keys "
                                     "to workspace.")
        # node list
        nodelist_parser = workspace_subparsers.add_parser(
            'node-list',
            help='List nodes, managed by workspace')
        nodelist_parser.add_argument(
            "-n", "--name",
            help="Workspace name").completer = completers.workspace_list
        nodelist_parser.add_argument(
            "-g", "--group",
            help="List nodes in specific group"
        ).completer = completers.group_list
        nodelist_parser.add_argument(
            "-f", "--format", choices=['fancy', 'json'], default='fancy',
            help="Output format")

        # group list
        grouplist_parser = workspace_subparsers.add_parser(
            'group-list',
            help='List groups, managed by workspace')
        grouplist_parser.add_argument(
            "-n", "--name",
            help="Workspace name").completer = completers.workspace_list

    def spec_handler(self, parser, args):
        """Handles all the plugin manager commands

        :param parser: the infrared parser object.
        :param args: the list of arguments received from cli.
        """
        pargs = parser.parse_args(args)
        subcommand = pargs.command0

        if subcommand == 'create':
            self._create_workspace(pargs.name)
        elif subcommand == 'checkout':
            self._checkout_workspace(pargs.name, pargs.checkout_create)
        elif subcommand == 'inventory':
            self._fetch_inventory(pargs.name)
        elif subcommand == 'list':
            if pargs.print_active:
                print(self.workspace_manager.get_active_workspace().name)
            else:
                workspaces = self.workspace_manager.list()
                headers = ("Name", "Active")
                workspaces = sorted([workspace.name for workspace in
                                     self.workspace_manager.list()])
                print(fancy_table(
                    headers,
                    *[(workspace, ' ' * (len(headers[-1]) // 2) + "*" if
                        self.workspace_manager.is_active(workspace) else "")
                      for workspace in workspaces]))
        elif subcommand == 'delete':
            for workspace_name in pargs.name:
                self.workspace_manager.delete(workspace_name)
                print("Workspace '{}' deleted".format(workspace_name))
        elif subcommand == 'cleanup':
            self.workspace_manager.cleanup(pargs.name)
        elif subcommand == 'export':
            self.workspace_manager.export_workspace(
                pargs.workspacename, pargs.filename, pargs.copykeys)
        elif subcommand == 'import':
            self.workspace_manager.import_workspace(
                pargs.filename, pargs.workspacename)
        elif subcommand == 'node-list':
            nodes = self.workspace_manager.node_list(pargs.name, pargs.group)
            if pargs.format == 'json':
                nodes_dict = [
                    {'name': name, 'address': address, 'groups': groups}
                    for name, address, groups in nodes]
                print(json.dumps({'nodes': nodes_dict}))
            else:
                print(fancy_table(
                    ("Name", "Address", "Groups"),
                    *[node_name for node_name in nodes]))
        elif subcommand == "group-list":
            groups = self.workspace_manager.group_list(pargs.name)
            print(fancy_table(
                ("Name", "Nodes"), *[group_name for group_name in groups]))

    def _create_workspace(self, name):
        """Creates a workspace

        :param name: Name of the workspace to create
        """
        self.workspace_manager.create(name)
        print("Workspace '{}' has been added".format(name))

    def _checkout_workspace(self, name, create=False):
        """Checkouts (activate) a workspace

        :param name: The name of the workspace to checkout
        :param create: if set to true will create a new workspace
        before checking out to it
        """
        if create:
            self._create_workspace(name)
        self.workspace_manager.activate(name)
        print("Now using workspace: '{}'".format(name))

    def _fetch_inventory(self, name):
        """fetch inventory file for workspace.

        if no active workspace found - create a new workspace
        """
        if name:
            wkspc = self.workspace_manager.get(name)
        else:
            wkspc = self.workspace_manager.get_active_workspace()
        if not wkspc:
            raise exceptions.IRNoActiveWorkspaceFound()
        print(wkspc.inventory)


class PluginManagerSpec(api.SpecObject):

    def __init__(self, name, *args, **kwargs):
        super(PluginManagerSpec, self).__init__(name, *args, **kwargs)
        self.plugin_manager = CoreServices.plugins_manager()

    def extend_cli(self, root_subparsers):
        plugin_parser = root_subparsers.add_parser(
            self.name,
            help=self.kwargs["description"],
            **self.kwargs)
        plugin_subparsers = plugin_parser.add_subparsers(dest="command0")

        # Add plugin
        add_parser = plugin_subparsers.add_parser(
            'add', help='Add a plugin')
        add_parser.add_argument("src", nargs='+',
                                help="Plugin Source (name/path/git URL)\n'all'"
                                     " will install all available plugins")
        add_parser.add_argument("--revision", help="git branch/tag/revision"
                                " sourced plugins. Ignored for"
                                "'plugin add all' command.")

        add_parser.add_argument("--src-path",
                                help="Relative path within the repository "
                                     "where infrared plugin can be found.\n"
                                     "(Required with --link-roles")

        add_parser.add_argument("--link-roles", action='store_true',
                                help="Auto creates symbolic 'roles' directory "
                                     "in the path provided with '--src-path' "
                                     "which points to the 'roles' directory "
                                     "inside the project's root dir if exists,"
                                     " otherwise to the project's root dir "
                                     "itself.")

        add_parser.add_argument("--skip-roles", action='store_true',
                                help="Skip the from file roles installation. "
                                     "(Don't install Ansible roles from "
                                     "'requirements.yml' or "
                                     "'requirements.yaml' file)")

        # Remove plugin
        remove_parser = plugin_subparsers.add_parser(
            "remove",
            help="Remove a plugin, 'all' will remove all installed plugins")
        remove_parser.add_argument(
            "name", nargs='+',
            help="Plugin name").completer = completers.plugin_list

        # List command
        list_parser = plugin_subparsers.add_parser(
            'list', help='List all the available plugins')
        list_parser.add_argument(
            "--available", action='store_true',
            help="Prints all available plugins in addition "
                 "to installed plugins")
        list_parser.add_argument(
            "--versions", action='store_true',
            help="Prints version of each installed plugins")

        # Update plugin
        update_parser = plugin_subparsers.add_parser(
            "update",
            help="Update a Git-based plugin")
        update_parser.add_argument(
            "name",
            help="Name of the plugin to update")
        update_parser.add_argument(
            "revision", nargs='?', default='latest',
            help="Revision number to checkout (if not given, will only pull "
                 "changes from the remote)")
        update_parser.add_argument(
            '--skip_reqs', '-s', action='store_true',
            help="Skips plugin's requirements installation")
        update_parser.add_argument(
            '--hard-reset', action='store_true',
            help="Drop all local changes using hard "
                 "reset (changes will be stashed")

        plugin_subparsers.add_parser(
            "freeze", help="Run through installed plugins. For git sourced "
            "one writes its current revision to plugins registry.")

        # search all plugins from github organization
        plugin_subparsers.add_parser(
            'search', help='Search and list all the available plugins from '
            "rhos-infra organization on GitHub")

        # import plugins from registry yml file
        import_parser = plugin_subparsers.add_parser(
            'import', help='Install plugins from a registry YML file')
        import_parser.add_argument("src",
                                   help="The registry YML file Source")

    def spec_handler(self, parser, args):
        """Handles all the plugin manager commands

        :param parser: the infrared parser object.
        :param args: the list of arguments received from cli.
        """
        pargs = parser.parse_args(args)
        subcommand = pargs.command0

        if subcommand == 'list':
            self._list_plugins(pargs.available, pargs.versions)
        elif subcommand == 'add':
            if 'all' in pargs.src:
                self.plugin_manager.add_all_available()
                self._list_plugins(print_available=False, print_version=False)
            else:
                if len(pargs.src) > 1 and (pargs.revision or pargs.src_path):
                    raise exceptions.IRFailedToAddPlugin(
                        "'--revision' works with one plugin source only.")
                for _plugin in pargs.src:
                    self.plugin_manager.add_plugin(
                        _plugin, rev=pargs.revision,
                        plugin_src_path=pargs.src_path,
                        skip_roles=pargs.skip_roles,
                        link_roles=pargs.link_roles)
        elif subcommand == 'remove':
            if 'all' in pargs.name:
                self.plugin_manager.remove_all()
                self._list_plugins(print_available=False, print_version=False)
            else:
                for _plugin in pargs.name:
                    self.plugin_manager.remove_plugin(_plugin)
        elif subcommand == 'freeze':
            self.plugin_manager.freeze()
        elif subcommand == 'update':
            self.plugin_manager.update_plugin(
                pargs.name, pargs.revision, pargs.skip_reqs, pargs.hard_reset)
        elif subcommand == 'search':
            self._search_plugins()
        elif subcommand == 'import':
            self.plugin_manager.import_plugins(pargs.src)

    def _list_plugins(self, print_available=False, print_version=False):
        """Print a list of installed & available plugins"""
        table_rows = []
        table_headers = ["Type", "Name"]
        installed_mark = ' ' * (len('Installed') // 2) + '*'

        plugins_dict = \
            self.plugin_manager.get_all_plugins() \
            if print_available \
            else self.plugin_manager.get_installed_plugins()

        for plugins_type, plugins in plugins_dict.items():
            installed_plugins_list = \
                self.plugin_manager.get_installed_plugins(plugins_type).keys()
            plugins_names = list(plugins.keys())
            plugins_names.sort()

            if print_available:
                all_plugins_list = []
                for plugin_name in plugins_names:
                    all_plugins_list.append(plugin_name)
                installed_plugins_mark_list = \
                    [installed_mark if plugin_name in installed_plugins_list
                     else '' for plugin_name in all_plugins_list]

                plugins_descs = \
                    [PLUGINS_REGISTRY.get(plugin, {}).get('desc', '')
                     for plugin in all_plugins_list]

                row = [plugins_type, '\n'.join(all_plugins_list),
                       '\n'.join(installed_plugins_mark_list),
                       '\n'.join(plugins_descs)]

                if print_version:
                    plugins_version = [
                        self.plugin_manager.get_plugin_version(plugin_name)
                        if plugin_name in installed_plugins_list else ''
                        for plugin_name in all_plugins_list]

                    row.append('\n'.join(plugins_version))

            else:
                row = [
                    plugins_type,
                    '\n'.join(installed_plugins_list)]

                if print_version:
                    plugins_version = [self.plugin_manager.get_plugin_version(
                        plugin_name) for plugin_name in installed_plugins_list]
                    row.append('\n'.join(plugins_version))

            table_rows.append(row)

        if print_available:
            table_headers.append("Installed")
            table_headers.append("Description")

        if print_version:
            table_headers.append("Version")

        print(fancy_table(table_headers, *table_rows))

    def _search_plugins(self):
        """Search git organizations and print a list of available plugins """

        table_rows = []
        table_headers = ["Type", "Name", "Description", "Source"]

        plugins_dict = \
            self.plugin_manager.get_all_git_plugins()

        for plugins_type, plugins in plugins_dict.items():
            # prepare empty lists
            all_plugins_list = []
            plugins_descs = []
            plugins_sources = []

            for plugin_name in sorted(plugins.iterkeys()):
                # get all plugin names
                all_plugins_list.append(plugin_name)
                # get all plugin descriptions
                plugins_descs.append(plugins[plugin_name]["desc"])
                # get all plugins sources
                plugins_sources.append(plugins[plugin_name]["src"])

            table_rows.append([
                plugins_type,
                '\n'.join(all_plugins_list),
                '\n'.join(plugins_descs),
                '\n'.join(plugins_sources)])

        print(fancy_table(table_headers, *table_rows))


class SSHSpec(api.SpecObject):

    def __init__(self, name, *args, **kwargs):
        super(SSHSpec, self).__init__(name, *args, **kwargs)

    def extend_cli(self, root_subparsers):
        issh_parser = root_subparsers.add_parser(
            self.name,
            help=self.kwargs["description"],
            **self.kwargs)

        issh_parser.add_argument("node_name", help="Node name. "
                                 "Ex.: controller-0"
                                 ).completer = completers.node_list
        issh_parser.add_argument("remote_command", nargs="?", help="Run "
                                 "provided command line on remote host and "
                                 "return its output.")

    def spec_handler(self, parser, args):
        """Handles the ssh command

        :param parser: the infrared parser object.
        :param args: the list of arguments received from cli.
        """
        pargs = parser.parse_args(args)
        return interactive_ssh.ssh_to_host(
            pargs.node_name, remote_command=pargs.remote_command)


def main(args=None):
    CoreServices.setup()

    # inject ansible config file
    CoreServices.ansible_config_manager().inject_config()

    specs_manager = api.SpecManager()

    # Init Managers
    specs_manager.register_spec(
        WorkspaceManagerSpec('workspace',
                             description="Workspace manager. "
                                         "Allows to create and use an "
                                         "isolated environment for plugins "
                                         "execution."))
    specs_manager.register_spec(
        PluginManagerSpec('plugin',
                          description="Plugin management"))

    specs_manager.register_spec(
        SSHSpec(
            'ssh',
            description="Interactive ssh session to node from inventory."))

    # register all plugins
    for plugin in CoreServices.plugins_manager().PLUGINS_DICT.values():
        specs_manager.register_spec(api.InfraredPluginsSpec(plugin))

    argcomplete.autocomplete(specs_manager.parser)
    return specs_manager.run_specs(args) or 0


# if __name__ == '__main__':
#     sys.exit(int(main() or 0))

if __name__ == "__main__":
    isT=True
    from infrared.core.services import plugins
    plugin_dir = "/home/travis/builds/repos/redhat-openstack---infrared/tests/example"
    test_plugin = plugins.InfraredPlugin(plugin_dir=plugin_dir)
    from infrared.api import InfraredPluginsSpec
    spec = InfraredPluginsSpec(test_plugin)
    input_string = ['example']
    spec_manager = api.SpecManager()
    spec_manager.register_spec(spec)
    if spec_manager.spec_objects is not None:
        temp1=spec_manager.spec_objects.get("example")
        temp2=temp1.plugin
        ist1=temp2.entry_point=="main.yml"
        ist2=temp2.name=="example"
        ist3=temp2.path=="/home/travis/builds/repos/redhat-openstack---infrared/tests/example"
        if not ist1 or not ist2 or not ist3:
            isT=False
    else:
        isT=False

    if not isT:
        raise Exception("Result not True!!!")

----------------------------
/home/travis/builds/repos/standalone/ocfl-dispositor-strip_root.py
import os


def strip_root(path, root):
    """Remove root from path, throw exception on failure."""
    root = root.rstrip(os.sep)  # ditch any trailing path separator
    if os.path.commonprefix((path, root)) == root:
        return os.path.relpath(path, start=root)
    raise Exception("Path %s is not in root %s" % (path, root))


def test_strip_root():
    """
    Check the corretness of strip_root
    """
    assert strip_root("/home/user/test", "/home/user") == "test"
    assert strip_root("/home/user/test/", "/home/user") == "test"
    assert strip_root("/home/user/test/", "/home/user/") == "test"
    assert strip_root("/home/user/test/", "/home/user/test") == '.'
    assert strip_root("/home/user/test/", "/home/user/test/") == '.'


if __name__ == "__main__":
    test_strip_root()


